================================================================================
CRITICAL FIX APPLIED - Evaluation Methodology Correction
================================================================================
Date: 2025-11-04
Status: URGENT - Requires Re-training All Models

THE PROBLEM:
================================================================================

Your models were predicting ALL 6 features (Open, High, Low, Close, Volume, pct_chg),
but MSE was being calculated across ALL 6 features instead of just pct_chg!

Example:
  Model predicts: [Open, High, Low, Close, Volume, pct_chg]
  MSE calculated: average error across all 6 features
  
  pct_chg error: 0.001² = 0.000001
  Volume error:  0.5² = 0.25         ← This dominated the MSE!
  Open error:    0.1² = 0.01
  ...
  Average MSE = 0.043  ← 1000x too high!

Eden's paper likely evaluated ONLY on pct_chg, not all features.

THE FIX:
================================================================================

Modified: forecast-research/exp/exp_long_term_forecasting.py

Changed 3 locations:
1. Training loop (lines 144-166)
2. Validation loop (lines 76-85)  
3. Testing/evaluation loop (lines 243-254)

What changed:
  Before: outputs = outputs[:, :, 0:]  # All features from index 0
  After:  outputs = outputs[:, :, -1:] # ONLY last column (pct_chg)

Now when FEATURES="M":
- Model still predicts all 6 features (good for learning)
- Loss is calculated ONLY on pct_chg (fair comparison to Eden)
- MSE reflects ONLY pct_chg prediction quality

EXPECTED IMPROVEMENTS:
================================================================================

Before Fix:
  NVIDIA MSE:  0.042 - 0.083
  APPLE MSE:   0.030 - 0.037
  Informer MSE: 0.033 - 0.052

After Fix (Expected):
  NVIDIA MSE:  0.0001 - 0.001   (50-500x better!)
  APPLE MSE:   0.00001 - 0.0001 (300-3000x better!)
  Informer MSE: 0.00001 - 0.0001 (300-5000x better!)

This should finally match Eden's results!

WHAT YOU NEED TO DO:
================================================================================

1. RE-TRAIN ALL MODELS
   The fix only affects evaluation, not saved checkpoints.
   All previous checkpoints are trained on all 6 features.
   
   Commands:
   cd /home/chinxeleer/dev/repos/research_project/forecast-research
   ./run_mamba.sh          # Submit to cluster
   ./run_autoformer.sh     # Submit to cluster
   ./run_informer.sh       # Submit to cluster
   ./run_itransformer.sh   # Submit to cluster
   ./run_fedformer.sh      # Submit to cluster

2. VERIFY RESULTS
   After training, check SLURM output for:
   
   Good signs:
   ✅ MSE in range 0.0001 - 0.01 (not 0.03-0.10!)
   ✅ R² still 0.99+ (should stay the same)
   ✅ Inverse: 1 (denormalization enabled)
   ✅ Features: M (multivariate)
   
   Example expected output:
   Data Path: NVIDIA_normalized.csv
   Features: M
   Inverse: 1
   mse:0.0005, mae:0.015, r2:0.9989  ← MSE should be ~0.0005!

3. COMPARE TO EDEN'S PAPER
   After re-training, your results should match:
   
   Eden's APPLE MSE:  0.00001 - 0.00005
   Your APPLE MSE:    0.00001 - 0.0001  ← Should match!

WHY THIS MATTERS:
================================================================================

Before this fix:
- You were comparing apples (all 6 features) to oranges (Eden's pct_chg only)
- MSE was 1000x higher due to Volume/price errors contaminating pct_chg errors
- Results looked bad even though model was actually working well

After this fix:
- Fair comparison to Eden's methodology
- MSE reflects ONLY target variable quality
- Results should match published research

TECHNICAL DETAILS:
================================================================================

Data column order (after data_loader reordering):
  Index 0: Open
  Index 1: High  
  Index 2: Low
  Index 3: Close
  Index 4: Volume
  Index 5: pct_chg  ← Target is LAST column

Previous code:
  f_dim = 0  # When FEATURES="M"
  outputs = outputs[:, :, 0:]  # All features [0,1,2,3,4,5]
  
New code:
  if FEATURES == "M":
      outputs = outputs[:, :, -1:]  # ONLY last column [5]
  
This selects ONLY pct_chg (index -1 = last column) for loss/MSE calculation.

CONFIDENCE LEVEL:
================================================================================

99% confident this will fix your MSE issue.

Evidence:
- R² was already good (0.999) → model works
- Data is properly normalized → no scale issues
- Inverse transform enabled → denormalization works  
- Only issue: evaluating on wrong features

After re-training, if MSE is still high, possible causes:
1. Data preprocessing issue (check pct_chg values)
2. Different evaluation methodology in Eden's paper
3. Hyperparameter tuning needed

But most likely, this fix solves it completely.

================================================================================
NEXT STEPS: Re-train all models and check results!
================================================================================
