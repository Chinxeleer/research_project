##############################################################################
# Training iTransformer Model on All Datasets
##############################################################################
Training: iTransformer on NVIDIA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_091419-gu0bdqua
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/gu0bdqua
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_NVIDIA_H3Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/gu0bdqua
>>>>>>>start training : long_term_forecast_iTransformer_NVIDIA_H3_iTransformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.2583118 Vali Loss: 0.1896148 Test Loss: 0.2570041
Validation loss decreased (inf --> 0.189615).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2325065 Vali Loss: 0.1971426 Test Loss: 0.2602984
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2137340 Vali Loss: 0.1907470 Test Loss: 0.2506275
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2017216 Vali Loss: 0.1922514 Test Loss: 0.2543822
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.25831180037860585, 'val/loss': 0.18961483426392078, 'test/loss': 0.2570040598511696, '_timestamp': 1762326870.2619476}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23250646071326464, 'val/loss': 0.1971426047384739, 'test/loss': 0.26029836386442184, '_timestamp': 1762326873.6132262}).
Epoch: 5, Steps: 133 | Train Loss: 0.1940085 Vali Loss: 0.2124250 Test Loss: 0.2510350
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1896551 Vali Loss: 0.1829569 Test Loss: 0.2505477
Validation loss decreased (0.189615 --> 0.182957).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1876729 Vali Loss: 0.1860456 Test Loss: 0.2497783
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1861874 Vali Loss: 0.1868236 Test Loss: 0.2498113
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1857012 Vali Loss: 0.1835138 Test Loss: 0.2502073
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1853568 Vali Loss: 0.1852354 Test Loss: 0.2503345
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1846948 Vali Loss: 0.1896385 Test Loss: 0.2503443
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1847437 Vali Loss: 0.1851997 Test Loss: 0.2503447
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1852160 Vali Loss: 0.1882999 Test Loss: 0.2503417
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1858742 Vali Loss: 0.1866371 Test Loss: 0.2503426
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1856413 Vali Loss: 0.1912335 Test Loss: 0.2503440
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1854949 Vali Loss: 0.2074247 Test Loss: 0.2503427
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_NVIDIA_H3_iTransformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.0012519097654148936, mae:0.026739466935396194, rmse:0.035382337868213654, r2:-0.1159963607788086, dtw:Not calculated


VAL - MSE: 0.0013, MAE: 0.0267, RMSE: 0.0354, RÂ²: -0.1160, MAPE: 596243.94%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.490 MB of 0.490 MB uploadedwandb: \ 0.490 MB of 0.490 MB uploadedwandb: | 0.490 MB of 0.490 MB uploadedwandb: / 0.490 MB of 0.490 MB uploadedwandb: - 0.490 MB of 0.490 MB uploadedwandb: \ 0.490 MB of 0.490 MB uploadedwandb: | 0.490 MB of 0.490 MB uploadedwandb: / 0.490 MB of 0.490 MB uploadedwandb: - 0.595 MB of 0.763 MB uploaded (0.002 MB deduped)wandb: \ 0.763 MB of 0.763 MB uploaded (0.002 MB deduped)wandb: | 0.763 MB of 0.763 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‚â–ˆâ–ƒâ–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–ƒâ–ˆâ–â–‚â–‚â–â–‚â–ƒâ–‚â–‚â–‚â–ƒâ–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6338563
wandb:     model/trainable_params 6338563
wandb:                  test/loss 0.25034
wandb:                 train/loss 0.18549
wandb:   val/directional_accuracy 48.10127
wandb:                   val/loss 0.20742
wandb:                    val/mae 0.02674
wandb:                   val/mape 59624393.75
wandb:                    val/mse 0.00125
wandb:                     val/r2 -0.116
wandb:                   val/rmse 0.03538
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/gu0bdqua
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_091419-gu0bdqua/logs
Completed: NVIDIA H=3

Training: iTransformer on NVIDIA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_091547-dhsaw5gx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/dhsaw5gx
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_NVIDIA_H5Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/dhsaw5gx
>>>>>>>start training : long_term_forecast_iTransformer_NVIDIA_H5_iTransformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.2599049 Vali Loss: 0.1881770 Test Loss: 0.2653353
Validation loss decreased (inf --> 0.188177).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2341833 Vali Loss: 0.2085087 Test Loss: 0.2752061
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2191885 Vali Loss: 0.1895927 Test Loss: 0.2576921
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2091372 Vali Loss: 0.1826140 Test Loss: 0.2582329
Validation loss decreased (0.188177 --> 0.182614).  Saving model ...
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.25990488430611175, 'val/loss': 0.18817702494561672, 'test/loss': 0.26533532701432705, '_timestamp': 1762326957.882155}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23418330933366502, 'val/loss': 0.20850868709385395, 'test/loss': 0.2752061262726784, '_timestamp': 1762326961.2025526}).
Epoch: 5, Steps: 133 | Train Loss: 0.2024948 Vali Loss: 0.1970936 Test Loss: 0.2557411
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1994623 Vali Loss: 0.1830376 Test Loss: 0.2571272
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1975221 Vali Loss: 0.1974466 Test Loss: 0.2568628
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1968436 Vali Loss: 0.1822709 Test Loss: 0.2571976
Validation loss decreased (0.182614 --> 0.182271).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1970234 Vali Loss: 0.1852842 Test Loss: 0.2572477
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1966778 Vali Loss: 0.1831714 Test Loss: 0.2572488
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1956112 Vali Loss: 0.1825107 Test Loss: 0.2572652
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1954002 Vali Loss: 0.1816619 Test Loss: 0.2572755
Validation loss decreased (0.182271 --> 0.181662).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1958283 Vali Loss: 0.1826147 Test Loss: 0.2572727
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1951583 Vali Loss: 0.1847076 Test Loss: 0.2572747
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1955672 Vali Loss: 0.1884194 Test Loss: 0.2572765
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1952808 Vali Loss: 0.1808212 Test Loss: 0.2572775
Validation loss decreased (0.181662 --> 0.180821).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1958460 Vali Loss: 0.1822096 Test Loss: 0.2572774
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1964163 Vali Loss: 0.1953923 Test Loss: 0.2572778
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1956613 Vali Loss: 0.1815369 Test Loss: 0.2572778
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1962578 Vali Loss: 0.2028864 Test Loss: 0.2572776
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1948617 Vali Loss: 0.2026921 Test Loss: 0.2572775
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1949548 Vali Loss: 0.1994315 Test Loss: 0.2572776
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1956968 Vali Loss: 0.2023378 Test Loss: 0.2572776
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.1962362 Vali Loss: 0.1815607 Test Loss: 0.2572776
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.1954386 Vali Loss: 0.1815469 Test Loss: 0.2572775
EarlyStopping counter: 9 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.1959758 Vali Loss: 0.1835155 Test Loss: 0.2572776
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_NVIDIA_H5_iTransformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.0012371230404824018, mae:0.026719197630882263, rmse:0.03517276048660278, r2:-0.09569108486175537, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0267, RMSE: 0.0352, RÂ²: -0.0957, MAPE: 503426.62%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.523 MB of 0.523 MB uploadedwandb: \ 0.523 MB of 0.523 MB uploadedwandb: | 0.523 MB of 0.523 MB uploadedwandb: / 0.523 MB of 0.523 MB uploadedwandb: - 0.523 MB of 0.693 MB uploadedwandb: \ 0.693 MB of 0.693 MB uploadedwandb: | 0.693 MB of 0.693 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–†â–ˆâ–â–…â–„â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–‚â–†â–‚â–†â–â–‚â–‚â–‚â–â–‚â–‚â–ƒâ–â–â–†â–â–ˆâ–ˆâ–‡â–ˆâ–â–â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 25
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6339589
wandb:     model/trainable_params 6339589
wandb:                  test/loss 0.25728
wandb:                 train/loss 0.19598
wandb:   val/directional_accuracy 46.48936
wandb:                   val/loss 0.18352
wandb:                    val/mae 0.02672
wandb:                   val/mape 50342662.5
wandb:                    val/mse 0.00124
wandb:                     val/r2 -0.09569
wandb:                   val/rmse 0.03517
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/dhsaw5gx
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_091547-dhsaw5gx/logs
Completed: NVIDIA H=5

Training: iTransformer on NVIDIA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_091742-okqfw66r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/okqfw66r
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_NVIDIA_H10Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/okqfw66r
>>>>>>>start training : long_term_forecast_iTransformer_NVIDIA_H10_iTransformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2601322 Vali Loss: 0.2060866 Test Loss: 0.2893011
Validation loss decreased (inf --> 0.206087).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2403920 Vali Loss: 0.1922853 Test Loss: 0.2881165
Validation loss decreased (0.206087 --> 0.192285).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2254269 Vali Loss: 0.2021755 Test Loss: 0.2786655
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2176707 Vali Loss: 0.2049233 Test Loss: 0.2756295
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.26013216925294774, 'val/loss': 0.20608657412230968, 'test/loss': 0.28930113930255175, '_timestamp': 1762327072.5062249}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24039197326602793, 'val/loss': 0.19228528812527657, 'test/loss': 0.28811645321547985, '_timestamp': 1762327075.8584905}).
Epoch: 5, Steps: 133 | Train Loss: 0.2131573 Vali Loss: 0.1812963 Test Loss: 0.2748178
Validation loss decreased (0.192285 --> 0.181296).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2099400 Vali Loss: 0.2073234 Test Loss: 0.2745618
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2091538 Vali Loss: 0.1800235 Test Loss: 0.2749343
Validation loss decreased (0.181296 --> 0.180023).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2089479 Vali Loss: 0.2024739 Test Loss: 0.2745497
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2073695 Vali Loss: 0.1779518 Test Loss: 0.2744844
Validation loss decreased (0.180023 --> 0.177952).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2067959 Vali Loss: 0.1893586 Test Loss: 0.2744067
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2078732 Vali Loss: 0.1819912 Test Loss: 0.2744102
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2076433 Vali Loss: 0.1798421 Test Loss: 0.2744070
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2072685 Vali Loss: 0.1984333 Test Loss: 0.2744045
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2070536 Vali Loss: 0.1799708 Test Loss: 0.2744099
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2075267 Vali Loss: 0.1774851 Test Loss: 0.2744094
Validation loss decreased (0.177952 --> 0.177485).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2071602 Vali Loss: 0.1940873 Test Loss: 0.2744088
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2074731 Vali Loss: 0.2166057 Test Loss: 0.2744091
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2078151 Vali Loss: 0.1809810 Test Loss: 0.2744089
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2075655 Vali Loss: 0.1807464 Test Loss: 0.2744090
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2075747 Vali Loss: 0.1815847 Test Loss: 0.2744090
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2072370 Vali Loss: 0.1782834 Test Loss: 0.2744089
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2074314 Vali Loss: 0.1919746 Test Loss: 0.2744089
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2073259 Vali Loss: 0.1774470 Test Loss: 0.2744090
Validation loss decreased (0.177485 --> 0.177447).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2075011 Vali Loss: 0.2210215 Test Loss: 0.2744090
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2074140 Vali Loss: 0.1820926 Test Loss: 0.2744090
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2068537 Vali Loss: 0.2158468 Test Loss: 0.2744090
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.2068773 Vali Loss: 0.1779160 Test Loss: 0.2744090
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.2072113 Vali Loss: 0.1793079 Test Loss: 0.2744090
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.2090313 Vali Loss: 0.2190137 Test Loss: 0.2744090
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 133 | Train Loss: 0.2068729 Vali Loss: 0.1782013 Test Loss: 0.2744090
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 133 | Train Loss: 0.2068260 Vali Loss: 0.1807533 Test Loss: 0.2744090
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 133 | Train Loss: 0.2076261 Vali Loss: 0.1783037 Test Loss: 0.2744090
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 133 | Train Loss: 0.2076874 Vali Loss: 0.1823074 Test Loss: 0.2744090
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_NVIDIA_H10_iTransformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.0012339205713942647, mae:0.02679450251162052, rmse:0.03512720391154289, r2:-0.07566344738006592, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0268, RMSE: 0.0351, RÂ²: -0.0757, MAPE: 309234.19%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.578 MB of 0.579 MB uploadedwandb: \ 0.578 MB of 0.579 MB uploadedwandb: | 0.579 MB of 0.579 MB uploadedwandb: / 0.579 MB of 0.750 MB uploadedwandb: - 0.579 MB of 0.750 MB uploadedwandb: \ 0.750 MB of 0.750 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–…â–‚â–†â–â–…â–â–ƒâ–‚â–â–„â–â–â–„â–‡â–‚â–‚â–‚â–â–ƒâ–â–ˆâ–‚â–‡â–â–â–ˆâ–â–‚â–â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 32
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6342154
wandb:     model/trainable_params 6342154
wandb:                  test/loss 0.27441
wandb:                 train/loss 0.20769
wandb:   val/directional_accuracy 45.65217
wandb:                   val/loss 0.18231
wandb:                    val/mae 0.02679
wandb:                   val/mape 30923418.75
wandb:                    val/mse 0.00123
wandb:                     val/r2 -0.07566
wandb:                   val/rmse 0.03513
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/okqfw66r
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_091742-okqfw66r/logs
Completed: NVIDIA H=10

Training: iTransformer on NVIDIA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_092001-043tv4d1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/043tv4d1
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_NVIDIA_H22Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/043tv4d1
>>>>>>>start training : long_term_forecast_iTransformer_NVIDIA_H22_iTransformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2641191 Vali Loss: 0.1915213 Test Loss: 0.3519974
Validation loss decreased (inf --> 0.191521).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2459343 Vali Loss: 0.1906515 Test Loss: 0.3479012
Validation loss decreased (0.191521 --> 0.190651).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2357431 Vali Loss: 0.1865240 Test Loss: 0.3398691
Validation loss decreased (0.190651 --> 0.186524).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2291459 Vali Loss: 0.1873052 Test Loss: 0.3387126
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2641191174360839, 'val/loss': 0.19152131038052694, 'test/loss': 0.3519974244492395, '_timestamp': 1762327210.9406254}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2459343129938299, 'val/loss': 0.19065147638320923, 'test/loss': 0.3479012208325522, '_timestamp': 1762327214.2759438}).
Epoch: 5, Steps: 132 | Train Loss: 0.2262799 Vali Loss: 0.1857829 Test Loss: 0.3337672
Validation loss decreased (0.186524 --> 0.185783).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2237938 Vali Loss: 0.1862884 Test Loss: 0.3332375
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2231043 Vali Loss: 0.1862700 Test Loss: 0.3329795
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2225461 Vali Loss: 0.1859520 Test Loss: 0.3329191
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2223921 Vali Loss: 0.1857323 Test Loss: 0.3328615
Validation loss decreased (0.185783 --> 0.185732).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2220231 Vali Loss: 0.1859334 Test Loss: 0.3328465
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2221208 Vali Loss: 0.1867960 Test Loss: 0.3328046
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2220299 Vali Loss: 0.1869268 Test Loss: 0.3328062
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2221572 Vali Loss: 0.1870283 Test Loss: 0.3328030
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2225144 Vali Loss: 0.1861480 Test Loss: 0.3328010
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2223572 Vali Loss: 0.1864102 Test Loss: 0.3327994
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2220882 Vali Loss: 0.1861339 Test Loss: 0.3327987
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2221864 Vali Loss: 0.1865103 Test Loss: 0.3327986
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2222299 Vali Loss: 0.1874212 Test Loss: 0.3327985
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2222243 Vali Loss: 0.1856817 Test Loss: 0.3327984
Validation loss decreased (0.185732 --> 0.185682).  Saving model ...
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2222495 Vali Loss: 0.1874795 Test Loss: 0.3327984
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2220710 Vali Loss: 0.1859284 Test Loss: 0.3327983
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2218811 Vali Loss: 0.1873751 Test Loss: 0.3327984
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.2220146 Vali Loss: 0.1856872 Test Loss: 0.3327983
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 132 | Train Loss: 0.2215873 Vali Loss: 0.1864504 Test Loss: 0.3327984
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 132 | Train Loss: 0.2221115 Vali Loss: 0.1847680 Test Loss: 0.3327983
Validation loss decreased (0.185682 --> 0.184768).  Saving model ...
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 132 | Train Loss: 0.2219091 Vali Loss: 0.1856127 Test Loss: 0.3327983
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 132 | Train Loss: 0.2220953 Vali Loss: 0.1868714 Test Loss: 0.3327983
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 132 | Train Loss: 0.2222487 Vali Loss: 0.1864832 Test Loss: 0.3327983
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 132 | Train Loss: 0.2225208 Vali Loss: 0.1873463 Test Loss: 0.3327983
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 132 | Train Loss: 0.2219576 Vali Loss: 0.1864924 Test Loss: 0.3327983
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 132 | Train Loss: 0.2219784 Vali Loss: 0.1890364 Test Loss: 0.3327983
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 132 | Train Loss: 0.2224504 Vali Loss: 0.1864741 Test Loss: 0.3327983
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 132 | Train Loss: 0.2223568 Vali Loss: 0.1863679 Test Loss: 0.3327983
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 132 | Train Loss: 0.2223718 Vali Loss: 0.1854257 Test Loss: 0.3327983
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1641532182693482e-14
Epoch: 35, Steps: 132 | Train Loss: 0.2220180 Vali Loss: 0.1859558 Test Loss: 0.3327983
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_NVIDIA_H22_iTransformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.0012475940166041255, mae:0.026781752705574036, rmse:0.03532129526138306, r2:-0.05741250514984131, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0268, RMSE: 0.0353, RÂ²: -0.0574, MAPE: 587078.19%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.600 MB of 0.601 MB uploadedwandb: \ 0.600 MB of 0.601 MB uploadedwandb: | 0.601 MB of 0.601 MB uploadedwandb: / 0.601 MB of 0.601 MB uploadedwandb: - 0.601 MB of 0.773 MB uploadedwandb: \ 0.773 MB of 0.773 MB uploadedwandb: | 0.773 MB of 0.773 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‡â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–…â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–…â–…â–ƒâ–„â–ƒâ–„â–…â–‚â–…â–ƒâ–…â–ƒâ–„â–â–‚â–„â–„â–…â–„â–ˆâ–„â–„â–‚â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 34
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6348310
wandb:     model/trainable_params 6348310
wandb:                  test/loss 0.3328
wandb:                 train/loss 0.22202
wandb:   val/directional_accuracy 46.67977
wandb:                   val/loss 0.18596
wandb:                    val/mae 0.02678
wandb:                   val/mape 58707818.75
wandb:                    val/mse 0.00125
wandb:                     val/r2 -0.05741
wandb:                   val/rmse 0.03532
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/043tv4d1
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_092001-043tv4d1/logs
Completed: NVIDIA H=22

Training: iTransformer on NVIDIA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_092226-p3htkgf0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/p3htkgf0
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_NVIDIA_H50Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/p3htkgf0
>>>>>>>start training : long_term_forecast_iTransformer_NVIDIA_H50_iTransformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.2750434 Vali Loss: 0.2034818 Test Loss: 0.4956385
Validation loss decreased (inf --> 0.203482).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2561124 Vali Loss: 0.1998892 Test Loss: 0.4823108
Validation loss decreased (0.203482 --> 0.199889).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2506646 Vali Loss: 0.1978405 Test Loss: 0.4713143
Validation loss decreased (0.199889 --> 0.197840).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2470718 Vali Loss: 0.1973219 Test Loss: 0.4669083
Validation loss decreased (0.197840 --> 0.197322).  Saving model ...
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.27504344668352243, 'val/loss': 0.20348180830478668, 'test/loss': 0.49563852200905484, '_timestamp': 1762327356.9652214}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2561123983539415, 'val/loss': 0.1998891606926918, 'test/loss': 0.48231084148089093, '_timestamp': 1762327360.3375304}).
Epoch: 5, Steps: 132 | Train Loss: 0.2444768 Vali Loss: 0.1973887 Test Loss: 0.4657349
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2446972 Vali Loss: 0.1969807 Test Loss: 0.4663371
Validation loss decreased (0.197322 --> 0.196981).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2436581 Vali Loss: 0.1968440 Test Loss: 0.4653196
Validation loss decreased (0.196981 --> 0.196844).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2432728 Vali Loss: 0.1971178 Test Loss: 0.4654728
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2441945 Vali Loss: 0.1971567 Test Loss: 0.4654194
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2421515 Vali Loss: 0.1968579 Test Loss: 0.4653983
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2447175 Vali Loss: 0.1968232 Test Loss: 0.4654095
Validation loss decreased (0.196844 --> 0.196823).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2431680 Vali Loss: 0.1969085 Test Loss: 0.4653985
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2426023 Vali Loss: 0.1968868 Test Loss: 0.4653983
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2424719 Vali Loss: 0.1967085 Test Loss: 0.4653992
Validation loss decreased (0.196823 --> 0.196709).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2432490 Vali Loss: 0.1971288 Test Loss: 0.4654009
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2424868 Vali Loss: 0.1970007 Test Loss: 0.4654005
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2431873 Vali Loss: 0.1970923 Test Loss: 0.4654005
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2427957 Vali Loss: 0.1969644 Test Loss: 0.4654006
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2430005 Vali Loss: 0.1967820 Test Loss: 0.4654006
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2424399 Vali Loss: 0.1966829 Test Loss: 0.4654006
Validation loss decreased (0.196709 --> 0.196683).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2422644 Vali Loss: 0.1971207 Test Loss: 0.4654007
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2426824 Vali Loss: 0.1969044 Test Loss: 0.4654007
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.2421109 Vali Loss: 0.1970654 Test Loss: 0.4654007
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 132 | Train Loss: 0.2422091 Vali Loss: 0.1966443 Test Loss: 0.4654007
Validation loss decreased (0.196683 --> 0.196644).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 132 | Train Loss: 0.2422524 Vali Loss: 0.1967536 Test Loss: 0.4654007
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 132 | Train Loss: 0.2421418 Vali Loss: 0.1968579 Test Loss: 0.4654007
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 132 | Train Loss: 0.2425756 Vali Loss: 0.1969424 Test Loss: 0.4654007
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 132 | Train Loss: 0.2420082 Vali Loss: 0.1968646 Test Loss: 0.4654006
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 132 | Train Loss: 0.2426305 Vali Loss: 0.1970749 Test Loss: 0.4654006
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 132 | Train Loss: 0.2435039 Vali Loss: 0.1968900 Test Loss: 0.4654006
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 132 | Train Loss: 0.2419572 Vali Loss: 0.1968000 Test Loss: 0.4654006
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 132 | Train Loss: 0.2425308 Vali Loss: 0.1970073 Test Loss: 0.4654006
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 132 | Train Loss: 0.2417204 Vali Loss: 0.1969350 Test Loss: 0.4654006
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 132 | Train Loss: 0.2428934 Vali Loss: 0.1970737 Test Loss: 0.4654006
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_NVIDIA_H50_iTransformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.0012280582450330257, mae:0.026852713897824287, rmse:0.035043660551309586, r2:-0.03198862075805664, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0269, RMSE: 0.0350, RÂ²: -0.0320, MAPE: 559747.38%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.653 MB of 0.655 MB uploadedwandb: \ 0.653 MB of 0.655 MB uploadedwandb: | 0.653 MB of 0.655 MB uploadedwandb: / 0.655 MB of 0.655 MB uploadedwandb: - 0.655 MB of 0.828 MB uploadedwandb: \ 0.828 MB of 0.828 MB uploadedwandb: | 0.828 MB of 0.828 MB uploadedwandb: / 0.828 MB of 0.828 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–‚â–â–‚â–‚â–â–‚â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–…â–ƒâ–‚â–„â–„â–‚â–‚â–ƒâ–‚â–â–„â–ƒâ–„â–ƒâ–‚â–â–„â–ƒâ–ƒâ–â–‚â–‚â–ƒâ–‚â–„â–‚â–‚â–ƒâ–ƒâ–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 33
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6362674
wandb:     model/trainable_params 6362674
wandb:                  test/loss 0.4654
wandb:                 train/loss 0.24289
wandb:   val/directional_accuracy 49.65628
wandb:                   val/loss 0.19707
wandb:                    val/mae 0.02685
wandb:                   val/mape 55974737.5
wandb:                    val/mse 0.00123
wandb:                     val/r2 -0.03199
wandb:                   val/rmse 0.03504
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/p3htkgf0
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_092226-p3htkgf0/logs
Completed: NVIDIA H=50

Training: iTransformer on NVIDIA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_092449-2jv9hdtb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/2jv9hdtb
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_NVIDIA_H100Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/2jv9hdtb
>>>>>>>start training : long_term_forecast_iTransformer_NVIDIA_H100_iTransformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.2965499 Vali Loss: 0.2359933 Test Loss: 0.7524862
Validation loss decreased (inf --> 0.235993).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2761123 Vali Loss: 0.2231081 Test Loss: 0.7191257
Validation loss decreased (0.235993 --> 0.223108).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.2716331 Vali Loss: 0.2189076 Test Loss: 0.7180630
Validation loss decreased (0.223108 --> 0.218908).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2699174 Vali Loss: 0.2195848 Test Loss: 0.7080348
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2965498997614934, 'val/loss': 0.2359933376312256, 'test/loss': 0.7524862468242646, '_timestamp': 1762327499.077201}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2761123389005661, 'val/loss': 0.22310809493064881, 'test/loss': 0.7191257089376449, '_timestamp': 1762327502.3567214}).
Epoch: 5, Steps: 130 | Train Loss: 0.2680249 Vali Loss: 0.2251055 Test Loss: 0.7089471
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2671529 Vali Loss: 0.2248544 Test Loss: 0.7087167
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2668545 Vali Loss: 0.2249789 Test Loss: 0.7087179
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2662587 Vali Loss: 0.2179619 Test Loss: 0.7079545
Validation loss decreased (0.218908 --> 0.217962).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2670203 Vali Loss: 0.2250838 Test Loss: 0.7078649
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2665123 Vali Loss: 0.2186043 Test Loss: 0.7079829
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2666045 Vali Loss: 0.2191509 Test Loss: 0.7080338
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2664164 Vali Loss: 0.2211276 Test Loss: 0.7079946
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2663222 Vali Loss: 0.2202919 Test Loss: 0.7079994
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.2664869 Vali Loss: 0.2197817 Test Loss: 0.7080058
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.2663165 Vali Loss: 0.2161372 Test Loss: 0.7079988
Validation loss decreased (0.217962 --> 0.216137).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 130 | Train Loss: 0.2663860 Vali Loss: 0.2204944 Test Loss: 0.7079989
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 130 | Train Loss: 0.2666832 Vali Loss: 0.2211174 Test Loss: 0.7079991
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 130 | Train Loss: 0.2665048 Vali Loss: 0.2193937 Test Loss: 0.7079991
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 130 | Train Loss: 0.2663755 Vali Loss: 0.2234563 Test Loss: 0.7079990
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 130 | Train Loss: 0.2664048 Vali Loss: 0.2175231 Test Loss: 0.7079991
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 130 | Train Loss: 0.2664427 Vali Loss: 0.2236247 Test Loss: 0.7079991
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 130 | Train Loss: 0.2664265 Vali Loss: 0.2202076 Test Loss: 0.7079992
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 130 | Train Loss: 0.2663512 Vali Loss: 0.2162123 Test Loss: 0.7079991
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 130 | Train Loss: 0.2664784 Vali Loss: 0.2237000 Test Loss: 0.7079991
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 130 | Train Loss: 0.2664954 Vali Loss: 0.2231262 Test Loss: 0.7079992
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_NVIDIA_H100_iTransformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.001309285406023264, mae:0.027810323983430862, rmse:0.036184050142765045, r2:-0.01698756217956543, dtw:Not calculated


VAL - MSE: 0.0013, MAE: 0.0278, RMSE: 0.0362, RÂ²: -0.0170, MAPE: 342748.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.718 MB of 0.723 MB uploadedwandb: \ 0.718 MB of 0.723 MB uploadedwandb: | 0.723 MB of 0.723 MB uploadedwandb: / 0.723 MB of 0.893 MB uploadedwandb: - 0.723 MB of 0.893 MB uploadedwandb: \ 0.893 MB of 0.893 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–†â–ƒâ–‚â–‚â–â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–„â–ˆâ–ˆâ–ˆâ–‚â–ˆâ–ƒâ–ƒâ–…â–„â–„â–â–„â–…â–„â–‡â–‚â–‡â–„â–â–‡â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 24
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6388324
wandb:     model/trainable_params 6388324
wandb:                  test/loss 0.708
wandb:                 train/loss 0.2665
wandb:   val/directional_accuracy 50.17316
wandb:                   val/loss 0.22313
wandb:                    val/mae 0.02781
wandb:                   val/mape 34274800.0
wandb:                    val/mse 0.00131
wandb:                     val/r2 -0.01699
wandb:                   val/rmse 0.03618
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/2jv9hdtb
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_092449-2jv9hdtb/logs
Completed: NVIDIA H=100

Training: iTransformer on APPLE for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_092638-mfmhjxs5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/mfmhjxs5
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_APPLE_H3Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/mfmhjxs5
>>>>>>>start training : long_term_forecast_iTransformer_APPLE_H3_iTransformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.2477157 Vali Loss: 0.0966151 Test Loss: 0.1406802
Validation loss decreased (inf --> 0.096615).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2213527 Vali Loss: 0.0897334 Test Loss: 0.1404280
Validation loss decreased (0.096615 --> 0.089733).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2025960 Vali Loss: 0.0890430 Test Loss: 0.1323234
Validation loss decreased (0.089733 --> 0.089043).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1915375 Vali Loss: 0.0875871 Test Loss: 0.1296988
Validation loss decreased (0.089043 --> 0.087587).  Saving model ...
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.24771568918586673, 'val/loss': 0.09661505650728941, 'test/loss': 0.14068022277206182, '_timestamp': 1762327608.8210685}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2213527016845861, 'val/loss': 0.08973341435194016, 'test/loss': 0.14042795449495316, '_timestamp': 1762327612.1269212}).
Epoch: 5, Steps: 133 | Train Loss: 0.1835458 Vali Loss: 0.0895213 Test Loss: 0.1309942
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1802533 Vali Loss: 0.0904250 Test Loss: 0.1310457
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1783175 Vali Loss: 0.0870764 Test Loss: 0.1309542
Validation loss decreased (0.087587 --> 0.087076).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1764793 Vali Loss: 0.0890990 Test Loss: 0.1309205
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1766802 Vali Loss: 0.0871004 Test Loss: 0.1309447
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1752047 Vali Loss: 0.0890084 Test Loss: 0.1309459
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1750441 Vali Loss: 0.0899606 Test Loss: 0.1309577
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1750416 Vali Loss: 0.0896501 Test Loss: 0.1309572
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1756184 Vali Loss: 0.0872359 Test Loss: 0.1309586
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1751754 Vali Loss: 0.0894840 Test Loss: 0.1309570
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1753761 Vali Loss: 0.0874473 Test Loss: 0.1309574
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1758920 Vali Loss: 0.0874908 Test Loss: 0.1309581
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1747488 Vali Loss: 0.0857336 Test Loss: 0.1309585
Validation loss decreased (0.087076 --> 0.085734).  Saving model ...
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1759068 Vali Loss: 0.0865279 Test Loss: 0.1309585
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1760342 Vali Loss: 0.0897645 Test Loss: 0.1309583
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1755490 Vali Loss: 0.0880551 Test Loss: 0.1309585
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1759256 Vali Loss: 0.0878228 Test Loss: 0.1309583
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1751615 Vali Loss: 0.0876534 Test Loss: 0.1309583
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1743625 Vali Loss: 0.0867998 Test Loss: 0.1309583
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.1757722 Vali Loss: 0.0855135 Test Loss: 0.1309583
Validation loss decreased (0.085734 --> 0.085514).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.1766901 Vali Loss: 0.0864914 Test Loss: 0.1309583
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.1763966 Vali Loss: 0.0892431 Test Loss: 0.1309584
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.1755123 Vali Loss: 0.0890501 Test Loss: 0.1309584
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.1749748 Vali Loss: 0.0867577 Test Loss: 0.1309584
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.1758886 Vali Loss: 0.0883857 Test Loss: 0.1309584
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 133 | Train Loss: 0.1755874 Vali Loss: 0.0902465 Test Loss: 0.1309584
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 133 | Train Loss: 0.1753887 Vali Loss: 0.0891193 Test Loss: 0.1309584
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 133 | Train Loss: 0.1748369 Vali Loss: 0.0875349 Test Loss: 0.1309584
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 133 | Train Loss: 0.1755376 Vali Loss: 0.0880171 Test Loss: 0.1309584
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 133 | Train Loss: 0.1751444 Vali Loss: 0.0863134 Test Loss: 0.1309584
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_APPLE_H3_iTransformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.0002226001670351252, mae:0.010904060676693916, rmse:0.014919791370630264, r2:-0.1133195161819458, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0109, RMSE: 0.0149, RÂ²: -0.1133, MAPE: 416200.59%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.486 MB of 0.486 MB uploadedwandb: \ 0.486 MB of 0.486 MB uploadedwandb: | 0.486 MB of 0.486 MB uploadedwandb: / 0.486 MB of 0.486 MB uploadedwandb: - 0.486 MB of 0.486 MB uploadedwandb: \ 0.486 MB of 0.486 MB uploadedwandb: | 0.486 MB of 0.486 MB uploadedwandb: / 0.591 MB of 0.763 MB uploaded (0.002 MB deduped)wandb: - 0.763 MB of 0.763 MB uploaded (0.002 MB deduped)wandb: \ 0.763 MB of 0.763 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–„â–…â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–„â–‡â–ˆâ–ƒâ–†â–ƒâ–†â–‡â–‡â–ƒâ–‡â–„â–„â–â–‚â–‡â–…â–„â–„â–ƒâ–â–‚â–†â–†â–ƒâ–…â–ˆâ–†â–„â–…â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 33
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6338563
wandb:     model/trainable_params 6338563
wandb:                  test/loss 0.13096
wandb:                 train/loss 0.17514
wandb:   val/directional_accuracy 44.93671
wandb:                   val/loss 0.08631
wandb:                    val/mae 0.0109
wandb:                   val/mape 41620059.375
wandb:                    val/mse 0.00022
wandb:                     val/r2 -0.11332
wandb:                   val/rmse 0.01492
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/mfmhjxs5
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_092638-mfmhjxs5/logs
Completed: APPLE H=3

Training: iTransformer on APPLE for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_092902-4msibk8v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/4msibk8v
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_APPLE_H5Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/4msibk8v
>>>>>>>start training : long_term_forecast_iTransformer_APPLE_H5_iTransformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.2469638 Vali Loss: 0.0930470 Test Loss: 0.1392085
Validation loss decreased (inf --> 0.093047).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2242605 Vali Loss: 0.0924441 Test Loss: 0.1368920
Validation loss decreased (0.093047 --> 0.092444).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2073322 Vali Loss: 0.0892219 Test Loss: 0.1333819
Validation loss decreased (0.092444 --> 0.089222).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1967727 Vali Loss: 0.0897305 Test Loss: 0.1310862
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.24696382869006997, 'val/loss': 0.09304699208587408, 'test/loss': 0.139208503998816, '_timestamp': 1762327752.4923377}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2242605268843192, 'val/loss': 0.09244409389793873, 'test/loss': 0.13689201325178146, '_timestamp': 1762327755.8921626}).
Epoch: 5, Steps: 133 | Train Loss: 0.1896048 Vali Loss: 0.0888203 Test Loss: 0.1313944
Validation loss decreased (0.089222 --> 0.088820).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1865634 Vali Loss: 0.0898265 Test Loss: 0.1315031
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1845064 Vali Loss: 0.0912595 Test Loss: 0.1315172
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1843645 Vali Loss: 0.0870277 Test Loss: 0.1315508
Validation loss decreased (0.088820 --> 0.087028).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1843090 Vali Loss: 0.0896593 Test Loss: 0.1315507
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1842798 Vali Loss: 0.0864968 Test Loss: 0.1315647
Validation loss decreased (0.087028 --> 0.086497).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1834943 Vali Loss: 0.0890654 Test Loss: 0.1315615
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1834130 Vali Loss: 0.0881468 Test Loss: 0.1315629
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1845940 Vali Loss: 0.0868264 Test Loss: 0.1315625
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1834001 Vali Loss: 0.0882006 Test Loss: 0.1315633
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1833143 Vali Loss: 0.0882006 Test Loss: 0.1315640
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1836670 Vali Loss: 0.0864038 Test Loss: 0.1315641
Validation loss decreased (0.086497 --> 0.086404).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1844348 Vali Loss: 0.0873259 Test Loss: 0.1315643
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1833205 Vali Loss: 0.0849682 Test Loss: 0.1315640
Validation loss decreased (0.086404 --> 0.084968).  Saving model ...
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1839006 Vali Loss: 0.0856054 Test Loss: 0.1315643
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1836107 Vali Loss: 0.0853906 Test Loss: 0.1315641
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1834617 Vali Loss: 0.0900865 Test Loss: 0.1315641
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1826702 Vali Loss: 0.0878318 Test Loss: 0.1315641
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1838496 Vali Loss: 0.0879140 Test Loss: 0.1315641
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.1839297 Vali Loss: 0.0878189 Test Loss: 0.1315641
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.1836819 Vali Loss: 0.0892210 Test Loss: 0.1315641
EarlyStopping counter: 7 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.1839085 Vali Loss: 0.0870855 Test Loss: 0.1315641
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.1838791 Vali Loss: 0.0867777 Test Loss: 0.1315641
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.1833268 Vali Loss: 0.0879104 Test Loss: 0.1315641
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_APPLE_H5_iTransformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.0002195507986471057, mae:0.010822916403412819, rmse:0.014817246235907078, r2:-0.09305620193481445, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0108, RMSE: 0.0148, RÂ²: -0.0931, MAPE: 221016.53%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.515 MB of 0.516 MB uploadedwandb: \ 0.515 MB of 0.516 MB uploadedwandb: | 0.515 MB of 0.516 MB uploadedwandb: / 0.516 MB of 0.516 MB uploadedwandb: - 0.516 MB of 0.686 MB uploadedwandb: \ 0.686 MB of 0.686 MB uploadedwandb: | 0.686 MB of 0.686 MB uploadedwandb: / 0.686 MB of 0.686 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–†â–…â–†â–ˆâ–ƒâ–†â–ƒâ–†â–…â–ƒâ–…â–…â–ƒâ–„â–â–‚â–â–‡â–„â–„â–„â–†â–ƒâ–ƒâ–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 27
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6339589
wandb:     model/trainable_params 6339589
wandb:                  test/loss 0.13156
wandb:                 train/loss 0.18333
wandb:   val/directional_accuracy 45.21277
wandb:                   val/loss 0.08791
wandb:                    val/mae 0.01082
wandb:                   val/mape 22101653.125
wandb:                    val/mse 0.00022
wandb:                     val/r2 -0.09306
wandb:                   val/rmse 0.01482
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/4msibk8v
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_092902-4msibk8v/logs
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 268, in check_network_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 848, in deliver_network_status
    return self._deliver_network_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 510, in _deliver_network_status
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Exception in thread ChkStopThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 286, in check_stop_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 840, in deliver_stop_status
    return self._deliver_stop_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 494, in _deliver_stop_status
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: APPLE H=5

Training: iTransformer on APPLE for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_093105-x58nthgh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/x58nthgh
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_APPLE_H10Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/x58nthgh
>>>>>>>start training : long_term_forecast_iTransformer_APPLE_H10_iTransformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2496254 Vali Loss: 0.0912007 Test Loss: 0.1368925
Validation loss decreased (inf --> 0.091201).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2270842 Vali Loss: 0.0882168 Test Loss: 0.1352058
Validation loss decreased (0.091201 --> 0.088217).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2136081 Vali Loss: 0.0885972 Test Loss: 0.1331937
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2061527 Vali Loss: 0.0893111 Test Loss: 0.1316385
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2496254338805837, 'val/loss': 0.09120071586221457, 'test/loss': 0.13689249847084284, '_timestamp': 1762327876.0440228}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2270842025378593, 'val/loss': 0.08821684122085571, 'test/loss': 0.1352057969197631, '_timestamp': 1762327879.3879433}).
Epoch: 5, Steps: 133 | Train Loss: 0.2013008 Vali Loss: 0.0879340 Test Loss: 0.1319883
Validation loss decreased (0.088217 --> 0.087934).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1990770 Vali Loss: 0.0858240 Test Loss: 0.1320861
Validation loss decreased (0.087934 --> 0.085824).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1973154 Vali Loss: 0.0873064 Test Loss: 0.1320315
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1966887 Vali Loss: 0.0886416 Test Loss: 0.1320908
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1962908 Vali Loss: 0.0842449 Test Loss: 0.1321121
Validation loss decreased (0.085824 --> 0.084245).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1956918 Vali Loss: 0.0871888 Test Loss: 0.1321235
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1965822 Vali Loss: 0.0852350 Test Loss: 0.1321211
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1962037 Vali Loss: 0.0871250 Test Loss: 0.1321226
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1963725 Vali Loss: 0.0836508 Test Loss: 0.1321234
Validation loss decreased (0.084245 --> 0.083651).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1961817 Vali Loss: 0.0892323 Test Loss: 0.1321238
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1963788 Vali Loss: 0.0869932 Test Loss: 0.1321237
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1957244 Vali Loss: 0.0871245 Test Loss: 0.1321239
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1960737 Vali Loss: 0.0854036 Test Loss: 0.1321240
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1958712 Vali Loss: 0.0866875 Test Loss: 0.1321239
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1967927 Vali Loss: 0.0904376 Test Loss: 0.1321239
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1956066 Vali Loss: 0.0908302 Test Loss: 0.1321239
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1960690 Vali Loss: 0.0865579 Test Loss: 0.1321239
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1961189 Vali Loss: 0.0897329 Test Loss: 0.1321239
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1955195 Vali Loss: 0.0856586 Test Loss: 0.1321239
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_APPLE_H10_iTransformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.00022052008716855198, mae:0.010844172909855843, rmse:0.014849918894469738, r2:-0.08916699886322021, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0108, RMSE: 0.0148, RÂ²: -0.0892, MAPE: 445675.84%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.567 MB of 0.568 MB uploadedwandb: \ 0.568 MB of 0.568 MB uploadedwandb: | 0.568 MB of 0.568 MB uploadedwandb: / 0.568 MB of 0.738 MB uploadedwandb: - 0.738 MB of 0.738 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–‡â–…â–ƒâ–…â–†â–‚â–„â–ƒâ–„â–â–†â–„â–„â–ƒâ–„â–ˆâ–ˆâ–„â–‡â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6342154
wandb:     model/trainable_params 6342154
wandb:                  test/loss 0.13212
wandb:                 train/loss 0.19552
wandb:   val/directional_accuracy 45.99034
wandb:                   val/loss 0.08566
wandb:                    val/mae 0.01084
wandb:                   val/mape 44567584.375
wandb:                    val/mse 0.00022
wandb:                     val/r2 -0.08917
wandb:                   val/rmse 0.01485
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/x58nthgh
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_093105-x58nthgh/logs
Completed: APPLE H=10

Training: iTransformer on APPLE for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_093252-vbqb01cz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/vbqb01cz
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_APPLE_H22Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/vbqb01cz
>>>>>>>start training : long_term_forecast_iTransformer_APPLE_H22_iTransformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2540406 Vali Loss: 0.0887726 Test Loss: 0.1374866
Validation loss decreased (inf --> 0.088773).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2359366 Vali Loss: 0.0879924 Test Loss: 0.1360977
Validation loss decreased (0.088773 --> 0.087992).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2250386 Vali Loss: 0.0865322 Test Loss: 0.1344791
Validation loss decreased (0.087992 --> 0.086532).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2180670 Vali Loss: 0.0866735 Test Loss: 0.1335660
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.25404058600013907, 'val/loss': 0.08877264601843697, 'test/loss': 0.13748660683631897, '_timestamp': 1762327981.3359907}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2359365759925409, 'val/loss': 0.08799235948494502, 'test/loss': 0.1360976876957076, '_timestamp': 1762327984.653036}).
Epoch: 5, Steps: 132 | Train Loss: 0.2147054 Vali Loss: 0.0865757 Test Loss: 0.1339636
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2130259 Vali Loss: 0.0865798 Test Loss: 0.1339653
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2122969 Vali Loss: 0.0866325 Test Loss: 0.1339517
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2113697 Vali Loss: 0.0867929 Test Loss: 0.1339443
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2114526 Vali Loss: 0.0864619 Test Loss: 0.1339521
Validation loss decreased (0.086532 --> 0.086462).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2111607 Vali Loss: 0.0862914 Test Loss: 0.1339514
Validation loss decreased (0.086462 --> 0.086291).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2114024 Vali Loss: 0.0866406 Test Loss: 0.1339470
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2112888 Vali Loss: 0.0868025 Test Loss: 0.1339490
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2111981 Vali Loss: 0.0865090 Test Loss: 0.1339495
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2112612 Vali Loss: 0.0864063 Test Loss: 0.1339501
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2110862 Vali Loss: 0.0867338 Test Loss: 0.1339499
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2114763 Vali Loss: 0.0864575 Test Loss: 0.1339499
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2113172 Vali Loss: 0.0866420 Test Loss: 0.1339502
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2114050 Vali Loss: 0.0866102 Test Loss: 0.1339502
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2111123 Vali Loss: 0.0864770 Test Loss: 0.1339502
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2112449 Vali Loss: 0.0865317 Test Loss: 0.1339501
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_APPLE_H22_iTransformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.00022250344045460224, mae:0.010827557183802128, rmse:0.014916549436748028, r2:-0.07228255271911621, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0108, RMSE: 0.0149, RÂ²: -0.0723, MAPE: 487886.19%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.657 MB of 0.658 MB uploadedwandb: \ 0.657 MB of 0.658 MB uploadedwandb: | 0.657 MB of 0.658 MB uploadedwandb: / 0.658 MB of 0.658 MB uploadedwandb: - 0.658 MB of 0.828 MB uploadedwandb: \ 0.658 MB of 0.828 MB uploadedwandb: | 0.828 MB of 0.828 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–†â–…â–…â–†â–ˆâ–ƒâ–â–†â–ˆâ–„â–ƒâ–‡â–ƒâ–†â–…â–„â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6348310
wandb:     model/trainable_params 6348310
wandb:                  test/loss 0.13395
wandb:                 train/loss 0.21124
wandb:   val/directional_accuracy 46.89821
wandb:                   val/loss 0.08653
wandb:                    val/mae 0.01083
wandb:                   val/mape 48788618.75
wandb:                    val/mse 0.00022
wandb:                     val/r2 -0.07228
wandb:                   val/rmse 0.01492
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/vbqb01cz
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_093252-vbqb01cz/logs
Completed: APPLE H=22

Training: iTransformer on APPLE for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_093427-kn4vbzzx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/kn4vbzzx
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_APPLE_H50Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/kn4vbzzx
>>>>>>>start training : long_term_forecast_iTransformer_APPLE_H50_iTransformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.2676767 Vali Loss: 0.0902747 Test Loss: 0.1496331
Validation loss decreased (inf --> 0.090275).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2489401 Vali Loss: 0.0895241 Test Loss: 0.1487906
Validation loss decreased (0.090275 --> 0.089524).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2399665 Vali Loss: 0.0891054 Test Loss: 0.1483588
Validation loss decreased (0.089524 --> 0.089105).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2359176 Vali Loss: 0.0892946 Test Loss: 0.1486556
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2676767419572129, 'val/loss': 0.09027469158172607, 'test/loss': 0.14963306983311972, '_timestamp': 1762328077.372064}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24894011635897736, 'val/loss': 0.08952410519123077, 'test/loss': 0.14879061778386435, '_timestamp': 1762328080.6950073}).
Epoch: 5, Steps: 132 | Train Loss: 0.2337964 Vali Loss: 0.0891323 Test Loss: 0.1489692
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2332823 Vali Loss: 0.0894278 Test Loss: 0.1492473
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2321270 Vali Loss: 0.0895156 Test Loss: 0.1493364
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2370110 Vali Loss: 0.0894878 Test Loss: 0.1492728
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2329796 Vali Loss: 0.0895126 Test Loss: 0.1493466
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2309652 Vali Loss: 0.0895382 Test Loss: 0.1493238
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2309798 Vali Loss: 0.0895031 Test Loss: 0.1493154
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2325675 Vali Loss: 0.0894987 Test Loss: 0.1493135
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2311273 Vali Loss: 0.0895651 Test Loss: 0.1493188
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_APPLE_H50_iTransformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.00023328838869929314, mae:0.01101688388735056, rmse:0.01527378149330616, r2:-0.05144059658050537, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0110, RMSE: 0.0153, RÂ²: -0.0514, MAPE: 573280.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.725 MB of 0.728 MB uploadedwandb: \ 0.725 MB of 0.728 MB uploadedwandb: | 0.728 MB of 0.728 MB uploadedwandb: / 0.728 MB of 0.728 MB uploadedwandb: - 0.728 MB of 0.896 MB uploadedwandb: \ 0.896 MB of 0.896 MB uploadedwandb: | 0.896 MB of 0.896 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ƒâ–…â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–†â–ƒâ–â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–„â–â–†â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6362674
wandb:     model/trainable_params 6362674
wandb:                  test/loss 0.14932
wandb:                 train/loss 0.23113
wandb:   val/directional_accuracy 49.33405
wandb:                   val/loss 0.08957
wandb:                    val/mae 0.01102
wandb:                   val/mape 57328000.0
wandb:                    val/mse 0.00023
wandb:                     val/r2 -0.05144
wandb:                   val/rmse 0.01527
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/kn4vbzzx
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_093427-kn4vbzzx/logs
Completed: APPLE H=50

Training: iTransformer on APPLE for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_093541-3s8woctk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/3s8woctk
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_APPLE_H100Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/3s8woctk
>>>>>>>start training : long_term_forecast_iTransformer_APPLE_H100_iTransformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.2878854 Vali Loss: 0.0968229 Test Loss: 0.1633588
Validation loss decreased (inf --> 0.096823).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2669303 Vali Loss: 0.0955897 Test Loss: 0.1642313
Validation loss decreased (0.096823 --> 0.095590).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.2613730 Vali Loss: 0.0945332 Test Loss: 0.1625588
Validation loss decreased (0.095590 --> 0.094533).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2594913 Vali Loss: 0.0957445 Test Loss: 0.1640125
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2878853548031587, 'val/loss': 0.09682293087244034, 'test/loss': 0.16335884034633635, '_timestamp': 1762328150.1311152}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26693030217519176, 'val/loss': 0.09558974504470825, 'test/loss': 0.16423129439353942, '_timestamp': 1762328153.4545772}).
Epoch: 5, Steps: 130 | Train Loss: 0.2573762 Vali Loss: 0.0963757 Test Loss: 0.1648840
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2563600 Vali Loss: 0.0968078 Test Loss: 0.1653508
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2562957 Vali Loss: 0.0967241 Test Loss: 0.1651593
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2555523 Vali Loss: 0.0957155 Test Loss: 0.1653278
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2560424 Vali Loss: 0.0962405 Test Loss: 0.1654140
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2557447 Vali Loss: 0.0955691 Test Loss: 0.1654305
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2558612 Vali Loss: 0.0954122 Test Loss: 0.1653957
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2558558 Vali Loss: 0.0960446 Test Loss: 0.1653783
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2557805 Vali Loss: 0.0961696 Test Loss: 0.1653794
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_APPLE_H100_iTransformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.00024498713901266456, mae:0.011285796761512756, rmse:0.015652064234018326, r2:-0.0375896692276001, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0113, RMSE: 0.0157, RÂ²: -0.0376, MAPE: 743179.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.745 MB of 0.745 MB uploadedwandb: \ 0.745 MB of 0.745 MB uploadedwandb: | 0.745 MB of 0.745 MB uploadedwandb: / 0.745 MB of 0.745 MB uploadedwandb: - 0.745 MB of 0.913 MB uploadedwandb: \ 0.745 MB of 0.913 MB uploadedwandb: | 0.913 MB of 0.913 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–…â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–†â–ƒâ–‚â–‚â–â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–…â–‡â–ˆâ–ˆâ–…â–†â–„â–„â–†â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6388324
wandb:     model/trainable_params 6388324
wandb:                  test/loss 0.16538
wandb:                 train/loss 0.25578
wandb:   val/directional_accuracy 50.88023
wandb:                   val/loss 0.09617
wandb:                    val/mae 0.01129
wandb:                   val/mape 74317975.0
wandb:                    val/mse 0.00024
wandb:                     val/r2 -0.03759
wandb:                   val/rmse 0.01565
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/3s8woctk
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_093541-3s8woctk/logs
Completed: APPLE H=100

Training: iTransformer on SP500 for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_093654-qgtju0m2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/qgtju0m2
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_SP500_H3Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/qgtju0m2
>>>>>>>start training : long_term_forecast_iTransformer_SP500_H3_iTransformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 3
============================================================
train 4241
val 237
test 238
Epoch: 1, Steps: 133 | Train Loss: 0.2169187 Vali Loss: 0.0821169 Test Loss: 0.0896225
Validation loss decreased (inf --> 0.082117).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.1859270 Vali Loss: 0.0766587 Test Loss: 0.0854207
Validation loss decreased (0.082117 --> 0.076659).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.1566724 Vali Loss: 0.0713075 Test Loss: 0.0792095
Validation loss decreased (0.076659 --> 0.071308).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1428438 Vali Loss: 0.0712719 Test Loss: 0.0769252
Validation loss decreased (0.071308 --> 0.071272).  Saving model ...
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2169186840380045, 'val/loss': 0.08211686089634895, 'test/loss': 0.08962254412472248, '_timestamp': 1762328222.0469801}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.1859269728113834, 'val/loss': 0.07665868662297726, 'test/loss': 0.08542068535462022, '_timestamp': 1762328225.43122}).
Epoch: 5, Steps: 133 | Train Loss: 0.1343395 Vali Loss: 0.0691827 Test Loss: 0.0770592
Validation loss decreased (0.071272 --> 0.069183).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1295734 Vali Loss: 0.0685369 Test Loss: 0.0770920
Validation loss decreased (0.069183 --> 0.068537).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1266141 Vali Loss: 0.0680937 Test Loss: 0.0770471
Validation loss decreased (0.068537 --> 0.068094).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1270741 Vali Loss: 0.0681218 Test Loss: 0.0769595
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1276190 Vali Loss: 0.0686662 Test Loss: 0.0769943
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1264105 Vali Loss: 0.0687919 Test Loss: 0.0770091
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1260964 Vali Loss: 0.0680943 Test Loss: 0.0770173
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1240735 Vali Loss: 0.0678847 Test Loss: 0.0770185
Validation loss decreased (0.068094 --> 0.067885).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1257113 Vali Loss: 0.0684918 Test Loss: 0.0770180
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1245882 Vali Loss: 0.0672514 Test Loss: 0.0770166
Validation loss decreased (0.067885 --> 0.067251).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1254691 Vali Loss: 0.0687874 Test Loss: 0.0770160
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1256227 Vali Loss: 0.0674034 Test Loss: 0.0770166
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1248560 Vali Loss: 0.0685125 Test Loss: 0.0770163
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1248735 Vali Loss: 0.0670369 Test Loss: 0.0770163
Validation loss decreased (0.067251 --> 0.067037).  Saving model ...
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1249443 Vali Loss: 0.0680149 Test Loss: 0.0770165
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1244050 Vali Loss: 0.0663402 Test Loss: 0.0770163
Validation loss decreased (0.067037 --> 0.066340).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1256879 Vali Loss: 0.0693067 Test Loss: 0.0770164
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1254787 Vali Loss: 0.0688671 Test Loss: 0.0770164
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1254382 Vali Loss: 0.0680100 Test Loss: 0.0770164
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.1251767 Vali Loss: 0.0680814 Test Loss: 0.0770164
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.1256093 Vali Loss: 0.0677879 Test Loss: 0.0770164
EarlyStopping counter: 5 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.1263502 Vali Loss: 0.0687819 Test Loss: 0.0770164
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.1250965 Vali Loss: 0.0685763 Test Loss: 0.0770164
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.1245349 Vali Loss: 0.0692154 Test Loss: 0.0770164
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.1251258 Vali Loss: 0.0685798 Test Loss: 0.0770164
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 133 | Train Loss: 0.1253383 Vali Loss: 0.0694002 Test Loss: 0.0770164
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_SP500_H3_iTransformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 238
test shape: (238, 3, 1) (238, 3, 1)
test shape: (238, 3, 1) (238, 3, 1)


	mse:6.99837037245743e-05, mae:0.006252633407711983, rmse:0.008365626446902752, r2:-0.07648599147796631, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0063, RMSE: 0.0084, RÂ²: -0.0765, MAPE: 5.35%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.516 MB of 0.516 MB uploadedwandb: \ 0.516 MB of 0.516 MB uploadedwandb: | 0.516 MB of 0.516 MB uploadedwandb: / 0.516 MB of 0.516 MB uploadedwandb: - 0.516 MB of 0.516 MB uploadedwandb: \ 0.516 MB of 0.516 MB uploadedwandb: | 0.516 MB of 0.516 MB uploadedwandb: / 0.622 MB of 0.792 MB uploaded (0.002 MB deduped)wandb: - 0.792 MB of 0.792 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ˆâ–…â–„â–ƒâ–„â–„â–„â–ƒâ–ƒâ–„â–‚â–„â–‚â–„â–‚â–ƒâ–â–…â–…â–ƒâ–ƒâ–ƒâ–„â–„â–…â–„â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 29
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6338563
wandb:     model/trainable_params 6338563
wandb:                  test/loss 0.07702
wandb:                 train/loss 0.12534
wandb:   val/directional_accuracy 46.84874
wandb:                   val/loss 0.0694
wandb:                    val/mae 0.00625
wandb:                   val/mape 534.81789
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.07649
wandb:                   val/rmse 0.00837
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/qgtju0m2
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_093654-qgtju0m2/logs
Completed: SP500 H=3

Training: iTransformer on SP500 for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_093902-l1wl1yq2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/l1wl1yq2
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_SP500_H5Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/l1wl1yq2
>>>>>>>start training : long_term_forecast_iTransformer_SP500_H5_iTransformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 5
============================================================
train 4239
val 235
test 236
Epoch: 1, Steps: 133 | Train Loss: 0.2119004 Vali Loss: 0.0789706 Test Loss: 0.0834702
Validation loss decreased (inf --> 0.078971).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.1847458 Vali Loss: 0.0757470 Test Loss: 0.0844297
Validation loss decreased (0.078971 --> 0.075747).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.1615261 Vali Loss: 0.0713271 Test Loss: 0.0796907
Validation loss decreased (0.075747 --> 0.071327).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.21190039863935986, 'val/loss': 0.07897062785923481, 'test/loss': 0.08347019040957093, '_timestamp': 1762328351.9989743}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.18474582201780232, 'val/loss': 0.07574695302173495, 'test/loss': 0.08442972740158439, '_timestamp': 1762328355.7108452}).
Epoch: 4, Steps: 133 | Train Loss: 0.1478156 Vali Loss: 0.0687781 Test Loss: 0.0786188
Validation loss decreased (0.071327 --> 0.068778).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1403417 Vali Loss: 0.0695666 Test Loss: 0.0779402
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1377501 Vali Loss: 0.0670635 Test Loss: 0.0779881
Validation loss decreased (0.068778 --> 0.067064).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1361068 Vali Loss: 0.0698898 Test Loss: 0.0779733
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1341276 Vali Loss: 0.0695147 Test Loss: 0.0779602
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1336115 Vali Loss: 0.0687990 Test Loss: 0.0779898
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1335856 Vali Loss: 0.0704220 Test Loss: 0.0779709
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1332247 Vali Loss: 0.0695221 Test Loss: 0.0779723
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1322914 Vali Loss: 0.0694446 Test Loss: 0.0779730
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1340026 Vali Loss: 0.0681528 Test Loss: 0.0779715
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1329251 Vali Loss: 0.0692087 Test Loss: 0.0779717
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1332765 Vali Loss: 0.0708950 Test Loss: 0.0779715
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1327694 Vali Loss: 0.0693680 Test Loss: 0.0779717
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_SP500_H5_iTransformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
test shape: (236, 5, 1) (236, 5, 1)
test shape: (236, 5, 1) (236, 5, 1)


	mse:6.980150646995753e-05, mae:0.006255507934838533, rmse:0.00835472997277975, r2:-0.07450926303863525, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0063, RMSE: 0.0084, RÂ²: -0.0745, MAPE: 5.16%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.544 MB of 0.544 MB uploadedwandb: \ 0.544 MB of 0.544 MB uploadedwandb: | 0.544 MB of 0.544 MB uploadedwandb: / 0.544 MB of 0.544 MB uploadedwandb: - 0.544 MB of 0.544 MB uploadedwandb: \ 0.544 MB of 0.713 MB uploadedwandb: | 0.713 MB of 0.713 MB uploadedwandb: / 0.713 MB of 0.713 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–…â–â–†â–…â–„â–‡â–…â–…â–ƒâ–…â–‡â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6339589
wandb:     model/trainable_params 6339589
wandb:                  test/loss 0.07797
wandb:                 train/loss 0.13277
wandb:   val/directional_accuracy 47.98729
wandb:                   val/loss 0.06937
wandb:                    val/mae 0.00626
wandb:                   val/mape 515.71794
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.07451
wandb:                   val/rmse 0.00835
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/l1wl1yq2
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_093902-l1wl1yq2/logs
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread ChkStopThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 268, in check_network_status
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 286, in check_stop_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 848, in deliver_network_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 840, in deliver_stop_status
    return self._deliver_stop_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 494, in _deliver_stop_status
    return self._deliver_network_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 510, in _deliver_network_status
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: SP500 H=5

Training: iTransformer on SP500 for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_094031-cl6ptzys
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/cl6ptzys
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_SP500_H10Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/cl6ptzys
>>>>>>>start training : long_term_forecast_iTransformer_SP500_H10_iTransformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 10
============================================================
train 4234
val 230
test 231
Epoch: 1, Steps: 133 | Train Loss: 0.2050618 Vali Loss: 0.0803825 Test Loss: 0.0871320
Validation loss decreased (inf --> 0.080382).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.1843443 Vali Loss: 0.0786982 Test Loss: 0.0841011
Validation loss decreased (0.080382 --> 0.078698).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.1640462 Vali Loss: 0.0717135 Test Loss: 0.0815188
Validation loss decreased (0.078698 --> 0.071713).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1538793 Vali Loss: 0.0733892 Test Loss: 0.0807272
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2050617737765599, 'val/loss': 0.08038248494267464, 'test/loss': 0.08713196264579892, '_timestamp': 1762328441.122277}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.18434430882894903, 'val/loss': 0.07869823090732098, 'test/loss': 0.08410111116245389, '_timestamp': 1762328444.4488144}).
Epoch: 5, Steps: 133 | Train Loss: 0.1478962 Vali Loss: 0.0728411 Test Loss: 0.0804684
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1460092 Vali Loss: 0.0723065 Test Loss: 0.0804587
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1438546 Vali Loss: 0.0719379 Test Loss: 0.0804272
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1436395 Vali Loss: 0.0707200 Test Loss: 0.0804280
Validation loss decreased (0.071713 --> 0.070720).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1439110 Vali Loss: 0.0690004 Test Loss: 0.0804311
Validation loss decreased (0.070720 --> 0.069000).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1423320 Vali Loss: 0.0709796 Test Loss: 0.0804288
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1423390 Vali Loss: 0.0706540 Test Loss: 0.0804278
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1430577 Vali Loss: 0.0691166 Test Loss: 0.0804265
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1424733 Vali Loss: 0.0717017 Test Loss: 0.0804257
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1427598 Vali Loss: 0.0723604 Test Loss: 0.0804254
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1426034 Vali Loss: 0.0702669 Test Loss: 0.0804253
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1426510 Vali Loss: 0.0716929 Test Loss: 0.0804254
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1417951 Vali Loss: 0.0736898 Test Loss: 0.0804254
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1419834 Vali Loss: 0.0708508 Test Loss: 0.0804254
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1430581 Vali Loss: 0.0700938 Test Loss: 0.0804254
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_SP500_H10_iTransformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 231
test shape: (231, 10, 1) (231, 10, 1)
test shape: (231, 10, 1) (231, 10, 1)


	mse:6.98659714544192e-05, mae:0.006256133317947388, rmse:0.008358586579561234, r2:-0.0748147964477539, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0063, RMSE: 0.0084, RÂ²: -0.0748, MAPE: 4.33%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.563 MB of 0.564 MB uploadedwandb: \ 0.563 MB of 0.564 MB uploadedwandb: | 0.563 MB of 0.564 MB uploadedwandb: / 0.564 MB of 0.564 MB uploadedwandb: - 0.564 MB of 0.564 MB uploadedwandb: \ 0.564 MB of 0.733 MB uploadedwandb: | 0.733 MB of 0.733 MB uploadedwandb: / 0.733 MB of 0.733 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–ˆâ–‡â–†â–…â–„â–â–„â–ƒâ–â–…â–†â–ƒâ–…â–ˆâ–„â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 18
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6342154
wandb:     model/trainable_params 6342154
wandb:                  test/loss 0.08043
wandb:                 train/loss 0.14306
wandb:   val/directional_accuracy 47.90765
wandb:                   val/loss 0.07009
wandb:                    val/mae 0.00626
wandb:                   val/mape 432.59025
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.07481
wandb:                   val/rmse 0.00836
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/cl6ptzys
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_094031-cl6ptzys/logs
Completed: SP500 H=10

Training: iTransformer on SP500 for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_094205-kzvw2662
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/kzvw2662
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_SP500_H22Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/kzvw2662
>>>>>>>start training : long_term_forecast_iTransformer_SP500_H22_iTransformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 22
============================================================
train 4222
val 218
test 219
Epoch: 1, Steps: 132 | Train Loss: 0.2052636 Vali Loss: 0.0767814 Test Loss: 0.0759409
Validation loss decreased (inf --> 0.076781).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.1842300 Vali Loss: 0.0759721 Test Loss: 0.0746407
Validation loss decreased (0.076781 --> 0.075972).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.1716085 Vali Loss: 0.0747149 Test Loss: 0.0734680
Validation loss decreased (0.075972 --> 0.074715).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.1640850 Vali Loss: 0.0743615 Test Loss: 0.0731393
Validation loss decreased (0.074715 --> 0.074361).  Saving model ...
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2052635888032841, 'val/loss': 0.07678137719631195, 'test/loss': 0.07594093041760581, '_timestamp': 1762328533.5918708}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.18423004550012675, 'val/loss': 0.0759721313204084, 'test/loss': 0.07464074077350753, '_timestamp': 1762328536.9251597}).
Epoch: 5, Steps: 132 | Train Loss: 0.1599951 Vali Loss: 0.0741175 Test Loss: 0.0728665
Validation loss decreased (0.074361 --> 0.074117).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.1580677 Vali Loss: 0.0739333 Test Loss: 0.0726595
Validation loss decreased (0.074117 --> 0.073933).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.1574824 Vali Loss: 0.0738105 Test Loss: 0.0726180
Validation loss decreased (0.073933 --> 0.073811).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.1568166 Vali Loss: 0.0739085 Test Loss: 0.0726059
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.1564151 Vali Loss: 0.0737307 Test Loss: 0.0726059
Validation loss decreased (0.073811 --> 0.073731).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.1565377 Vali Loss: 0.0740167 Test Loss: 0.0726029
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.1566161 Vali Loss: 0.0739388 Test Loss: 0.0726037
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.1562755 Vali Loss: 0.0735483 Test Loss: 0.0726030
Validation loss decreased (0.073731 --> 0.073548).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.1563876 Vali Loss: 0.0737111 Test Loss: 0.0726034
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.1559906 Vali Loss: 0.0739250 Test Loss: 0.0726036
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.1558753 Vali Loss: 0.0736806 Test Loss: 0.0726036
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.1558206 Vali Loss: 0.0737310 Test Loss: 0.0726036
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.1564828 Vali Loss: 0.0736950 Test Loss: 0.0726036
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.1557030 Vali Loss: 0.0740101 Test Loss: 0.0726036
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.1562814 Vali Loss: 0.0739453 Test Loss: 0.0726037
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.1566212 Vali Loss: 0.0739014 Test Loss: 0.0726037
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.1558565 Vali Loss: 0.0737076 Test Loss: 0.0726036
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.1561172 Vali Loss: 0.0738078 Test Loss: 0.0726037
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_SP500_H22_iTransformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 219
test shape: (219, 22, 1) (219, 22, 1)
test shape: (219, 22, 1) (219, 22, 1)


	mse:6.733742338838056e-05, mae:0.006126113701611757, rmse:0.008205938152968884, r2:-0.054694414138793945, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0082, RÂ²: -0.0547, MAPE: 3.37%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.637 MB of 0.638 MB uploadedwandb: \ 0.637 MB of 0.638 MB uploadedwandb: | 0.637 MB of 0.638 MB uploadedwandb: / 0.638 MB of 0.638 MB uploadedwandb: - 0.638 MB of 0.808 MB uploadedwandb: \ 0.638 MB of 0.808 MB uploadedwandb: | 0.808 MB of 0.808 MB uploadedwandb: / 0.808 MB of 0.808 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–„â–ƒâ–ƒâ–ƒâ–‚â–„â–ƒâ–â–‚â–ƒâ–‚â–‚â–‚â–„â–ƒâ–ƒâ–‚â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6348310
wandb:     model/trainable_params 6348310
wandb:                  test/loss 0.0726
wandb:                 train/loss 0.15612
wandb:   val/directional_accuracy 48.24962
wandb:                   val/loss 0.07381
wandb:                    val/mae 0.00613
wandb:                   val/mape 336.5231
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.05469
wandb:                   val/rmse 0.00821
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/kzvw2662
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_094205-kzvw2662/logs
Completed: SP500 H=22

Training: iTransformer on SP500 for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_094348-74r6rmes
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/74r6rmes
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_SP500_H50Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/74r6rmes
>>>>>>>start training : long_term_forecast_iTransformer_SP500_H50_iTransformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 50
============================================================
train 4194
val 190
test 191
Epoch: 1, Steps: 132 | Train Loss: 0.2088920 Vali Loss: 0.0770017 Test Loss: 0.0787717
Validation loss decreased (inf --> 0.077002).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.1879954 Vali Loss: 0.0752557 Test Loss: 0.0771810
Validation loss decreased (0.077002 --> 0.075256).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.1787689 Vali Loss: 0.0748471 Test Loss: 0.0763088
Validation loss decreased (0.075256 --> 0.074847).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.1764982 Vali Loss: 0.0743524 Test Loss: 0.0757983
Validation loss decreased (0.074847 --> 0.074352).  Saving model ...
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.20889196618262565, 'val/loss': 0.07700169210632642, 'test/loss': 0.07877166879673798, '_timestamp': 1762328638.4974077}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.18799536703436664, 'val/loss': 0.07525572304924329, 'test/loss': 0.07718099902073543, '_timestamp': 1762328641.7938955}).
Epoch: 5, Steps: 132 | Train Loss: 0.1723355 Vali Loss: 0.0741668 Test Loss: 0.0755758
Validation loss decreased (0.074352 --> 0.074167).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.1710257 Vali Loss: 0.0741278 Test Loss: 0.0755041
Validation loss decreased (0.074167 --> 0.074128).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.1704585 Vali Loss: 0.0741142 Test Loss: 0.0754668
Validation loss decreased (0.074128 --> 0.074114).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.1703609 Vali Loss: 0.0740701 Test Loss: 0.0754536
Validation loss decreased (0.074114 --> 0.074070).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.1741277 Vali Loss: 0.0740816 Test Loss: 0.0754351
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.1697299 Vali Loss: 0.0741426 Test Loss: 0.0754397
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.1701534 Vali Loss: 0.0741119 Test Loss: 0.0754368
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.1699765 Vali Loss: 0.0741236 Test Loss: 0.0754361
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.1695659 Vali Loss: 0.0741037 Test Loss: 0.0754351
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.1697808 Vali Loss: 0.0741160 Test Loss: 0.0754348
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.1703724 Vali Loss: 0.0740588 Test Loss: 0.0754346
Validation loss decreased (0.074070 --> 0.074059).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.1688780 Vali Loss: 0.0740942 Test Loss: 0.0754346
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.1761214 Vali Loss: 0.0740757 Test Loss: 0.0754346
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.1701723 Vali Loss: 0.0741752 Test Loss: 0.0754346
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.1706731 Vali Loss: 0.0740971 Test Loss: 0.0754346
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.1705471 Vali Loss: 0.0740683 Test Loss: 0.0754346
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.1692600 Vali Loss: 0.0740842 Test Loss: 0.0754346
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.1693570 Vali Loss: 0.0740836 Test Loss: 0.0754346
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.1705187 Vali Loss: 0.0740999 Test Loss: 0.0754346
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 132 | Train Loss: 0.1701493 Vali Loss: 0.0741376 Test Loss: 0.0754346
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 132 | Train Loss: 0.1702543 Vali Loss: 0.0740933 Test Loss: 0.0754346
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_SP500_H50_iTransformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 191
test shape: (191, 50, 1) (191, 50, 1)
test shape: (191, 50, 1) (191, 50, 1)


	mse:6.739294622093439e-05, mae:0.0061053503304719925, rmse:0.008209320716559887, r2:-0.03629577159881592, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0082, RÂ²: -0.0363, MAPE: 2.92%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.716 MB of 0.719 MB uploadedwandb: \ 0.716 MB of 0.719 MB uploadedwandb: | 0.719 MB of 0.719 MB uploadedwandb: / 0.719 MB of 0.719 MB uploadedwandb: - 0.719 MB of 0.889 MB uploadedwandb: \ 0.889 MB of 0.889 MB uploadedwandb: | 0.889 MB of 0.889 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–†â–ƒâ–ƒâ–‚â–‚â–…â–‚â–‚â–‚â–â–‚â–‚â–â–†â–‚â–‚â–‚â–â–â–‚â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–‚â–‚â–â–â–â–‚â–â–‚â–â–‚â–â–â–â–‚â–â–â–â–â–â–‚â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 24
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6362674
wandb:     model/trainable_params 6362674
wandb:                  test/loss 0.07543
wandb:                 train/loss 0.17025
wandb:   val/directional_accuracy 48.68041
wandb:                   val/loss 0.07409
wandb:                    val/mae 0.00611
wandb:                   val/mape 291.74991
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.0363
wandb:                   val/rmse 0.00821
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/74r6rmes
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_094348-74r6rmes/logs
Completed: SP500 H=50

Training: iTransformer on SP500 for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_094538-osxa9773
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/osxa9773
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_SP500_H100Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/osxa9773
>>>>>>>start training : long_term_forecast_iTransformer_SP500_H100_iTransformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 100
============================================================
train 4144
val 140
test 141
Epoch: 1, Steps: 130 | Train Loss: 0.2186092 Vali Loss: 0.0765500 Test Loss: 0.0896200
Validation loss decreased (inf --> 0.076550).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.1968408 Vali Loss: 0.0736619 Test Loss: 0.0851166
Validation loss decreased (0.076550 --> 0.073662).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.1912615 Vali Loss: 0.0722289 Test Loss: 0.0830561
Validation loss decreased (0.073662 --> 0.072229).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.1884133 Vali Loss: 0.0719432 Test Loss: 0.0823502
Validation loss decreased (0.072229 --> 0.071943).  Saving model ...
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.21860923125193668, 'val/loss': 0.07655002325773239, 'test/loss': 0.08961998224258423, '_timestamp': 1762328748.7036374}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19684080206430876, 'val/loss': 0.073661907017231, 'test/loss': 0.08511659055948258, '_timestamp': 1762328752.0463839}).
Epoch: 5, Steps: 130 | Train Loss: 0.1871114 Vali Loss: 0.0720733 Test Loss: 0.0821253
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.1863546 Vali Loss: 0.0720969 Test Loss: 0.0820213
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.1860739 Vali Loss: 0.0718722 Test Loss: 0.0819641
Validation loss decreased (0.071943 --> 0.071872).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.1851920 Vali Loss: 0.0716063 Test Loss: 0.0819296
Validation loss decreased (0.071872 --> 0.071606).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.1860262 Vali Loss: 0.0718607 Test Loss: 0.0819311
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.1862006 Vali Loss: 0.0714634 Test Loss: 0.0819215
Validation loss decreased (0.071606 --> 0.071463).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.1851615 Vali Loss: 0.0713838 Test Loss: 0.0819200
Validation loss decreased (0.071463 --> 0.071384).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.1853559 Vali Loss: 0.0718777 Test Loss: 0.0819196
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.1851505 Vali Loss: 0.0717793 Test Loss: 0.0819186
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.1852422 Vali Loss: 0.0715967 Test Loss: 0.0819181
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.1855170 Vali Loss: 0.0712536 Test Loss: 0.0819179
Validation loss decreased (0.071384 --> 0.071254).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 130 | Train Loss: 0.1852207 Vali Loss: 0.0715108 Test Loss: 0.0819179
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 130 | Train Loss: 0.1859869 Vali Loss: 0.0718498 Test Loss: 0.0819179
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 130 | Train Loss: 0.1850339 Vali Loss: 0.0715074 Test Loss: 0.0819178
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 130 | Train Loss: 0.1851427 Vali Loss: 0.0715973 Test Loss: 0.0819179
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 130 | Train Loss: 0.1850923 Vali Loss: 0.0716269 Test Loss: 0.0819178
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 130 | Train Loss: 0.1854957 Vali Loss: 0.0719728 Test Loss: 0.0819178
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 130 | Train Loss: 0.1856544 Vali Loss: 0.0717306 Test Loss: 0.0819178
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 130 | Train Loss: 0.1854640 Vali Loss: 0.0713802 Test Loss: 0.0819178
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 130 | Train Loss: 0.1855493 Vali Loss: 0.0718118 Test Loss: 0.0819178
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 130 | Train Loss: 0.1854092 Vali Loss: 0.0716174 Test Loss: 0.0819178
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_SP500_H100_iTransformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 141
test shape: (141, 100, 1) (141, 100, 1)
test shape: (141, 100, 1) (141, 100, 1)


	mse:6.936158024473116e-05, mae:0.006161628756672144, rmse:0.008328359574079514, r2:-0.012454509735107422, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0062, RMSE: 0.0083, RÂ²: -0.0125, MAPE: 2.62%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.810 MB of 0.815 MB uploadedwandb: \ 0.815 MB of 0.815 MB uploadedwandb: | 0.815 MB of 0.815 MB uploadedwandb: / 0.815 MB of 0.986 MB uploadedwandb: - 0.986 MB of 0.986 MB uploadedwandb: \ 0.986 MB of 0.986 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–‚â–‚â–â–â–â–â–‚â–â–‚â–â–â–â–‚â–‚â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–‡â–‡â–…â–„â–…â–ƒâ–‚â–…â–…â–ƒâ–â–ƒâ–…â–ƒâ–ƒâ–„â–†â–„â–‚â–…â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 24
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6388324
wandb:     model/trainable_params 6388324
wandb:                  test/loss 0.08192
wandb:                 train/loss 0.18541
wandb:   val/directional_accuracy 49.14392
wandb:                   val/loss 0.07162
wandb:                    val/mae 0.00616
wandb:                   val/mape 261.68883
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.01245
wandb:                   val/rmse 0.00833
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/osxa9773
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_094538-osxa9773/logs
Completed: SP500 H=100

Training: iTransformer on NASDAQ for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_094731-pnwsto3i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/pnwsto3i
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_NASDAQ_H3Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/pnwsto3i
>>>>>>>start training : long_term_forecast_iTransformer_NASDAQ_H3_iTransformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.2777822 Vali Loss: 0.1469078 Test Loss: 0.1359014
Validation loss decreased (inf --> 0.146908).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2372469 Vali Loss: 0.1406406 Test Loss: 0.1364761
Validation loss decreased (0.146908 --> 0.140641).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2111786 Vali Loss: 0.1300303 Test Loss: 0.1276729
Validation loss decreased (0.140641 --> 0.130030).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1924696 Vali Loss: 0.1329278 Test Loss: 0.1260555
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.27778221465143044, 'val/loss': 0.1469078278169036, 'test/loss': 0.13590136030688882, '_timestamp': 1762328861.5661821}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23724687614835294, 'val/loss': 0.14064062852412462, 'test/loss': 0.13647611998021603, '_timestamp': 1762328864.8613522}).
Epoch: 5, Steps: 133 | Train Loss: 0.1831357 Vali Loss: 0.1414445 Test Loss: 0.1256718
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1792051 Vali Loss: 0.1280089 Test Loss: 0.1257292
Validation loss decreased (0.130030 --> 0.128009).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1770825 Vali Loss: 0.1304577 Test Loss: 0.1254152
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1756493 Vali Loss: 0.1304603 Test Loss: 0.1253342
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1746366 Vali Loss: 0.1420253 Test Loss: 0.1253501
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1756223 Vali Loss: 0.1406478 Test Loss: 0.1253683
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1741865 Vali Loss: 0.1319331 Test Loss: 0.1253752
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1743498 Vali Loss: 0.1286429 Test Loss: 0.1253734
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1737422 Vali Loss: 0.1313767 Test Loss: 0.1253739
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1746591 Vali Loss: 0.1302899 Test Loss: 0.1253726
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1745055 Vali Loss: 0.1279807 Test Loss: 0.1253734
Validation loss decreased (0.128009 --> 0.127981).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1740659 Vali Loss: 0.1301377 Test Loss: 0.1253731
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1748119 Vali Loss: 0.1425308 Test Loss: 0.1253735
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1732105 Vali Loss: 0.1300509 Test Loss: 0.1253734
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1741297 Vali Loss: 0.1311313 Test Loss: 0.1253734
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1741680 Vali Loss: 0.1282891 Test Loss: 0.1253733
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1734054 Vali Loss: 0.1275795 Test Loss: 0.1253734
Validation loss decreased (0.127981 --> 0.127580).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1750170 Vali Loss: 0.1328539 Test Loss: 0.1253733
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1736963 Vali Loss: 0.1524249 Test Loss: 0.1253734
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.1751547 Vali Loss: 0.1293225 Test Loss: 0.1253734
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.1739474 Vali Loss: 0.1446562 Test Loss: 0.1253735
EarlyStopping counter: 4 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.1750068 Vali Loss: 0.1311315 Test Loss: 0.1253734
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.1752627 Vali Loss: 0.1290133 Test Loss: 0.1253734
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.1740149 Vali Loss: 0.1288180 Test Loss: 0.1253734
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.1740987 Vali Loss: 0.1284640 Test Loss: 0.1253734
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 133 | Train Loss: 0.1750582 Vali Loss: 0.1321304 Test Loss: 0.1253734
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 133 | Train Loss: 0.1755616 Vali Loss: 0.1283839 Test Loss: 0.1253734
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_NASDAQ_H3_iTransformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.00014754643780179322, mae:0.00890523660928011, rmse:0.01214686967432499, r2:-0.08403289318084717, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0089, RMSE: 0.0121, RÂ²: -0.0840, MAPE: 7301682.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.493 MB of 0.493 MB uploadedwandb: \ 0.493 MB of 0.493 MB uploadedwandb: | 0.493 MB of 0.493 MB uploadedwandb: / 0.493 MB of 0.493 MB uploadedwandb: - 0.493 MB of 0.493 MB uploadedwandb: \ 0.493 MB of 0.493 MB uploadedwandb: | 0.493 MB of 0.493 MB uploadedwandb: / 0.493 MB of 0.493 MB uploadedwandb: - 0.493 MB of 0.493 MB uploadedwandb: \ 0.598 MB of 0.769 MB uploaded (0.002 MB deduped)wandb: | 0.769 MB of 0.769 MB uploaded (0.002 MB deduped)wandb: / 0.769 MB of 0.769 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–ƒâ–…â–â–‚â–‚â–…â–…â–‚â–â–‚â–‚â–â–‚â–…â–‚â–‚â–â–â–‚â–ˆâ–â–†â–‚â–â–â–â–‚â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 30
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6338563
wandb:     model/trainable_params 6338563
wandb:                  test/loss 0.12537
wandb:                 train/loss 0.17556
wandb:   val/directional_accuracy 52.95359
wandb:                   val/loss 0.12838
wandb:                    val/mae 0.00891
wandb:                   val/mape 730168250.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.08403
wandb:                   val/rmse 0.01215
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/pnwsto3i
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_094731-pnwsto3i/logs
Completed: NASDAQ H=3

Training: iTransformer on NASDAQ for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_094949-4dcbaxk3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/4dcbaxk3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_NASDAQ_H5Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/4dcbaxk3
>>>>>>>start training : long_term_forecast_iTransformer_NASDAQ_H5_iTransformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.2757723 Vali Loss: 0.1426856 Test Loss: 0.1333859
Validation loss decreased (inf --> 0.142686).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2408314 Vali Loss: 0.1461671 Test Loss: 0.1348313
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2180137 Vali Loss: 0.1348752 Test Loss: 0.1319001
Validation loss decreased (0.142686 --> 0.134875).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2041577 Vali Loss: 0.1433839 Test Loss: 0.1302541
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.27577231342631175, 'val/loss': 0.14268558751791716, 'test/loss': 0.13338585756719112, '_timestamp': 1762328999.5654645}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24083142240244643, 'val/loss': 0.14616707246750593, 'test/loss': 0.13483129395172, '_timestamp': 1762329002.9226515}).
Epoch: 5, Steps: 133 | Train Loss: 0.1953613 Vali Loss: 0.1427726 Test Loss: 0.1297936
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1916975 Vali Loss: 0.1320220 Test Loss: 0.1294669
Validation loss decreased (0.134875 --> 0.132022).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1896054 Vali Loss: 0.1333589 Test Loss: 0.1295700
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1888640 Vali Loss: 0.1319799 Test Loss: 0.1295907
Validation loss decreased (0.132022 --> 0.131980).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1878139 Vali Loss: 0.1354793 Test Loss: 0.1296027
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1891158 Vali Loss: 0.1351735 Test Loss: 0.1296123
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1875746 Vali Loss: 0.1349686 Test Loss: 0.1296326
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1880301 Vali Loss: 0.1356469 Test Loss: 0.1296326
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1882290 Vali Loss: 0.1354452 Test Loss: 0.1296341
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1875453 Vali Loss: 0.1360200 Test Loss: 0.1296363
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1869502 Vali Loss: 0.1357779 Test Loss: 0.1296368
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1870099 Vali Loss: 0.1321933 Test Loss: 0.1296364
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1883586 Vali Loss: 0.1454015 Test Loss: 0.1296365
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1875352 Vali Loss: 0.1330633 Test Loss: 0.1296362
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_NASDAQ_H5_iTransformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.00014612132508773357, mae:0.008884375914931297, rmse:0.012088065035641193, r2:-0.06811559200286865, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0089, RMSE: 0.0121, RÂ²: -0.0681, MAPE: 4666710.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.537 MB of 0.537 MB uploadedwandb: \ 0.537 MB of 0.537 MB uploadedwandb: | 0.537 MB of 0.537 MB uploadedwandb: / 0.537 MB of 0.537 MB uploadedwandb: - 0.537 MB of 0.537 MB uploadedwandb: \ 0.537 MB of 0.705 MB uploadedwandb: | 0.705 MB of 0.705 MB uploadedwandb: / 0.705 MB of 0.705 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–‡â–‡â–â–‚â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–ˆâ–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 17
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6339589
wandb:     model/trainable_params 6339589
wandb:                  test/loss 0.12964
wandb:                 train/loss 0.18754
wandb:   val/directional_accuracy 50.21277
wandb:                   val/loss 0.13306
wandb:                    val/mae 0.00888
wandb:                   val/mape 466671000.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.06812
wandb:                   val/rmse 0.01209
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/4dcbaxk3
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_094949-4dcbaxk3/logs
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread ChkStopThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 268, in check_network_status
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 286, in check_stop_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 848, in deliver_network_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 840, in deliver_stop_status
    return self._deliver_network_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 510, in _deliver_network_status
    return self._deliver_stop_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 494, in _deliver_stop_status
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: NASDAQ H=5

Training: iTransformer on NASDAQ for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_095121-nt8llc3g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/nt8llc3g
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_NASDAQ_H10Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/nt8llc3g
>>>>>>>start training : long_term_forecast_iTransformer_NASDAQ_H10_iTransformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2749085 Vali Loss: 0.1469320 Test Loss: 0.1390416
Validation loss decreased (inf --> 0.146932).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2470057 Vali Loss: 0.1485470 Test Loss: 0.1370464
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2290112 Vali Loss: 0.1518644 Test Loss: 0.1327176
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2171358 Vali Loss: 0.1425338 Test Loss: 0.1329552
Validation loss decreased (0.146932 --> 0.142534).  Saving model ...
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2749085416247074, 'val/loss': 0.14693202544003725, 'test/loss': 0.13904164172708988, '_timestamp': 1762329089.1377685}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2470057409508784, 'val/loss': 0.14854699652642012, 'test/loss': 0.13704638555645943, '_timestamp': 1762329092.5188787}).
Epoch: 5, Steps: 133 | Train Loss: 0.2114563 Vali Loss: 0.1412464 Test Loss: 0.1323923
Validation loss decreased (0.142534 --> 0.141246).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2085582 Vali Loss: 0.1535791 Test Loss: 0.1323278
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2061315 Vali Loss: 0.1531211 Test Loss: 0.1322582
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2060651 Vali Loss: 0.1398637 Test Loss: 0.1322583
Validation loss decreased (0.141246 --> 0.139864).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2049818 Vali Loss: 0.1514933 Test Loss: 0.1322797
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2038061 Vali Loss: 0.1387625 Test Loss: 0.1322861
Validation loss decreased (0.139864 --> 0.138763).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2048082 Vali Loss: 0.1536163 Test Loss: 0.1322832
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2040671 Vali Loss: 0.1383854 Test Loss: 0.1322878
Validation loss decreased (0.138763 --> 0.138385).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2042484 Vali Loss: 0.1524020 Test Loss: 0.1322874
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2043303 Vali Loss: 0.1412005 Test Loss: 0.1322877
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2041037 Vali Loss: 0.1420509 Test Loss: 0.1322880
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2038488 Vali Loss: 0.1398177 Test Loss: 0.1322880
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2056185 Vali Loss: 0.1387987 Test Loss: 0.1322879
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2052261 Vali Loss: 0.1409275 Test Loss: 0.1322880
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2048560 Vali Loss: 0.1394918 Test Loss: 0.1322878
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2052909 Vali Loss: 0.1409819 Test Loss: 0.1322878
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2053640 Vali Loss: 0.1407436 Test Loss: 0.1322879
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2043959 Vali Loss: 0.1388658 Test Loss: 0.1322879
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_NASDAQ_H10_iTransformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.0001451186544727534, mae:0.00885506346821785, rmse:0.012046520598232746, r2:-0.0483475923538208, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0089, RMSE: 0.0120, RÂ²: -0.0483, MAPE: 3412582.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.592 MB of 0.593 MB uploadedwandb: \ 0.592 MB of 0.593 MB uploadedwandb: | 0.593 MB of 0.593 MB uploadedwandb: / 0.593 MB of 0.593 MB uploadedwandb: - 0.593 MB of 0.762 MB uploadedwandb: \ 0.762 MB of 0.762 MB uploadedwandb: | 0.762 MB of 0.762 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–†â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–ƒâ–‚â–ˆâ–ˆâ–‚â–‡â–â–ˆâ–â–‡â–‚â–ƒâ–‚â–â–‚â–‚â–‚â–‚â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6342154
wandb:     model/trainable_params 6342154
wandb:                  test/loss 0.13229
wandb:                 train/loss 0.2044
wandb:   val/directional_accuracy 51.93237
wandb:                   val/loss 0.13887
wandb:                    val/mae 0.00886
wandb:                   val/mape 341258225.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.04835
wandb:                   val/rmse 0.01205
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/nt8llc3g
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_095121-nt8llc3g/logs
Completed: NASDAQ H=10

Training: iTransformer on NASDAQ for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_095302-lvy4eank
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/lvy4eank
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_NASDAQ_H22Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/lvy4eank
>>>>>>>start training : long_term_forecast_iTransformer_NASDAQ_H22_iTransformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2778892 Vali Loss: 0.1573361 Test Loss: 0.1401648
Validation loss decreased (inf --> 0.157336).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2549071 Vali Loss: 0.1559108 Test Loss: 0.1392429
Validation loss decreased (0.157336 --> 0.155911).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2398906 Vali Loss: 0.1530868 Test Loss: 0.1364400
Validation loss decreased (0.155911 --> 0.153087).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2312651 Vali Loss: 0.1522449 Test Loss: 0.1357500
Validation loss decreased (0.153087 --> 0.152245).  Saving model ...
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.27788924572594237, 'val/loss': 0.15733609667846135, 'test/loss': 0.1401647882802146, '_timestamp': 1762329192.3519695}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25490705680215, 'val/loss': 0.15591083467006683, 'test/loss': 0.13924294071538107, '_timestamp': 1762329195.7769537}).
Epoch: 5, Steps: 132 | Train Loss: 0.2278281 Vali Loss: 0.1523415 Test Loss: 0.1353059
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2248081 Vali Loss: 0.1523146 Test Loss: 0.1352509
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2245276 Vali Loss: 0.1519930 Test Loss: 0.1352594
Validation loss decreased (0.152245 --> 0.151993).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2236253 Vali Loss: 0.1510193 Test Loss: 0.1352519
Validation loss decreased (0.151993 --> 0.151019).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2234143 Vali Loss: 0.1524543 Test Loss: 0.1352438
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2229992 Vali Loss: 0.1525758 Test Loss: 0.1352404
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2232591 Vali Loss: 0.1517112 Test Loss: 0.1352362
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2233826 Vali Loss: 0.1512968 Test Loss: 0.1352365
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2230419 Vali Loss: 0.1520108 Test Loss: 0.1352368
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2230122 Vali Loss: 0.1520146 Test Loss: 0.1352367
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2235101 Vali Loss: 0.1520883 Test Loss: 0.1352366
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2233104 Vali Loss: 0.1527380 Test Loss: 0.1352366
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2227217 Vali Loss: 0.1522672 Test Loss: 0.1352366
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2228039 Vali Loss: 0.1527253 Test Loss: 0.1352366
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_NASDAQ_H22_iTransformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.00014691220712848008, mae:0.008860795758664608, rmse:0.01212073490023613, r2:-0.04993176460266113, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0089, RMSE: 0.0121, RÂ²: -0.0499, MAPE: 1937231.88%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.678 MB of 0.679 MB uploadedwandb: \ 0.678 MB of 0.679 MB uploadedwandb: | 0.678 MB of 0.679 MB uploadedwandb: / 0.679 MB of 0.679 MB uploadedwandb: - 0.679 MB of 0.848 MB uploadedwandb: \ 0.848 MB of 0.848 MB uploadedwandb: | 0.848 MB of 0.848 MB uploadedwandb: / 0.848 MB of 0.848 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–…â–…â–„â–â–†â–†â–ƒâ–‚â–„â–„â–…â–‡â–…â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 17
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6348310
wandb:     model/trainable_params 6348310
wandb:                  test/loss 0.13524
wandb:                 train/loss 0.2228
wandb:   val/directional_accuracy 52.73045
wandb:                   val/loss 0.15273
wandb:                    val/mae 0.00886
wandb:                   val/mape 193723187.5
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.04993
wandb:                   val/rmse 0.01212
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/lvy4eank
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_095302-lvy4eank/logs
Exception in thread ChkStopThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 286, in check_stop_status
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 268, in check_network_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 840, in deliver_stop_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 848, in deliver_network_status
    return self._deliver_stop_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 494, in _deliver_stop_status
    return self._deliver_network_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 510, in _deliver_network_status
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: NASDAQ H=22

Training: iTransformer on NASDAQ for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_095435-fckg94qz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/fckg94qz
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_NASDAQ_H50Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/fckg94qz
>>>>>>>start training : long_term_forecast_iTransformer_NASDAQ_H50_iTransformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.2864487 Vali Loss: 0.1650824 Test Loss: 0.1482062
Validation loss decreased (inf --> 0.165082).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2634744 Vali Loss: 0.1641057 Test Loss: 0.1474745
Validation loss decreased (0.165082 --> 0.164106).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2528915 Vali Loss: 0.1625905 Test Loss: 0.1455479
Validation loss decreased (0.164106 --> 0.162591).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2476942 Vali Loss: 0.1609092 Test Loss: 0.1450294
Validation loss decreased (0.162591 --> 0.160909).  Saving model ...
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.28644867263960117, 'val/loss': 0.16508235285679498, 'test/loss': 0.1482061669230461, '_timestamp': 1762329283.0994327}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2634744264298316, 'val/loss': 0.16410565624634424, 'test/loss': 0.14747449507315954, '_timestamp': 1762329286.901816}).
Epoch: 5, Steps: 132 | Train Loss: 0.2455101 Vali Loss: 0.1609327 Test Loss: 0.1447249
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2460399 Vali Loss: 0.1606130 Test Loss: 0.1446499
Validation loss decreased (0.160909 --> 0.160613).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2427178 Vali Loss: 0.1603930 Test Loss: 0.1445770
Validation loss decreased (0.160613 --> 0.160393).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2421776 Vali Loss: 0.1605374 Test Loss: 0.1445496
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2446444 Vali Loss: 0.1607063 Test Loss: 0.1445442
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2416393 Vali Loss: 0.1604967 Test Loss: 0.1445432
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2418809 Vali Loss: 0.1604951 Test Loss: 0.1445381
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2430746 Vali Loss: 0.1607264 Test Loss: 0.1445385
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2419840 Vali Loss: 0.1603780 Test Loss: 0.1445385
Validation loss decreased (0.160393 --> 0.160378).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2437194 Vali Loss: 0.1604480 Test Loss: 0.1445382
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2422165 Vali Loss: 0.1608382 Test Loss: 0.1445383
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2420097 Vali Loss: 0.1604995 Test Loss: 0.1445380
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2444774 Vali Loss: 0.1605179 Test Loss: 0.1445380
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2417734 Vali Loss: 0.1605698 Test Loss: 0.1445380
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2418380 Vali Loss: 0.1604559 Test Loss: 0.1445380
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2424676 Vali Loss: 0.1604736 Test Loss: 0.1445380
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2422238 Vali Loss: 0.1606133 Test Loss: 0.1445380
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2417008 Vali Loss: 0.1606314 Test Loss: 0.1445380
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.2416716 Vali Loss: 0.1606746 Test Loss: 0.1445380
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_NASDAQ_H50_iTransformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.0001504316460341215, mae:0.008885692805051804, rmse:0.012265058234333992, r2:-0.03856527805328369, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0089, RMSE: 0.0123, RÂ²: -0.0386, MAPE: 2587404.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.715 MB of 0.718 MB uploadedwandb: \ 0.715 MB of 0.718 MB uploadedwandb: | 0.715 MB of 0.718 MB uploadedwandb: / 0.718 MB of 0.718 MB uploadedwandb: - 0.718 MB of 0.887 MB uploadedwandb: \ 0.718 MB of 0.887 MB uploadedwandb: | 0.887 MB of 0.887 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–„â–‚â–â–ƒâ–â–â–‚â–â–‚â–â–â–ƒâ–â–â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–ƒâ–‚â–â–‚â–‚â–â–â–‚â–â–â–‚â–â–â–‚â–â–â–‚â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6362674
wandb:     model/trainable_params 6362674
wandb:                  test/loss 0.14454
wandb:                 train/loss 0.24167
wandb:   val/directional_accuracy 51.9334
wandb:                   val/loss 0.16067
wandb:                    val/mae 0.00889
wandb:                   val/mape 258740450.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.03857
wandb:                   val/rmse 0.01227
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/fckg94qz
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_095435-fckg94qz/logs
Exception in thread ChkStopThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread IntMsgThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 286, in check_stop_status
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 300, in check_internal_messages
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 840, in deliver_stop_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 856, in deliver_internal_messages
    return self._deliver_stop_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 494, in _deliver_stop_status
    return self._deliver_internal_messages(internal_message)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 516, in _deliver_internal_messages
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: NASDAQ H=50

Training: iTransformer on NASDAQ for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_095621-a274k9y6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/a274k9y6
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_NASDAQ_H100Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/a274k9y6
>>>>>>>start training : long_term_forecast_iTransformer_NASDAQ_H100_iTransformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.2976236 Vali Loss: 0.1794012 Test Loss: 0.1586454
Validation loss decreased (inf --> 0.179401).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2740485 Vali Loss: 0.1746111 Test Loss: 0.1588740
Validation loss decreased (0.179401 --> 0.174611).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.2685039 Vali Loss: 0.1719948 Test Loss: 0.1587158
Validation loss decreased (0.174611 --> 0.171995).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2660492 Vali Loss: 0.1715162 Test Loss: 0.1586339
Validation loss decreased (0.171995 --> 0.171516).  Saving model ...
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2976236132475046, 'val/loss': 0.17940119802951812, 'test/loss': 0.15864540338516236, '_timestamp': 1762329391.5644789}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.27404851752978104, 'val/loss': 0.17461109757423401, 'test/loss': 0.15887404084205628, '_timestamp': 1762329394.917575}).
Epoch: 5, Steps: 130 | Train Loss: 0.2639634 Vali Loss: 0.1760200 Test Loss: 0.1584705
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2629646 Vali Loss: 0.1747689 Test Loss: 0.1584008
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2625962 Vali Loss: 0.1747341 Test Loss: 0.1583279
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2618190 Vali Loss: 0.1714755 Test Loss: 0.1583389
Validation loss decreased (0.171516 --> 0.171476).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2624770 Vali Loss: 0.1749321 Test Loss: 0.1583430
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2622923 Vali Loss: 0.1706595 Test Loss: 0.1583444
Validation loss decreased (0.171476 --> 0.170660).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2623103 Vali Loss: 0.1721584 Test Loss: 0.1583436
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2620967 Vali Loss: 0.1720854 Test Loss: 0.1583444
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2620751 Vali Loss: 0.1724256 Test Loss: 0.1583442
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.2620756 Vali Loss: 0.1730157 Test Loss: 0.1583439
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.2620371 Vali Loss: 0.1697927 Test Loss: 0.1583438
Validation loss decreased (0.170660 --> 0.169793).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 130 | Train Loss: 0.2620861 Vali Loss: 0.1731257 Test Loss: 0.1583438
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 130 | Train Loss: 0.2625110 Vali Loss: 0.1732814 Test Loss: 0.1583437
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 130 | Train Loss: 0.2622911 Vali Loss: 0.1722456 Test Loss: 0.1583438
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 130 | Train Loss: 0.2618636 Vali Loss: 0.1728413 Test Loss: 0.1583438
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 130 | Train Loss: 0.2618377 Vali Loss: 0.1711733 Test Loss: 0.1583438
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 130 | Train Loss: 0.2618866 Vali Loss: 0.1742885 Test Loss: 0.1583438
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 130 | Train Loss: 0.2619531 Vali Loss: 0.1715969 Test Loss: 0.1583438
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 130 | Train Loss: 0.2618266 Vali Loss: 0.1702614 Test Loss: 0.1583438
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 130 | Train Loss: 0.2620130 Vali Loss: 0.1746612 Test Loss: 0.1583438
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 130 | Train Loss: 0.2619687 Vali Loss: 0.1734415 Test Loss: 0.1583438
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_NASDAQ_H100_iTransformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0001573579356772825, mae:0.008828293532133102, rmse:0.012544238939881325, r2:-0.037474870681762695, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0088, RMSE: 0.0125, RÂ²: -0.0375, MAPE: 2033990.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.774 MB of 0.779 MB uploadedwandb: \ 0.774 MB of 0.779 MB uploadedwandb: | 0.779 MB of 0.779 MB uploadedwandb: / 0.779 MB of 0.779 MB uploadedwandb: - 0.779 MB of 0.949 MB uploadedwandb: \ 0.949 MB of 0.949 MB uploadedwandb: | 0.949 MB of 0.949 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‡â–„â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–‚â–â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–ƒâ–ˆâ–‡â–‡â–ƒâ–‡â–‚â–„â–„â–„â–…â–â–…â–…â–„â–„â–ƒâ–†â–ƒâ–‚â–†â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 24
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6388324
wandb:     model/trainable_params 6388324
wandb:                  test/loss 0.15834
wandb:                 train/loss 0.26197
wandb:   val/directional_accuracy 49.87013
wandb:                   val/loss 0.17344
wandb:                    val/mae 0.00883
wandb:                   val/mape 203399025.0
wandb:                    val/mse 0.00016
wandb:                     val/r2 -0.03747
wandb:                   val/rmse 0.01254
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/a274k9y6
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_095621-a274k9y6/logs
Completed: NASDAQ H=100

Training: iTransformer on ABSA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_095820-ox9dfarv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/ox9dfarv
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_ABSA_H3Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/ox9dfarv
>>>>>>>start training : long_term_forecast_iTransformer_ABSA_H3_iTransformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 238
Epoch: 1, Steps: 133 | Train Loss: 0.3291668 Vali Loss: 0.1813523 Test Loss: 0.1735908
Validation loss decreased (inf --> 0.181352).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2809232 Vali Loss: 0.1789691 Test Loss: 0.1709037
Validation loss decreased (0.181352 --> 0.178969).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2506082 Vali Loss: 0.1795691 Test Loss: 0.1678330
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2312036 Vali Loss: 0.1760964 Test Loss: 0.1672460
Validation loss decreased (0.178969 --> 0.176096).  Saving model ...
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.32916675676080515, 'val/loss': 0.18135233409702778, 'test/loss': 0.17359076533466578, '_timestamp': 1762329511.2197042}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.28092315255251143, 'val/loss': 0.17896906659007072, 'test/loss': 0.1709036948159337, '_timestamp': 1762329514.6927972}).
Epoch: 5, Steps: 133 | Train Loss: 0.2214094 Vali Loss: 0.1750151 Test Loss: 0.1673466
Validation loss decreased (0.176096 --> 0.175015).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2161198 Vali Loss: 0.1785781 Test Loss: 0.1672743
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2135674 Vali Loss: 0.1722469 Test Loss: 0.1671820
Validation loss decreased (0.175015 --> 0.172247).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2116269 Vali Loss: 0.1681991 Test Loss: 0.1674547
Validation loss decreased (0.172247 --> 0.168199).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2123004 Vali Loss: 0.1742271 Test Loss: 0.1674012
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2101666 Vali Loss: 0.1724624 Test Loss: 0.1674767
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2104258 Vali Loss: 0.1732024 Test Loss: 0.1674699
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2102636 Vali Loss: 0.1775670 Test Loss: 0.1674797
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2103321 Vali Loss: 0.1700506 Test Loss: 0.1674736
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2104001 Vali Loss: 0.1737255 Test Loss: 0.1674777
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2101115 Vali Loss: 0.1723469 Test Loss: 0.1674793
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2109551 Vali Loss: 0.1735547 Test Loss: 0.1674800
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2102704 Vali Loss: 0.1788733 Test Loss: 0.1674799
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2105433 Vali Loss: 0.1742385 Test Loss: 0.1674797
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_ABSA_H3_iTransformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 238
test shape: (238, 3, 1) (238, 3, 1)
test shape: (238, 3, 1) (238, 3, 1)


	mse:0.0005006106221117079, mae:0.016904016956686974, rmse:0.022374330088496208, r2:-0.09937500953674316, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0169, RMSE: 0.0224, RÂ²: -0.0994, MAPE: 2.42%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.499 MB of 0.500 MB uploadedwandb: \ 0.499 MB of 0.500 MB uploadedwandb: | 0.500 MB of 0.500 MB uploadedwandb: / 0.500 MB of 0.500 MB uploadedwandb: - 0.500 MB of 0.500 MB uploadedwandb: \ 0.500 MB of 0.500 MB uploadedwandb: | 0.500 MB of 0.500 MB uploadedwandb: / 0.500 MB of 0.500 MB uploadedwandb: - 0.605 MB of 0.773 MB uploaded (0.002 MB deduped)wandb: \ 0.773 MB of 0.773 MB uploaded (0.002 MB deduped)wandb: | 0.773 MB of 0.773 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–ƒâ–‚â–â–„â–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–„
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–…â–‡â–ƒâ–â–…â–„â–„â–‡â–‚â–„â–„â–„â–ˆâ–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 17
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6338563
wandb:     model/trainable_params 6338563
wandb:                  test/loss 0.16748
wandb:                 train/loss 0.21054
wandb:   val/directional_accuracy 51.2605
wandb:                   val/loss 0.17424
wandb:                    val/mae 0.0169
wandb:                   val/mape 242.38775
wandb:                    val/mse 0.0005
wandb:                     val/r2 -0.09938
wandb:                   val/rmse 0.02237
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/ox9dfarv
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_095820-ox9dfarv/logs
Completed: ABSA H=3

Training: iTransformer on ABSA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_095955-2knlftqu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/2knlftqu
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_ABSA_H5Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/2knlftqu
>>>>>>>start training : long_term_forecast_iTransformer_ABSA_H5_iTransformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 236
Epoch: 1, Steps: 133 | Train Loss: 0.3247493 Vali Loss: 0.1927921 Test Loss: 0.1817052
Validation loss decreased (inf --> 0.192792).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2912788 Vali Loss: 0.1880542 Test Loss: 0.1774824
Validation loss decreased (0.192792 --> 0.188054).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2569387 Vali Loss: 0.1819010 Test Loss: 0.1746580
Validation loss decreased (0.188054 --> 0.181901).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2416130 Vali Loss: 0.1803141 Test Loss: 0.1713919
Validation loss decreased (0.181901 --> 0.180314).  Saving model ...
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.32474925296199053, 'val/loss': 0.19279210828244686, 'test/loss': 0.18170522805303335, '_timestamp': 1762329604.3879719}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.29127876281290127, 'val/loss': 0.18805416859686375, 'test/loss': 0.1774824419990182, '_timestamp': 1762329607.7195282}).
Epoch: 5, Steps: 133 | Train Loss: 0.2331646 Vali Loss: 0.1764024 Test Loss: 0.1711961
Validation loss decreased (0.180314 --> 0.176402).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2283891 Vali Loss: 0.1761859 Test Loss: 0.1709597
Validation loss decreased (0.176402 --> 0.176186).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2254735 Vali Loss: 0.1746723 Test Loss: 0.1710847
Validation loss decreased (0.176186 --> 0.174672).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2251864 Vali Loss: 0.1729228 Test Loss: 0.1710975
Validation loss decreased (0.174672 --> 0.172923).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2232996 Vali Loss: 0.1801640 Test Loss: 0.1711868
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2234554 Vali Loss: 0.1759506 Test Loss: 0.1712142
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2232207 Vali Loss: 0.1778010 Test Loss: 0.1712273
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2230013 Vali Loss: 0.1734549 Test Loss: 0.1712408
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2233171 Vali Loss: 0.1797100 Test Loss: 0.1712433
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2233480 Vali Loss: 0.1728229 Test Loss: 0.1712446
Validation loss decreased (0.172923 --> 0.172823).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2233878 Vali Loss: 0.1766623 Test Loss: 0.1712455
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2243529 Vali Loss: 0.1800629 Test Loss: 0.1712463
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2225752 Vali Loss: 0.1791921 Test Loss: 0.1712470
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2236460 Vali Loss: 0.1784593 Test Loss: 0.1712470
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2230045 Vali Loss: 0.1806561 Test Loss: 0.1712468
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2233437 Vali Loss: 0.1779867 Test Loss: 0.1712468
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2231765 Vali Loss: 0.1784866 Test Loss: 0.1712468
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2238676 Vali Loss: 0.1785528 Test Loss: 0.1712468
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2226233 Vali Loss: 0.1799774 Test Loss: 0.1712469
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2228950 Vali Loss: 0.1750404 Test Loss: 0.1712468
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_ABSA_H5_iTransformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
test shape: (236, 5, 1) (236, 5, 1)
test shape: (236, 5, 1) (236, 5, 1)


	mse:0.0005066614248789847, mae:0.01701672561466694, rmse:0.022509140893816948, r2:-0.1061936616897583, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0170, RMSE: 0.0225, RÂ²: -0.1062, MAPE: 2.18%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.536 MB of 0.536 MB uploadedwandb: \ 0.536 MB of 0.536 MB uploadedwandb: | 0.536 MB of 0.705 MB uploadedwandb: / 0.705 MB of 0.705 MB uploadedwandb: - 0.705 MB of 0.705 MB uploadedwandb: \ 0.705 MB of 0.705 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‡â–„â–„â–‚â–â–‡â–ƒâ–…â–â–†â–â–„â–‡â–†â–…â–‡â–…â–…â–…â–‡â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 23
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6339589
wandb:     model/trainable_params 6339589
wandb:                  test/loss 0.17125
wandb:                 train/loss 0.2229
wandb:   val/directional_accuracy 50.84746
wandb:                   val/loss 0.17504
wandb:                    val/mae 0.01702
wandb:                   val/mape 218.29093
wandb:                    val/mse 0.00051
wandb:                     val/r2 -0.10619
wandb:                   val/rmse 0.02251
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/2knlftqu
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_095955-2knlftqu/logs
Completed: ABSA H=5

Training: iTransformer on ABSA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_100142-v49a5nyc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/v49a5nyc
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_ABSA_H10Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/v49a5nyc
>>>>>>>start training : long_term_forecast_iTransformer_ABSA_H10_iTransformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 231
Epoch: 1, Steps: 133 | Train Loss: 0.3275207 Vali Loss: 0.1909555 Test Loss: 0.1770051
Validation loss decreased (inf --> 0.190955).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2962093 Vali Loss: 0.1904032 Test Loss: 0.1753682
Validation loss decreased (0.190955 --> 0.190403).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2707914 Vali Loss: 0.1805791 Test Loss: 0.1730114
Validation loss decreased (0.190403 --> 0.180579).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2578355 Vali Loss: 0.1840731 Test Loss: 0.1704452
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3275207032386522, 'val/loss': 0.19095546938478947, 'test/loss': 0.1770050898194313, '_timestamp': 1762329712.449953}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.29620928748657827, 'val/loss': 0.190403176471591, 'test/loss': 0.17536820098757744, '_timestamp': 1762329716.1372435}).
Epoch: 5, Steps: 133 | Train Loss: 0.2509158 Vali Loss: 0.1770538 Test Loss: 0.1705887
Validation loss decreased (0.180579 --> 0.177054).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2464455 Vali Loss: 0.1835837 Test Loss: 0.1707679
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2440376 Vali Loss: 0.1807159 Test Loss: 0.1707431
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2430972 Vali Loss: 0.1782161 Test Loss: 0.1707223
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2437737 Vali Loss: 0.1851984 Test Loss: 0.1707711
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2419440 Vali Loss: 0.1784517 Test Loss: 0.1707759
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2450622 Vali Loss: 0.1775404 Test Loss: 0.1707662
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2421900 Vali Loss: 0.1779824 Test Loss: 0.1707639
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2418754 Vali Loss: 0.1857093 Test Loss: 0.1707623
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2412097 Vali Loss: 0.1830601 Test Loss: 0.1707632
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2425348 Vali Loss: 0.1846518 Test Loss: 0.1707628
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_ABSA_H10_iTransformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 231
test shape: (231, 10, 1) (231, 10, 1)
test shape: (231, 10, 1) (231, 10, 1)


	mse:0.0004997850046493113, mae:0.016881093382835388, rmse:0.02235587127506733, r2:-0.08254504203796387, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0169, RMSE: 0.0224, RÂ²: -0.0825, MAPE: 1.99%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.576 MB of 0.576 MB uploadedwandb: \ 0.576 MB of 0.576 MB uploadedwandb: | 0.576 MB of 0.576 MB uploadedwandb: / 0.576 MB of 0.576 MB uploadedwandb: - 0.576 MB of 0.576 MB uploadedwandb: \ 0.576 MB of 0.744 MB uploadedwandb: | 0.744 MB of 0.744 MB uploadedwandb: / 0.744 MB of 0.744 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–‚â–â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–‡â–â–†â–„â–‚â–ˆâ–‚â–â–‚â–ˆâ–†â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6342154
wandb:     model/trainable_params 6342154
wandb:                  test/loss 0.17076
wandb:                 train/loss 0.24253
wandb:   val/directional_accuracy 51.41895
wandb:                   val/loss 0.18465
wandb:                    val/mae 0.01688
wandb:                   val/mape 199.01788
wandb:                    val/mse 0.0005
wandb:                     val/r2 -0.08255
wandb:                   val/rmse 0.02236
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/v49a5nyc
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_100142-v49a5nyc/logs
Completed: ABSA H=10

Training: iTransformer on ABSA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_100311-zes5a2lq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/zes5a2lq
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_ABSA_H22Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/zes5a2lq
>>>>>>>start training : long_term_forecast_iTransformer_ABSA_H22_iTransformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 219
Epoch: 1, Steps: 132 | Train Loss: 0.3402396 Vali Loss: 0.1877077 Test Loss: 0.1711772
Validation loss decreased (inf --> 0.187708).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.3110086 Vali Loss: 0.1879443 Test Loss: 0.1705572
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2910584 Vali Loss: 0.1877728 Test Loss: 0.1685870
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2807110 Vali Loss: 0.1861733 Test Loss: 0.1683664
Validation loss decreased (0.187708 --> 0.186173).  Saving model ...
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.34023963129430107, 'val/loss': 0.1877076838697706, 'test/loss': 0.17117724461214884, '_timestamp': 1762329803.9464712}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.31100859380129614, 'val/loss': 0.18794429302215576, 'test/loss': 0.17055719026497432, '_timestamp': 1762329807.2499816}).
Epoch: 5, Steps: 132 | Train Loss: 0.2751362 Vali Loss: 0.1862384 Test Loss: 0.1689250
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2717569 Vali Loss: 0.1862515 Test Loss: 0.1688034
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2705262 Vali Loss: 0.1861176 Test Loss: 0.1689367
Validation loss decreased (0.186173 --> 0.186118).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2696341 Vali Loss: 0.1863796 Test Loss: 0.1689617
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2692320 Vali Loss: 0.1863362 Test Loss: 0.1689456
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2687745 Vali Loss: 0.1865838 Test Loss: 0.1689387
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2688497 Vali Loss: 0.1866141 Test Loss: 0.1689379
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2692507 Vali Loss: 0.1861028 Test Loss: 0.1689377
Validation loss decreased (0.186118 --> 0.186103).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2685466 Vali Loss: 0.1863656 Test Loss: 0.1689364
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2689774 Vali Loss: 0.1868769 Test Loss: 0.1689371
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2683145 Vali Loss: 0.1859813 Test Loss: 0.1689371
Validation loss decreased (0.186103 --> 0.185981).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2689436 Vali Loss: 0.1864634 Test Loss: 0.1689373
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2680715 Vali Loss: 0.1864856 Test Loss: 0.1689372
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2689551 Vali Loss: 0.1866003 Test Loss: 0.1689373
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2687911 Vali Loss: 0.1867776 Test Loss: 0.1689373
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2692577 Vali Loss: 0.1863463 Test Loss: 0.1689373
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2693584 Vali Loss: 0.1862261 Test Loss: 0.1689373
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2688529 Vali Loss: 0.1864405 Test Loss: 0.1689373
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.2684869 Vali Loss: 0.1863595 Test Loss: 0.1689374
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 132 | Train Loss: 0.2691651 Vali Loss: 0.1861754 Test Loss: 0.1689373
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 132 | Train Loss: 0.2683945 Vali Loss: 0.1865704 Test Loss: 0.1689373
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_ABSA_H22_iTransformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 219
test shape: (219, 22, 1) (219, 22, 1)
test shape: (219, 22, 1) (219, 22, 1)


	mse:0.0005015438655391335, mae:0.017002400010824203, rmse:0.022395174950361252, r2:-0.07048225402832031, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0170, RMSE: 0.0224, RÂ²: -0.0705, MAPE: 2.18%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.667 MB of 0.668 MB uploadedwandb: \ 0.667 MB of 0.668 MB uploadedwandb: | 0.668 MB of 0.668 MB uploadedwandb: / 0.668 MB of 0.838 MB uploadedwandb: - 0.838 MB of 0.838 MB uploadedwandb: \ 0.838 MB of 0.838 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–„â–â–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–â–ƒâ–„â–â–ƒâ–ƒâ–ƒâ–„â–‚â–‚â–ƒâ–‚â–‚â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 24
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6348310
wandb:     model/trainable_params 6348310
wandb:                  test/loss 0.16894
wandb:                 train/loss 0.26839
wandb:   val/directional_accuracy 50.09785
wandb:                   val/loss 0.18657
wandb:                    val/mae 0.017
wandb:                   val/mape 217.75255
wandb:                    val/mse 0.0005
wandb:                     val/r2 -0.07048
wandb:                   val/rmse 0.0224
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/zes5a2lq
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_100311-zes5a2lq/logs
Completed: ABSA H=22

Training: iTransformer on ABSA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_100508-z88rxjzr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/z88rxjzr
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_ABSA_H50Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/z88rxjzr
>>>>>>>start training : long_term_forecast_iTransformer_ABSA_H50_iTransformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 191
Epoch: 1, Steps: 132 | Train Loss: 0.3659654 Vali Loss: 0.1853398 Test Loss: 0.1668341
Validation loss decreased (inf --> 0.185340).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.3365685 Vali Loss: 0.1823925 Test Loss: 0.1660846
Validation loss decreased (0.185340 --> 0.182393).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.3226871 Vali Loss: 0.1809045 Test Loss: 0.1666232
Validation loss decreased (0.182393 --> 0.180904).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.3150138 Vali Loss: 0.1812687 Test Loss: 0.1663422
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.365965367831064, 'val/loss': 0.18533983578284582, 'test/loss': 0.16683407003680864, '_timestamp': 1762329921.0299308}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.33656851507046004, 'val/loss': 0.1823925276597341, 'test/loss': 0.16608460371692976, '_timestamp': 1762329924.3576097}).
Epoch: 5, Steps: 132 | Train Loss: 0.3122298 Vali Loss: 0.1816003 Test Loss: 0.1663880
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.3092202 Vali Loss: 0.1818235 Test Loss: 0.1665136
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.3074317 Vali Loss: 0.1820660 Test Loss: 0.1665729
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.3073041 Vali Loss: 0.1820068 Test Loss: 0.1666595
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.3069260 Vali Loss: 0.1819305 Test Loss: 0.1666922
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.3059719 Vali Loss: 0.1820240 Test Loss: 0.1667004
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.3059608 Vali Loss: 0.1819560 Test Loss: 0.1667085
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.3066192 Vali Loss: 0.1820591 Test Loss: 0.1667148
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.3071241 Vali Loss: 0.1819237 Test Loss: 0.1667166
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_ABSA_H50_iTransformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 191
test shape: (191, 50, 1) (191, 50, 1)
test shape: (191, 50, 1) (191, 50, 1)


	mse:0.0005131588550284505, mae:0.017366718500852585, rmse:0.022653009742498398, r2:-0.05481910705566406, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0174, RMSE: 0.0227, RÂ²: -0.0548, MAPE: 1.65%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.777 MB of 0.780 MB uploadedwandb: \ 0.777 MB of 0.780 MB uploadedwandb: | 0.777 MB of 0.780 MB uploadedwandb: / 0.780 MB of 0.780 MB uploadedwandb: - 0.780 MB of 0.780 MB uploadedwandb: \ 0.780 MB of 0.948 MB uploadedwandb: | 0.948 MB of 0.948 MB uploadedwandb: / 0.948 MB of 0.948 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–†â–â–‚â–„â–…â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ƒâ–…â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6362674
wandb:     model/trainable_params 6362674
wandb:                  test/loss 0.16672
wandb:                 train/loss 0.30712
wandb:   val/directional_accuracy 50.09082
wandb:                   val/loss 0.18192
wandb:                    val/mae 0.01737
wandb:                   val/mape 165.2503
wandb:                    val/mse 0.00051
wandb:                     val/r2 -0.05482
wandb:                   val/rmse 0.02265
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/z88rxjzr
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_100508-z88rxjzr/logs
Completed: ABSA H=50

Training: iTransformer on ABSA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_100625-eosun358
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/eosun358
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_ABSA_H100Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/eosun358
>>>>>>>start training : long_term_forecast_iTransformer_ABSA_H100_iTransformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 141
Epoch: 1, Steps: 130 | Train Loss: 0.4021309 Vali Loss: 0.1913497 Test Loss: 0.1683399
Validation loss decreased (inf --> 0.191350).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.3734146 Vali Loss: 0.1947713 Test Loss: 0.1674120
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.3627711 Vali Loss: 0.1946649 Test Loss: 0.1683018
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.3572069 Vali Loss: 0.1982272 Test Loss: 0.1682908
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.4021308850783568, 'val/loss': 0.19134969115257264, 'test/loss': 0.16833986043930055, '_timestamp': 1762329993.4881299}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3734146372630046, 'val/loss': 0.19477129876613616, 'test/loss': 0.1674119636416435, '_timestamp': 1762329996.775668}).
Epoch: 5, Steps: 130 | Train Loss: 0.3530559 Vali Loss: 0.1967411 Test Loss: 0.1685012
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.3509598 Vali Loss: 0.1965522 Test Loss: 0.1686065
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.3497534 Vali Loss: 0.1983231 Test Loss: 0.1688187
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.3492736 Vali Loss: 0.1949202 Test Loss: 0.1687204
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.3504498 Vali Loss: 0.1961056 Test Loss: 0.1687761
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.3496487 Vali Loss: 0.1981752 Test Loss: 0.1687808
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.3491759 Vali Loss: 0.1971779 Test Loss: 0.1687976
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_ABSA_H100_iTransformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 141
test shape: (141, 100, 1) (141, 100, 1)
test shape: (141, 100, 1) (141, 100, 1)


	mse:0.0005402045790106058, mae:0.01769166626036167, rmse:0.023242302238941193, r2:-0.04683947563171387, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0177, RMSE: 0.0232, RÂ²: -0.0468, MAPE: 1.45%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.883 MB of 0.887 MB uploadedwandb: \ 0.883 MB of 0.887 MB uploadedwandb: | 0.887 MB of 0.887 MB uploadedwandb: / 0.887 MB of 1.056 MB uploadedwandb: - 0.887 MB of 1.056 MB uploadedwandb: \ 1.056 MB of 1.056 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–â–„â–…â–ˆâ–‡â–‡â–‡â–ˆ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ˆâ–…â–…â–ˆâ–â–„â–ˆâ–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6388324
wandb:     model/trainable_params 6388324
wandb:                  test/loss 0.1688
wandb:                 train/loss 0.34918
wandb:   val/directional_accuracy 49.66688
wandb:                   val/loss 0.19718
wandb:                    val/mae 0.01769
wandb:                   val/mape 144.50601
wandb:                    val/mse 0.00054
wandb:                     val/r2 -0.04684
wandb:                   val/rmse 0.02324
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/eosun358
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_100625-eosun358/logs
Completed: ABSA H=100

Training: iTransformer on SASOL for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_100728-k7rmp317
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/k7rmp317
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_SASOL_H3Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/k7rmp317
>>>>>>>start training : long_term_forecast_iTransformer_SASOL_H3_iTransformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 3
============================================================
train 3772
val 211
test 212
Epoch: 1, Steps: 118 | Train Loss: 0.2409880 Vali Loss: 0.1073708 Test Loss: 0.1628248
Validation loss decreased (inf --> 0.107371).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2086577 Vali Loss: 0.1084831 Test Loss: 0.1556139
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 118 | Train Loss: 0.1850795 Vali Loss: 0.1052625 Test Loss: 0.1494027
Validation loss decreased (0.107371 --> 0.105262).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.1705256 Vali Loss: 0.1054335 Test Loss: 0.1484117
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.24098798828357357, 'val/loss': 0.1073707789182663, 'test/loss': 0.16282479677881515, '_timestamp': 1762330055.9618063}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.20865765655949964, 'val/loss': 0.10848310589790344, 'test/loss': 0.15561392584017344, '_timestamp': 1762330059.1433496}).
Epoch: 5, Steps: 118 | Train Loss: 0.1633124 Vali Loss: 0.1018571 Test Loss: 0.1472444
Validation loss decreased (0.105262 --> 0.101857).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.1584930 Vali Loss: 0.1026007 Test Loss: 0.1474421
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.1566717 Vali Loss: 0.1036334 Test Loss: 0.1476567
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.1558048 Vali Loss: 0.1035997 Test Loss: 0.1477363
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.1551882 Vali Loss: 0.1006191 Test Loss: 0.1478109
Validation loss decreased (0.101857 --> 0.100619).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.1550274 Vali Loss: 0.1021396 Test Loss: 0.1478169
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.1542534 Vali Loss: 0.1015114 Test Loss: 0.1478335
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.1552438 Vali Loss: 0.1009033 Test Loss: 0.1478428
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.1552226 Vali Loss: 0.1025163 Test Loss: 0.1478467
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.1548333 Vali Loss: 0.1035486 Test Loss: 0.1478493
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.1549499 Vali Loss: 0.1026403 Test Loss: 0.1478506
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.1547662 Vali Loss: 0.1017488 Test Loss: 0.1478513
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.1547452 Vali Loss: 0.1005876 Test Loss: 0.1478511
Validation loss decreased (0.100619 --> 0.100588).  Saving model ...
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.1539255 Vali Loss: 0.1023608 Test Loss: 0.1478512
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.1557975 Vali Loss: 0.1025599 Test Loss: 0.1478515
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.1535453 Vali Loss: 0.1027961 Test Loss: 0.1478514
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 118 | Train Loss: 0.1552039 Vali Loss: 0.1053991 Test Loss: 0.1478514
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 118 | Train Loss: 0.1544653 Vali Loss: 0.1024481 Test Loss: 0.1478515
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 118 | Train Loss: 0.1541660 Vali Loss: 0.1044281 Test Loss: 0.1478515
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 118 | Train Loss: 0.1546285 Vali Loss: 0.0996231 Test Loss: 0.1478515
Validation loss decreased (0.100588 --> 0.099623).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 118 | Train Loss: 0.1553314 Vali Loss: 0.0992426 Test Loss: 0.1478515
Validation loss decreased (0.099623 --> 0.099243).  Saving model ...
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 118 | Train Loss: 0.1542661 Vali Loss: 0.0994897 Test Loss: 0.1478515
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 118 | Train Loss: 0.1553098 Vali Loss: 0.1005986 Test Loss: 0.1478515
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 118 | Train Loss: 0.1539992 Vali Loss: 0.1030041 Test Loss: 0.1478515
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 118 | Train Loss: 0.1551948 Vali Loss: 0.0992108 Test Loss: 0.1478515
Validation loss decreased (0.099243 --> 0.099211).  Saving model ...
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 118 | Train Loss: 0.1549796 Vali Loss: 0.0998312 Test Loss: 0.1478515
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 118 | Train Loss: 0.1554296 Vali Loss: 0.0993174 Test Loss: 0.1478515
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 118 | Train Loss: 0.1557476 Vali Loss: 0.0986235 Test Loss: 0.1478515
Validation loss decreased (0.099211 --> 0.098624).  Saving model ...
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 118 | Train Loss: 0.1540541 Vali Loss: 0.1005857 Test Loss: 0.1478515
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 118 | Train Loss: 0.1544868 Vali Loss: 0.1002369 Test Loss: 0.1478515
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.1641532182693482e-14
Epoch: 35, Steps: 118 | Train Loss: 0.1542418 Vali Loss: 0.1037076 Test Loss: 0.1478515
EarlyStopping counter: 3 out of 10
Updating learning rate to 5.820766091346741e-15
Epoch: 36, Steps: 118 | Train Loss: 0.1549096 Vali Loss: 0.1014539 Test Loss: 0.1478515
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.9103830456733705e-15
Epoch: 37, Steps: 118 | Train Loss: 0.1554783 Vali Loss: 0.1043956 Test Loss: 0.1478515
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.4551915228366853e-15
Epoch: 38, Steps: 118 | Train Loss: 0.1545762 Vali Loss: 0.1014377 Test Loss: 0.1478515
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.275957614183426e-16
Epoch: 39, Steps: 118 | Train Loss: 0.1549596 Vali Loss: 0.1001677 Test Loss: 0.1478515
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.637978807091713e-16
Epoch: 40, Steps: 118 | Train Loss: 0.1548001 Vali Loss: 0.0998394 Test Loss: 0.1478515
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.8189894035458566e-16
Epoch: 41, Steps: 118 | Train Loss: 0.1536135 Vali Loss: 0.0994642 Test Loss: 0.1478515
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.094947017729283e-17
Epoch: 42, Steps: 118 | Train Loss: 0.1547535 Vali Loss: 0.1054988 Test Loss: 0.1478515
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_SASOL_H3_iTransformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 212
test shape: (212, 3, 1) (212, 3, 1)
test shape: (212, 3, 1) (212, 3, 1)


	mse:0.002330603776499629, mae:0.035854585468769073, rmse:0.048276327550411224, r2:-0.05792093276977539, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0359, RMSE: 0.0483, RÂ²: -0.0579, MAPE: 15490299.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.463 MB of 0.463 MB uploadedwandb: \ 0.463 MB of 0.463 MB uploadedwandb: | 0.463 MB of 0.463 MB uploadedwandb: / 0.463 MB of 0.463 MB uploadedwandb: - 0.463 MB of 0.463 MB uploadedwandb: \ 0.463 MB of 0.463 MB uploadedwandb: | 0.463 MB of 0.463 MB uploadedwandb: / 0.569 MB of 0.741 MB uploaded (0.002 MB deduped)wandb: - 0.569 MB of 0.741 MB uploaded (0.002 MB deduped)wandb: \ 0.741 MB of 0.741 MB uploaded (0.002 MB deduped)wandb: | 0.741 MB of 0.741 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ˆâ–„â–…â–†â–†â–ƒâ–…â–„â–ƒâ–…â–†â–…â–„â–ƒâ–…â–…â–…â–ˆâ–…â–‡â–‚â–‚â–‚â–ƒâ–…â–‚â–‚â–‚â–â–ƒâ–ƒâ–†â–„â–‡â–„â–ƒâ–‚â–‚â–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 41
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6338563
wandb:     model/trainable_params 6338563
wandb:                  test/loss 0.14785
wandb:                 train/loss 0.15475
wandb:   val/directional_accuracy 47.64151
wandb:                   val/loss 0.1055
wandb:                    val/mae 0.03585
wandb:                   val/mape 1549029900.0
wandb:                    val/mse 0.00233
wandb:                     val/r2 -0.05792
wandb:                   val/rmse 0.04828
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/k7rmp317
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_100728-k7rmp317/logs
Completed: SASOL H=3

Training: iTransformer on SASOL for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_101013-xmtxq6ga
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/xmtxq6ga
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_SASOL_H5Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/xmtxq6ga
>>>>>>>start training : long_term_forecast_iTransformer_SASOL_H5_iTransformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 5
============================================================
train 3770
val 209
test 210
Epoch: 1, Steps: 118 | Train Loss: 0.2387393 Vali Loss: 0.1119464 Test Loss: 0.1552405
Validation loss decreased (inf --> 0.111946).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2115171 Vali Loss: 0.1076746 Test Loss: 0.1558590
Validation loss decreased (0.111946 --> 0.107675).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 118 | Train Loss: 0.1913899 Vali Loss: 0.1040876 Test Loss: 0.1510100
Validation loss decreased (0.107675 --> 0.104088).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.1796160 Vali Loss: 0.1077094 Test Loss: 0.1491413
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2387392645922758, 'val/loss': 0.11194640930209841, 'test/loss': 0.15524046548775264, '_timestamp': 1762330224.3303683}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.21151706241702628, 'val/loss': 0.1076746370111193, 'test/loss': 0.15585898182221822, '_timestamp': 1762330227.3846653}).
Epoch: 5, Steps: 118 | Train Loss: 0.1722156 Vali Loss: 0.1019826 Test Loss: 0.1481557
Validation loss decreased (0.104088 --> 0.101983).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.1683377 Vali Loss: 0.1086390 Test Loss: 0.1483473
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.1673153 Vali Loss: 0.1040106 Test Loss: 0.1483781
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.1658482 Vali Loss: 0.1022358 Test Loss: 0.1485119
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.1661056 Vali Loss: 0.1050287 Test Loss: 0.1485531
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.1653409 Vali Loss: 0.1018583 Test Loss: 0.1485700
Validation loss decreased (0.101983 --> 0.101858).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.1660102 Vali Loss: 0.1029186 Test Loss: 0.1485766
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.1654687 Vali Loss: 0.1057750 Test Loss: 0.1485748
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.1653587 Vali Loss: 0.1063690 Test Loss: 0.1485764
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.1655077 Vali Loss: 0.1045272 Test Loss: 0.1485795
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.1651701 Vali Loss: 0.1031518 Test Loss: 0.1485799
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.1656298 Vali Loss: 0.1062843 Test Loss: 0.1485809
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.1653510 Vali Loss: 0.1034363 Test Loss: 0.1485808
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.1659555 Vali Loss: 0.1076962 Test Loss: 0.1485810
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.1652144 Vali Loss: 0.1024832 Test Loss: 0.1485809
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.1655004 Vali Loss: 0.1033937 Test Loss: 0.1485810
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_SASOL_H5_iTransformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 210
test shape: (210, 5, 1) (210, 5, 1)
test shape: (210, 5, 1) (210, 5, 1)


	mse:0.002325142966583371, mae:0.03578411415219307, rmse:0.04821973666548729, r2:-0.047644853591918945, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0358, RMSE: 0.0482, RÂ²: -0.0476, MAPE: 15031749.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.487 MB of 0.487 MB uploadedwandb: \ 0.487 MB of 0.487 MB uploadedwandb: | 0.487 MB of 0.487 MB uploadedwandb: / 0.487 MB of 0.656 MB uploadedwandb: - 0.656 MB of 0.656 MB uploadedwandb: \ 0.656 MB of 0.656 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–‡â–â–ˆâ–ƒâ–â–„â–â–‚â–…â–†â–„â–‚â–†â–ƒâ–‡â–‚â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6339589
wandb:     model/trainable_params 6339589
wandb:                  test/loss 0.14858
wandb:                 train/loss 0.1655
wandb:   val/directional_accuracy 47.38095
wandb:                   val/loss 0.10339
wandb:                    val/mae 0.03578
wandb:                   val/mape 1503174900.0
wandb:                    val/mse 0.00233
wandb:                     val/r2 -0.04764
wandb:                   val/rmse 0.04822
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/xmtxq6ga
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_101013-xmtxq6ga/logs
Completed: SASOL H=5

Training: iTransformer on SASOL for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_101145-u5xj2l3e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/u5xj2l3e
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_SASOL_H10Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/u5xj2l3e
>>>>>>>start training : long_term_forecast_iTransformer_SASOL_H10_iTransformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 10
============================================================
train 3765
val 204
test 205
Epoch: 1, Steps: 118 | Train Loss: 0.2385451 Vali Loss: 0.1129075 Test Loss: 0.1606857
Validation loss decreased (inf --> 0.112907).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2146439 Vali Loss: 0.1094712 Test Loss: 0.1535176
Validation loss decreased (0.112907 --> 0.109471).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 118 | Train Loss: 0.2007973 Vali Loss: 0.1077777 Test Loss: 0.1497841
Validation loss decreased (0.109471 --> 0.107778).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.1908484 Vali Loss: 0.1067605 Test Loss: 0.1494005
Validation loss decreased (0.107778 --> 0.106760).  Saving model ...
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23854506950257187, 'val/loss': 0.11290749268872398, 'test/loss': 0.16068568612848008, '_timestamp': 1762330313.134333}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2146439178515289, 'val/loss': 0.10947115932192121, 'test/loss': 0.15351755172014236, '_timestamp': 1762330316.1952107}).
Epoch: 5, Steps: 118 | Train Loss: 0.1846386 Vali Loss: 0.1100280 Test Loss: 0.1492433
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.1820460 Vali Loss: 0.1082540 Test Loss: 0.1492437
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.1813421 Vali Loss: 0.1046067 Test Loss: 0.1492918
Validation loss decreased (0.106760 --> 0.104607).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.1806287 Vali Loss: 0.1075503 Test Loss: 0.1492226
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.1797123 Vali Loss: 0.1071650 Test Loss: 0.1492247
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.1799995 Vali Loss: 0.1064199 Test Loss: 0.1492344
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.1798486 Vali Loss: 0.1107082 Test Loss: 0.1492440
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.1792987 Vali Loss: 0.1080921 Test Loss: 0.1492464
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.1794057 Vali Loss: 0.1080131 Test Loss: 0.1492487
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.1798560 Vali Loss: 0.1073649 Test Loss: 0.1492496
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.1795094 Vali Loss: 0.1091347 Test Loss: 0.1492497
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.1793068 Vali Loss: 0.1052734 Test Loss: 0.1492495
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.1799782 Vali Loss: 0.1065786 Test Loss: 0.1492496
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_SASOL_H10_iTransformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 205
test shape: (205, 10, 1) (205, 10, 1)
test shape: (205, 10, 1) (205, 10, 1)


	mse:0.0023102709092199802, mae:0.035456154495477676, rmse:0.04806527867913246, r2:-0.040802836418151855, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0355, RMSE: 0.0481, RÂ²: -0.0408, MAPE: 12688446.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.561 MB of 0.561 MB uploadedwandb: \ 0.561 MB of 0.561 MB uploadedwandb: | 0.561 MB of 0.561 MB uploadedwandb: / 0.561 MB of 0.730 MB uploadedwandb: - 0.730 MB of 0.730 MB uploadedwandb: \ 0.730 MB of 0.730 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–ƒâ–‡â–…â–â–„â–„â–ƒâ–ˆâ–…â–…â–„â–†â–‚â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6342154
wandb:     model/trainable_params 6342154
wandb:                  test/loss 0.14925
wandb:                 train/loss 0.17998
wandb:   val/directional_accuracy 47.75068
wandb:                   val/loss 0.10658
wandb:                    val/mae 0.03546
wandb:                   val/mape 1268844600.0
wandb:                    val/mse 0.00231
wandb:                     val/r2 -0.0408
wandb:                   val/rmse 0.04807
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/u5xj2l3e
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_101145-u5xj2l3e/logs
Completed: SASOL H=10

Training: iTransformer on SASOL for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_101303-re0xpson
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/re0xpson
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_SASOL_H22Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/re0xpson
>>>>>>>start training : long_term_forecast_iTransformer_SASOL_H22_iTransformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 22
============================================================
train 3753
val 192
test 193
Epoch: 1, Steps: 118 | Train Loss: 0.2526430 Vali Loss: 0.1141649 Test Loss: 0.1591609
Validation loss decreased (inf --> 0.114165).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2290213 Vali Loss: 0.1119657 Test Loss: 0.1606230
Validation loss decreased (0.114165 --> 0.111966).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 118 | Train Loss: 0.2166729 Vali Loss: 0.1102677 Test Loss: 0.1559536
Validation loss decreased (0.111966 --> 0.110268).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2075946 Vali Loss: 0.1097263 Test Loss: 0.1556478
Validation loss decreased (0.110268 --> 0.109726).  Saving model ...
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.25264298448623235, 'val/loss': 0.11416491369406383, 'test/loss': 0.15916085562535695, '_timestamp': 1762330391.1992564}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.22902125865221024, 'val/loss': 0.11196567366520564, 'test/loss': 0.16062295543295996, '_timestamp': 1762330394.2407637}).
Epoch: 5, Steps: 118 | Train Loss: 0.2034688 Vali Loss: 0.1096513 Test Loss: 0.1547134
Validation loss decreased (0.109726 --> 0.109651).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2016450 Vali Loss: 0.1096235 Test Loss: 0.1544498
Validation loss decreased (0.109651 --> 0.109624).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2008400 Vali Loss: 0.1094795 Test Loss: 0.1543661
Validation loss decreased (0.109624 --> 0.109480).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2008895 Vali Loss: 0.1094955 Test Loss: 0.1543250
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.1999723 Vali Loss: 0.1094751 Test Loss: 0.1542981
Validation loss decreased (0.109480 --> 0.109475).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.1998806 Vali Loss: 0.1094882 Test Loss: 0.1543026
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.1999884 Vali Loss: 0.1094876 Test Loss: 0.1542995
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.1991766 Vali Loss: 0.1094919 Test Loss: 0.1543007
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.1988949 Vali Loss: 0.1094933 Test Loss: 0.1542985
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.1997538 Vali Loss: 0.1094934 Test Loss: 0.1543000
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.1995221 Vali Loss: 0.1094938 Test Loss: 0.1543003
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.1992611 Vali Loss: 0.1094940 Test Loss: 0.1543000
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.1999857 Vali Loss: 0.1094940 Test Loss: 0.1543001
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.1994305 Vali Loss: 0.1094940 Test Loss: 0.1543001
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.1993205 Vali Loss: 0.1094940 Test Loss: 0.1543001
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_SASOL_H22_iTransformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 193
test shape: (193, 22, 1) (193, 22, 1)
test shape: (193, 22, 1) (193, 22, 1)


	mse:0.002334543038159609, mae:0.0353444442152977, rmse:0.04831710830330849, r2:-0.04028964042663574, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0353, RMSE: 0.0483, RÂ²: -0.0403, MAPE: 12399792.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.624 MB of 0.625 MB uploadedwandb: \ 0.624 MB of 0.625 MB uploadedwandb: | 0.624 MB of 0.625 MB uploadedwandb: / 0.625 MB of 0.625 MB uploadedwandb: - 0.625 MB of 0.625 MB uploadedwandb: \ 0.625 MB of 0.794 MB uploadedwandb: | 0.794 MB of 0.794 MB uploadedwandb: / 0.794 MB of 0.794 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‡â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 18
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6348310
wandb:     model/trainable_params 6348310
wandb:                  test/loss 0.1543
wandb:                 train/loss 0.19932
wandb:   val/directional_accuracy 47.59437
wandb:                   val/loss 0.10949
wandb:                    val/mae 0.03534
wandb:                   val/mape 1239979200.0
wandb:                    val/mse 0.00233
wandb:                     val/r2 -0.04029
wandb:                   val/rmse 0.04832
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/re0xpson
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_101303-re0xpson/logs
Exception in thread IntMsgThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 300, in check_internal_messages
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 268, in check_network_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 856, in deliver_internal_messages
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 848, in deliver_network_status
    return self._deliver_internal_messages(internal_message)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 516, in _deliver_internal_messages
    return self._deliver_network_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 510, in _deliver_network_status
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: SASOL H=22

Training: iTransformer on SASOL for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_101437-90za70p9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/90za70p9
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_SASOL_H50Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/90za70p9
>>>>>>>start training : long_term_forecast_iTransformer_SASOL_H50_iTransformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 50
============================================================
train 3725
val 164
test 165
Epoch: 1, Steps: 117 | Train Loss: 0.2829805 Vali Loss: 0.1160190 Test Loss: 0.1697444
Validation loss decreased (inf --> 0.116019).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 117 | Train Loss: 0.2588377 Vali Loss: 0.1097884 Test Loss: 0.1699439
Validation loss decreased (0.116019 --> 0.109788).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 117 | Train Loss: 0.2466740 Vali Loss: 0.1063245 Test Loss: 0.1689936
Validation loss decreased (0.109788 --> 0.106324).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 117 | Train Loss: 0.2405713 Vali Loss: 0.1095735 Test Loss: 0.1694542
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2829805013970432, 'val/loss': 0.11601899688442548, 'test/loss': 0.16974442700544992, '_timestamp': 1762330486.0191283}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2588377038383076, 'val/loss': 0.10978840912381808, 'test/loss': 0.1699438914656639, '_timestamp': 1762330489.1091835}).
Epoch: 5, Steps: 117 | Train Loss: 0.2371516 Vali Loss: 0.1097161 Test Loss: 0.1676134
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 117 | Train Loss: 0.2361689 Vali Loss: 0.1073684 Test Loss: 0.1685863
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 117 | Train Loss: 0.2350197 Vali Loss: 0.1071590 Test Loss: 0.1686429
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 117 | Train Loss: 0.2350576 Vali Loss: 0.1100740 Test Loss: 0.1686554
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 117 | Train Loss: 0.2341370 Vali Loss: 0.1068894 Test Loss: 0.1686393
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 117 | Train Loss: 0.2342540 Vali Loss: 0.1084737 Test Loss: 0.1686365
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 117 | Train Loss: 0.2342167 Vali Loss: 0.1110756 Test Loss: 0.1686481
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 117 | Train Loss: 0.2340401 Vali Loss: 0.1076363 Test Loss: 0.1686571
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 117 | Train Loss: 0.2347827 Vali Loss: 0.1092572 Test Loss: 0.1686539
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_SASOL_H50_iTransformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 165
test shape: (165, 50, 1) (165, 50, 1)
test shape: (165, 50, 1) (165, 50, 1)


	mse:0.002129900036379695, mae:0.03409022465348244, rmse:0.04615084081888199, r2:-0.03960311412811279, dtw:Not calculated


VAL - MSE: 0.0021, MAE: 0.0341, RMSE: 0.0462, RÂ²: -0.0396, MAPE: 12761763.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.646 MB of 0.648 MB uploadedwandb: \ 0.646 MB of 0.648 MB uploadedwandb: | 0.646 MB of 0.648 MB uploadedwandb: / 0.648 MB of 0.648 MB uploadedwandb: - 0.648 MB of 0.648 MB uploadedwandb: \ 0.648 MB of 0.816 MB uploadedwandb: | 0.816 MB of 0.816 MB uploadedwandb: / 0.816 MB of 0.816 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–†â–ˆâ–â–…â–…â–…â–…â–…â–…â–…â–…
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–†â–†â–ƒâ–‚â–‡â–‚â–„â–ˆâ–ƒâ–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6362674
wandb:     model/trainable_params 6362674
wandb:                  test/loss 0.16865
wandb:                 train/loss 0.23478
wandb:   val/directional_accuracy 48.55906
wandb:                   val/loss 0.10926
wandb:                    val/mae 0.03409
wandb:                   val/mape 1276176300.0
wandb:                    val/mse 0.00213
wandb:                     val/r2 -0.0396
wandb:                   val/rmse 0.04615
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/90za70p9
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_101437-90za70p9/logs
Completed: SASOL H=50

Training: iTransformer on SASOL for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_101545-0be9butk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/0be9butk
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_SASOL_H100Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/0be9butk
>>>>>>>start training : long_term_forecast_iTransformer_SASOL_H100_iTransformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 100
============================================================
train 3675
val 114
test 115
Epoch: 1, Steps: 115 | Train Loss: 0.3464702 Vali Loss: 0.1294734 Test Loss: 0.1770292
Validation loss decreased (inf --> 0.129473).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 115 | Train Loss: 0.3162818 Vali Loss: 0.1251334 Test Loss: 0.1764686
Validation loss decreased (0.129473 --> 0.125133).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 115 | Train Loss: 0.3053067 Vali Loss: 0.1226277 Test Loss: 0.1768563
Validation loss decreased (0.125133 --> 0.122628).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 115 | Train Loss: 0.3000879 Vali Loss: 0.1236607 Test Loss: 0.1757628
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 115 | Train Loss: 0.2970147 Vali Loss: 0.1240225 Test Loss: 0.1760582
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.34647021475045575, 'val/loss': 0.12947341427206993, 'test/loss': 0.17702916637063026, '_timestamp': 1762330553.4121304}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3162817945946818, 'val/loss': 0.12513336539268494, 'test/loss': 0.17646856606006622, '_timestamp': 1762330556.3970702}).
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.34647021475045575, 'val/loss': 0.12947341427206993, 'test/loss': 0.17702916637063026, '_timestamp': 1762330553.4121304}).
Epoch: 6, Steps: 115 | Train Loss: 0.2954656 Vali Loss: 0.1230893 Test Loss: 0.1764698
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 115 | Train Loss: 0.2945747 Vali Loss: 0.1234754 Test Loss: 0.1767054
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 115 | Train Loss: 0.2942399 Vali Loss: 0.1238371 Test Loss: 0.1766247
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 115 | Train Loss: 0.2937497 Vali Loss: 0.1229707 Test Loss: 0.1766647
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 115 | Train Loss: 0.2940803 Vali Loss: 0.1235446 Test Loss: 0.1766753
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 115 | Train Loss: 0.2939799 Vali Loss: 0.1237579 Test Loss: 0.1766943
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 115 | Train Loss: 0.2937793 Vali Loss: 0.1230756 Test Loss: 0.1766984
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 115 | Train Loss: 0.2938310 Vali Loss: 0.1220144 Test Loss: 0.1766984
Validation loss decreased (0.122628 --> 0.122014).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 115 | Train Loss: 0.2938776 Vali Loss: 0.1216415 Test Loss: 0.1766991
Validation loss decreased (0.122014 --> 0.121642).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 115 | Train Loss: 0.2937057 Vali Loss: 0.1234347 Test Loss: 0.1766998
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 115 | Train Loss: 0.2940083 Vali Loss: 0.1219867 Test Loss: 0.1766998
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 115 | Train Loss: 0.2940516 Vali Loss: 0.1220761 Test Loss: 0.1766998
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 115 | Train Loss: 0.2936666 Vali Loss: 0.1212308 Test Loss: 0.1766999
Validation loss decreased (0.121642 --> 0.121231).  Saving model ...
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 115 | Train Loss: 0.2938791 Vali Loss: 0.1239866 Test Loss: 0.1766999
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 115 | Train Loss: 0.2938384 Vali Loss: 0.1236837 Test Loss: 0.1766999
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 115 | Train Loss: 0.2938080 Vali Loss: 0.1229953 Test Loss: 0.1766999
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 115 | Train Loss: 0.2939797 Vali Loss: 0.1228293 Test Loss: 0.1766999
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 115 | Train Loss: 0.2938145 Vali Loss: 0.1216905 Test Loss: 0.1766999
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 115 | Train Loss: 0.2938819 Vali Loss: 0.1228156 Test Loss: 0.1766999
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 115 | Train Loss: 0.2938276 Vali Loss: 0.1227371 Test Loss: 0.1766999
EarlyStopping counter: 7 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 115 | Train Loss: 0.2941461 Vali Loss: 0.1229239 Test Loss: 0.1766999
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 115 | Train Loss: 0.2941114 Vali Loss: 0.1222109 Test Loss: 0.1766999
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 115 | Train Loss: 0.2938693 Vali Loss: 0.1220911 Test Loss: 0.1766999
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_SASOL_H100_iTransformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 115
test shape: (115, 100, 1) (115, 100, 1)
test shape: (115, 100, 1) (115, 100, 1)


	mse:0.0020208489149808884, mae:0.0333939827978611, rmse:0.044953852891922, r2:-0.01835775375366211, dtw:Not calculated


VAL - MSE: 0.0020, MAE: 0.0334, RMSE: 0.0450, RÂ²: -0.0184, MAPE: 11200290.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.669 MB of 0.674 MB uploadedwandb: \ 0.674 MB of 0.674 MB uploadedwandb: | 0.674 MB of 0.674 MB uploadedwandb: / 0.674 MB of 0.845 MB uploadedwandb: - 0.845 MB of 0.845 MB uploadedwandb: \ 0.845 MB of 0.845 MB uploadedwandb: | 0.845 MB of 0.845 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–ƒâ–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–‡â–ˆâ–†â–‡â–ˆâ–…â–‡â–‡â–†â–ƒâ–‚â–‡â–ƒâ–ƒâ–â–ˆâ–‡â–…â–…â–‚â–…â–…â–…â–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 27
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6388324
wandb:     model/trainable_params 6388324
wandb:                  test/loss 0.1767
wandb:                 train/loss 0.29387
wandb:   val/directional_accuracy 49.10848
wandb:                   val/loss 0.12209
wandb:                    val/mae 0.03339
wandb:                   val/mape 1120029000.0
wandb:                    val/mse 0.00202
wandb:                     val/r2 -0.01836
wandb:                   val/rmse 0.04495
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/0be9butk
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_101545-0be9butk/logs
Completed: SASOL H=100

iTransformer training completed for all datasets!
