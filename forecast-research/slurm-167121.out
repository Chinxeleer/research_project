##############################################################################
# Training Autoformer Model on All Datasets
##############################################################################
Training: Autoformer on NVIDIA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_143007-331eqhj1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/331eqhj1
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NVIDIA_H3Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/331eqhj1
>>>>>>>start training : long_term_forecast_Autoformer_NVIDIA_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.3041070 Vali Loss: 0.1951895 Test Loss: 0.3391317
Validation loss decreased (inf --> 0.195189).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3041069768649295, 'val/loss': 0.19518946390599012, 'test/loss': 0.3391316644847393, '_timestamp': 1762777822.3676739}).
Epoch: 2, Steps: 133 | Train Loss: 0.2598917 Vali Loss: 0.1839015 Test Loss: 0.3173382
Validation loss decreased (0.195189 --> 0.183902).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2463120 Vali Loss: 0.1819031 Test Loss: 0.3134973
Validation loss decreased (0.183902 --> 0.181903).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2413276 Vali Loss: 0.1737629 Test Loss: 0.3023002
Validation loss decreased (0.181903 --> 0.173763).  Saving model ...
Updating learning rate to 1.25e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25989169229690295, 'val/loss': 0.18390154838562012, 'test/loss': 0.31733815651386976, '_timestamp': 1762777829.3345895}).
Epoch: 5, Steps: 133 | Train Loss: 0.2367137 Vali Loss: 0.1961618 Test Loss: 0.3029124
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2342389 Vali Loss: 0.2012539 Test Loss: 0.3023744
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2330156 Vali Loss: 0.1710258 Test Loss: 0.3028661
Validation loss decreased (0.173763 --> 0.171026).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2334555 Vali Loss: 0.1762559 Test Loss: 0.3022568
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2331360 Vali Loss: 0.1805576 Test Loss: 0.3019227
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2331233 Vali Loss: 0.1727740 Test Loss: 0.3017954
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2330313 Vali Loss: 0.1782215 Test Loss: 0.3018191
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2325103 Vali Loss: 0.1745144 Test Loss: 0.3018085
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2325671 Vali Loss: 0.1747719 Test Loss: 0.3018570
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2326104 Vali Loss: 0.1746237 Test Loss: 0.3018614
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2324174 Vali Loss: 0.1861602 Test Loss: 0.3018628
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2321510 Vali Loss: 0.1781653 Test Loss: 0.3018631
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2315436 Vali Loss: 0.1939243 Test Loss: 0.3018639
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NVIDIA_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.0011538239195942879, mae:0.025427406653761864, rmse:0.03396798297762871, r2:-0.028559207916259766, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0254, RMSE: 0.0340, RÂ²: -0.0286, MAPE: 758538.44%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.465 MB of 0.465 MB uploadedwandb: \ 0.465 MB of 0.465 MB uploadedwandb: | 0.465 MB of 0.465 MB uploadedwandb: / 0.465 MB of 0.465 MB uploadedwandb: - 0.465 MB of 0.465 MB uploadedwandb: \ 0.465 MB of 0.465 MB uploadedwandb: | 0.465 MB of 0.465 MB uploadedwandb: / 0.465 MB of 0.465 MB uploadedwandb: - 0.596 MB of 0.812 MB uploaded (0.002 MB deduped)wandb: \ 0.812 MB of 0.812 MB uploaded (0.002 MB deduped)wandb: | 0.812 MB of 0.812 MB uploaded (0.002 MB deduped)wandb: / 0.812 MB of 0.812 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–†â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–‚â–‡â–ˆâ–â–‚â–ƒâ–â–ƒâ–‚â–‚â–‚â–…â–ƒâ–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.30186
wandb:                 train/loss 0.23154
wandb:   val/directional_accuracy 50.42194
wandb:                   val/loss 0.19392
wandb:                    val/mae 0.02543
wandb:                   val/mape 75853843.75
wandb:                    val/mse 0.00115
wandb:                     val/r2 -0.02856
wandb:                   val/rmse 0.03397
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/331eqhj1
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_143007-331eqhj1/logs
Completed: NVIDIA H=3

Training: Autoformer on NVIDIA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_143239-zdxyxe38
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/zdxyxe38
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NVIDIA_H5Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/zdxyxe38
>>>>>>>start training : long_term_forecast_Autoformer_NVIDIA_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.2974592 Vali Loss: 0.2024044 Test Loss: 0.3360844
Validation loss decreased (inf --> 0.202404).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2597451 Vali Loss: 0.1822921 Test Loss: 0.3363513
Validation loss decreased (0.202404 --> 0.182292).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29745923464459584, 'val/loss': 0.20240439474582672, 'test/loss': 0.33608442079275846, '_timestamp': 1762777972.8055673}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2597451431858808, 'val/loss': 0.18229206837713718, 'test/loss': 0.3363513210788369, '_timestamp': 1762777979.9817626}).
Epoch: 3, Steps: 133 | Train Loss: 0.2446362 Vali Loss: 0.1813648 Test Loss: 0.3374882
Validation loss decreased (0.182292 --> 0.181365).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2399571 Vali Loss: 0.1811198 Test Loss: 0.3281515
Validation loss decreased (0.181365 --> 0.181120).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2363239 Vali Loss: 0.1787973 Test Loss: 0.3293593
Validation loss decreased (0.181120 --> 0.178797).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2356609 Vali Loss: 0.1788929 Test Loss: 0.3284677
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2357084 Vali Loss: 0.1802036 Test Loss: 0.3267726
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2350292 Vali Loss: 0.1795145 Test Loss: 0.3272770
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2343159 Vali Loss: 0.1803956 Test Loss: 0.3264963
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2347257 Vali Loss: 0.1932401 Test Loss: 0.3266915
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2355363 Vali Loss: 0.1763412 Test Loss: 0.3266756
Validation loss decreased (0.178797 --> 0.176341).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2347314 Vali Loss: 0.1809423 Test Loss: 0.3267338
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2340970 Vali Loss: 0.2010192 Test Loss: 0.3267137
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2343192 Vali Loss: 0.1951384 Test Loss: 0.3266807
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2347224 Vali Loss: 0.1942659 Test Loss: 0.3266751
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2346298 Vali Loss: 0.1930581 Test Loss: 0.3266741
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2341678 Vali Loss: 0.1865919 Test Loss: 0.3266745
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2349160 Vali Loss: 0.1950130 Test Loss: 0.3266743
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2339308 Vali Loss: 0.1860513 Test Loss: 0.3266742
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2347028 Vali Loss: 0.1798961 Test Loss: 0.3266738
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2347149 Vali Loss: 0.1791022 Test Loss: 0.3266744
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NVIDIA_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.0011718100868165493, mae:0.025821805000305176, rmse:0.03423171117901802, r2:-0.03784489631652832, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0258, RMSE: 0.0342, RÂ²: -0.0378, MAPE: 847163.38%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.523 MB of 0.524 MB uploadedwandb: \ 0.523 MB of 0.524 MB uploadedwandb: | 0.523 MB of 0.524 MB uploadedwandb: / 0.524 MB of 0.524 MB uploadedwandb: - 0.524 MB of 0.524 MB uploadedwandb: \ 0.524 MB of 0.740 MB uploadedwandb: | 0.740 MB of 0.740 MB uploadedwandb: / 0.740 MB of 0.740 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–‚â–â–â–‚â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–†â–â–‚â–ˆâ–†â–†â–†â–„â–†â–„â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.32667
wandb:                 train/loss 0.23471
wandb:   val/directional_accuracy 46.2766
wandb:                   val/loss 0.1791
wandb:                    val/mae 0.02582
wandb:                   val/mape 84716337.5
wandb:                    val/mse 0.00117
wandb:                     val/r2 -0.03784
wandb:                   val/rmse 0.03423
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/zdxyxe38
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_143239-zdxyxe38/logs
Completed: NVIDIA H=5

Training: Autoformer on NVIDIA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_143533-pzbvbje9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/pzbvbje9
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NVIDIA_H10Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/pzbvbje9
>>>>>>>start training : long_term_forecast_Autoformer_NVIDIA_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2896230 Vali Loss: 0.1963091 Test Loss: 0.3669248
Validation loss decreased (inf --> 0.196309).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2540234 Vali Loss: 0.1866751 Test Loss: 0.3626031
Validation loss decreased (0.196309 --> 0.186675).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.28962302756936925, 'val/loss': 0.19630907848477364, 'test/loss': 0.3669248363003135, '_timestamp': 1762778145.975765}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2540234079710523, 'val/loss': 0.18667514249682426, 'test/loss': 0.3626030804589391, '_timestamp': 1762778152.9827826}).
Epoch: 3, Steps: 133 | Train Loss: 0.2442917 Vali Loss: 0.1822685 Test Loss: 0.3537407
Validation loss decreased (0.186675 --> 0.182268).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2401210 Vali Loss: 0.2067090 Test Loss: 0.3566712
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2386945 Vali Loss: 0.1932483 Test Loss: 0.3503198
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2368202 Vali Loss: 0.1749866 Test Loss: 0.3508429
Validation loss decreased (0.182268 --> 0.174987).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2367679 Vali Loss: 0.1834312 Test Loss: 0.3519870
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2367967 Vali Loss: 0.1839780 Test Loss: 0.3518184
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2373430 Vali Loss: 0.1813322 Test Loss: 0.3516780
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2369455 Vali Loss: 0.1947003 Test Loss: 0.3516387
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2364549 Vali Loss: 0.1895609 Test Loss: 0.3515368
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2363086 Vali Loss: 0.1744950 Test Loss: 0.3516046
Validation loss decreased (0.174987 --> 0.174495).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2366865 Vali Loss: 0.2007325 Test Loss: 0.3516526
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2367198 Vali Loss: 0.1854510 Test Loss: 0.3516725
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2355114 Vali Loss: 0.1919923 Test Loss: 0.3516554
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2363967 Vali Loss: 0.1758718 Test Loss: 0.3516590
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2378136 Vali Loss: 0.2138817 Test Loss: 0.3516582
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2364294 Vali Loss: 0.1738453 Test Loss: 0.3516580
Validation loss decreased (0.174495 --> 0.173845).  Saving model ...
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2358574 Vali Loss: 0.1812159 Test Loss: 0.3516578
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2366780 Vali Loss: 0.1781487 Test Loss: 0.3516580
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2366017 Vali Loss: 0.1788267 Test Loss: 0.3516584
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2364550 Vali Loss: 0.2098390 Test Loss: 0.3516582
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2375230 Vali Loss: 0.1802568 Test Loss: 0.3516582
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2366643 Vali Loss: 0.1759708 Test Loss: 0.3516578
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2359216 Vali Loss: 0.2008035 Test Loss: 0.3516579
EarlyStopping counter: 7 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2361975 Vali Loss: 0.1755930 Test Loss: 0.3516583
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.2356255 Vali Loss: 0.1751821 Test Loss: 0.3516583
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.2360424 Vali Loss: 0.1789473 Test Loss: 0.3516581
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NVIDIA_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.0011730206897482276, mae:0.02587970532476902, rmse:0.03424939140677452, r2:-0.022574305534362793, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0259, RMSE: 0.0342, RÂ²: -0.0226, MAPE: 807556.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.563 MB of 0.564 MB uploadedwandb: \ 0.563 MB of 0.564 MB uploadedwandb: | 0.563 MB of 0.564 MB uploadedwandb: / 0.564 MB of 0.564 MB uploadedwandb: - 0.564 MB of 0.564 MB uploadedwandb: \ 0.564 MB of 0.782 MB uploadedwandb: | 0.782 MB of 0.782 MB uploadedwandb: / 0.782 MB of 0.782 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–ˆâ–â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–ƒâ–‚â–â–‚â–‚â–‚â–ƒâ–‚â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–‡â–„â–â–ƒâ–ƒâ–‚â–…â–„â–â–†â–ƒâ–„â–â–ˆâ–â–‚â–‚â–‚â–‡â–‚â–â–†â–â–â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 27
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.35166
wandb:                 train/loss 0.23604
wandb:   val/directional_accuracy 51.54589
wandb:                   val/loss 0.17895
wandb:                    val/mae 0.02588
wandb:                   val/mape 80755600.0
wandb:                    val/mse 0.00117
wandb:                     val/r2 -0.02257
wandb:                   val/rmse 0.03425
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/pzbvbje9
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_143533-pzbvbje9/logs
Completed: NVIDIA H=10

Training: Autoformer on NVIDIA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_143909-mw4js6xg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/mw4js6xg
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NVIDIA_H22Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/mw4js6xg
>>>>>>>start training : long_term_forecast_Autoformer_NVIDIA_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2827455 Vali Loss: 0.1948409 Test Loss: 0.4371992
Validation loss decreased (inf --> 0.194841).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2527918 Vali Loss: 0.1893269 Test Loss: 0.4284905
Validation loss decreased (0.194841 --> 0.189327).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2827454677811175, 'val/loss': 0.19484093359538487, 'test/loss': 0.43719917110034395, '_timestamp': 1762778362.6485868}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25279179565382726, 'val/loss': 0.18932691003595079, 'test/loss': 0.428490549325943, '_timestamp': 1762778369.9267676}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25279179565382726, 'val/loss': 0.18932691003595079, 'test/loss': 0.428490549325943, '_timestamp': 1762778369.9267676}).
Epoch: 3, Steps: 132 | Train Loss: 0.2457310 Vali Loss: 0.1878524 Test Loss: 0.4445080
Validation loss decreased (0.189327 --> 0.187852).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2425277 Vali Loss: 0.1880129 Test Loss: 0.4251001
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2405563 Vali Loss: 0.1872824 Test Loss: 0.4299311
Validation loss decreased (0.187852 --> 0.187282).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2395878 Vali Loss: 0.1907556 Test Loss: 0.4309899
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2392992 Vali Loss: 0.1886403 Test Loss: 0.4278645
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2388750 Vali Loss: 0.1892554 Test Loss: 0.4284467
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2387166 Vali Loss: 0.1896418 Test Loss: 0.4280360
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2386739 Vali Loss: 0.1892917 Test Loss: 0.4284201
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2385730 Vali Loss: 0.1890868 Test Loss: 0.4284077
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2383602 Vali Loss: 0.1879371 Test Loss: 0.4284988
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2383466 Vali Loss: 0.1888275 Test Loss: 0.4285213
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2384372 Vali Loss: 0.1879498 Test Loss: 0.4285280
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2385487 Vali Loss: 0.1899586 Test Loss: 0.4285330
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NVIDIA_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.0012169756228104234, mae:0.026283761486411095, rmse:0.03488517925143242, r2:-0.031461477279663086, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0263, RMSE: 0.0349, RÂ²: -0.0315, MAPE: 565375.06%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.586 MB of 0.587 MB uploadedwandb: \ 0.586 MB of 0.587 MB uploadedwandb: | 0.586 MB of 0.587 MB uploadedwandb: / 0.587 MB of 0.587 MB uploadedwandb: - 0.587 MB of 0.587 MB uploadedwandb: \ 0.587 MB of 0.803 MB uploadedwandb: | 0.803 MB of 0.803 MB uploadedwandb: / 0.803 MB of 0.803 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–‚â–â–ˆâ–„â–…â–†â–…â–…â–‚â–„â–‚â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.42853
wandb:                 train/loss 0.23855
wandb:   val/directional_accuracy 50.37134
wandb:                   val/loss 0.18996
wandb:                    val/mae 0.02628
wandb:                   val/mape 56537506.25
wandb:                    val/mse 0.00122
wandb:                     val/r2 -0.03146
wandb:                   val/rmse 0.03489
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/mw4js6xg
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_143909-mw4js6xg/logs
Completed: NVIDIA H=22

Training: Autoformer on NVIDIA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_144127-4vnhlmzo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/4vnhlmzo
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NVIDIA_H50Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/4vnhlmzo
>>>>>>>start training : long_term_forecast_Autoformer_NVIDIA_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.2879563 Vali Loss: 0.2083407 Test Loss: 0.5586434
Validation loss decreased (inf --> 0.208341).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2601156 Vali Loss: 0.2106596 Test Loss: 0.5675399
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.28795625258124236, 'val/loss': 0.2083407317598661, 'test/loss': 0.5586433857679367, '_timestamp': 1762778500.2478068}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26011557244893274, 'val/loss': 0.2106595958272616, 'test/loss': 0.5675398682554563, '_timestamp': 1762778507.213881}).
Epoch: 3, Steps: 132 | Train Loss: 0.2536864 Vali Loss: 0.2035299 Test Loss: 0.5438888
Validation loss decreased (0.208341 --> 0.203530).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2513566 Vali Loss: 0.2055419 Test Loss: 0.5532464
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2498254 Vali Loss: 0.2061165 Test Loss: 0.5373499
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2483722 Vali Loss: 0.2064391 Test Loss: 0.5350789
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2477931 Vali Loss: 0.2064938 Test Loss: 0.5357929
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2475573 Vali Loss: 0.2075866 Test Loss: 0.5359167
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2477706 Vali Loss: 0.2069637 Test Loss: 0.5365532
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2477018 Vali Loss: 0.2077255 Test Loss: 0.5368459
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2474495 Vali Loss: 0.2076621 Test Loss: 0.5369746
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2471662 Vali Loss: 0.2076519 Test Loss: 0.5370598
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2469055 Vali Loss: 0.2077406 Test Loss: 0.5370372
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NVIDIA_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.001208090572617948, mae:0.026512041687965393, rmse:0.034757595509290695, r2:-0.015208959579467773, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0265, RMSE: 0.0348, RÂ²: -0.0152, MAPE: 334613.62%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.632 MB of 0.634 MB uploadedwandb: \ 0.632 MB of 0.634 MB uploadedwandb: | 0.634 MB of 0.634 MB uploadedwandb: / 0.634 MB of 0.634 MB uploadedwandb: - 0.634 MB of 0.849 MB uploadedwandb: \ 0.849 MB of 0.849 MB uploadedwandb: | 0.849 MB of 0.849 MB uploadedwandb: / 0.849 MB of 0.849 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–„â–ˆâ–‚â–â–â–â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–†â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–„â–…â–†â–†â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.53704
wandb:                 train/loss 0.24691
wandb:   val/directional_accuracy 51.28894
wandb:                   val/loss 0.20774
wandb:                    val/mae 0.02651
wandb:                   val/mape 33461362.5
wandb:                    val/mse 0.00121
wandb:                     val/r2 -0.01521
wandb:                   val/rmse 0.03476
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/4vnhlmzo
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_144127-4vnhlmzo/logs
Completed: NVIDIA H=50

Training: Autoformer on NVIDIA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_144326-h7m3g6fx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/h7m3g6fx
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NVIDIA_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/h7m3g6fx
>>>>>>>start training : long_term_forecast_Autoformer_NVIDIA_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.3054018 Vali Loss: 0.2303094 Test Loss: 0.8291393
Validation loss decreased (inf --> 0.230309).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2802191 Vali Loss: 0.2287483 Test Loss: 0.8282062
Validation loss decreased (0.230309 --> 0.228748).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.30540181879813855, 'val/loss': 0.23030944764614106, 'test/loss': 0.8291392982006073, '_timestamp': 1762778619.5846064}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2802191322812667, 'val/loss': 0.22874827384948732, 'test/loss': 0.828206205368042, '_timestamp': 1762778626.3805497}).
Epoch: 3, Steps: 130 | Train Loss: 0.2755358 Vali Loss: 0.2373755 Test Loss: 0.8075907
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2734030 Vali Loss: 0.2284192 Test Loss: 0.8197067
Validation loss decreased (0.228748 --> 0.228419).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2721373 Vali Loss: 0.2356130 Test Loss: 0.8149079
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2715898 Vali Loss: 0.2368631 Test Loss: 0.8119754
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2718637 Vali Loss: 0.2423611 Test Loss: 0.8200043
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2715518 Vali Loss: 0.2385790 Test Loss: 0.8175327
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2715472 Vali Loss: 0.2333573 Test Loss: 0.8176838
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2712464 Vali Loss: 0.2371184 Test Loss: 0.8177170
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2713411 Vali Loss: 0.2408919 Test Loss: 0.8176182
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2715080 Vali Loss: 0.2374095 Test Loss: 0.8175798
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2714388 Vali Loss: 0.2391463 Test Loss: 0.8175742
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.2712569 Vali Loss: 0.2397068 Test Loss: 0.8175783
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NVIDIA_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0013063057558611035, mae:0.027484415099024773, rmse:0.036142852157354355, r2:-0.014673113822937012, dtw:Not calculated


VAL - MSE: 0.0013, MAE: 0.0275, RMSE: 0.0361, RÂ²: -0.0147, MAPE: 237246.72%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.682 MB of 0.687 MB uploadedwandb: \ 0.682 MB of 0.687 MB uploadedwandb: | 0.682 MB of 0.687 MB uploadedwandb: / 0.687 MB of 0.687 MB uploadedwandb: - 0.687 MB of 0.902 MB uploadedwandb: \ 0.902 MB of 0.902 MB uploadedwandb: | 0.902 MB of 0.902 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–…â–ƒâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:                 train/loss â–ˆâ–…â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–â–…â–…â–ˆâ–†â–ƒâ–…â–‡â–†â–†â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 13
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.81758
wandb:                 train/loss 0.27126
wandb:   val/directional_accuracy 49.75469
wandb:                   val/loss 0.23971
wandb:                    val/mae 0.02748
wandb:                   val/mape 23724671.875
wandb:                    val/mse 0.00131
wandb:                     val/r2 -0.01467
wandb:                   val/rmse 0.03614
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/h7m3g6fx
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_144326-h7m3g6fx/logs
Completed: NVIDIA H=100

Training: Autoformer on APPLE for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_144530-hqdk62el
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/hqdk62el
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_APPLE_H3 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/hqdk62el
>>>>>>>start training : long_term_forecast_Autoformer_APPLE_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.2977461 Vali Loss: 0.0966512 Test Loss: 0.1430481
Validation loss decreased (inf --> 0.096651).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2530758 Vali Loss: 0.0969874 Test Loss: 0.1429346
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2977460611583595, 'val/loss': 0.09665118996053934, 'test/loss': 0.14304814767092466, '_timestamp': 1762778743.3674192}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25307575316357434, 'val/loss': 0.0969874169677496, 'test/loss': 0.14293460827320814, '_timestamp': 1762778750.3544927}).
Epoch: 3, Steps: 133 | Train Loss: 0.2357233 Vali Loss: 0.0885447 Test Loss: 0.1392762
Validation loss decreased (0.096651 --> 0.088545).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2294457 Vali Loss: 0.0893837 Test Loss: 0.1348981
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2246658 Vali Loss: 0.0860006 Test Loss: 0.1345609
Validation loss decreased (0.088545 --> 0.086001).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2238398 Vali Loss: 0.0830138 Test Loss: 0.1331972
Validation loss decreased (0.086001 --> 0.083014).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2224924 Vali Loss: 0.0852276 Test Loss: 0.1333334
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2223738 Vali Loss: 0.0848981 Test Loss: 0.1331964
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2218652 Vali Loss: 0.0855492 Test Loss: 0.1330430
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2213962 Vali Loss: 0.0835218 Test Loss: 0.1326053
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2220764 Vali Loss: 0.0849006 Test Loss: 0.1325628
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2226793 Vali Loss: 0.0893661 Test Loss: 0.1325515
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2219786 Vali Loss: 0.0865454 Test Loss: 0.1325384
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2217945 Vali Loss: 0.0879383 Test Loss: 0.1325336
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2217219 Vali Loss: 0.0892471 Test Loss: 0.1325317
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2214186 Vali Loss: 0.0829262 Test Loss: 0.1325315
Validation loss decreased (0.083014 --> 0.082926).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2214126 Vali Loss: 0.0863632 Test Loss: 0.1325312
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2212637 Vali Loss: 0.0872445 Test Loss: 0.1325312
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2211629 Vali Loss: 0.0837378 Test Loss: 0.1325313
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2204397 Vali Loss: 0.0862748 Test Loss: 0.1325309
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2214498 Vali Loss: 0.0842961 Test Loss: 0.1325317
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2220218 Vali Loss: 0.0835322 Test Loss: 0.1325316
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2212784 Vali Loss: 0.0830613 Test Loss: 0.1325313
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2218275 Vali Loss: 0.0832689 Test Loss: 0.1325313
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2216362 Vali Loss: 0.0831924 Test Loss: 0.1325308
EarlyStopping counter: 9 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2217699 Vali Loss: 0.0862357 Test Loss: 0.1325312
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_APPLE_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.00020779170154128224, mae:0.010592186823487282, rmse:0.014414981938898563, r2:-0.03925597667694092, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0106, RMSE: 0.0144, RÂ²: -0.0393, MAPE: 886372.12%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.479 MB of 0.480 MB uploadedwandb: \ 0.479 MB of 0.480 MB uploadedwandb: | 0.480 MB of 0.480 MB uploadedwandb: / 0.480 MB of 0.480 MB uploadedwandb: - 0.480 MB of 0.480 MB uploadedwandb: \ 0.480 MB of 0.480 MB uploadedwandb: | 0.480 MB of 0.480 MB uploadedwandb: / 0.610 MB of 0.828 MB uploaded (0.002 MB deduped)wandb: - 0.828 MB of 0.828 MB uploaded (0.002 MB deduped)wandb: \ 0.828 MB of 0.828 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–‚â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–ˆâ–„â–â–ƒâ–ƒâ–„â–‚â–ƒâ–ˆâ–…â–†â–ˆâ–â–…â–†â–‚â–…â–‚â–‚â–â–â–â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 25
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.13253
wandb:                 train/loss 0.22177
wandb:   val/directional_accuracy 47.46835
wandb:                   val/loss 0.08624
wandb:                    val/mae 0.01059
wandb:                   val/mape 88637212.5
wandb:                    val/mse 0.00021
wandb:                     val/r2 -0.03926
wandb:                   val/rmse 0.01441
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/hqdk62el
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_144530-hqdk62el/logs
Completed: APPLE H=3

Training: Autoformer on APPLE for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_144856-c4pgsfy9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/c4pgsfy9
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_APPLE_H5 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/c4pgsfy9
>>>>>>>start training : long_term_forecast_Autoformer_APPLE_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.2882582 Vali Loss: 0.0955474 Test Loss: 0.1452191
Validation loss decreased (inf --> 0.095547).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2494175 Vali Loss: 0.0899844 Test Loss: 0.1440096
Validation loss decreased (0.095547 --> 0.089984).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2882581803583561, 'val/loss': 0.09554741438478231, 'test/loss': 0.1452191025018692, '_timestamp': 1762778949.7107873}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2494175078949534, 'val/loss': 0.08998439833521843, 'test/loss': 0.1440096264705062, '_timestamp': 1762778956.8008776}).
Epoch: 3, Steps: 133 | Train Loss: 0.2364685 Vali Loss: 0.0912543 Test Loss: 0.1436065
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2313918 Vali Loss: 0.0893446 Test Loss: 0.1392393
Validation loss decreased (0.089984 --> 0.089345).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2297289 Vali Loss: 0.0900109 Test Loss: 0.1367060
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2262336 Vali Loss: 0.0857598 Test Loss: 0.1371115
Validation loss decreased (0.089345 --> 0.085760).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2272254 Vali Loss: 0.0851606 Test Loss: 0.1368928
Validation loss decreased (0.085760 --> 0.085161).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2270112 Vali Loss: 0.0871644 Test Loss: 0.1366285
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2273915 Vali Loss: 0.0855850 Test Loss: 0.1364751
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2265544 Vali Loss: 0.0845726 Test Loss: 0.1363286
Validation loss decreased (0.085161 --> 0.084573).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2261082 Vali Loss: 0.0845823 Test Loss: 0.1362364
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2269771 Vali Loss: 0.0850457 Test Loss: 0.1362961
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2257164 Vali Loss: 0.0851921 Test Loss: 0.1362992
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2261620 Vali Loss: 0.0846062 Test Loss: 0.1362979
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2265847 Vali Loss: 0.0851932 Test Loss: 0.1362961
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2264700 Vali Loss: 0.0841082 Test Loss: 0.1362949
Validation loss decreased (0.084573 --> 0.084108).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2259059 Vali Loss: 0.0856502 Test Loss: 0.1362950
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2251397 Vali Loss: 0.0846183 Test Loss: 0.1362946
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2263733 Vali Loss: 0.0854264 Test Loss: 0.1362948
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2260943 Vali Loss: 0.0881972 Test Loss: 0.1362943
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2274387 Vali Loss: 0.0846382 Test Loss: 0.1362945
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2260864 Vali Loss: 0.0883461 Test Loss: 0.1362944
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2262207 Vali Loss: 0.0867372 Test Loss: 0.1362946
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2259763 Vali Loss: 0.0840228 Test Loss: 0.1362946
Validation loss decreased (0.084108 --> 0.084023).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2260637 Vali Loss: 0.0862627 Test Loss: 0.1362947
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2260488 Vali Loss: 0.0846182 Test Loss: 0.1362949
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.2261978 Vali Loss: 0.0841306 Test Loss: 0.1362948
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.2267098 Vali Loss: 0.0850142 Test Loss: 0.1362947
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.2264491 Vali Loss: 0.0858376 Test Loss: 0.1362946
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 133 | Train Loss: 0.2254863 Vali Loss: 0.0871742 Test Loss: 0.1362946
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 133 | Train Loss: 0.2256938 Vali Loss: 0.0861205 Test Loss: 0.1362946
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 133 | Train Loss: 0.2261253 Vali Loss: 0.0850516 Test Loss: 0.1362946
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 133 | Train Loss: 0.2258817 Vali Loss: 0.0877174 Test Loss: 0.1362946
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 133 | Train Loss: 0.2265095 Vali Loss: 0.0849047 Test Loss: 0.1362946
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_APPLE_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.0002133488596882671, mae:0.010591390542685986, rmse:0.014606466516852379, r2:-0.06217920780181885, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0106, RMSE: 0.0146, RÂ²: -0.0622, MAPE: 285111.66%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.521 MB of 0.522 MB uploadedwandb: \ 0.521 MB of 0.522 MB uploadedwandb: | 0.521 MB of 0.522 MB uploadedwandb: / 0.522 MB of 0.522 MB uploadedwandb: - 0.522 MB of 0.522 MB uploadedwandb: \ 0.522 MB of 0.740 MB uploadedwandb: | 0.740 MB of 0.740 MB uploadedwandb: / 0.740 MB of 0.740 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–‡â–ƒâ–‚â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–ƒâ–‚â–‚â–…â–‚â–…â–„â–â–ƒâ–‚â–â–‚â–ƒâ–„â–ƒâ–‚â–…â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 33
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.13629
wandb:                 train/loss 0.22651
wandb:   val/directional_accuracy 44.68085
wandb:                   val/loss 0.0849
wandb:                    val/mae 0.01059
wandb:                   val/mape 28511165.625
wandb:                    val/mse 0.00021
wandb:                     val/r2 -0.06218
wandb:                   val/rmse 0.01461
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/c4pgsfy9
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_144856-c4pgsfy9/logs
Completed: APPLE H=5

Training: Autoformer on APPLE for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_145313-1igp00zi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/1igp00zi
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_APPLE_H10Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/1igp00zi
>>>>>>>start training : long_term_forecast_Autoformer_APPLE_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2774969 Vali Loss: 0.1027214 Test Loss: 0.1470551
Validation loss decreased (inf --> 0.102721).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2446195 Vali Loss: 0.0970819 Test Loss: 0.1401248
Validation loss decreased (0.102721 --> 0.097082).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2774968942753354, 'val/loss': 0.10272137448191643, 'test/loss': 0.14705507177859545, '_timestamp': 1762779206.4954095}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2446195379011613, 'val/loss': 0.09708192758262157, 'test/loss': 0.14012475218623877, '_timestamp': 1762779213.5021043}).
Epoch: 3, Steps: 133 | Train Loss: 0.2350078 Vali Loss: 0.0925857 Test Loss: 0.1435913
Validation loss decreased (0.097082 --> 0.092586).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2294169 Vali Loss: 0.0953058 Test Loss: 0.1393726
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2277940 Vali Loss: 0.0910670 Test Loss: 0.1403387
Validation loss decreased (0.092586 --> 0.091067).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2255515 Vali Loss: 0.0911512 Test Loss: 0.1392742
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2250118 Vali Loss: 0.0904631 Test Loss: 0.1387598
Validation loss decreased (0.091067 --> 0.090463).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2252415 Vali Loss: 0.0892012 Test Loss: 0.1391641
Validation loss decreased (0.090463 --> 0.089201).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2256408 Vali Loss: 0.0892802 Test Loss: 0.1386707
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2242805 Vali Loss: 0.0909831 Test Loss: 0.1383128
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2243106 Vali Loss: 0.0885369 Test Loss: 0.1382823
Validation loss decreased (0.089201 --> 0.088537).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2237541 Vali Loss: 0.0933585 Test Loss: 0.1383325
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2247478 Vali Loss: 0.0900600 Test Loss: 0.1382949
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2251468 Vali Loss: 0.0917083 Test Loss: 0.1382935
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2238498 Vali Loss: 0.0897895 Test Loss: 0.1382940
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2244497 Vali Loss: 0.0907528 Test Loss: 0.1383734
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2252092 Vali Loss: 0.0906152 Test Loss: 0.1383736
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2238672 Vali Loss: 0.0916845 Test Loss: 0.1383736
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2243026 Vali Loss: 0.0910613 Test Loss: 0.1383734
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2249003 Vali Loss: 0.0906287 Test Loss: 0.1383735
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2257334 Vali Loss: 0.0908580 Test Loss: 0.1383737
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_APPLE_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.00021889344498049468, mae:0.010741034522652626, rmse:0.014795048162341118, r2:-0.08113288879394531, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0107, RMSE: 0.0148, RÂ²: -0.0811, MAPE: 626410.06%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.582 MB of 0.583 MB uploadedwandb: \ 0.582 MB of 0.583 MB uploadedwandb: | 0.583 MB of 0.583 MB uploadedwandb: / 0.583 MB of 0.583 MB uploadedwandb: - 0.583 MB of 0.799 MB uploadedwandb: \ 0.583 MB of 0.799 MB uploadedwandb: | 0.799 MB of 0.799 MB uploadedwandb: / 0.799 MB of 0.799 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–â–â–â–‚â–‚â–â–â–‚â–â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–ˆâ–„â–„â–ƒâ–‚â–‚â–„â–â–†â–ƒâ–„â–‚â–ƒâ–ƒâ–„â–„â–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.13837
wandb:                 train/loss 0.22573
wandb:   val/directional_accuracy 50.14493
wandb:                   val/loss 0.09086
wandb:                    val/mae 0.01074
wandb:                   val/mape 62641006.25
wandb:                    val/mse 0.00022
wandb:                     val/r2 -0.08113
wandb:                   val/rmse 0.0148
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/1igp00zi
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_145313-1igp00zi/logs
Completed: APPLE H=10

Training: Autoformer on APPLE for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_145608-ansgws0f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/ansgws0f
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_APPLE_H22Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/ansgws0f
>>>>>>>start training : long_term_forecast_Autoformer_APPLE_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2778098 Vali Loss: 0.0946683 Test Loss: 0.1386282
Validation loss decreased (inf --> 0.094668).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2467057 Vali Loss: 0.0918064 Test Loss: 0.1357699
Validation loss decreased (0.094668 --> 0.091806).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2778097735435674, 'val/loss': 0.09466833621263504, 'test/loss': 0.13862817734479904, '_timestamp': 1762779381.2541635}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24670571529052474, 'val/loss': 0.09180635746036257, 'test/loss': 0.13576986427818025, '_timestamp': 1762779388.1187005}).
Epoch: 3, Steps: 132 | Train Loss: 0.2389144 Vali Loss: 0.0919657 Test Loss: 0.1407272
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2353403 Vali Loss: 0.0925171 Test Loss: 0.1415200
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2333721 Vali Loss: 0.0905859 Test Loss: 0.1397778
Validation loss decreased (0.091806 --> 0.090586).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2320306 Vali Loss: 0.0907641 Test Loss: 0.1399795
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2318699 Vali Loss: 0.0901962 Test Loss: 0.1395105
Validation loss decreased (0.090586 --> 0.090196).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2315734 Vali Loss: 0.0906011 Test Loss: 0.1398411
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2313611 Vali Loss: 0.0904811 Test Loss: 0.1398116
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2310514 Vali Loss: 0.0908583 Test Loss: 0.1399203
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2310491 Vali Loss: 0.0905046 Test Loss: 0.1399644
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2306667 Vali Loss: 0.0907811 Test Loss: 0.1399198
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2306159 Vali Loss: 0.0907725 Test Loss: 0.1399201
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2305808 Vali Loss: 0.0904693 Test Loss: 0.1399158
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2309128 Vali Loss: 0.0904309 Test Loss: 0.1399182
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2309052 Vali Loss: 0.0905177 Test Loss: 0.1399191
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2313405 Vali Loss: 0.0906961 Test Loss: 0.1399191
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_APPLE_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.00022216103388927877, mae:0.01075816247612238, rmse:0.014905067160725594, r2:-0.0706324577331543, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0108, RMSE: 0.0149, RÂ²: -0.0706, MAPE: 351123.09%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.649 MB of 0.650 MB uploadedwandb: \ 0.649 MB of 0.650 MB uploadedwandb: | 0.649 MB of 0.650 MB uploadedwandb: / 0.650 MB of 0.650 MB uploadedwandb: - 0.650 MB of 0.650 MB uploadedwandb: \ 0.650 MB of 0.866 MB uploadedwandb: | 0.866 MB of 0.866 MB uploadedwandb: / 0.866 MB of 0.866 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–ˆâ–‚â–ƒâ–â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–ˆâ–‚â–ƒâ–â–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.13992
wandb:                 train/loss 0.23134
wandb:   val/directional_accuracy 48.95151
wandb:                   val/loss 0.0907
wandb:                    val/mae 0.01076
wandb:                   val/mape 35112309.375
wandb:                    val/mse 0.00022
wandb:                     val/r2 -0.07063
wandb:                   val/rmse 0.01491
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/ansgws0f
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_145608-ansgws0f/logs
Completed: APPLE H=22

Training: Autoformer on APPLE for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_145835-sctl2su1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/sctl2su1
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_APPLE_H50Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/sctl2su1
>>>>>>>start training : long_term_forecast_Autoformer_APPLE_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.2839286 Vali Loss: 0.1002720 Test Loss: 0.1611216
Validation loss decreased (inf --> 0.100272).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2548241 Vali Loss: 0.0961136 Test Loss: 0.1555229
Validation loss decreased (0.100272 --> 0.096114).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.283928552811796, 'val/loss': 0.10027201597889264, 'test/loss': 0.16112161676088968, '_timestamp': 1762779528.59208}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25482407234834903, 'val/loss': 0.09611357500155766, 'test/loss': 0.15552289163072905, '_timestamp': 1762779535.6956754}).
Epoch: 3, Steps: 132 | Train Loss: 0.2450044 Vali Loss: 0.0942678 Test Loss: 0.1545711
Validation loss decreased (0.096114 --> 0.094268).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2419355 Vali Loss: 0.0930890 Test Loss: 0.1561786
Validation loss decreased (0.094268 --> 0.093089).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2406018 Vali Loss: 0.0909069 Test Loss: 0.1534962
Validation loss decreased (0.093089 --> 0.090907).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2394408 Vali Loss: 0.0929322 Test Loss: 0.1557991
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2383848 Vali Loss: 0.0934851 Test Loss: 0.1565547
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2378540 Vali Loss: 0.0934861 Test Loss: 0.1563616
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2384113 Vali Loss: 0.0937712 Test Loss: 0.1567835
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2385875 Vali Loss: 0.0938107 Test Loss: 0.1568959
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2386374 Vali Loss: 0.0937534 Test Loss: 0.1567328
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2380865 Vali Loss: 0.0936248 Test Loss: 0.1566576
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2380385 Vali Loss: 0.0936496 Test Loss: 0.1566122
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2374199 Vali Loss: 0.0937265 Test Loss: 0.1566110
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2377335 Vali Loss: 0.0936545 Test Loss: 0.1566042
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_APPLE_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.00023597302788402885, mae:0.011133220046758652, rmse:0.015361413359642029, r2:-0.06354033946990967, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0111, RMSE: 0.0154, RÂ²: -0.0635, MAPE: 368490.19%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.729 MB of 0.732 MB uploadedwandb: \ 0.729 MB of 0.732 MB uploadedwandb: | 0.729 MB of 0.732 MB uploadedwandb: / 0.732 MB of 0.732 MB uploadedwandb: - 0.732 MB of 0.732 MB uploadedwandb: \ 0.732 MB of 0.947 MB uploadedwandb: | 0.947 MB of 0.947 MB uploadedwandb: / 0.947 MB of 0.947 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ƒâ–‡â–â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡
wandb:                 train/loss â–ˆâ–…â–„â–ƒâ–‚â–â–‚â–‚â–‚â–‚â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.1566
wandb:                 train/loss 0.23773
wandb:   val/directional_accuracy 48.24919
wandb:                   val/loss 0.09365
wandb:                    val/mae 0.01113
wandb:                   val/mape 36849018.75
wandb:                    val/mse 0.00024
wandb:                     val/r2 -0.06354
wandb:                   val/rmse 0.01536
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/sctl2su1
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_145835-sctl2su1/logs
Completed: APPLE H=50

Training: Autoformer on APPLE for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_150050-abt6sm7w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/abt6sm7w
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_APPLE_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/abt6sm7w
>>>>>>>start training : long_term_forecast_Autoformer_APPLE_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.2972879 Vali Loss: 0.0995323 Test Loss: 0.1655732
Validation loss decreased (inf --> 0.099532).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2709301 Vali Loss: 0.0952673 Test Loss: 0.1651528
Validation loss decreased (0.099532 --> 0.095267).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2972879305481911, 'val/loss': 0.09953228533267974, 'test/loss': 0.1655731827020645, '_timestamp': 1762779664.3909934}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.27093009146360253, 'val/loss': 0.09526734054088593, 'test/loss': 0.16515279710292816, '_timestamp': 1762779671.4152305}).
Epoch: 3, Steps: 130 | Train Loss: 0.2642930 Vali Loss: 0.0998364 Test Loss: 0.1680555
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2614497 Vali Loss: 0.0970256 Test Loss: 0.1680681
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2599090 Vali Loss: 0.0973079 Test Loss: 0.1692850
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2595233 Vali Loss: 0.0976762 Test Loss: 0.1676180
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2591214 Vali Loss: 0.0981610 Test Loss: 0.1684154
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2582971 Vali Loss: 0.0984166 Test Loss: 0.1676549
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2582393 Vali Loss: 0.0975663 Test Loss: 0.1679291
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2580623 Vali Loss: 0.0978086 Test Loss: 0.1678945
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2583199 Vali Loss: 0.0978243 Test Loss: 0.1678481
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2585324 Vali Loss: 0.0980314 Test Loss: 0.1678731
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_APPLE_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.00024839333491399884, mae:0.011314773932099342, rmse:0.015760499984025955, r2:-0.05201590061187744, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0113, RMSE: 0.0158, RÂ²: -0.0520, MAPE: 977556.56%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.730 MB of 0.734 MB uploadedwandb: \ 0.730 MB of 0.734 MB uploadedwandb: | 0.734 MB of 0.734 MB uploadedwandb: / 0.734 MB of 0.734 MB uploadedwandb: - 0.734 MB of 0.949 MB uploadedwandb: \ 0.949 MB of 0.949 MB uploadedwandb: | 0.949 MB of 0.949 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ƒâ–ƒâ–ˆâ–â–„â–â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–‚â–ƒâ–„â–„â–‚â–ƒâ–ƒâ–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.16787
wandb:                 train/loss 0.25853
wandb:   val/directional_accuracy 48.99711
wandb:                   val/loss 0.09803
wandb:                    val/mae 0.01131
wandb:                   val/mape 97755656.25
wandb:                    val/mse 0.00025
wandb:                     val/r2 -0.05202
wandb:                   val/rmse 0.01576
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/abt6sm7w
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_150050-abt6sm7w/logs
Completed: APPLE H=100

Training: Autoformer on SP500 for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_150244-m9fofw0f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/m9fofw0f
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SP500_H3 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/m9fofw0f
>>>>>>>start training : long_term_forecast_Autoformer_SP500_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 3
============================================================
train 4241
val 237
test 238
Epoch: 1, Steps: 133 | Train Loss: 0.2429843 Vali Loss: 0.0950817 Test Loss: 0.1059345
Validation loss decreased (inf --> 0.095082).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2059110 Vali Loss: 0.0781615 Test Loss: 0.0862168
Validation loss decreased (0.095082 --> 0.078162).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.24298433892260818, 'val/loss': 0.09508167766034603, 'test/loss': 0.10593450907617807, '_timestamp': 1762779777.3074064}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.20591096221504354, 'val/loss': 0.07816150318831205, 'test/loss': 0.08621684648096561, '_timestamp': 1762779784.3239555}).
Epoch: 3, Steps: 133 | Train Loss: 0.1933705 Vali Loss: 0.0716806 Test Loss: 0.0788340
Validation loss decreased (0.078162 --> 0.071681).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1880216 Vali Loss: 0.0693410 Test Loss: 0.0752074
Validation loss decreased (0.071681 --> 0.069341).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1854971 Vali Loss: 0.0696802 Test Loss: 0.0745680
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1841882 Vali Loss: 0.0677363 Test Loss: 0.0740769
Validation loss decreased (0.069341 --> 0.067736).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1839489 Vali Loss: 0.0689801 Test Loss: 0.0736471
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1825574 Vali Loss: 0.0686963 Test Loss: 0.0738630
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1830903 Vali Loss: 0.0698320 Test Loss: 0.0738533
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1842789 Vali Loss: 0.0693802 Test Loss: 0.0738600
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1828426 Vali Loss: 0.0680934 Test Loss: 0.0741202
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1829804 Vali Loss: 0.0683965 Test Loss: 0.0739304
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1831558 Vali Loss: 0.0663796 Test Loss: 0.0739086
Validation loss decreased (0.067736 --> 0.066380).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1818269 Vali Loss: 0.0688556 Test Loss: 0.0739079
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1822339 Vali Loss: 0.0716706 Test Loss: 0.0739048
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1839413 Vali Loss: 0.0690917 Test Loss: 0.0739058
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1817103 Vali Loss: 0.0678202 Test Loss: 0.0739047
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1826416 Vali Loss: 0.0678606 Test Loss: 0.0739047
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1819112 Vali Loss: 0.0696373 Test Loss: 0.0739043
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1824364 Vali Loss: 0.0687555 Test Loss: 0.0739042
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1829732 Vali Loss: 0.0681401 Test Loss: 0.0739046
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1831987 Vali Loss: 0.0688988 Test Loss: 0.0739043
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1845701 Vali Loss: 0.0691631 Test Loss: 0.0739043
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SP500_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 238
test shape: (238, 3, 1) (238, 3, 1)
test shape: (238, 3, 1) (238, 3, 1)


	mse:6.608110561501235e-05, mae:0.00601145951077342, rmse:0.008129028603434563, r2:-0.01645636558532715, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0060, RMSE: 0.0081, RÂ²: -0.0165, MAPE: 1.90%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.493 MB of 0.494 MB uploadedwandb: \ 0.493 MB of 0.494 MB uploadedwandb: | 0.494 MB of 0.494 MB uploadedwandb: / 0.494 MB of 0.494 MB uploadedwandb: - 0.494 MB of 0.494 MB uploadedwandb: \ 0.494 MB of 0.494 MB uploadedwandb: | 0.494 MB of 0.494 MB uploadedwandb: / 0.624 MB of 0.841 MB uploaded (0.002 MB deduped)wandb: - 0.841 MB of 0.841 MB uploaded (0.002 MB deduped)wandb: \ 0.841 MB of 0.841 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–â–â–‚â–â–‚â–â–â–‚â–‚â–ƒ
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–…â–ƒâ–„â–„â–†â–…â–ƒâ–„â–â–„â–ˆâ–…â–ƒâ–ƒâ–…â–„â–ƒâ–„â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.0739
wandb:                 train/loss 0.18457
wandb:   val/directional_accuracy 54.41176
wandb:                   val/loss 0.06916
wandb:                    val/mae 0.00601
wandb:                   val/mape 189.80575
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.01646
wandb:                   val/rmse 0.00813
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/m9fofw0f
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_150244-m9fofw0f/logs
Completed: SP500 H=3

Training: Autoformer on SP500 for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_150552-8eddjdc9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/8eddjdc9
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SP500_H5 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/8eddjdc9
>>>>>>>start training : long_term_forecast_Autoformer_SP500_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 5
============================================================
train 4239
val 235
test 236
Epoch: 1, Steps: 133 | Train Loss: 0.2376393 Vali Loss: 0.0925296 Test Loss: 0.1064274
Validation loss decreased (inf --> 0.092530).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.1993132 Vali Loss: 0.0739793 Test Loss: 0.0862153
Validation loss decreased (0.092530 --> 0.073979).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23763925593374366, 'val/loss': 0.0925295976921916, 'test/loss': 0.1064273826777935, '_timestamp': 1762779965.1822708}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19931315322567647, 'val/loss': 0.07397928135469556, 'test/loss': 0.08621531631797552, '_timestamp': 1762779972.3533916}).
Epoch: 3, Steps: 133 | Train Loss: 0.1875359 Vali Loss: 0.0763552 Test Loss: 0.0802386
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1838706 Vali Loss: 0.0742564 Test Loss: 0.0798969
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1810372 Vali Loss: 0.0723810 Test Loss: 0.0798291
Validation loss decreased (0.073979 --> 0.072381).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1801675 Vali Loss: 0.0702315 Test Loss: 0.0803299
Validation loss decreased (0.072381 --> 0.070232).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1788339 Vali Loss: 0.0700445 Test Loss: 0.0783232
Validation loss decreased (0.070232 --> 0.070045).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1787399 Vali Loss: 0.0706414 Test Loss: 0.0783982
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1794745 Vali Loss: 0.0687941 Test Loss: 0.0785187
Validation loss decreased (0.070045 --> 0.068794).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1783346 Vali Loss: 0.0706390 Test Loss: 0.0784485
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1784777 Vali Loss: 0.0706633 Test Loss: 0.0783901
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1787365 Vali Loss: 0.0703285 Test Loss: 0.0783941
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1794736 Vali Loss: 0.0711655 Test Loss: 0.0784076
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1780819 Vali Loss: 0.0699267 Test Loss: 0.0784018
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1785427 Vali Loss: 0.0690218 Test Loss: 0.0784039
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1784068 Vali Loss: 0.0694617 Test Loss: 0.0784064
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1780739 Vali Loss: 0.0722829 Test Loss: 0.0784069
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1780425 Vali Loss: 0.0708932 Test Loss: 0.0784064
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1782921 Vali Loss: 0.0728374 Test Loss: 0.0784067
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SP500_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
test shape: (236, 5, 1) (236, 5, 1)
test shape: (236, 5, 1) (236, 5, 1)


	mse:6.912615208420902e-05, mae:0.006220916751772165, rmse:0.008314213715493679, r2:-0.06411302089691162, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0062, RMSE: 0.0083, RÂ²: -0.0641, MAPE: 2.62%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.536 MB of 0.536 MB uploadedwandb: \ 0.536 MB of 0.536 MB uploadedwandb: | 0.536 MB of 0.536 MB uploadedwandb: / 0.536 MB of 0.536 MB uploadedwandb: - 0.536 MB of 0.536 MB uploadedwandb: \ 0.536 MB of 0.752 MB uploadedwandb: | 0.636 MB of 0.752 MB uploadedwandb: / 0.752 MB of 0.752 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–†â–†â–ˆâ–â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–‚â–‚â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–„â–‚â–‚â–ƒâ–â–ƒâ–ƒâ–‚â–ƒâ–‚â–â–‚â–„â–ƒâ–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 18
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.07841
wandb:                 train/loss 0.17829
wandb:   val/directional_accuracy 43.64407
wandb:                   val/loss 0.07284
wandb:                    val/mae 0.00622
wandb:                   val/mape 262.3013
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.06411
wandb:                   val/rmse 0.00831
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/8eddjdc9
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_150552-8eddjdc9/logs
Completed: SP500 H=5

Training: Autoformer on SP500 for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_150839-nzyvg2b4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/nzyvg2b4
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SP500_H10Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/nzyvg2b4
>>>>>>>start training : long_term_forecast_Autoformer_SP500_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 10
============================================================
train 4234
val 230
test 231
Epoch: 1, Steps: 133 | Train Loss: 0.2235764 Vali Loss: 0.0748892 Test Loss: 0.0842192
Validation loss decreased (inf --> 0.074889).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.1923247 Vali Loss: 0.0731793 Test Loss: 0.0836566
Validation loss decreased (0.074889 --> 0.073179).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2235763782173171, 'val/loss': 0.07488923706114292, 'test/loss': 0.08421919634565711, '_timestamp': 1762780132.5507464}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.1923247374090037, 'val/loss': 0.0731792775914073, 'test/loss': 0.08365656714886427, '_timestamp': 1762780139.6521723}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.1923247374090037, 'val/loss': 0.0731792775914073, 'test/loss': 0.08365656714886427, '_timestamp': 1762780139.6521723}).
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2235763782173171, 'val/loss': 0.07488923706114292, 'test/loss': 0.08421919634565711, '_timestamp': 1762780132.5507464}).
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
Epoch: 3, Steps: 133 | Train Loss: 0.1853550 Vali Loss: 0.0735042 Test Loss: 0.0811079
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1813662 Vali Loss: 0.0732295 Test Loss: 0.0807934
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1806643 Vali Loss: 0.0726440 Test Loss: 0.0800081
Validation loss decreased (0.073179 --> 0.072644).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1790188 Vali Loss: 0.0714929 Test Loss: 0.0805088
Validation loss decreased (0.072644 --> 0.071493).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1789575 Vali Loss: 0.0731321 Test Loss: 0.0801549
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1791481 Vali Loss: 0.0696423 Test Loss: 0.0799297
Validation loss decreased (0.071493 --> 0.069642).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1784585 Vali Loss: 0.0701311 Test Loss: 0.0799946
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1782543 Vali Loss: 0.0699780 Test Loss: 0.0800362
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1784172 Vali Loss: 0.0719664 Test Loss: 0.0800499
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1795726 Vali Loss: 0.0698368 Test Loss: 0.0800479
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1788479 Vali Loss: 0.0725281 Test Loss: 0.0800510
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1782398 Vali Loss: 0.0708478 Test Loss: 0.0800491
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1780448 Vali Loss: 0.0696113 Test Loss: 0.0800492
Validation loss decreased (0.069642 --> 0.069611).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1784474 Vali Loss: 0.0695044 Test Loss: 0.0800487
Validation loss decreased (0.069611 --> 0.069504).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1817906 Vali Loss: 0.0708404 Test Loss: 0.0800487
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1786267 Vali Loss: 0.0708572 Test Loss: 0.0800489
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1784848 Vali Loss: 0.0737459 Test Loss: 0.0800491
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1794575 Vali Loss: 0.0723192 Test Loss: 0.0800489
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1783949 Vali Loss: 0.0745796 Test Loss: 0.0800488
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1790274 Vali Loss: 0.0718126 Test Loss: 0.0800490
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1783082 Vali Loss: 0.0696085 Test Loss: 0.0800487
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.1785493 Vali Loss: 0.0707799 Test Loss: 0.0800488
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.1803831 Vali Loss: 0.0702419 Test Loss: 0.0800489
EarlyStopping counter: 9 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.1792890 Vali Loss: 0.0698542 Test Loss: 0.0800489
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SP500_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 231
test shape: (231, 10, 1) (231, 10, 1)
test shape: (231, 10, 1) (231, 10, 1)


	mse:6.733799818903208e-05, mae:0.0060880957171320915, rmse:0.008205973543226719, r2:-0.035924553871154785, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0082, RÂ²: -0.0359, MAPE: 2.73%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.556 MB of 0.557 MB uploadedwandb: \ 0.556 MB of 0.557 MB uploadedwandb: | 0.556 MB of 0.557 MB uploadedwandb: / 0.557 MB of 0.557 MB uploadedwandb: - 0.557 MB of 0.557 MB uploadedwandb: \ 0.557 MB of 0.557 MB uploadedwandb: | 0.557 MB of 0.774 MB uploadedwandb: / 0.658 MB of 0.774 MB uploadedwandb: - 0.774 MB of 0.774 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–†â–â–„â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–„â–„â–‚â–‚â–‚â–â–â–â–‚â–‚â–â–â–â–…â–‚â–â–‚â–â–‚â–â–â–ƒâ–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–†â–…â–„â–†â–â–‚â–‚â–„â–â–…â–ƒâ–â–â–ƒâ–ƒâ–‡â–…â–ˆâ–„â–â–ƒâ–‚â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 25
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.08005
wandb:                 train/loss 0.17929
wandb:   val/directional_accuracy 47.52285
wandb:                   val/loss 0.06985
wandb:                    val/mae 0.00609
wandb:                   val/mape 273.44148
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.03592
wandb:                   val/rmse 0.00821
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/nzyvg2b4
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_150839-nzyvg2b4/logs
Completed: SP500 H=10

Training: Autoformer on SP500 for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_151209-rmmh4ec8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/rmmh4ec8
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SP500_H22Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/rmmh4ec8
>>>>>>>start training : long_term_forecast_Autoformer_SP500_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 22
============================================================
train 4222
val 218
test 219
Epoch: 1, Steps: 132 | Train Loss: 0.2191719 Vali Loss: 0.0727128 Test Loss: 0.0730022
Validation loss decreased (inf --> 0.072713).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.1900598 Vali Loss: 0.0746423 Test Loss: 0.0728030
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2191718719673879, 'val/loss': 0.07271279394626617, 'test/loss': 0.07300223250474248, '_timestamp': 1762780343.0664878}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19005983949384905, 'val/loss': 0.07464226867471423, 'test/loss': 0.0728029917393412, '_timestamp': 1762780350.2069013}).
Epoch: 3, Steps: 132 | Train Loss: 0.1843969 Vali Loss: 0.0737772 Test Loss: 0.0747693
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.1819405 Vali Loss: 0.0721312 Test Loss: 0.0723161
Validation loss decreased (0.072713 --> 0.072131).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.1807540 Vali Loss: 0.0718968 Test Loss: 0.0722295
Validation loss decreased (0.072131 --> 0.071897).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.1802000 Vali Loss: 0.0718425 Test Loss: 0.0728656
Validation loss decreased (0.071897 --> 0.071843).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.1800505 Vali Loss: 0.0721773 Test Loss: 0.0724927
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.1797377 Vali Loss: 0.0720287 Test Loss: 0.0726148
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.1793470 Vali Loss: 0.0720416 Test Loss: 0.0724710
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.1794090 Vali Loss: 0.0719206 Test Loss: 0.0724970
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.1794021 Vali Loss: 0.0719932 Test Loss: 0.0725271
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.1794987 Vali Loss: 0.0722746 Test Loss: 0.0725024
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.1793911 Vali Loss: 0.0718830 Test Loss: 0.0724982
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.1794932 Vali Loss: 0.0720332 Test Loss: 0.0724976
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.1794877 Vali Loss: 0.0720157 Test Loss: 0.0724952
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.1794565 Vali Loss: 0.0719574 Test Loss: 0.0724995
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SP500_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 219
test shape: (219, 22, 1) (219, 22, 1)
test shape: (219, 22, 1) (219, 22, 1)


	mse:6.643757660640404e-05, mae:0.006068876478821039, rmse:0.008150924928486347, r2:-0.04060029983520508, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0082, RÂ²: -0.0406, MAPE: 3.58%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.634 MB of 0.635 MB uploadedwandb: \ 0.634 MB of 0.635 MB uploadedwandb: | 0.634 MB of 0.635 MB uploadedwandb: / 0.635 MB of 0.635 MB uploadedwandb: - 0.635 MB of 0.635 MB uploadedwandb: \ 0.635 MB of 0.851 MB uploadedwandb: | 0.851 MB of 0.851 MB uploadedwandb: / 0.851 MB of 0.851 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‚â–â–â–‚â–‚â–‚â–â–‚â–ƒâ–â–‚â–‚â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.0725
wandb:                 train/loss 0.17946
wandb:   val/directional_accuracy 48.33659
wandb:                   val/loss 0.07196
wandb:                    val/mae 0.00607
wandb:                   val/mape 357.91538
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.0406
wandb:                   val/rmse 0.00815
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/rmmh4ec8
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_151209-rmmh4ec8/logs
Completed: SP500 H=22

Training: Autoformer on SP500 for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_151431-kqwupan3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/kqwupan3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SP500_H50Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/kqwupan3
>>>>>>>start training : long_term_forecast_Autoformer_SP500_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 50
============================================================
train 4194
val 190
test 191
Epoch: 1, Steps: 132 | Train Loss: 0.2199751 Vali Loss: 0.0776509 Test Loss: 0.0816606
Validation loss decreased (inf --> 0.077651).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.1929130 Vali Loss: 0.0751079 Test Loss: 0.0789332
Validation loss decreased (0.077651 --> 0.075108).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2199751210935188, 'val/loss': 0.07765088602900505, 'test/loss': 0.08166059230764706, '_timestamp': 1762780484.3311164}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19291297182666534, 'val/loss': 0.0751078873872757, 'test/loss': 0.07893323712050915, '_timestamp': 1762780492.0324204}).
Epoch: 3, Steps: 132 | Train Loss: 0.1881115 Vali Loss: 0.0752137 Test Loss: 0.0790478
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.1855910 Vali Loss: 0.0727190 Test Loss: 0.0748592
Validation loss decreased (0.075108 --> 0.072719).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.1908692 Vali Loss: 0.0729028 Test Loss: 0.0763870
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.1844124 Vali Loss: 0.0731215 Test Loss: 0.0768339
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.1843305 Vali Loss: 0.0727597 Test Loss: 0.0757683
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.1892711 Vali Loss: 0.0729130 Test Loss: 0.0762916
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.1853171 Vali Loss: 0.0730020 Test Loss: 0.0761009
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.1837820 Vali Loss: 0.0729714 Test Loss: 0.0759987
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.1837466 Vali Loss: 0.0729372 Test Loss: 0.0760056
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.1837379 Vali Loss: 0.0729546 Test Loss: 0.0759819
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.1840815 Vali Loss: 0.0729655 Test Loss: 0.0759662
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.1841173 Vali Loss: 0.0729384 Test Loss: 0.0759615
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SP500_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 191
test shape: (191, 50, 1) (191, 50, 1)
test shape: (191, 50, 1) (191, 50, 1)


	mse:6.611856224481016e-05, mae:0.006022725719958544, rmse:0.008131331764161587, r2:-0.016699790954589844, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0060, RMSE: 0.0081, RÂ²: -0.0167, MAPE: 3.18%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.709 MB of 0.711 MB uploadedwandb: \ 0.709 MB of 0.711 MB uploadedwandb: | 0.709 MB of 0.711 MB uploadedwandb: / 0.711 MB of 0.711 MB uploadedwandb: - 0.711 MB of 0.711 MB uploadedwandb: \ 0.711 MB of 0.927 MB uploadedwandb: | 0.927 MB of 0.927 MB uploadedwandb: / 0.927 MB of 0.927 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–…â–ƒâ–ˆâ–‚â–‚â–†â–ƒâ–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 13
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.07596
wandb:                 train/loss 0.18412
wandb:   val/directional_accuracy 51.92862
wandb:                   val/loss 0.07294
wandb:                    val/mae 0.00602
wandb:                   val/mape 317.61603
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.0167
wandb:                   val/rmse 0.00813
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/kqwupan3
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_151431-kqwupan3/logs
Completed: SP500 H=50

Training: Autoformer on SP500 for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_151640-sl3c2h8k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/sl3c2h8k
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SP500_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/sl3c2h8k
>>>>>>>start training : long_term_forecast_Autoformer_SP500_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 100
============================================================
train 4144
val 140
test 141
Epoch: 1, Steps: 130 | Train Loss: 0.2300285 Vali Loss: 0.0692017 Test Loss: 0.0836325
Validation loss decreased (inf --> 0.069202).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2033486 Vali Loss: 0.0678885 Test Loss: 0.0841520
Validation loss decreased (0.069202 --> 0.067888).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23002851828932763, 'val/loss': 0.06920173317193985, 'test/loss': 0.08363247066736221, '_timestamp': 1762780613.4419487}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2033486032142089, 'val/loss': 0.06788846701383591, 'test/loss': 0.08415196686983109, '_timestamp': 1762780620.493522}).
Epoch: 3, Steps: 130 | Train Loss: 0.1982704 Vali Loss: 0.0702752 Test Loss: 0.0854339
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.1969302 Vali Loss: 0.0688131 Test Loss: 0.0842764
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.1956712 Vali Loss: 0.0697917 Test Loss: 0.0852634
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.1956490 Vali Loss: 0.0691979 Test Loss: 0.0848297
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.1963802 Vali Loss: 0.0703013 Test Loss: 0.0860489
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.1951566 Vali Loss: 0.0694932 Test Loss: 0.0852221
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.1955609 Vali Loss: 0.0694162 Test Loss: 0.0853282
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.1952924 Vali Loss: 0.0696131 Test Loss: 0.0853818
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.1955154 Vali Loss: 0.0697646 Test Loss: 0.0854022
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.1952502 Vali Loss: 0.0693448 Test Loss: 0.0853971
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SP500_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 141
test shape: (141, 100, 1) (141, 100, 1)
test shape: (141, 100, 1) (141, 100, 1)


	mse:6.958066660445184e-05, mae:0.006156690884381533, rmse:0.008341502398252487, r2:-0.01565253734588623, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0062, RMSE: 0.0083, RÂ²: -0.0157, MAPE: 3.48%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.793 MB of 0.798 MB uploadedwandb: \ 0.793 MB of 0.798 MB uploadedwandb: | 0.798 MB of 0.798 MB uploadedwandb: / 0.798 MB of 0.798 MB uploadedwandb: - 0.798 MB of 1.013 MB uploadedwandb: \ 0.798 MB of 1.013 MB uploadedwandb: | 1.013 MB of 1.013 MB uploadedwandb: / 1.013 MB of 1.013 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–†â–â–…â–ƒâ–ˆâ–…â–…â–…â–…â–…
wandb:                 train/loss â–ˆâ–…â–‚â–‚â–„â–â–‚â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–†â–ƒâ–ˆâ–„â–„â–…â–…â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.0854
wandb:                 train/loss 0.19525
wandb:   val/directional_accuracy 51.92349
wandb:                   val/loss 0.06934
wandb:                    val/mae 0.00616
wandb:                   val/mape 348.49424
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.01565
wandb:                   val/rmse 0.00834
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/sl3c2h8k
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_151640-sl3c2h8k/logs
Completed: SP500 H=100

Training: Autoformer on NASDAQ for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_151833-tkmokmtf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/tkmokmtf
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NASDAQ_H3Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/tkmokmtf
>>>>>>>start training : long_term_forecast_Autoformer_NASDAQ_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.3219788 Vali Loss: 0.1676297 Test Loss: 0.1560226
Validation loss decreased (inf --> 0.167630).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2746878 Vali Loss: 0.1444791 Test Loss: 0.1388947
Validation loss decreased (0.167630 --> 0.144479).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.32197881999768707, 'val/loss': 0.16762973461300135, 'test/loss': 0.1560226222500205, '_timestamp': 1762780727.2925026}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2746878357085967, 'val/loss': 0.14447913970798254, 'test/loss': 0.13889468926936388, '_timestamp': 1762780734.4071202}).
Epoch: 3, Steps: 133 | Train Loss: 0.2586469 Vali Loss: 0.1438687 Test Loss: 0.1302504
Validation loss decreased (0.144479 --> 0.143869).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2505399 Vali Loss: 0.1385564 Test Loss: 0.1289393
Validation loss decreased (0.143869 --> 0.138556).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2454030 Vali Loss: 0.1364594 Test Loss: 0.1275214
Validation loss decreased (0.138556 --> 0.136459).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2434964 Vali Loss: 0.1393771 Test Loss: 0.1279798
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2413216 Vali Loss: 0.1363069 Test Loss: 0.1269147
Validation loss decreased (0.136459 --> 0.136307).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2403975 Vali Loss: 0.1336903 Test Loss: 0.1269949
Validation loss decreased (0.136307 --> 0.133690).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2404653 Vali Loss: 0.1320435 Test Loss: 0.1270930
Validation loss decreased (0.133690 --> 0.132044).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2406363 Vali Loss: 0.1351619 Test Loss: 0.1270475
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2391821 Vali Loss: 0.1329046 Test Loss: 0.1269616
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2407432 Vali Loss: 0.1351771 Test Loss: 0.1270002
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2397023 Vali Loss: 0.1379668 Test Loss: 0.1269816
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2399756 Vali Loss: 0.1353610 Test Loss: 0.1269739
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2395053 Vali Loss: 0.1367634 Test Loss: 0.1269753
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2401002 Vali Loss: 0.1351374 Test Loss: 0.1269750
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2407517 Vali Loss: 0.1352374 Test Loss: 0.1269742
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2398169 Vali Loss: 0.1349567 Test Loss: 0.1269742
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2394790 Vali Loss: 0.1373602 Test Loss: 0.1269744
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NASDAQ_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.00014094539801590145, mae:0.008727695792913437, rmse:0.011872042901813984, r2:-0.03553462028503418, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0087, RMSE: 0.0119, RÂ²: -0.0355, MAPE: 2343152.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.504 MB of 0.504 MB uploadedwandb: \ 0.504 MB of 0.504 MB uploadedwandb: | 0.504 MB of 0.504 MB uploadedwandb: / 0.504 MB of 0.504 MB uploadedwandb: - 0.504 MB of 0.504 MB uploadedwandb: \ 0.504 MB of 0.504 MB uploadedwandb: | 0.504 MB of 0.504 MB uploadedwandb: / 0.504 MB of 0.504 MB uploadedwandb: - 0.635 MB of 0.851 MB uploaded (0.002 MB deduped)wandb: \ 0.851 MB of 0.851 MB uploaded (0.002 MB deduped)wandb: | 0.851 MB of 0.851 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–‚â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–â–â–‚â–â–‚â–â–â–â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–„â–…â–„â–‚â–â–ƒâ–‚â–ƒâ–…â–ƒâ–„â–ƒâ–ƒâ–ƒâ–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 18
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.12697
wandb:                 train/loss 0.23948
wandb:   val/directional_accuracy 50.63291
wandb:                   val/loss 0.13736
wandb:                    val/mae 0.00873
wandb:                   val/mape 234315200.0
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.03553
wandb:                   val/rmse 0.01187
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/tkmokmtf
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_151833-tkmokmtf/logs
Completed: NASDAQ H=3

Training: Autoformer on NASDAQ for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_152120-y0tqrkki
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/y0tqrkki
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NASDAQ_H5Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/y0tqrkki
>>>>>>>start training : long_term_forecast_Autoformer_NASDAQ_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.3078916 Vali Loss: 0.1532928 Test Loss: 0.1434834
Validation loss decreased (inf --> 0.153293).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2698362 Vali Loss: 0.1565893 Test Loss: 0.1367702
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.30789159966590707, 'val/loss': 0.15329275373369455, 'test/loss': 0.1434834487736225, '_timestamp': 1762780893.5590727}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26983618008014854, 'val/loss': 0.15658933483064175, 'test/loss': 0.1367701767012477, '_timestamp': 1762780900.9429123}).
Epoch: 3, Steps: 133 | Train Loss: 0.2541091 Vali Loss: 0.1505544 Test Loss: 0.1306311
Validation loss decreased (0.153293 --> 0.150554).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2471760 Vali Loss: 0.1446401 Test Loss: 0.1341690
Validation loss decreased (0.150554 --> 0.144640).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2442166 Vali Loss: 0.1360396 Test Loss: 0.1320038
Validation loss decreased (0.144640 --> 0.136040).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2418530 Vali Loss: 0.1358007 Test Loss: 0.1313382
Validation loss decreased (0.136040 --> 0.135801).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2414683 Vali Loss: 0.1457933 Test Loss: 0.1325026
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2414728 Vali Loss: 0.1459199 Test Loss: 0.1321709
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2411675 Vali Loss: 0.1364597 Test Loss: 0.1326957
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2409841 Vali Loss: 0.1449060 Test Loss: 0.1328424
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2406286 Vali Loss: 0.1363967 Test Loss: 0.1328110
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2415897 Vali Loss: 0.1404677 Test Loss: 0.1328807
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2406396 Vali Loss: 0.1464333 Test Loss: 0.1328765
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2408927 Vali Loss: 0.1378581 Test Loss: 0.1328772
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2409503 Vali Loss: 0.1456914 Test Loss: 0.1328813
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2405761 Vali Loss: 0.1370190 Test Loss: 0.1328827
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NASDAQ_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.00014757901953998953, mae:0.00887412391602993, rmse:0.012148210778832436, r2:-0.07877111434936523, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0089, RMSE: 0.0121, RÂ²: -0.0788, MAPE: 5582380.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.511 MB of 0.512 MB uploadedwandb: \ 0.511 MB of 0.512 MB uploadedwandb: | 0.512 MB of 0.512 MB uploadedwandb: / 0.512 MB of 0.727 MB uploadedwandb: - 0.727 MB of 0.727 MB uploadedwandb: \ 0.727 MB of 0.727 MB uploadedwandb: | 0.727 MB of 0.727 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–„â–‚â–…â–„â–…â–…â–…â–…â–…â–…â–…â–…
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–â–â–â–â–â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–â–â–†â–†â–â–…â–â–ƒâ–†â–‚â–†â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.13288
wandb:                 train/loss 0.24058
wandb:   val/directional_accuracy 49.04255
wandb:                   val/loss 0.13702
wandb:                    val/mae 0.00887
wandb:                   val/mape 558238050.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.07877
wandb:                   val/rmse 0.01215
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/y0tqrkki
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_152120-y0tqrkki/logs
Completed: NASDAQ H=5

Training: Autoformer on NASDAQ for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_152343-vnvopu2p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/vnvopu2p
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NASDAQ_H10Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/vnvopu2p
>>>>>>>start training : long_term_forecast_Autoformer_NASDAQ_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2956629 Vali Loss: 0.1606235 Test Loss: 0.1466464
Validation loss decreased (inf --> 0.160623).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2619469 Vali Loss: 0.1530018 Test Loss: 0.1410001
Validation loss decreased (0.160623 --> 0.153002).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2956629313696596, 'val/loss': 0.1606234684586525, 'test/loss': 0.14664635621011257, '_timestamp': 1762781035.9569223}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2619469412287375, 'val/loss': 0.15300177410244942, 'test/loss': 0.14100008457899094, '_timestamp': 1762781042.9863126}).
Epoch: 3, Steps: 133 | Train Loss: 0.2514118 Vali Loss: 0.1475450 Test Loss: 0.1417999
Validation loss decreased (0.153002 --> 0.147545).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2462297 Vali Loss: 0.1453760 Test Loss: 0.1382854
Validation loss decreased (0.147545 --> 0.145376).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2448781 Vali Loss: 0.1435804 Test Loss: 0.1371892
Validation loss decreased (0.145376 --> 0.143580).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2425788 Vali Loss: 0.1433236 Test Loss: 0.1364273
Validation loss decreased (0.143580 --> 0.143324).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2412281 Vali Loss: 0.1579617 Test Loss: 0.1372552
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2427589 Vali Loss: 0.1457282 Test Loss: 0.1373954
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2423894 Vali Loss: 0.1468407 Test Loss: 0.1374587
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2408214 Vali Loss: 0.1420099 Test Loss: 0.1372324
Validation loss decreased (0.143324 --> 0.142010).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2400783 Vali Loss: 0.1532933 Test Loss: 0.1371571
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2416740 Vali Loss: 0.1456997 Test Loss: 0.1371389
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2408832 Vali Loss: 0.1469579 Test Loss: 0.1371396
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2422235 Vali Loss: 0.1451124 Test Loss: 0.1371445
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2412568 Vali Loss: 0.1452245 Test Loss: 0.1371426
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2408320 Vali Loss: 0.1440642 Test Loss: 0.1371426
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2423350 Vali Loss: 0.1460364 Test Loss: 0.1371435
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2402076 Vali Loss: 0.1465831 Test Loss: 0.1371441
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2411662 Vali Loss: 0.1561448 Test Loss: 0.1371440
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2416396 Vali Loss: 0.1576430 Test Loss: 0.1371437
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NASDAQ_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.00015066984633449465, mae:0.009039553813636303, rmse:0.012274764478206635, r2:-0.08844971656799316, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0090, RMSE: 0.0123, RÂ²: -0.0884, MAPE: 8538697.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.603 MB of 0.604 MB uploadedwandb: \ 0.603 MB of 0.604 MB uploadedwandb: | 0.604 MB of 0.604 MB uploadedwandb: / 0.604 MB of 0.604 MB uploadedwandb: - 0.604 MB of 0.820 MB uploadedwandb: \ 0.604 MB of 0.820 MB uploadedwandb: | 0.820 MB of 0.820 MB uploadedwandb: / 0.820 MB of 0.820 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–„â–ƒâ–‚â–ƒâ–‚â–â–â–‚â–â–‚â–‚â–â–‚â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–‚â–‚â–‚â–ˆâ–ƒâ–ƒâ–â–†â–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–‡â–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.13714
wandb:                 train/loss 0.24164
wandb:   val/directional_accuracy 49.61353
wandb:                   val/loss 0.15764
wandb:                    val/mae 0.00904
wandb:                   val/mape 853869700.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.08845
wandb:                   val/rmse 0.01227
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/vnvopu2p
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_152343-vnvopu2p/logs
Completed: NASDAQ H=10

Training: Autoformer on NASDAQ for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_152632-sibsoftb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/sibsoftb
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NASDAQ_H22Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/sibsoftb
>>>>>>>start training : long_term_forecast_Autoformer_NASDAQ_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2921133 Vali Loss: 0.1570803 Test Loss: 0.1333306
Validation loss decreased (inf --> 0.157080).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2602331 Vali Loss: 0.1530315 Test Loss: 0.1369058
Validation loss decreased (0.157080 --> 0.153032).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.292113298042254, 'val/loss': 0.15708032250404358, 'test/loss': 0.13333061231034143, '_timestamp': 1762781204.841502}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26023306381521805, 'val/loss': 0.15303153012480056, 'test/loss': 0.13690576808793203, '_timestamp': 1762781211.974607}).
Epoch: 3, Steps: 132 | Train Loss: 0.2505685 Vali Loss: 0.1578919 Test Loss: 0.1376896
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2458353 Vali Loss: 0.1554997 Test Loss: 0.1371914
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2433620 Vali Loss: 0.1558164 Test Loss: 0.1375083
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2425984 Vali Loss: 0.1559900 Test Loss: 0.1372434
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2410397 Vali Loss: 0.1563216 Test Loss: 0.1377122
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2410517 Vali Loss: 0.1565588 Test Loss: 0.1388177
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2408652 Vali Loss: 0.1557345 Test Loss: 0.1388425
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2407181 Vali Loss: 0.1550191 Test Loss: 0.1389634
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2405054 Vali Loss: 0.1557081 Test Loss: 0.1389797
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2406606 Vali Loss: 0.1551442 Test Loss: 0.1390248
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NASDAQ_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.00015165082004386932, mae:0.009020797908306122, rmse:0.012314658612012863, r2:-0.08379697799682617, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0090, RMSE: 0.0123, RÂ²: -0.0838, MAPE: 4173628.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.653 MB of 0.655 MB uploadedwandb: \ 0.653 MB of 0.655 MB uploadedwandb: | 0.655 MB of 0.655 MB uploadedwandb: / 0.655 MB of 0.869 MB uploadedwandb: - 0.655 MB of 0.869 MB uploadedwandb: \ 0.869 MB of 0.869 MB uploadedwandb: | 0.869 MB of 0.869 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ƒâ–â–‚â–â–ƒâ–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‚â–ƒâ–ƒâ–„â–…â–ƒâ–â–ƒâ–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.13902
wandb:                 train/loss 0.24066
wandb:   val/directional_accuracy 48.38357
wandb:                   val/loss 0.15514
wandb:                    val/mae 0.00902
wandb:                   val/mape 417362850.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.0838
wandb:                   val/rmse 0.01231
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/sibsoftb
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_152632-sibsoftb/logs
Completed: NASDAQ H=22

Training: Autoformer on NASDAQ for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_152831-2lmflxij
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/2lmflxij
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NASDAQ_H50Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/2lmflxij
>>>>>>>start training : long_term_forecast_Autoformer_NASDAQ_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.2907473 Vali Loss: 0.1777451 Test Loss: 0.1392460
Validation loss decreased (inf --> 0.177745).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2621646 Vali Loss: 0.1704644 Test Loss: 0.1455789
Validation loss decreased (0.177745 --> 0.170464).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29074730532187404, 'val/loss': 0.17774507900079092, 'test/loss': 0.1392459695537885, '_timestamp': 1762781324.5043428}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2621645646339113, 'val/loss': 0.1704644113779068, 'test/loss': 0.14557891835769018, '_timestamp': 1762781331.544563}).
Epoch: 3, Steps: 132 | Train Loss: 0.2551756 Vali Loss: 0.1712031 Test Loss: 0.1396488
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2514469 Vali Loss: 0.1707327 Test Loss: 0.1395318
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2505622 Vali Loss: 0.1702022 Test Loss: 0.1396479
Validation loss decreased (0.170464 --> 0.170202).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2483758 Vali Loss: 0.1697294 Test Loss: 0.1389624
Validation loss decreased (0.170202 --> 0.169729).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2471483 Vali Loss: 0.1704302 Test Loss: 0.1382978
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2469043 Vali Loss: 0.1705773 Test Loss: 0.1386239
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2473694 Vali Loss: 0.1708570 Test Loss: 0.1382457
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2477434 Vali Loss: 0.1709099 Test Loss: 0.1383330
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2474669 Vali Loss: 0.1706558 Test Loss: 0.1383920
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2474541 Vali Loss: 0.1706748 Test Loss: 0.1384118
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2469841 Vali Loss: 0.1706098 Test Loss: 0.1383712
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2465617 Vali Loss: 0.1706199 Test Loss: 0.1383819
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2467006 Vali Loss: 0.1706601 Test Loss: 0.1383868
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2475683 Vali Loss: 0.1703924 Test Loss: 0.1383852
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NASDAQ_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.00015331832400988787, mae:0.008986967615783215, rmse:0.012382177636027336, r2:-0.0584946870803833, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0090, RMSE: 0.0124, RÂ²: -0.0585, MAPE: 6177465.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.721 MB of 0.724 MB uploadedwandb: \ 0.721 MB of 0.724 MB uploadedwandb: | 0.721 MB of 0.724 MB uploadedwandb: / 0.724 MB of 0.724 MB uploadedwandb: - 0.724 MB of 0.939 MB uploadedwandb: \ 0.939 MB of 0.939 MB uploadedwandb: | 0.939 MB of 0.939 MB uploadedwandb: / 0.939 MB of 0.939 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‡â–ˆâ–…â–â–ƒâ–â–â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–„â–‚â–â–â–‚â–‚â–‚â–‚â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–ƒâ–â–„â–…â–†â–‡â–…â–…â–…â–…â–…â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.13839
wandb:                 train/loss 0.24757
wandb:   val/directional_accuracy 53.06122
wandb:                   val/loss 0.17039
wandb:                    val/mae 0.00899
wandb:                   val/mape 617746500.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.05849
wandb:                   val/rmse 0.01238
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/2lmflxij
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_152831-2lmflxij/logs
Completed: NASDAQ H=50

Training: Autoformer on NASDAQ for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_153052-feojy41k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/feojy41k
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NASDAQ_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/feojy41k
>>>>>>>start training : long_term_forecast_Autoformer_NASDAQ_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.2994044 Vali Loss: 0.1910509 Test Loss: 0.1444534
Validation loss decreased (inf --> 0.191051).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2723751 Vali Loss: 0.1798799 Test Loss: 0.1481732
Validation loss decreased (0.191051 --> 0.179880).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29940444013247125, 'val/loss': 0.19105091392993928, 'test/loss': 0.14445341974496842, '_timestamp': 1762781465.045498}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.27237514555454256, 'val/loss': 0.17987993955612183, 'test/loss': 0.14817319214344024, '_timestamp': 1762781472.1663575}).
Epoch: 3, Steps: 130 | Train Loss: 0.2659621 Vali Loss: 0.1863929 Test Loss: 0.1468453
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2632432 Vali Loss: 0.1878305 Test Loss: 0.1461131
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2616549 Vali Loss: 0.1867484 Test Loss: 0.1456392
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2604855 Vali Loss: 0.1847121 Test Loss: 0.1463454
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2605143 Vali Loss: 0.1872670 Test Loss: 0.1464350
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2599725 Vali Loss: 0.1859953 Test Loss: 0.1464688
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2598474 Vali Loss: 0.1820821 Test Loss: 0.1464507
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2592532 Vali Loss: 0.1850118 Test Loss: 0.1464524
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2593545 Vali Loss: 0.1860295 Test Loss: 0.1464833
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2595183 Vali Loss: 0.1853293 Test Loss: 0.1464591
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NASDAQ_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.00015787062875460833, mae:0.008816289715468884, rmse:0.01256465818732977, r2:-0.04085516929626465, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0088, RMSE: 0.0126, RÂ²: -0.0409, MAPE: 2920784.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.731 MB of 0.736 MB uploadedwandb: \ 0.731 MB of 0.736 MB uploadedwandb: | 0.731 MB of 0.736 MB uploadedwandb: / 0.736 MB of 0.736 MB uploadedwandb: - 0.736 MB of 0.736 MB uploadedwandb: \ 0.736 MB of 0.951 MB uploadedwandb: | 0.736 MB of 0.951 MB uploadedwandb: / 0.951 MB of 0.951 MB uploadedwandb: - 0.951 MB of 0.951 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–â–…â–†â–†â–†â–†â–†â–†
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–ˆâ–‡â–„â–‡â–†â–â–…â–†â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.14646
wandb:                 train/loss 0.25952
wandb:   val/directional_accuracy 51.81097
wandb:                   val/loss 0.18533
wandb:                    val/mae 0.00882
wandb:                   val/mape 292078450.0
wandb:                    val/mse 0.00016
wandb:                     val/r2 -0.04086
wandb:                   val/rmse 0.01256
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/feojy41k
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_153052-feojy41k/logs
Completed: NASDAQ H=100

Training: Autoformer on ABSA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_153248-06zi10ps
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/06zi10ps
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ABSA_H3  Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/06zi10ps
>>>>>>>start training : long_term_forecast_Autoformer_ABSA_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 238
Epoch: 1, Steps: 133 | Train Loss: 0.3692647 Vali Loss: 0.1924858 Test Loss: 0.1777168
Validation loss decreased (inf --> 0.192486).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.3182573 Vali Loss: 0.1814816 Test Loss: 0.1774408
Validation loss decreased (0.192486 --> 0.181482).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3692646948690701, 'val/loss': 0.19248580560088158, 'test/loss': 0.17771677114069462, '_timestamp': 1762781582.4984303}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.31825733016756247, 'val/loss': 0.18148157931864262, 'test/loss': 0.17744079139083624, '_timestamp': 1762781589.6338317}).
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
Epoch: 3, Steps: 133 | Train Loss: 0.3012766 Vali Loss: 0.1706836 Test Loss: 0.1643100
Validation loss decreased (0.181482 --> 0.170684).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2878439 Vali Loss: 0.1702387 Test Loss: 0.1573691
Validation loss decreased (0.170684 --> 0.170239).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2830745 Vali Loss: 0.1680397 Test Loss: 0.1597084
Validation loss decreased (0.170239 --> 0.168040).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2833292 Vali Loss: 0.1651370 Test Loss: 0.1582468
Validation loss decreased (0.168040 --> 0.165137).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2810369 Vali Loss: 0.1650210 Test Loss: 0.1570865
Validation loss decreased (0.165137 --> 0.165021).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2799936 Vali Loss: 0.1648842 Test Loss: 0.1566086
Validation loss decreased (0.165021 --> 0.164884).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2798818 Vali Loss: 0.1642920 Test Loss: 0.1570904
Validation loss decreased (0.164884 --> 0.164292).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2802890 Vali Loss: 0.1645107 Test Loss: 0.1571789
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2788260 Vali Loss: 0.1653629 Test Loss: 0.1572367
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2798855 Vali Loss: 0.1657527 Test Loss: 0.1573568
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2793977 Vali Loss: 0.1594907 Test Loss: 0.1573530
Validation loss decreased (0.164292 --> 0.159491).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2792972 Vali Loss: 0.1624445 Test Loss: 0.1573421
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2787628 Vali Loss: 0.1636367 Test Loss: 0.1573508
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2799058 Vali Loss: 0.1651807 Test Loss: 0.1573378
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2803882 Vali Loss: 0.1663999 Test Loss: 0.1573378
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2781367 Vali Loss: 0.1674901 Test Loss: 0.1573499
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2797396 Vali Loss: 0.1664144 Test Loss: 0.1573493
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2783588 Vali Loss: 0.1698015 Test Loss: 0.1573491
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2782941 Vali Loss: 0.1667398 Test Loss: 0.1573496
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2800493 Vali Loss: 0.1627274 Test Loss: 0.1573495
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2792887 Vali Loss: 0.1640847 Test Loss: 0.1573492
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ABSA_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 238
test shape: (238, 3, 1) (238, 3, 1)
test shape: (238, 3, 1) (238, 3, 1)


	mse:0.00047577227815054357, mae:0.016750916838645935, rmse:0.021812204271554947, r2:-0.04482829570770264, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0168, RMSE: 0.0218, RÂ²: -0.0448, MAPE: 1.64%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.497 MB of 0.497 MB uploadedwandb: \ 0.497 MB of 0.497 MB uploadedwandb: | 0.497 MB of 0.497 MB uploadedwandb: / 0.497 MB of 0.497 MB uploadedwandb: - 0.497 MB of 0.497 MB uploadedwandb: \ 0.497 MB of 0.497 MB uploadedwandb: | 0.497 MB of 0.497 MB uploadedwandb: / 0.628 MB of 0.845 MB uploaded (0.002 MB deduped)wandb: - 0.845 MB of 0.845 MB uploaded (0.002 MB deduped)wandb: \ 0.845 MB of 0.845 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–„â–‚â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–„â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–â–â–‚â–‚â–â–â–â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ˆâ–†â–…â–„â–„â–„â–„â–…â–…â–â–ƒâ–„â–…â–…â–†â–…â–‡â–†â–ƒâ–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.15735
wandb:                 train/loss 0.27929
wandb:   val/directional_accuracy 49.36975
wandb:                   val/loss 0.16408
wandb:                    val/mae 0.01675
wandb:                   val/mape 164.11933
wandb:                    val/mse 0.00048
wandb:                     val/r2 -0.04483
wandb:                   val/rmse 0.02181
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/06zi10ps
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_153248-06zi10ps/logs
Completed: ABSA H=3

Training: Autoformer on ABSA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_153601-qc1jsosr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/qc1jsosr
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ABSA_H5  Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/qc1jsosr
>>>>>>>start training : long_term_forecast_Autoformer_ABSA_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 236
Epoch: 1, Steps: 133 | Train Loss: 0.3658021 Vali Loss: 0.1916354 Test Loss: 0.1815151
Validation loss decreased (inf --> 0.191635).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.3232521 Vali Loss: 0.1934849 Test Loss: 0.1755852
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3658020918754707, 'val/loss': 0.19163536839187145, 'test/loss': 0.18151511810719967, '_timestamp': 1762781774.5994732}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3232520870248178, 'val/loss': 0.1934848688542843, 'test/loss': 0.17558523267507553, '_timestamp': 1762781781.695558}).
Epoch: 3, Steps: 133 | Train Loss: 0.3039387 Vali Loss: 0.1780597 Test Loss: 0.1682204
Validation loss decreased (0.191635 --> 0.178060).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2966946 Vali Loss: 0.1784855 Test Loss: 0.1675169
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2923192 Vali Loss: 0.1773519 Test Loss: 0.1683711
Validation loss decreased (0.178060 --> 0.177352).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2904385 Vali Loss: 0.1695609 Test Loss: 0.1664133
Validation loss decreased (0.177352 --> 0.169561).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2891267 Vali Loss: 0.1739574 Test Loss: 0.1653805
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2876386 Vali Loss: 0.1765959 Test Loss: 0.1651119
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2895506 Vali Loss: 0.1739135 Test Loss: 0.1651886
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2873459 Vali Loss: 0.1702052 Test Loss: 0.1651070
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2877255 Vali Loss: 0.1744224 Test Loss: 0.1651011
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2874809 Vali Loss: 0.1750714 Test Loss: 0.1650785
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2870409 Vali Loss: 0.1729369 Test Loss: 0.1650781
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2868446 Vali Loss: 0.1741955 Test Loss: 0.1650856
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2871467 Vali Loss: 0.1737856 Test Loss: 0.1650831
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2878810 Vali Loss: 0.1741242 Test Loss: 0.1650249
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ABSA_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
test shape: (236, 5, 1) (236, 5, 1)
test shape: (236, 5, 1) (236, 5, 1)


	mse:0.0004880137275904417, mae:0.016756368800997734, rmse:0.022091032937169075, r2:-0.06548011302947998, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0168, RMSE: 0.0221, RÂ²: -0.0655, MAPE: 1.45%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.533 MB of 0.534 MB uploadedwandb: \ 0.533 MB of 0.534 MB uploadedwandb: | 0.533 MB of 0.534 MB uploadedwandb: / 0.534 MB of 0.534 MB uploadedwandb: - 0.534 MB of 0.534 MB uploadedwandb: \ 0.534 MB of 0.749 MB uploadedwandb: | 0.749 MB of 0.749 MB uploadedwandb: / 0.749 MB of 0.749 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–†â–ˆâ–„â–‚â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–‚â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ˆâ–‡â–â–„â–‡â–„â–‚â–…â–…â–„â–…â–„â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.16502
wandb:                 train/loss 0.28788
wandb:   val/directional_accuracy 49.57627
wandb:                   val/loss 0.17412
wandb:                    val/mae 0.01676
wandb:                   val/mape 144.51367
wandb:                    val/mse 0.00049
wandb:                     val/r2 -0.06548
wandb:                   val/rmse 0.02209
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/qc1jsosr
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_153601-qc1jsosr/logs
Completed: ABSA H=5

Training: Autoformer on ABSA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_153823-f7kidk2z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/f7kidk2z
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ABSA_H10 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/f7kidk2z
>>>>>>>start training : long_term_forecast_Autoformer_ABSA_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 231
Epoch: 1, Steps: 133 | Train Loss: 0.3583406 Vali Loss: 0.1839476 Test Loss: 0.1734661
Validation loss decreased (inf --> 0.183948).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.3207987 Vali Loss: 0.1966218 Test Loss: 0.1705302
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.35834056441497086, 'val/loss': 0.1839476376771927, 'test/loss': 0.17346611339598894, '_timestamp': 1762781917.41535}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3207986887012209, 'val/loss': 0.19662184827029705, 'test/loss': 0.17053017392754555, '_timestamp': 1762781924.5019407}).
Epoch: 3, Steps: 133 | Train Loss: 0.3069896 Vali Loss: 0.1834359 Test Loss: 0.1696702
Validation loss decreased (0.183948 --> 0.183436).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.3005026 Vali Loss: 0.1805376 Test Loss: 0.1641874
Validation loss decreased (0.183436 --> 0.180538).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2988613 Vali Loss: 0.1772732 Test Loss: 0.1654122
Validation loss decreased (0.180538 --> 0.177273).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2940462 Vali Loss: 0.1823876 Test Loss: 0.1650040
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2936456 Vali Loss: 0.1843862 Test Loss: 0.1649433
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2938817 Vali Loss: 0.1774456 Test Loss: 0.1647442
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2940989 Vali Loss: 0.1813872 Test Loss: 0.1645165
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2923557 Vali Loss: 0.1799271 Test Loss: 0.1644667
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2928182 Vali Loss: 0.1786923 Test Loss: 0.1644847
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2919658 Vali Loss: 0.1765594 Test Loss: 0.1645425
Validation loss decreased (0.177273 --> 0.176559).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2935045 Vali Loss: 0.1826334 Test Loss: 0.1645200
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2942527 Vali Loss: 0.1812940 Test Loss: 0.1645283
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2922845 Vali Loss: 0.1772278 Test Loss: 0.1645314
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2912177 Vali Loss: 0.1759346 Test Loss: 0.1645374
Validation loss decreased (0.176559 --> 0.175935).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2951419 Vali Loss: 0.1823514 Test Loss: 0.1645372
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2929605 Vali Loss: 0.1803738 Test Loss: 0.1645375
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2929698 Vali Loss: 0.1815197 Test Loss: 0.1645376
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2915223 Vali Loss: 0.1853238 Test Loss: 0.1645374
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2926382 Vali Loss: 0.1893085 Test Loss: 0.1645379
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2924323 Vali Loss: 0.1844834 Test Loss: 0.1645377
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2956718 Vali Loss: 0.1791989 Test Loss: 0.1645376
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2958507 Vali Loss: 0.1790095 Test Loss: 0.1645378
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2932284 Vali Loss: 0.1781291 Test Loss: 0.1645378
EarlyStopping counter: 9 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2926204 Vali Loss: 0.1808468 Test Loss: 0.1645379
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ABSA_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 231
test shape: (231, 10, 1) (231, 10, 1)
test shape: (231, 10, 1) (231, 10, 1)


	mse:0.00048698330647312105, mae:0.016828317195177078, rmse:0.02206769771873951, r2:-0.054816246032714844, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0168, RMSE: 0.0221, RÂ²: -0.0548, MAPE: 1.69%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.575 MB of 0.576 MB uploadedwandb: \ 0.575 MB of 0.576 MB uploadedwandb: | 0.576 MB of 0.576 MB uploadedwandb: / 0.576 MB of 0.576 MB uploadedwandb: - 0.576 MB of 0.793 MB uploadedwandb: \ 0.793 MB of 0.793 MB uploadedwandb: | 0.793 MB of 0.793 MB uploadedwandb: / 0.793 MB of 0.793 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–ƒâ–‚â–‚â–â–‚â–‚â–ƒâ–ƒâ–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–ƒâ–‚â–„â–…â–‚â–„â–ƒâ–‚â–â–…â–„â–‚â–â–„â–ƒâ–„â–†â–ˆâ–…â–ƒâ–ƒâ–‚â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 25
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.16454
wandb:                 train/loss 0.29262
wandb:   val/directional_accuracy 51.85185
wandb:                   val/loss 0.18085
wandb:                    val/mae 0.01683
wandb:                   val/mape 168.82544
wandb:                    val/mse 0.00049
wandb:                     val/r2 -0.05482
wandb:                   val/rmse 0.02207
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/f7kidk2z
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_153823-f7kidk2z/logs
Completed: ABSA H=10

Training: Autoformer on ABSA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_154152-ndlz1f10
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/ndlz1f10
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ABSA_H22 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/ndlz1f10
>>>>>>>start training : long_term_forecast_Autoformer_ABSA_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 219
Epoch: 1, Steps: 132 | Train Loss: 0.3592377 Vali Loss: 0.1831323 Test Loss: 0.1657674
Validation loss decreased (inf --> 0.183132).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.3257919 Vali Loss: 0.1831232 Test Loss: 0.1632767
Validation loss decreased (0.183132 --> 0.183123).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.35923767654281674, 'val/loss': 0.1831323355436325, 'test/loss': 0.16576743445226125, '_timestamp': 1762782125.4275043}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3257919114195939, 'val/loss': 0.1831232351916177, 'test/loss': 0.16327671813113348, '_timestamp': 1762782132.5888088}).
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
Epoch: 3, Steps: 132 | Train Loss: 0.3116956 Vali Loss: 0.1922399 Test Loss: 0.1683115
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.3044960 Vali Loss: 0.1883251 Test Loss: 0.1679014
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2994029 Vali Loss: 0.1892165 Test Loss: 0.1700559
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2970785 Vali Loss: 0.1890856 Test Loss: 0.1690449
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2959044 Vali Loss: 0.1891150 Test Loss: 0.1683014
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2963659 Vali Loss: 0.1891980 Test Loss: 0.1677555
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2951563 Vali Loss: 0.1886041 Test Loss: 0.1682042
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2958131 Vali Loss: 0.1885316 Test Loss: 0.1682355
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2949493 Vali Loss: 0.1887130 Test Loss: 0.1680889
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2954089 Vali Loss: 0.1890901 Test Loss: 0.1680905
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ABSA_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 219
test shape: (219, 22, 1) (219, 22, 1)
test shape: (219, 22, 1) (219, 22, 1)


	mse:0.00048665812937542796, mae:0.01692713238298893, rmse:0.022060329094529152, r2:-0.038710594177246094, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0169, RMSE: 0.0221, RÂ²: -0.0387, MAPE: 1.71%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.657 MB of 0.658 MB uploadedwandb: \ 0.657 MB of 0.658 MB uploadedwandb: | 0.657 MB of 0.658 MB uploadedwandb: / 0.657 MB of 0.658 MB uploadedwandb: - 0.658 MB of 0.658 MB uploadedwandb: \ 0.658 MB of 0.658 MB uploadedwandb: | 0.658 MB of 0.873 MB uploadedwandb: / 0.873 MB of 0.873 MB uploadedwandb: - 0.873 MB of 0.873 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ƒâ–â–ˆâ–…â–ƒâ–â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–ƒâ–‚â–‚â–ƒâ–â–â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.16809
wandb:                 train/loss 0.29541
wandb:   val/directional_accuracy 51.1633
wandb:                   val/loss 0.18909
wandb:                    val/mae 0.01693
wandb:                   val/mape 171.13913
wandb:                    val/mse 0.00049
wandb:                     val/r2 -0.03871
wandb:                   val/rmse 0.02206
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/ndlz1f10
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_154152-ndlz1f10/logs
Completed: ABSA H=22

Training: Autoformer on ABSA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_154350-qb3s1lc8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/qb3s1lc8
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ABSA_H50 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/qb3s1lc8
>>>>>>>start training : long_term_forecast_Autoformer_ABSA_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 191
Epoch: 1, Steps: 132 | Train Loss: 0.3697434 Vali Loss: 0.1875240 Test Loss: 0.1688281
Validation loss decreased (inf --> 0.187524).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.3386079 Vali Loss: 0.1945721 Test Loss: 0.1704198
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3697433897265882, 'val/loss': 0.1875239834189415, 'test/loss': 0.16882810990015665, '_timestamp': 1762782242.6277554}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3386079434192542, 'val/loss': 0.19457214574019113, 'test/loss': 0.17041984821359316, '_timestamp': 1762782249.683204}).
Epoch: 3, Steps: 132 | Train Loss: 0.3270043 Vali Loss: 0.1997279 Test Loss: 0.1724364
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.3219099 Vali Loss: 0.1997447 Test Loss: 0.1738069
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.3178896 Vali Loss: 0.1991023 Test Loss: 0.1755820
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.3161875 Vali Loss: 0.2037740 Test Loss: 0.1732122
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.3148373 Vali Loss: 0.2028170 Test Loss: 0.1733397
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.3141447 Vali Loss: 0.2025751 Test Loss: 0.1737735
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.3145375 Vali Loss: 0.2022147 Test Loss: 0.1734984
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.3153037 Vali Loss: 0.2024015 Test Loss: 0.1733687
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.3152814 Vali Loss: 0.2025765 Test Loss: 0.1732846
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ABSA_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 191
test shape: (191, 50, 1) (191, 50, 1)
test shape: (191, 50, 1) (191, 50, 1)


	mse:0.0005199767183512449, mae:0.017734719440340996, rmse:0.02280299738049507, r2:-0.0688333511352539, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0177, RMSE: 0.0228, RÂ²: -0.0688, MAPE: 1.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.769 MB of 0.771 MB uploadedwandb: \ 0.769 MB of 0.771 MB uploadedwandb: | 0.771 MB of 0.771 MB uploadedwandb: / 0.771 MB of 0.986 MB uploadedwandb: - 0.986 MB of 0.986 MB uploadedwandb: \ 0.986 MB of 0.986 MB uploadedwandb: | 0.986 MB of 0.986 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–„â–ˆâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–‚â–â–ˆâ–‡â–†â–†â–†â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.17328
wandb:                 train/loss 0.31528
wandb:   val/directional_accuracy 50.72123
wandb:                   val/loss 0.20258
wandb:                    val/mae 0.01773
wandb:                   val/mape 149.82663
wandb:                    val/mse 0.00052
wandb:                     val/r2 -0.06883
wandb:                   val/rmse 0.0228
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/qb3s1lc8
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_154350-qb3s1lc8/logs
Completed: ABSA H=50

Training: Autoformer on ABSA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_154535-85qz8n43
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/85qz8n43
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ABSA_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/85qz8n43
>>>>>>>start training : long_term_forecast_Autoformer_ABSA_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 141
Epoch: 1, Steps: 130 | Train Loss: 0.4089614 Vali Loss: 0.1916966 Test Loss: 0.1700890
Validation loss decreased (inf --> 0.191697).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.3808503 Vali Loss: 0.2052684 Test Loss: 0.1668179
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.40896144325916584, 'val/loss': 0.19169657826423644, 'test/loss': 0.17008899301290512, '_timestamp': 1762782348.5489156}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3808502586988302, 'val/loss': 0.2052684336900711, 'test/loss': 0.16681790202856064, '_timestamp': 1762782355.5101328}).
Epoch: 3, Steps: 130 | Train Loss: 0.3722792 Vali Loss: 0.2049174 Test Loss: 0.1676954
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.3682096 Vali Loss: 0.2044891 Test Loss: 0.1667171
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.3657292 Vali Loss: 0.2037420 Test Loss: 0.1681509
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.3645717 Vali Loss: 0.2050381 Test Loss: 0.1686760
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.3650544 Vali Loss: 0.2056844 Test Loss: 0.1687951
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.3635624 Vali Loss: 0.2049161 Test Loss: 0.1690609
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.3635838 Vali Loss: 0.2053324 Test Loss: 0.1690118
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.3631470 Vali Loss: 0.2042083 Test Loss: 0.1690514
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.3633417 Vali Loss: 0.2047963 Test Loss: 0.1690906
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ABSA_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 141
test shape: (141, 100, 1) (141, 100, 1)
test shape: (141, 100, 1) (141, 100, 1)


	mse:0.0005378985079005361, mae:0.017688605934381485, rmse:0.023192638531327248, r2:-0.04237067699432373, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0177, RMSE: 0.0232, RÂ²: -0.0424, MAPE: 1.24%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.806 MB of 0.811 MB uploadedwandb: \ 0.806 MB of 0.811 MB uploadedwandb: | 0.806 MB of 0.811 MB uploadedwandb: / 0.811 MB of 0.811 MB uploadedwandb: - 0.811 MB of 0.811 MB uploadedwandb: \ 0.811 MB of 1.026 MB uploadedwandb: | 1.026 MB of 1.026 MB uploadedwandb: / 1.026 MB of 1.026 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–„â–â–…â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–„â–â–†â–ˆâ–…â–‡â–ƒâ–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.16909
wandb:                 train/loss 0.36334
wandb:   val/directional_accuracy 49.02214
wandb:                   val/loss 0.2048
wandb:                    val/mae 0.01769
wandb:                   val/mape 123.8525
wandb:                    val/mse 0.00054
wandb:                     val/r2 -0.04237
wandb:                   val/rmse 0.02319
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/85qz8n43
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_154535-85qz8n43/logs
Completed: ABSA H=100

Training: Autoformer on SASOL for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_154720-gckkhr1b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/gckkhr1b
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SASOL_H3 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/gckkhr1b
>>>>>>>start training : long_term_forecast_Autoformer_SASOL_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 3
============================================================
train 3772
val 211
test 212
Epoch: 1, Steps: 118 | Train Loss: 0.2944106 Vali Loss: 0.1201981 Test Loss: 0.1622134
Validation loss decreased (inf --> 0.120198).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2368348 Vali Loss: 0.1210809 Test Loss: 0.1679778
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2944105719610796, 'val/loss': 0.12019808909722737, 'test/loss': 0.1622133765901838, '_timestamp': 1762782451.9469895}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23683476549083904, 'val/loss': 0.12108085091624941, 'test/loss': 0.16797779606921331, '_timestamp': 1762782458.3953984}).
Epoch: 3, Steps: 118 | Train Loss: 0.2197720 Vali Loss: 0.1156980 Test Loss: 0.1550701
Validation loss decreased (0.120198 --> 0.115698).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2125457 Vali Loss: 0.1131029 Test Loss: 0.1502747
Validation loss decreased (0.115698 --> 0.113103).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2100779 Vali Loss: 0.1091797 Test Loss: 0.1470781
Validation loss decreased (0.113103 --> 0.109180).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2066099 Vali Loss: 0.1099173 Test Loss: 0.1475049
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2061498 Vali Loss: 0.1111042 Test Loss: 0.1460812
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2054694 Vali Loss: 0.1127263 Test Loss: 0.1466848
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2057748 Vali Loss: 0.1089961 Test Loss: 0.1467721
Validation loss decreased (0.109180 --> 0.108996).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2040122 Vali Loss: 0.1096372 Test Loss: 0.1467378
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2054496 Vali Loss: 0.1095052 Test Loss: 0.1468360
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2057607 Vali Loss: 0.1071935 Test Loss: 0.1467738
Validation loss decreased (0.108996 --> 0.107193).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2046240 Vali Loss: 0.1092274 Test Loss: 0.1467902
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2037950 Vali Loss: 0.1085642 Test Loss: 0.1467616
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.2041834 Vali Loss: 0.1094528 Test Loss: 0.1467575
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.2049342 Vali Loss: 0.1072227 Test Loss: 0.1467583
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.2032099 Vali Loss: 0.1081539 Test Loss: 0.1467587
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.2045289 Vali Loss: 0.1093538 Test Loss: 0.1467586
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.2045313 Vali Loss: 0.1101351 Test Loss: 0.1467586
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.2041721 Vali Loss: 0.1101695 Test Loss: 0.1467586
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 118 | Train Loss: 0.2039926 Vali Loss: 0.1130060 Test Loss: 0.1467587
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 118 | Train Loss: 0.2045099 Vali Loss: 0.1101400 Test Loss: 0.1467586
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SASOL_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 212
test shape: (212, 3, 1) (212, 3, 1)
test shape: (212, 3, 1) (212, 3, 1)


	mse:0.0022755374666303396, mae:0.03518939018249512, rmse:0.047702595591545105, r2:-0.03292489051818848, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0352, RMSE: 0.0477, RÂ²: -0.0329, MAPE: 9007131.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.467 MB of 0.468 MB uploadedwandb: \ 0.467 MB of 0.468 MB uploadedwandb: | 0.467 MB of 0.468 MB uploadedwandb: / 0.468 MB of 0.468 MB uploadedwandb: - 0.468 MB of 0.468 MB uploadedwandb: \ 0.468 MB of 0.468 MB uploadedwandb: | 0.468 MB of 0.468 MB uploadedwandb: / 0.468 MB of 0.468 MB uploadedwandb: - 0.598 MB of 0.815 MB uploaded (0.002 MB deduped)wandb: \ 0.815 MB of 0.815 MB uploaded (0.002 MB deduped)wandb: | 0.815 MB of 0.815 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–‚â–â–‚â–‚â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–ƒâ–ƒâ–„â–†â–‚â–ƒâ–ƒâ–â–ƒâ–‚â–ƒâ–â–‚â–ƒâ–ƒâ–ƒâ–†â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.14676
wandb:                 train/loss 0.20451
wandb:   val/directional_accuracy 50.70755
wandb:                   val/loss 0.11014
wandb:                    val/mae 0.03519
wandb:                   val/mape 900713100.0
wandb:                    val/mse 0.00228
wandb:                     val/r2 -0.03292
wandb:                   val/rmse 0.0477
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/gckkhr1b
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_154720-gckkhr1b/logs
Completed: SASOL H=3

Training: Autoformer on SASOL for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_155009-pam4o4qp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/pam4o4qp
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SASOL_H5 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/pam4o4qp
>>>>>>>start training : long_term_forecast_Autoformer_SASOL_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 5
============================================================
train 3770
val 209
test 210
Epoch: 1, Steps: 118 | Train Loss: 0.2916861 Vali Loss: 0.1317952 Test Loss: 0.1683561
Validation loss decreased (inf --> 0.131795).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2411895 Vali Loss: 0.1188391 Test Loss: 0.1705146
Validation loss decreased (0.131795 --> 0.118839).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2916860997171725, 'val/loss': 0.1317952030471393, 'test/loss': 0.1683561024921281, '_timestamp': 1762782621.5937676}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2411895024321847, 'val/loss': 0.11883908723081861, 'test/loss': 0.17051464744976588, '_timestamp': 1762782628.0517411}).
Epoch: 3, Steps: 118 | Train Loss: 0.2215065 Vali Loss: 0.1097125 Test Loss: 0.1650555
Validation loss decreased (0.118839 --> 0.109712).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2150615 Vali Loss: 0.1134729 Test Loss: 0.1583583
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2113010 Vali Loss: 0.1127735 Test Loss: 0.1581230
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2093966 Vali Loss: 0.1121299 Test Loss: 0.1565628
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2073469 Vali Loss: 0.1110668 Test Loss: 0.1556881
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2076998 Vali Loss: 0.1103279 Test Loss: 0.1555187
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2075700 Vali Loss: 0.1105714 Test Loss: 0.1553959
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2074805 Vali Loss: 0.1102908 Test Loss: 0.1553594
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2071883 Vali Loss: 0.1129143 Test Loss: 0.1553510
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2068411 Vali Loss: 0.1112912 Test Loss: 0.1554160
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2083132 Vali Loss: 0.1124611 Test Loss: 0.1554118
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SASOL_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 210
test shape: (210, 5, 1) (210, 5, 1)
test shape: (210, 5, 1) (210, 5, 1)


	mse:0.0022932891733944416, mae:0.03518415242433548, rmse:0.04788829758763313, r2:-0.033292531967163086, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0352, RMSE: 0.0479, RÂ²: -0.0333, MAPE: 16378913.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.506 MB of 0.506 MB uploadedwandb: \ 0.506 MB of 0.506 MB uploadedwandb: | 0.506 MB of 0.506 MB uploadedwandb: / 0.506 MB of 0.506 MB uploadedwandb: - 0.506 MB of 0.506 MB uploadedwandb: \ 0.506 MB of 0.721 MB uploadedwandb: | 0.721 MB of 0.721 MB uploadedwandb: / 0.721 MB of 0.721 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–ƒâ–‚â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ˆâ–‡â–†â–„â–‚â–ƒâ–‚â–‡â–„â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.15541
wandb:                 train/loss 0.20831
wandb:   val/directional_accuracy 55.47619
wandb:                   val/loss 0.11246
wandb:                    val/mae 0.03518
wandb:                   val/mape 1637891300.0
wandb:                    val/mse 0.00229
wandb:                     val/r2 -0.03329
wandb:                   val/rmse 0.04789
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/pam4o4qp
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_155009-pam4o4qp/logs
Completed: SASOL H=5

Training: Autoformer on SASOL for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_155203-62mqyfj1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/62mqyfj1
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SASOL_H10Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/62mqyfj1
>>>>>>>start training : long_term_forecast_Autoformer_SASOL_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 10
============================================================
train 3765
val 204
test 205
Epoch: 1, Steps: 118 | Train Loss: 0.2812423 Vali Loss: 0.1219966 Test Loss: 0.1640204
Validation loss decreased (inf --> 0.121997).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2374638 Vali Loss: 0.1228576 Test Loss: 0.1716452
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.28124232299752155, 'val/loss': 0.12199659219809941, 'test/loss': 0.16402037335293634, '_timestamp': 1762782735.9197192}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2374638454136202, 'val/loss': 0.12285757809877396, 'test/loss': 0.17164523260934011, '_timestamp': 1762782742.4651113}).
Epoch: 3, Steps: 118 | Train Loss: 0.2234781 Vali Loss: 0.1118723 Test Loss: 0.1631069
Validation loss decreased (0.121997 --> 0.111872).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2181043 Vali Loss: 0.1117927 Test Loss: 0.1613668
Validation loss decreased (0.111872 --> 0.111793).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2148525 Vali Loss: 0.1107735 Test Loss: 0.1625779
Validation loss decreased (0.111793 --> 0.110774).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2129755 Vali Loss: 0.1110337 Test Loss: 0.1621272
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2127129 Vali Loss: 0.1169964 Test Loss: 0.1617420
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2121700 Vali Loss: 0.1123010 Test Loss: 0.1612159
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2126836 Vali Loss: 0.1115171 Test Loss: 0.1610408
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2119989 Vali Loss: 0.1116433 Test Loss: 0.1614253
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2121433 Vali Loss: 0.1144976 Test Loss: 0.1613977
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2123810 Vali Loss: 0.1122520 Test Loss: 0.1614236
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2131438 Vali Loss: 0.1146734 Test Loss: 0.1613910
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2124330 Vali Loss: 0.1137640 Test Loss: 0.1613854
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.2123801 Vali Loss: 0.1161170 Test Loss: 0.1613897
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SASOL_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 205
test shape: (205, 10, 1) (205, 10, 1)
test shape: (205, 10, 1) (205, 10, 1)


	mse:0.0022931708954274654, mae:0.035255737602710724, rmse:0.04788706451654434, r2:-0.03309905529022217, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0353, RMSE: 0.0479, RÂ²: -0.0331, MAPE: 11285900.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.549 MB of 0.550 MB uploadedwandb: \ 0.549 MB of 0.550 MB uploadedwandb: | 0.549 MB of 0.550 MB uploadedwandb: / 0.550 MB of 0.550 MB uploadedwandb: - 0.550 MB of 0.550 MB uploadedwandb: \ 0.550 MB of 0.765 MB uploadedwandb: | 0.765 MB of 0.765 MB uploadedwandb: / 0.765 MB of 0.765 MB uploadedwandb: - 0.765 MB of 0.765 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–†â–…â–ƒâ–‚â–â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–‚â–â–â–ˆâ–ƒâ–‚â–‚â–…â–ƒâ–…â–„â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.16139
wandb:                 train/loss 0.21238
wandb:   val/directional_accuracy 51.81572
wandb:                   val/loss 0.11612
wandb:                    val/mae 0.03526
wandb:                   val/mape 1128590000.0
wandb:                    val/mse 0.00229
wandb:                     val/r2 -0.0331
wandb:                   val/rmse 0.04789
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/62mqyfj1
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_155203-62mqyfj1/logs
Completed: SASOL H=10

Training: Autoformer on SASOL for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_155409-9f08isnc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/9f08isnc
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SASOL_H22Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/9f08isnc
>>>>>>>start training : long_term_forecast_Autoformer_SASOL_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 22
============================================================
train 3753
val 192
test 193
Epoch: 1, Steps: 118 | Train Loss: 0.2886763 Vali Loss: 0.1130940 Test Loss: 0.1686460
Validation loss decreased (inf --> 0.113094).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2529160 Vali Loss: 0.1223091 Test Loss: 0.1715030
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2886762913253348, 'val/loss': 0.11309404671192169, 'test/loss': 0.16864596413714544, '_timestamp': 1762782861.4416637}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25291596706640923, 'val/loss': 0.12230908125638962, 'test/loss': 0.17150303295680455, '_timestamp': 1762782867.9797318}).
Epoch: 3, Steps: 118 | Train Loss: 0.2380586 Vali Loss: 0.1158502 Test Loss: 0.1690588
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2314391 Vali Loss: 0.1154136 Test Loss: 0.1748250
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2274076 Vali Loss: 0.1151952 Test Loss: 0.1757516
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2263356 Vali Loss: 0.1157058 Test Loss: 0.1743428
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2240387 Vali Loss: 0.1154363 Test Loss: 0.1747166
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2231906 Vali Loss: 0.1156857 Test Loss: 0.1759329
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2234553 Vali Loss: 0.1155110 Test Loss: 0.1753655
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2242860 Vali Loss: 0.1156298 Test Loss: 0.1753187
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2234969 Vali Loss: 0.1155933 Test Loss: 0.1752966
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SASOL_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 193
test shape: (193, 22, 1) (193, 22, 1)
test shape: (193, 22, 1) (193, 22, 1)


	mse:0.0023865560069680214, mae:0.035778775811195374, rmse:0.04885239154100418, r2:-0.06346702575683594, dtw:Not calculated


VAL - MSE: 0.0024, MAE: 0.0358, RMSE: 0.0489, RÂ²: -0.0635, MAPE: 13317624.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.609 MB of 0.611 MB uploadedwandb: \ 0.609 MB of 0.611 MB uploadedwandb: | 0.609 MB of 0.611 MB uploadedwandb: / 0.611 MB of 0.611 MB uploadedwandb: - 0.611 MB of 0.611 MB uploadedwandb: \ 0.611 MB of 0.825 MB uploadedwandb: | 0.611 MB of 0.825 MB uploadedwandb: / 0.825 MB of 0.825 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–‡â–ˆâ–†â–‡â–ˆâ–‡â–‡â–‡
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–â–†â–„â–†â–„â–†â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.1753
wandb:                 train/loss 0.2235
wandb:   val/directional_accuracy 47.91512
wandb:                   val/loss 0.11559
wandb:                    val/mae 0.03578
wandb:                   val/mape 1331762400.0
wandb:                    val/mse 0.00239
wandb:                     val/r2 -0.06347
wandb:                   val/rmse 0.04885
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/9f08isnc
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_155409-9f08isnc/logs
Completed: SASOL H=22

Training: Autoformer on SASOL for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_155550-pph5dv52
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/pph5dv52
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SASOL_H50Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/pph5dv52
>>>>>>>start training : long_term_forecast_Autoformer_SASOL_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 50
============================================================
train 3725
val 164
test 165
Epoch: 1, Steps: 117 | Train Loss: 0.3166530 Vali Loss: 0.1080585 Test Loss: 0.1893589
Validation loss decreased (inf --> 0.108058).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 117 | Train Loss: 0.2692833 Vali Loss: 0.1186239 Test Loss: 0.2219068
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3166530478713859, 'val/loss': 0.10805849855144818, 'test/loss': 0.1893588751554489, '_timestamp': 1762782962.6078472}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2692833041024004, 'val/loss': 0.11862393096089363, 'test/loss': 0.22190682838360468, '_timestamp': 1762782969.0471375}).
Epoch: 3, Steps: 117 | Train Loss: 0.2513074 Vali Loss: 0.1258975 Test Loss: 0.2180268
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 117 | Train Loss: 0.2425701 Vali Loss: 0.1157274 Test Loss: 0.2199904
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 117 | Train Loss: 0.2387427 Vali Loss: 0.1234227 Test Loss: 0.2190358
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 117 | Train Loss: 0.2372944 Vali Loss: 0.1231425 Test Loss: 0.2200498
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 117 | Train Loss: 0.2356016 Vali Loss: 0.1238181 Test Loss: 0.2190918
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 117 | Train Loss: 0.2354960 Vali Loss: 0.1191316 Test Loss: 0.2204830
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 117 | Train Loss: 0.2350741 Vali Loss: 0.1218467 Test Loss: 0.2213648
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 117 | Train Loss: 0.2350036 Vali Loss: 0.1218704 Test Loss: 0.2217984
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 117 | Train Loss: 0.2345115 Vali Loss: 0.1182030 Test Loss: 0.2215606
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SASOL_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 165
test shape: (165, 50, 1) (165, 50, 1)
test shape: (165, 50, 1) (165, 50, 1)


	mse:0.0021906548645347357, mae:0.03470426797866821, rmse:0.046804431825876236, r2:-0.06925761699676514, dtw:Not calculated


VAL - MSE: 0.0022, MAE: 0.0347, RMSE: 0.0468, RÂ²: -0.0693, MAPE: 12549738.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.650 MB of 0.652 MB uploadedwandb: \ 0.650 MB of 0.652 MB uploadedwandb: | 0.652 MB of 0.652 MB uploadedwandb: / 0.652 MB of 0.867 MB uploadedwandb: - 0.867 MB of 0.867 MB uploadedwandb: \ 0.867 MB of 0.867 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–…â–ƒâ–…â–ƒâ–†â–‡â–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–†â–†â–‡â–ƒâ–…â–…â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.22156
wandb:                 train/loss 0.23451
wandb:   val/directional_accuracy 47.91589
wandb:                   val/loss 0.1182
wandb:                    val/mae 0.0347
wandb:                   val/mape 1254973800.0
wandb:                    val/mse 0.00219
wandb:                     val/r2 -0.06926
wandb:                   val/rmse 0.0468
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/pph5dv52
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_155550-pph5dv52/logs
Completed: SASOL H=50

Training: Autoformer on SASOL for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_155726-zje5bwi6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/zje5bwi6
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SASOL_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/zje5bwi6
>>>>>>>start training : long_term_forecast_Autoformer_SASOL_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 100
============================================================
train 3675
val 114
test 115
Epoch: 1, Steps: 115 | Train Loss: 0.3811456 Vali Loss: 0.1240215 Test Loss: 0.1873610
Validation loss decreased (inf --> 0.124021).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 115 | Train Loss: 0.3454182 Vali Loss: 0.1189027 Test Loss: 0.2084671
Validation loss decreased (0.124021 --> 0.118903).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.38114560013232024, 'val/loss': 0.12402147240936756, 'test/loss': 0.1873610056936741, '_timestamp': 1762783057.0902784}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3454181896603626, 'val/loss': 0.1189026515930891, 'test/loss': 0.20846714079380035, '_timestamp': 1762783063.4172251}).
Epoch: 3, Steps: 115 | Train Loss: 0.3303630 Vali Loss: 0.1258292 Test Loss: 0.2289682
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 115 | Train Loss: 0.3249523 Vali Loss: 0.1370942 Test Loss: 0.2375937
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 115 | Train Loss: 0.3223951 Vali Loss: 0.1355913 Test Loss: 0.2354824
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 115 | Train Loss: 0.3206144 Vali Loss: 0.1320000 Test Loss: 0.2304474
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 115 | Train Loss: 0.3195396 Vali Loss: 0.1349918 Test Loss: 0.2363388
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 115 | Train Loss: 0.3189978 Vali Loss: 0.1310373 Test Loss: 0.2319780
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 115 | Train Loss: 0.3189946 Vali Loss: 0.1315264 Test Loss: 0.2348735
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 115 | Train Loss: 0.3191953 Vali Loss: 0.1325808 Test Loss: 0.2325556
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 115 | Train Loss: 0.3186769 Vali Loss: 0.1313895 Test Loss: 0.2321515
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 115 | Train Loss: 0.3190967 Vali Loss: 0.1336958 Test Loss: 0.2324196
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SASOL_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 115
test shape: (115, 100, 1) (115, 100, 1)
test shape: (115, 100, 1) (115, 100, 1)


	mse:0.00201878952793777, mae:0.033016808331012726, rmse:0.044930942356586456, r2:-0.017319917678833008, dtw:Not calculated


VAL - MSE: 0.0020, MAE: 0.0330, RMSE: 0.0449, RÂ²: -0.0173, MAPE: 7447613.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.652 MB of 0.656 MB uploadedwandb: \ 0.652 MB of 0.656 MB uploadedwandb: | 0.652 MB of 0.656 MB uploadedwandb: / 0.656 MB of 0.656 MB uploadedwandb: - 0.656 MB of 0.656 MB uploadedwandb: \ 0.656 MB of 0.871 MB uploadedwandb: | 0.871 MB of 0.871 MB uploadedwandb: / 0.871 MB of 0.871 MB uploadedwandb: - 0.871 MB of 0.871 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–†â–‚â–‡â–ƒâ–†â–„â–„â–„
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ˆâ–‡â–…â–‡â–„â–…â–…â–„â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.23242
wandb:                 train/loss 0.3191
wandb:   val/directional_accuracy 50.50505
wandb:                   val/loss 0.1337
wandb:                    val/mae 0.03302
wandb:                   val/mape 744761350.0
wandb:                    val/mse 0.00202
wandb:                     val/r2 -0.01732
wandb:                   val/rmse 0.04493
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/zje5bwi6
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_155726-zje5bwi6/logs
Completed: SASOL H=100

Training: Autoformer on DRD_GOLD for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_155911-gna6t545
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/gna6t545
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_DRD_GOLD_H3Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/gna6t545
>>>>>>>start training : long_term_forecast_Autoformer_DRD_GOLD_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.3321154 Vali Loss: 0.1681517 Test Loss: 0.1762639
Validation loss decreased (inf --> 0.168152).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2691963 Vali Loss: 0.1547631 Test Loss: 0.1644990
Validation loss decreased (0.168152 --> 0.154763).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3321153788190139, 'val/loss': 0.16815168783068657, 'test/loss': 0.17626393027603626, '_timestamp': 1762783164.296436}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.269196258787822, 'val/loss': 0.1547630988061428, 'test/loss': 0.16449899598956108, '_timestamp': 1762783171.2987282}).
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
Epoch: 3, Steps: 133 | Train Loss: 0.2518331 Vali Loss: 0.1533277 Test Loss: 0.1500063
Validation loss decreased (0.154763 --> 0.153328).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2430809 Vali Loss: 0.1476541 Test Loss: 0.1493227
Validation loss decreased (0.153328 --> 0.147654).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2385161 Vali Loss: 0.1437509 Test Loss: 0.1453518
Validation loss decreased (0.147654 --> 0.143751).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2350053 Vali Loss: 0.1446571 Test Loss: 0.1438456
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2336693 Vali Loss: 0.1451278 Test Loss: 0.1437866
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2326863 Vali Loss: 0.1434227 Test Loss: 0.1436431
Validation loss decreased (0.143751 --> 0.143423).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2327941 Vali Loss: 0.1427667 Test Loss: 0.1434311
Validation loss decreased (0.143423 --> 0.142767).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2322494 Vali Loss: 0.1436095 Test Loss: 0.1435243
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2317839 Vali Loss: 0.1419918 Test Loss: 0.1435590
Validation loss decreased (0.142767 --> 0.141992).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2323424 Vali Loss: 0.1436660 Test Loss: 0.1435532
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2320386 Vali Loss: 0.1457426 Test Loss: 0.1436535
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2319956 Vali Loss: 0.1438389 Test Loss: 0.1436519
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2318278 Vali Loss: 0.1488183 Test Loss: 0.1436449
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2329308 Vali Loss: 0.1420246 Test Loss: 0.1436433
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2323736 Vali Loss: 0.1408990 Test Loss: 0.1436415
Validation loss decreased (0.141992 --> 0.140899).  Saving model ...
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2313462 Vali Loss: 0.1419930 Test Loss: 0.1436420
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2329193 Vali Loss: 0.1422080 Test Loss: 0.1436417
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2318872 Vali Loss: 0.1526850 Test Loss: 0.1436417
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2330281 Vali Loss: 0.1444769 Test Loss: 0.1436420
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2325480 Vali Loss: 0.1389279 Test Loss: 0.1436417
Validation loss decreased (0.140899 --> 0.138928).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2314997 Vali Loss: 0.1466199 Test Loss: 0.1436414
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2316639 Vali Loss: 0.1422345 Test Loss: 0.1436417
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2313965 Vali Loss: 0.1461686 Test Loss: 0.1436414
EarlyStopping counter: 3 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2328114 Vali Loss: 0.1415984 Test Loss: 0.1436414
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.2317023 Vali Loss: 0.1435745 Test Loss: 0.1436415
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.2315058 Vali Loss: 0.1447369 Test Loss: 0.1436415
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.2310280 Vali Loss: 0.1423757 Test Loss: 0.1436415
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 133 | Train Loss: 0.2320123 Vali Loss: 0.1423513 Test Loss: 0.1436415
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 133 | Train Loss: 0.2333471 Vali Loss: 0.1461122 Test Loss: 0.1436415
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 133 | Train Loss: 0.2326746 Vali Loss: 0.1423769 Test Loss: 0.1436415
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_DRD_GOLD_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.0009840826969593763, mae:0.022409211844205856, rmse:0.03137009218335152, r2:-0.06115007400512695, dtw:Not calculated


VAL - MSE: 0.0010, MAE: 0.0224, RMSE: 0.0314, RÂ²: -0.0612, MAPE: 2803805.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.475 MB of 0.476 MB uploadedwandb: \ 0.475 MB of 0.476 MB uploadedwandb: | 0.475 MB of 0.476 MB uploadedwandb: / 0.476 MB of 0.476 MB uploadedwandb: - 0.476 MB of 0.476 MB uploadedwandb: \ 0.476 MB of 0.476 MB uploadedwandb: | 0.476 MB of 0.476 MB uploadedwandb: / 0.476 MB of 0.476 MB uploadedwandb: - 0.606 MB of 0.824 MB uploaded (0.002 MB deduped)wandb: \ 0.824 MB of 0.824 MB uploaded (0.002 MB deduped)wandb: | 0.824 MB of 0.824 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‡â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–‚â–â–‚â–‚â–â–â–â–‚â–â–â–â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–ƒâ–„â–„â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–„â–ƒâ–†â–ƒâ–‚â–‚â–ƒâ–ˆâ–„â–â–…â–ƒâ–…â–‚â–ƒâ–„â–ƒâ–ƒâ–„â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 31
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.14364
wandb:                 train/loss 0.23267
wandb:   val/directional_accuracy 51.05485
wandb:                   val/loss 0.14238
wandb:                    val/mae 0.02241
wandb:                   val/mape 280380500.0
wandb:                    val/mse 0.00098
wandb:                     val/r2 -0.06115
wandb:                   val/rmse 0.03137
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/gna6t545
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_155911-gna6t545/logs
Completed: DRD_GOLD H=3

Training: Autoformer on DRD_GOLD for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_160328-rswtbwqq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/rswtbwqq
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_DRD_GOLD_H5Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/rswtbwqq
>>>>>>>start training : long_term_forecast_Autoformer_DRD_GOLD_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.3154588 Vali Loss: 0.1845096 Test Loss: 0.1691442
Validation loss decreased (inf --> 0.184510).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2685765 Vali Loss: 0.1669757 Test Loss: 0.1587311
Validation loss decreased (0.184510 --> 0.166976).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3154588236396474, 'val/loss': 0.18450960703194141, 'test/loss': 0.1691441759467125, '_timestamp': 1762783421.152975}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26857646840407434, 'val/loss': 0.1669757254421711, 'test/loss': 0.1587311401963234, '_timestamp': 1762783428.2688217}).
Epoch: 3, Steps: 133 | Train Loss: 0.2523015 Vali Loss: 0.1590459 Test Loss: 0.1509758
Validation loss decreased (0.166976 --> 0.159046).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2451654 Vali Loss: 0.1553090 Test Loss: 0.1467884
Validation loss decreased (0.159046 --> 0.155309).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2409758 Vali Loss: 0.1514179 Test Loss: 0.1460633
Validation loss decreased (0.155309 --> 0.151418).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2394395 Vali Loss: 0.1500593 Test Loss: 0.1457031
Validation loss decreased (0.151418 --> 0.150059).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2389474 Vali Loss: 0.1520126 Test Loss: 0.1459771
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2391293 Vali Loss: 0.1520061 Test Loss: 0.1455680
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2385450 Vali Loss: 0.1503658 Test Loss: 0.1457722
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2379112 Vali Loss: 0.1521513 Test Loss: 0.1457498
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2376138 Vali Loss: 0.1558105 Test Loss: 0.1458996
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2368386 Vali Loss: 0.1489212 Test Loss: 0.1460538
Validation loss decreased (0.150059 --> 0.148921).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2380478 Vali Loss: 0.1478837 Test Loss: 0.1460620
Validation loss decreased (0.148921 --> 0.147884).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2376075 Vali Loss: 0.1476592 Test Loss: 0.1460577
Validation loss decreased (0.147884 --> 0.147659).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2375673 Vali Loss: 0.1460616 Test Loss: 0.1460582
Validation loss decreased (0.147659 --> 0.146062).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2378136 Vali Loss: 0.1476358 Test Loss: 0.1460577
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2367658 Vali Loss: 0.1486350 Test Loss: 0.1460581
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2370417 Vali Loss: 0.1501415 Test Loss: 0.1460579
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2369088 Vali Loss: 0.1522031 Test Loss: 0.1460578
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2375722 Vali Loss: 0.1480508 Test Loss: 0.1460577
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2367294 Vali Loss: 0.1490994 Test Loss: 0.1460576
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2366604 Vali Loss: 0.1524706 Test Loss: 0.1460576
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2373185 Vali Loss: 0.1506783 Test Loss: 0.1460580
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2376577 Vali Loss: 0.1516838 Test Loss: 0.1460579
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2374061 Vali Loss: 0.1545903 Test Loss: 0.1460579
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_DRD_GOLD_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.000992238987237215, mae:0.022628173232078552, rmse:0.03149982541799545, r2:-0.06258869171142578, dtw:Not calculated


VAL - MSE: 0.0010, MAE: 0.0226, RMSE: 0.0315, RÂ²: -0.0626, MAPE: 3395076.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.507 MB of 0.507 MB uploadedwandb: \ 0.507 MB of 0.507 MB uploadedwandb: | 0.507 MB of 0.507 MB uploadedwandb: / 0.507 MB of 0.724 MB uploadedwandb: - 0.710 MB of 0.724 MB uploadedwandb: \ 0.724 MB of 0.724 MB uploadedwandb: | 0.724 MB of 0.724 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–â–‚â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–„â–ƒâ–„â–„â–ƒâ–„â–†â–ƒâ–‚â–‚â–â–‚â–‚â–ƒâ–„â–‚â–ƒâ–„â–ƒâ–„â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 24
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.14606
wandb:                 train/loss 0.23741
wandb:   val/directional_accuracy 54.3617
wandb:                   val/loss 0.15459
wandb:                    val/mae 0.02263
wandb:                   val/mape 339507625.0
wandb:                    val/mse 0.00099
wandb:                     val/r2 -0.06259
wandb:                   val/rmse 0.0315
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/rswtbwqq
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_160328-rswtbwqq/logs
Completed: DRD_GOLD H=5

Training: Autoformer on DRD_GOLD for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_160655-4o9f5ir3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/4o9f5ir3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_DRD_GOLD_H10Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/4o9f5ir3
>>>>>>>start training : long_term_forecast_Autoformer_DRD_GOLD_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.3218165 Vali Loss: 0.1731919 Test Loss: 0.1588213
Validation loss decreased (inf --> 0.173192).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2736988 Vali Loss: 0.1816834 Test Loss: 0.1574418
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3218164584018234, 'val/loss': 0.17319185845553875, 'test/loss': 0.15882129408419132, '_timestamp': 1762783628.6928165}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2736988351085132, 'val/loss': 0.1816833820194006, 'test/loss': 0.15744184330105782, '_timestamp': 1762783635.6787076}).
Epoch: 3, Steps: 133 | Train Loss: 0.2606236 Vali Loss: 0.1688820 Test Loss: 0.1541458
Validation loss decreased (0.173192 --> 0.168882).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2534489 Vali Loss: 0.1645756 Test Loss: 0.1496452
Validation loss decreased (0.168882 --> 0.164576).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2512566 Vali Loss: 0.1648224 Test Loss: 0.1472066
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2501914 Vali Loss: 0.1749365 Test Loss: 0.1471340
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2499804 Vali Loss: 0.1782899 Test Loss: 0.1476096
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2496821 Vali Loss: 0.1640623 Test Loss: 0.1477904
Validation loss decreased (0.164576 --> 0.164062).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2509825 Vali Loss: 0.1656808 Test Loss: 0.1468954
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2486116 Vali Loss: 0.1654285 Test Loss: 0.1471575
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2497288 Vali Loss: 0.1660221 Test Loss: 0.1473868
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2481384 Vali Loss: 0.1637639 Test Loss: 0.1473988
Validation loss decreased (0.164062 --> 0.163764).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2499349 Vali Loss: 0.1724777 Test Loss: 0.1474879
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2497010 Vali Loss: 0.1624643 Test Loss: 0.1474988
Validation loss decreased (0.163764 --> 0.162464).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2493739 Vali Loss: 0.1666567 Test Loss: 0.1474979
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2489474 Vali Loss: 0.1616054 Test Loss: 0.1474984
Validation loss decreased (0.162464 --> 0.161605).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2490779 Vali Loss: 0.1632931 Test Loss: 0.1474975
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2489062 Vali Loss: 0.1715388 Test Loss: 0.1474976
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2484198 Vali Loss: 0.1711399 Test Loss: 0.1474979
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2484685 Vali Loss: 0.1771640 Test Loss: 0.1474977
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2477528 Vali Loss: 0.1729327 Test Loss: 0.1474979
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2481583 Vali Loss: 0.1634143 Test Loss: 0.1474977
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2491526 Vali Loss: 0.1721953 Test Loss: 0.1474979
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2490569 Vali Loss: 0.1686347 Test Loss: 0.1474978
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2507563 Vali Loss: 0.1625888 Test Loss: 0.1474978
EarlyStopping counter: 9 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2478049 Vali Loss: 0.1708314 Test Loss: 0.1474979
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_DRD_GOLD_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.0009881154401227832, mae:0.022341664880514145, rmse:0.031434305012226105, r2:-0.04202723503112793, dtw:Not calculated


VAL - MSE: 0.0010, MAE: 0.0223, RMSE: 0.0314, RÂ²: -0.0420, MAPE: 2145780.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.528 MB of 0.529 MB uploadedwandb: \ 0.528 MB of 0.529 MB uploadedwandb: | 0.528 MB of 0.529 MB uploadedwandb: / 0.529 MB of 0.529 MB uploadedwandb: - 0.529 MB of 0.529 MB uploadedwandb: \ 0.529 MB of 0.746 MB uploadedwandb: | 0.746 MB of 0.746 MB uploadedwandb: / 0.746 MB of 0.746 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–â–â–‚â–‚â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–‚â–ƒâ–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–‚â–ƒâ–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–‚â–‚â–‡â–ˆâ–‚â–ƒâ–ƒâ–ƒâ–‚â–†â–â–ƒâ–â–‚â–…â–…â–ˆâ–†â–‚â–…â–„â–â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 25
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.1475
wandb:                 train/loss 0.2478
wandb:   val/directional_accuracy 53.57488
wandb:                   val/loss 0.17083
wandb:                    val/mae 0.02234
wandb:                   val/mape 214578000.0
wandb:                    val/mse 0.00099
wandb:                     val/r2 -0.04203
wandb:                   val/rmse 0.03143
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/4o9f5ir3
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_160655-4o9f5ir3/logs
Completed: DRD_GOLD H=10

Training: Autoformer on DRD_GOLD for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_161023-29kpq577
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/29kpq577
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_DRD_GOLD_H22Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/29kpq577
>>>>>>>start training : long_term_forecast_Autoformer_DRD_GOLD_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.3405514 Vali Loss: 0.1910905 Test Loss: 0.1617530
Validation loss decreased (inf --> 0.191090).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.34055140941883577, 'val/loss': 0.1910904539482934, 'test/loss': 0.16175296476909093, '_timestamp': 1762783835.8233383}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2935158884660764, 'val/loss': 0.18021090115819657, 'test/loss': 0.16188828434262956, '_timestamp': 1762783843.211475}).
Epoch: 2, Steps: 132 | Train Loss: 0.2935159 Vali Loss: 0.1802109 Test Loss: 0.1618883
Validation loss decreased (0.191090 --> 0.180211).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2777026 Vali Loss: 0.1821073 Test Loss: 0.1649215
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2704202 Vali Loss: 0.1758713 Test Loss: 0.1595345
Validation loss decreased (0.180211 --> 0.175871).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2677016 Vali Loss: 0.1743992 Test Loss: 0.1554166
Validation loss decreased (0.175871 --> 0.174399).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2661800 Vali Loss: 0.1727407 Test Loss: 0.1564132
Validation loss decreased (0.174399 --> 0.172741).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2651127 Vali Loss: 0.1729303 Test Loss: 0.1562490
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2646838 Vali Loss: 0.1720315 Test Loss: 0.1565089
Validation loss decreased (0.172741 --> 0.172031).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2643181 Vali Loss: 0.1714855 Test Loss: 0.1568427
Validation loss decreased (0.172031 --> 0.171486).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2644130 Vali Loss: 0.1713350 Test Loss: 0.1567250
Validation loss decreased (0.171486 --> 0.171335).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2642808 Vali Loss: 0.1717458 Test Loss: 0.1567369
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2642508 Vali Loss: 0.1719564 Test Loss: 0.1567463
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2644283 Vali Loss: 0.1713791 Test Loss: 0.1567305
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2642030 Vali Loss: 0.1719333 Test Loss: 0.1567308
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2648757 Vali Loss: 0.1715279 Test Loss: 0.1567332
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2640873 Vali Loss: 0.1715931 Test Loss: 0.1567302
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2646865 Vali Loss: 0.1718905 Test Loss: 0.1567296
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2641690 Vali Loss: 0.1715203 Test Loss: 0.1567301
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2644090 Vali Loss: 0.1719858 Test Loss: 0.1567300
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2646059 Vali Loss: 0.1713725 Test Loss: 0.1567300
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_DRD_GOLD_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.0009848190238699317, mae:0.022458920255303383, rmse:0.03138182684779167, r2:-0.034169673919677734, dtw:Not calculated


VAL - MSE: 0.0010, MAE: 0.0225, RMSE: 0.0314, RÂ²: -0.0342, MAPE: 2050655.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.607 MB of 0.608 MB uploadedwandb: \ 0.607 MB of 0.608 MB uploadedwandb: | 0.608 MB of 0.608 MB uploadedwandb: / 0.608 MB of 0.608 MB uploadedwandb: - 0.608 MB of 0.825 MB uploadedwandb: \ 0.825 MB of 0.825 MB uploadedwandb: | 0.825 MB of 0.825 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.15673
wandb:                 train/loss 0.26461
wandb:   val/directional_accuracy 54.39056
wandb:                   val/loss 0.17137
wandb:                    val/mae 0.02246
wandb:                   val/mape 205065550.0
wandb:                    val/mse 0.00098
wandb:                     val/r2 -0.03417
wandb:                   val/rmse 0.03138
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/29kpq577
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_161023-29kpq577/logs
Completed: DRD_GOLD H=22

Training: Autoformer on DRD_GOLD for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_161312-rn5l93hb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/rn5l93hb
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_DRD_GOLD_H50Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/rn5l93hb
>>>>>>>start training : long_term_forecast_Autoformer_DRD_GOLD_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.3776539 Vali Loss: 0.2316235 Test Loss: 0.1682362
Validation loss decreased (inf --> 0.231624).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.3334949 Vali Loss: 0.2205423 Test Loss: 0.1710031
Validation loss decreased (0.231624 --> 0.220542).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.37765392751404736, 'val/loss': 0.23162350555260977, 'test/loss': 0.16823617865641913, '_timestamp': 1762784005.0286255}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3334949299918883, 'val/loss': 0.22054229180018106, 'test/loss': 0.1710030511021614, '_timestamp': 1762784012.0034328}).
Epoch: 3, Steps: 132 | Train Loss: 0.3117130 Vali Loss: 0.2157786 Test Loss: 0.1854246
Validation loss decreased (0.220542 --> 0.215779).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2988085 Vali Loss: 0.2190148 Test Loss: 0.1895375
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2954038 Vali Loss: 0.2201178 Test Loss: 0.1869799
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2927832 Vali Loss: 0.2212546 Test Loss: 0.1894201
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2914854 Vali Loss: 0.2211899 Test Loss: 0.1886661
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2900808 Vali Loss: 0.2208471 Test Loss: 0.1876371
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2896796 Vali Loss: 0.2203002 Test Loss: 0.1871161
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2946504 Vali Loss: 0.2202617 Test Loss: 0.1870780
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2936367 Vali Loss: 0.2205434 Test Loss: 0.1875550
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2894181 Vali Loss: 0.2203178 Test Loss: 0.1874917
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2887802 Vali Loss: 0.2207396 Test Loss: 0.1875794
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_DRD_GOLD_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.000968737353105098, mae:0.022186504676938057, rmse:0.031124545261263847, r2:-0.040329933166503906, dtw:Not calculated


VAL - MSE: 0.0010, MAE: 0.0222, RMSE: 0.0311, RÂ²: -0.0403, MAPE: 3077857.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.710 MB of 0.713 MB uploadedwandb: \ 0.710 MB of 0.713 MB uploadedwandb: | 0.710 MB of 0.713 MB uploadedwandb: / 0.713 MB of 0.713 MB uploadedwandb: - 0.713 MB of 0.713 MB uploadedwandb: \ 0.713 MB of 0.928 MB uploadedwandb: | 0.928 MB of 0.928 MB uploadedwandb: / 0.928 MB of 0.928 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–„â–ˆâ–‡â–…â–„â–„â–…â–…â–…
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–â–ƒâ–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–…â–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.18758
wandb:                 train/loss 0.28878
wandb:   val/directional_accuracy 54.79055
wandb:                   val/loss 0.22074
wandb:                    val/mae 0.02219
wandb:                   val/mape 307785775.0
wandb:                    val/mse 0.00097
wandb:                     val/r2 -0.04033
wandb:                   val/rmse 0.03112
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/rn5l93hb
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_161312-rn5l93hb/logs
Completed: DRD_GOLD H=50

Training: Autoformer on DRD_GOLD for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_161513-g7zagnin
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/g7zagnin
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_DRD_GOLD_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/g7zagnin
>>>>>>>start training : long_term_forecast_Autoformer_DRD_GOLD_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.4656748 Vali Loss: 0.2816241 Test Loss: 0.1832372
Validation loss decreased (inf --> 0.281624).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.4334753 Vali Loss: 0.2520462 Test Loss: 0.1850722
Validation loss decreased (0.281624 --> 0.252046).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.46567479647122895, 'val/loss': 0.2816241443157196, 'test/loss': 0.18323717415332794, '_timestamp': 1762784126.574174}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.43347533207673294, 'val/loss': 0.25204615890979765, 'test/loss': 0.18507219851016998, '_timestamp': 1762784133.478639}).
Epoch: 3, Steps: 130 | Train Loss: 0.4213910 Vali Loss: 0.2649057 Test Loss: 0.1906450
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.4136619 Vali Loss: 0.2631066 Test Loss: 0.1884639
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.4093140 Vali Loss: 0.2595932 Test Loss: 0.1811377
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.4054799 Vali Loss: 0.2588578 Test Loss: 0.1858112
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.4037531 Vali Loss: 0.2605570 Test Loss: 0.1841153
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.4029430 Vali Loss: 0.2614527 Test Loss: 0.1846022
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.4026520 Vali Loss: 0.2547971 Test Loss: 0.1842041
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.4020845 Vali Loss: 0.2581150 Test Loss: 0.1844877
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.4022235 Vali Loss: 0.2591224 Test Loss: 0.1843848
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.4020789 Vali Loss: 0.2603999 Test Loss: 0.1842543
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_DRD_GOLD_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0009390359628014266, mae:0.021644657477736473, rmse:0.03064369410276413, r2:-0.039278507232666016, dtw:Not calculated


VAL - MSE: 0.0009, MAE: 0.0216, RMSE: 0.0306, RÂ²: -0.0393, MAPE: 1224469.38%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.782 MB of 0.787 MB uploadedwandb: \ 0.782 MB of 0.787 MB uploadedwandb: | 0.787 MB of 0.787 MB uploadedwandb: / 0.787 MB of 1.002 MB uploadedwandb: - 1.002 MB of 1.002 MB uploadedwandb: \ 1.002 MB of 1.002 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–†â–â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‡â–„â–„â–…â–†â–â–ƒâ–„â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.18425
wandb:                 train/loss 0.40208
wandb:   val/directional_accuracy 53.65801
wandb:                   val/loss 0.2604
wandb:                    val/mae 0.02164
wandb:                   val/mape 122446937.5
wandb:                    val/mse 0.00094
wandb:                     val/r2 -0.03928
wandb:                   val/rmse 0.03064
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/g7zagnin
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_161513-g7zagnin/logs
Completed: DRD_GOLD H=100

Training: Autoformer on ANGLO_AMERICAN for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_161703-9ibxmqwu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/9ibxmqwu
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ANGLO_AMERICAN_H3Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/9ibxmqwu
>>>>>>>start training : long_term_forecast_Autoformer_ANGLO_AMERICAN_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 3
============================================================
train 778
val 44
test 46
Epoch: 1, Steps: 25 | Train Loss: 0.4681381 Vali Loss: 0.6150567 Test Loss: 0.9083944
Validation loss decreased (inf --> 0.615057).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 25 | Train Loss: 0.3560636 Vali Loss: 0.6537048 Test Loss: 0.9248271
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 25 | Train Loss: 0.3160420 Vali Loss: 0.6567512 Test Loss: 0.9084268
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 25 | Train Loss: 0.3013970 Vali Loss: 0.6213713 Test Loss: 0.9279057
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 25 | Train Loss: 0.2942352 Vali Loss: 0.5974675 Test Loss: 0.9116985
Validation loss decreased (0.615057 --> 0.597468).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 25 | Train Loss: 0.2940027 Vali Loss: 0.5760696 Test Loss: 0.8965931
Validation loss decreased (0.597468 --> 0.576070).  Saving model ...
Updating learning rate to 3.125e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.4681380867958069, 'val/loss': 0.615056723356247, 'test/loss': 0.9083943665027618, '_timestamp': 1762784231.9073913}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.35606359124183656, 'val/loss': 0.6537047624588013, 'test/loss': 0.924827054142952, '_timestamp': 1762784234.3854973}).
Epoch: 7, Steps: 25 | Train Loss: 0.2872293 Vali Loss: 0.5942177 Test Loss: 0.9020531
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 25 | Train Loss: 0.2932363 Vali Loss: 0.6068605 Test Loss: 0.8801251
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 25 | Train Loss: 0.2926802 Vali Loss: 0.5653648 Test Loss: 0.8841143
Validation loss decreased (0.576070 --> 0.565365).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 25 | Train Loss: 0.2844519 Vali Loss: 0.5861608 Test Loss: 0.8821755
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 25 | Train Loss: 0.2927736 Vali Loss: 0.6226309 Test Loss: 0.8820230
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 25 | Train Loss: 0.2882495 Vali Loss: 0.5949535 Test Loss: 0.8818594
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 25 | Train Loss: 0.2907370 Vali Loss: 0.5510761 Test Loss: 0.8818180
Validation loss decreased (0.565365 --> 0.551076).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 25 | Train Loss: 0.2900759 Vali Loss: 0.5939555 Test Loss: 0.8818539
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 25 | Train Loss: 0.2965368 Vali Loss: 0.5858596 Test Loss: 0.8818626
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 25 | Train Loss: 0.2873657 Vali Loss: 0.6031345 Test Loss: 0.8818662
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 25 | Train Loss: 0.2922237 Vali Loss: 0.5457973 Test Loss: 0.8818697
Validation loss decreased (0.551076 --> 0.545797).  Saving model ...
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 25 | Train Loss: 0.2896093 Vali Loss: 0.6274521 Test Loss: 0.8818736
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 25 | Train Loss: 0.2891615 Vali Loss: 0.5939665 Test Loss: 0.8818709
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 25 | Train Loss: 0.2911829 Vali Loss: 0.5752154 Test Loss: 0.8818704
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 25 | Train Loss: 0.2937602 Vali Loss: 0.5694069 Test Loss: 0.8818751
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 25 | Train Loss: 0.2884754 Vali Loss: 0.5874506 Test Loss: 0.8818695
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 25 | Train Loss: 0.2879421 Vali Loss: 0.5775698 Test Loss: 0.8818675
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 25 | Train Loss: 0.2867817 Vali Loss: 0.5783206 Test Loss: 0.8818712
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 25 | Train Loss: 0.2844928 Vali Loss: 0.5836300 Test Loss: 0.8818700
EarlyStopping counter: 8 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 25 | Train Loss: 0.2913922 Vali Loss: 0.5689688 Test Loss: 0.8818699
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 25 | Train Loss: 0.2897271 Vali Loss: 0.6077410 Test Loss: 0.8818696
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ANGLO_AMERICAN_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 46
test shape: (46, 3, 1) (46, 3, 1)
test shape: (46, 3, 1) (46, 3, 1)


	mse:0.00717761367559433, mae:0.05953076109290123, rmse:0.08472079783678055, r2:-0.05944371223449707, dtw:Not calculated


VAL - MSE: 0.0072, MAE: 0.0595, RMSE: 0.0847, RÂ²: -0.0594, MAPE: 52255060.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.431 MB of 0.431 MB uploadedwandb: \ 0.431 MB of 0.431 MB uploadedwandb: | 0.431 MB of 0.431 MB uploadedwandb: / 0.431 MB of 0.431 MB uploadedwandb: - 0.431 MB of 0.431 MB uploadedwandb: \ 0.431 MB of 0.431 MB uploadedwandb: | 0.431 MB of 0.431 MB uploadedwandb: / 0.431 MB of 0.431 MB uploadedwandb: - 0.562 MB of 0.779 MB uploaded (0.002 MB deduped)wandb: \ 0.779 MB of 0.779 MB uploaded (0.002 MB deduped)wandb: | 0.779 MB of 0.779 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–ˆâ–†â–ƒâ–„â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–â–ƒâ–‚â–‚â–‚â–„â–‚â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–â–ƒâ–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–„â–ƒâ–„â–…â–‚â–„â–†â–„â–â–„â–„â–…â–â–†â–„â–ƒâ–‚â–„â–ƒâ–ƒâ–ƒâ–‚â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 26
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.88187
wandb:                 train/loss 0.28973
wandb:   val/directional_accuracy 41.30435
wandb:                   val/loss 0.60774
wandb:                    val/mae 0.05953
wandb:                   val/mape 5225506000.0
wandb:                    val/mse 0.00718
wandb:                     val/r2 -0.05944
wandb:                   val/rmse 0.08472
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/9ibxmqwu
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_161703-9ibxmqwu/logs
Completed: ANGLO_AMERICAN H=3

Training: Autoformer on ANGLO_AMERICAN for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_161835-u1rywzep
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/u1rywzep
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ANGLO_AMERICAN_H5Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/u1rywzep
>>>>>>>start training : long_term_forecast_Autoformer_ANGLO_AMERICAN_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 5
============================================================
train 776
val 42
test 44
Epoch: 1, Steps: 25 | Train Loss: 0.4551731 Vali Loss: 0.5825939 Test Loss: 0.8836616
Validation loss decreased (inf --> 0.582594).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 25 | Train Loss: 0.3585333 Vali Loss: 0.6051304 Test Loss: 0.8716078
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 25 | Train Loss: 0.3222961 Vali Loss: 0.6328813 Test Loss: 0.9370247
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 25 | Train Loss: 0.3088640 Vali Loss: 0.6588976 Test Loss: 0.9655900
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 25 | Train Loss: 0.3024484 Vali Loss: 0.6636773 Test Loss: 0.9382247
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 25 | Train Loss: 0.3010840 Vali Loss: 0.6003534 Test Loss: 0.9386930
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 25 | Train Loss: 0.2983267 Vali Loss: 0.5953602 Test Loss: 0.9451492
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.45517305374145506, 'val/loss': 0.5825938582420349, 'test/loss': 0.8836615979671478, '_timestamp': 1762784323.0186255}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.35853329062461853, 'val/loss': 0.6051304042339325, 'test/loss': 0.8716078102588654, '_timestamp': 1762784325.456484}).
Epoch: 8, Steps: 25 | Train Loss: 0.2974519 Vali Loss: 0.6512916 Test Loss: 0.9438339
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 25 | Train Loss: 0.2958261 Vali Loss: 0.6379353 Test Loss: 0.9431873
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 25 | Train Loss: 0.3045182 Vali Loss: 0.6616140 Test Loss: 0.9438735
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 25 | Train Loss: 0.2968661 Vali Loss: 0.6292595 Test Loss: 0.9436592
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ANGLO_AMERICAN_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 44
test shape: (44, 5, 1) (44, 5, 1)
test shape: (44, 5, 1) (44, 5, 1)


	mse:0.007467489689588547, mae:0.0608585961163044, rmse:0.086414635181427, r2:-0.058930397033691406, dtw:Not calculated


VAL - MSE: 0.0075, MAE: 0.0609, RMSE: 0.0864, RÂ²: -0.0589, MAPE: 42545284.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.474 MB of 0.474 MB uploadedwandb: \ 0.474 MB of 0.474 MB uploadedwandb: | 0.474 MB of 0.474 MB uploadedwandb: / 0.474 MB of 0.689 MB uploadedwandb: - 0.474 MB of 0.689 MB uploadedwandb: \ 0.689 MB of 0.689 MB uploadedwandb: | 0.689 MB of 0.689 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–â–ƒâ–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–ˆâ–ˆâ–‚â–â–‡â–…â–ˆâ–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.94366
wandb:                 train/loss 0.29687
wandb:   val/directional_accuracy 47.72727
wandb:                   val/loss 0.62926
wandb:                    val/mae 0.06086
wandb:                   val/mape 4254528400.0
wandb:                    val/mse 0.00747
wandb:                     val/r2 -0.05893
wandb:                   val/rmse 0.08641
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/u1rywzep
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_161835-u1rywzep/logs
Completed: ANGLO_AMERICAN H=5

Training: Autoformer on ANGLO_AMERICAN for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_161923-8c24d5i9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/8c24d5i9
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ANGLO_AMERICAN_H10Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/8c24d5i9
>>>>>>>start training : long_term_forecast_Autoformer_ANGLO_AMERICAN_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 10
============================================================
train 771
val 37
test 39
Epoch: 1, Steps: 25 | Train Loss: 0.4638062 Vali Loss: 0.6813492 Test Loss: 0.9155233
Validation loss decreased (inf --> 0.681349).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 25 | Train Loss: 0.3723544 Vali Loss: 0.6381451 Test Loss: 0.9078771
Validation loss decreased (0.681349 --> 0.638145).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 25 | Train Loss: 0.3422785 Vali Loss: 0.6593125 Test Loss: 0.8752797
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 25 | Train Loss: 0.3328968 Vali Loss: 0.6848503 Test Loss: 0.9097489
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 25 | Train Loss: 0.3233371 Vali Loss: 0.6441225 Test Loss: 0.9129547
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 25 | Train Loss: 0.3141869 Vali Loss: 0.6741017 Test Loss: 0.9154893
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 25 | Train Loss: 0.3177745 Vali Loss: 0.6417393 Test Loss: 0.9201859
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.4638062334060669, 'val/loss': 0.6813491582870483, 'test/loss': 0.9155233204364777, '_timestamp': 1762784369.562406}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3723543608188629, 'val/loss': 0.6381451487541199, 'test/loss': 0.9078770577907562, '_timestamp': 1762784372.0018604}).
Epoch: 8, Steps: 25 | Train Loss: 0.3086411 Vali Loss: 0.6894693 Test Loss: 0.9192141
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 25 | Train Loss: 0.3107364 Vali Loss: 0.6777370 Test Loss: 0.9209707
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 25 | Train Loss: 0.3208744 Vali Loss: 0.6511823 Test Loss: 0.9217324
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 25 | Train Loss: 0.3103451 Vali Loss: 0.6879473 Test Loss: 0.9216749
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 25 | Train Loss: 0.3128965 Vali Loss: 0.6820505 Test Loss: 0.9217331
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ANGLO_AMERICAN_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 39
test shape: (39, 10, 1) (39, 10, 1)
test shape: (39, 10, 1) (39, 10, 1)


	mse:0.008199385367333889, mae:0.06484968215227127, rmse:0.09055045992136002, r2:-0.05818939208984375, dtw:Not calculated


VAL - MSE: 0.0082, MAE: 0.0648, RMSE: 0.0906, RÂ²: -0.0582, MAPE: 37548736.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.510 MB of 0.510 MB uploadedwandb: \ 0.510 MB of 0.510 MB uploadedwandb: | 0.510 MB of 0.510 MB uploadedwandb: / 0.510 MB of 0.725 MB uploadedwandb: - 0.510 MB of 0.725 MB uploadedwandb: \ 0.725 MB of 0.725 MB uploadedwandb: | 0.725 MB of 0.725 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–†â–„â–‚â–ƒâ–â–â–„â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–‡â–â–†â–â–ˆâ–†â–‚â–ˆâ–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.92173
wandb:                 train/loss 0.3129
wandb:   val/directional_accuracy 29.05983
wandb:                   val/loss 0.68205
wandb:                    val/mae 0.06485
wandb:                   val/mape 3754873600.0
wandb:                    val/mse 0.0082
wandb:                     val/r2 -0.05819
wandb:                   val/rmse 0.09055
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/8c24d5i9
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_161923-8c24d5i9/logs
Completed: ANGLO_AMERICAN H=10

Training: Autoformer on ANGLO_AMERICAN for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_162017-bcn7vcx9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/bcn7vcx9
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ANGLO_AMERICAN_H22Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/bcn7vcx9
>>>>>>>start training : long_term_forecast_Autoformer_ANGLO_AMERICAN_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 22
============================================================
train 759
val 25
test 27
Epoch: 1, Steps: 24 | Train Loss: 0.4784269 Vali Loss: 0.6809640 Test Loss: 1.2652333
Validation loss decreased (inf --> 0.680964).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 24 | Train Loss: 0.3820043 Vali Loss: 0.7283101 Test Loss: 1.2679406
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 24 | Train Loss: 0.3573558 Vali Loss: 0.7270373 Test Loss: 1.2504225
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 24 | Train Loss: 0.3442671 Vali Loss: 0.7468009 Test Loss: 1.2349124
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 24 | Train Loss: 0.3399856 Vali Loss: 0.7597554 Test Loss: 1.2524903
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 24 | Train Loss: 0.3333771 Vali Loss: 0.7538462 Test Loss: 1.2486545
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 24 | Train Loss: 0.3307396 Vali Loss: 0.7578351 Test Loss: 1.2460986
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.4784269258379936, 'val/loss': 0.6809639930725098, 'test/loss': 1.2652332782745361, '_timestamp': 1762784424.054744}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3820042697091897, 'val/loss': 0.7283101081848145, 'test/loss': 1.267940640449524, '_timestamp': 1762784426.5208492}).
Epoch: 8, Steps: 24 | Train Loss: 0.3310479 Vali Loss: 0.7617487 Test Loss: 1.2443841
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 24 | Train Loss: 0.3306492 Vali Loss: 0.7626898 Test Loss: 1.2435801
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 24 | Train Loss: 0.3308688 Vali Loss: 0.7631683 Test Loss: 1.2461742
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 24 | Train Loss: 0.3293985 Vali Loss: 0.7632474 Test Loss: 1.2447321
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ANGLO_AMERICAN_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 27
test shape: (27, 22, 1) (27, 22, 1)
test shape: (27, 22, 1) (27, 22, 1)


	mse:0.00824542436748743, mae:0.06540389358997345, rmse:0.0908043161034584, r2:-0.050219178199768066, dtw:Not calculated


VAL - MSE: 0.0082, MAE: 0.0654, RMSE: 0.0908, RÂ²: -0.0502, MAPE: 20623706.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.490 MB of 0.491 MB uploadedwandb: \ 0.490 MB of 0.491 MB uploadedwandb: | 0.491 MB of 0.491 MB uploadedwandb: / 0.491 MB of 0.706 MB uploadedwandb: - 0.706 MB of 0.706 MB uploadedwandb: \ 0.706 MB of 0.706 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‡â–â–ˆâ–†â–…â–…â–„â–…â–…
wandb:                 train/loss â–ˆâ–…â–„â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–…â–‡â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 1.24473
wandb:                 train/loss 0.3294
wandb:   val/directional_accuracy 42.50441
wandb:                   val/loss 0.76325
wandb:                    val/mae 0.0654
wandb:                   val/mape 2062370600.0
wandb:                    val/mse 0.00825
wandb:                     val/r2 -0.05022
wandb:                   val/rmse 0.0908
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/bcn7vcx9
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_162017-bcn7vcx9/logs
Completed: ANGLO_AMERICAN H=22

Training: Autoformer on ANGLO_AMERICAN for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_162104-1n19i9br
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/1n19i9br
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ANGLO_AMERICAN_H50Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/1n19i9br
>>>>>>>start training : long_term_forecast_Autoformer_ANGLO_AMERICAN_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 50
============================================================
train 731
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.005 MB of 0.020 MB uploadedwandb: / 0.024 MB of 0.024 MB uploadedwandb: ðŸš€ View run Autoformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/1n19i9br
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_162104-1n19i9br/logs
Traceback (most recent call last):
  File "run.py", line 207, in <module>
    exp.train(setting)
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 93, in train
    vali_data, vali_loader = self._get_data(flag='val')
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 43, in _get_data
    data_set, data_loader = data_provider(self.args, flag)
  File "/home-mscluster/bkodze/research_project/forecast-research/data_provider/data_factory.py", line 79, in data_provider
    print(flag, len(data_set))
ValueError: __len__() should return >= 0
Completed: ANGLO_AMERICAN H=50

Training: Autoformer on ANGLO_AMERICAN for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_162126-b6523r3e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/b6523r3e
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ANGLO_AMERICAN_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/b6523r3e
>>>>>>>start training : long_term_forecast_Autoformer_ANGLO_AMERICAN_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 100
============================================================
train 681
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.005 MB of 0.020 MB uploadedwandb: / 0.024 MB of 0.024 MB uploadedwandb: ðŸš€ View run Autoformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/b6523r3e
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_162126-b6523r3e/logs
Traceback (most recent call last):
  File "run.py", line 207, in <module>
    exp.train(setting)
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 93, in train
    vali_data, vali_loader = self._get_data(flag='val')
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 43, in _get_data
    data_set, data_loader = data_provider(self.args, flag)
  File "/home-mscluster/bkodze/research_project/forecast-research/data_provider/data_factory.py", line 79, in data_provider
    print(flag, len(data_set))
ValueError: __len__() should return >= 0
Completed: ANGLO_AMERICAN H=100

Autoformer training completed for all datasets!
