##############################################################################
# Training Autoformer Model on All Datasets
##############################################################################
Training: Autoformer on NVIDIA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_193303-h6xjnj81
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/h6xjnj81
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NVIDIA_H3Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/h6xjnj81
>>>>>>>start training : long_term_forecast_Autoformer_NVIDIA_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.30410712778120114, 'val/loss': 0.19519002176821232, 'test/loss': 0.33913153782486916, '_timestamp': 1762882438.000433}).
Epoch: 1, Steps: 133 | Train Loss: 0.3041071 Vali Loss: 0.1951900 Test Loss: 0.3391315
Validation loss decreased (inf --> 0.195190).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2598915 Vali Loss: 0.1839028 Test Loss: 0.3174018
Validation loss decreased (0.195190 --> 0.183903).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25989153185733277, 'val/loss': 0.18390284944325686, 'test/loss': 0.31740181613713503, '_timestamp': 1762882447.2268147}).
Epoch: 3, Steps: 133 | Train Loss: 0.2462980 Vali Loss: 0.1817831 Test Loss: 0.3136882
Validation loss decreased (0.183903 --> 0.181783).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2412566 Vali Loss: 0.1729673 Test Loss: 0.3022293
Validation loss decreased (0.181783 --> 0.172967).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2365275 Vali Loss: 0.1968048 Test Loss: 0.3021703
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2343486 Vali Loss: 0.2011749 Test Loss: 0.3022712
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2331211 Vali Loss: 0.1707947 Test Loss: 0.3027661
Validation loss decreased (0.172967 --> 0.170795).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2336217 Vali Loss: 0.1762675 Test Loss: 0.3017719
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2332593 Vali Loss: 0.1810617 Test Loss: 0.3018031
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2331289 Vali Loss: 0.1727470 Test Loss: 0.3017532
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2328199 Vali Loss: 0.1783326 Test Loss: 0.3016985
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2325455 Vali Loss: 0.1747353 Test Loss: 0.3017469
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2327413 Vali Loss: 0.1750795 Test Loss: 0.3017645
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2326451 Vali Loss: 0.1749202 Test Loss: 0.3017368
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2324291 Vali Loss: 0.1864961 Test Loss: 0.3017381
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2322393 Vali Loss: 0.1785460 Test Loss: 0.3017357
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2316523 Vali Loss: 0.1947648 Test Loss: 0.3017341
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NVIDIA_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.0011521877022460103, mae:0.025424839928746223, rmse:0.033943891525268555, r2:-0.027100682258605957, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0254, RMSE: 0.0339, RÂ²: -0.0271, MAPE: 774368.88%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.467 MB of 0.467 MB uploadedwandb: \ 0.467 MB of 0.467 MB uploadedwandb: | 0.467 MB of 0.467 MB uploadedwandb: / 0.467 MB of 0.467 MB uploadedwandb: - 0.467 MB of 0.467 MB uploadedwandb: \ 0.467 MB of 0.467 MB uploadedwandb: | 0.467 MB of 0.467 MB uploadedwandb: / 0.467 MB of 0.467 MB uploadedwandb: - 0.467 MB of 0.467 MB uploadedwandb: \ 0.467 MB of 0.467 MB uploadedwandb: | 0.467 MB of 0.467 MB uploadedwandb: / 0.598 MB of 0.802 MB uploaded (0.002 MB deduped)wandb: - 0.598 MB of 0.802 MB uploaded (0.002 MB deduped)wandb: \ 0.802 MB of 0.802 MB uploaded (0.002 MB deduped)wandb: | 0.802 MB of 0.802 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–†â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–‚â–‡â–ˆâ–â–‚â–ƒâ–â–ƒâ–‚â–‚â–‚â–…â–ƒâ–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.30173
wandb:                 train/loss 0.23165
wandb:   val/directional_accuracy 48.52321
wandb:                   val/loss 0.19476
wandb:                    val/mae 0.02542
wandb:                   val/mape 77436887.5
wandb:                    val/mse 0.00115
wandb:                     val/r2 -0.0271
wandb:                   val/rmse 0.03394
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/h6xjnj81
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_193303-h6xjnj81/logs
Completed: NVIDIA H=3

Training: Autoformer on NVIDIA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_193730-lct1ptga
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/lct1ptga
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NVIDIA_H5Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/lct1ptga
>>>>>>>start training : long_term_forecast_Autoformer_NVIDIA_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.2974593 Vali Loss: 0.2024042 Test Loss: 0.3361513
Validation loss decreased (inf --> 0.202404).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29745933267855107, 'val/loss': 0.20240416564047337, 'test/loss': 0.3361512515693903, '_timestamp': 1762882682.7502432}).
Epoch: 2, Steps: 133 | Train Loss: 0.2597455 Vali Loss: 0.1822929 Test Loss: 0.3361465
Validation loss decreased (0.202404 --> 0.182293).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25974545140463606, 'val/loss': 0.18229291774332523, 'test/loss': 0.3361465474590659, '_timestamp': 1762882692.7169647}).
Epoch: 3, Steps: 133 | Train Loss: 0.2446369 Vali Loss: 0.1813642 Test Loss: 0.3374874
Validation loss decreased (0.182293 --> 0.181364).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2399573 Vali Loss: 0.1811207 Test Loss: 0.3281503
Validation loss decreased (0.181364 --> 0.181121).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2363239 Vali Loss: 0.1787977 Test Loss: 0.3293619
Validation loss decreased (0.181121 --> 0.178798).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2356610 Vali Loss: 0.1788923 Test Loss: 0.3284678
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2357083 Vali Loss: 0.1802031 Test Loss: 0.3267751
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2350271 Vali Loss: 0.1795183 Test Loss: 0.3272771
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2343162 Vali Loss: 0.1803952 Test Loss: 0.3264988
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2347255 Vali Loss: 0.1932403 Test Loss: 0.3266927
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2355365 Vali Loss: 0.1763500 Test Loss: 0.3266457
Validation loss decreased (0.178798 --> 0.176350).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2347316 Vali Loss: 0.1809440 Test Loss: 0.3267349
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2340921 Vali Loss: 0.2010205 Test Loss: 0.3267204
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2343193 Vali Loss: 0.1951393 Test Loss: 0.3266813
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2347223 Vali Loss: 0.1942671 Test Loss: 0.3266739
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2346299 Vali Loss: 0.1930598 Test Loss: 0.3266729
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2341679 Vali Loss: 0.1865925 Test Loss: 0.3266735
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2349156 Vali Loss: 0.1950135 Test Loss: 0.3266738
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2339309 Vali Loss: 0.1860524 Test Loss: 0.3266739
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2347026 Vali Loss: 0.1799061 Test Loss: 0.3266740
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2347149 Vali Loss: 0.1791286 Test Loss: 0.3266742
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NVIDIA_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.001171806245110929, mae:0.025821920484304428, rmse:0.034231655299663544, r2:-0.03784143924713135, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0258, RMSE: 0.0342, RÂ²: -0.0378, MAPE: 847138.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.524 MB of 0.524 MB uploadedwandb: \ 0.524 MB of 0.524 MB uploadedwandb: | 0.524 MB of 0.524 MB uploadedwandb: / 0.524 MB of 0.524 MB uploadedwandb: - 0.524 MB of 0.730 MB uploadedwandb: \ 0.524 MB of 0.730 MB uploadedwandb: | 0.730 MB of 0.730 MB uploadedwandb: / 0.730 MB of 0.730 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–‚â–â–â–‚â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–†â–â–‚â–ˆâ–†â–†â–†â–„â–†â–„â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.32667
wandb:                 train/loss 0.23471
wandb:   val/directional_accuracy 46.17021
wandb:                   val/loss 0.17913
wandb:                    val/mae 0.02582
wandb:                   val/mape 84713875.0
wandb:                    val/mse 0.00117
wandb:                     val/r2 -0.03784
wandb:                   val/rmse 0.03423
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/lct1ptga
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_193730-lct1ptga/logs
Completed: NVIDIA H=5

Training: Autoformer on NVIDIA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_194226-j23egwp3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/j23egwp3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NVIDIA_H10Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/j23egwp3
>>>>>>>start training : long_term_forecast_Autoformer_NVIDIA_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2896229 Vali Loss: 0.1963093 Test Loss: 0.3668997
Validation loss decreased (inf --> 0.196309).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2896229417476439, 'val/loss': 0.19630925171077251, 'test/loss': 0.3668996747583151, '_timestamp': 1762882979.0798893}).
Epoch: 2, Steps: 133 | Train Loss: 0.2540279 Vali Loss: 0.1866667 Test Loss: 0.3627859
Validation loss decreased (0.196309 --> 0.186667).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2445285 Vali Loss: 0.1820336 Test Loss: 0.3547019
Validation loss decreased (0.186667 --> 0.182034).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2540278891871746, 'val/loss': 0.1866666842252016, 'test/loss': 0.3627859381958842, '_timestamp': 1762882989.7478726}).
Epoch: 4, Steps: 133 | Train Loss: 0.2400852 Vali Loss: 0.2060735 Test Loss: 0.3564129
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2384722 Vali Loss: 0.1925819 Test Loss: 0.3503104
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2368145 Vali Loss: 0.1746722 Test Loss: 0.3504977
Validation loss decreased (0.182034 --> 0.174672).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2366007 Vali Loss: 0.1827872 Test Loss: 0.3528084
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2368215 Vali Loss: 0.1830265 Test Loss: 0.3524337
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2372319 Vali Loss: 0.1807514 Test Loss: 0.3521077
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2367236 Vali Loss: 0.1938275 Test Loss: 0.3518605
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2363580 Vali Loss: 0.1885332 Test Loss: 0.3518626
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2363533 Vali Loss: 0.1739643 Test Loss: 0.3520787
Validation loss decreased (0.174672 --> 0.173964).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2367182 Vali Loss: 0.1997906 Test Loss: 0.3524059
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2366904 Vali Loss: 0.1843632 Test Loss: 0.3523622
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2355601 Vali Loss: 0.1912231 Test Loss: 0.3523254
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2364687 Vali Loss: 0.1752213 Test Loss: 0.3523245
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2378480 Vali Loss: 0.2128732 Test Loss: 0.3523252
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2363379 Vali Loss: 0.1731319 Test Loss: 0.3523243
Validation loss decreased (0.173964 --> 0.173132).  Saving model ...
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2357246 Vali Loss: 0.1802839 Test Loss: 0.3523250
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2367806 Vali Loss: 0.1772408 Test Loss: 0.3523250
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2365531 Vali Loss: 0.1780913 Test Loss: 0.3523246
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2365293 Vali Loss: 0.2090924 Test Loss: 0.3523251
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2374564 Vali Loss: 0.1794328 Test Loss: 0.3523250
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2368225 Vali Loss: 0.1752796 Test Loss: 0.3523249
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2359129 Vali Loss: 0.2001196 Test Loss: 0.3523250
EarlyStopping counter: 7 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2361740 Vali Loss: 0.1748559 Test Loss: 0.3523252
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.2355578 Vali Loss: 0.1745129 Test Loss: 0.3523250
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.2358715 Vali Loss: 0.1780963 Test Loss: 0.3523250
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NVIDIA_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.0011744610965251923, mae:0.025905074551701546, rmse:0.03427041321992874, r2:-0.023830056190490723, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0259, RMSE: 0.0343, RÂ²: -0.0238, MAPE: 783712.38%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.562 MB of 0.562 MB uploadedwandb: \ 0.562 MB of 0.562 MB uploadedwandb: | 0.562 MB of 0.562 MB uploadedwandb: / 0.562 MB of 0.562 MB uploadedwandb: - 0.562 MB of 0.768 MB uploadedwandb: \ 0.768 MB of 0.768 MB uploadedwandb: | 0.768 MB of 0.768 MB uploadedwandb: / 0.768 MB of 0.768 MB uploadedwandb: - 0.768 MB of 0.768 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–†â–ˆâ–â–â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–ƒâ–‚â–â–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–‡â–„â–â–ƒâ–ƒâ–‚â–…â–„â–â–†â–ƒâ–„â–â–ˆâ–â–‚â–‚â–‚â–‡â–‚â–â–†â–â–â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 27
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.35233
wandb:                 train/loss 0.23587
wandb:   val/directional_accuracy 52.27053
wandb:                   val/loss 0.1781
wandb:                    val/mae 0.02591
wandb:                   val/mape 78371237.5
wandb:                    val/mse 0.00117
wandb:                     val/r2 -0.02383
wandb:                   val/rmse 0.03427
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/j23egwp3
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_194226-j23egwp3/logs
Completed: NVIDIA H=10

Training: Autoformer on NVIDIA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_194809-i64xe9fl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/i64xe9fl
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NVIDIA_H22Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/i64xe9fl
>>>>>>>start training : long_term_forecast_Autoformer_NVIDIA_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2827455 Vali Loss: 0.1948413 Test Loss: 0.4371987
Validation loss decreased (inf --> 0.194841).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.28274548889109585, 'val/loss': 0.1948413018669401, 'test/loss': 0.43719873258045744, '_timestamp': 1762883321.6981025}).
Epoch: 2, Steps: 132 | Train Loss: 0.2527918 Vali Loss: 0.1893195 Test Loss: 0.4284898
Validation loss decreased (0.194841 --> 0.189319).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2457309 Vali Loss: 0.1878520 Test Loss: 0.4445307
Validation loss decreased (0.189319 --> 0.187852).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25279178334908053, 'val/loss': 0.18931949990136282, 'test/loss': 0.42848983832768034, '_timestamp': 1762883333.4935217}).
Epoch: 4, Steps: 132 | Train Loss: 0.2425275 Vali Loss: 0.1880133 Test Loss: 0.4251010
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2405564 Vali Loss: 0.1872822 Test Loss: 0.4299313
Validation loss decreased (0.187852 --> 0.187282).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2395889 Vali Loss: 0.1907536 Test Loss: 0.4309912
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2392993 Vali Loss: 0.1886407 Test Loss: 0.4279311
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2388750 Vali Loss: 0.1892478 Test Loss: 0.4284644
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2387165 Vali Loss: 0.1896412 Test Loss: 0.4279462
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2386739 Vali Loss: 0.1892914 Test Loss: 0.4284197
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2385619 Vali Loss: 0.1890894 Test Loss: 0.4284297
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2383602 Vali Loss: 0.1879389 Test Loss: 0.4284988
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2383465 Vali Loss: 0.1888269 Test Loss: 0.4285209
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2384374 Vali Loss: 0.1879487 Test Loss: 0.4285278
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2385488 Vali Loss: 0.1899590 Test Loss: 0.4285450
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NVIDIA_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.0012169749243184924, mae:0.02628374472260475, rmse:0.03488516807556152, r2:-0.03146088123321533, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0263, RMSE: 0.0349, RÂ²: -0.0315, MAPE: 565388.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.586 MB of 0.587 MB uploadedwandb: \ 0.586 MB of 0.587 MB uploadedwandb: | 0.587 MB of 0.587 MB uploadedwandb: / 0.587 MB of 0.587 MB uploadedwandb: - 0.587 MB of 0.791 MB uploadedwandb: \ 0.791 MB of 0.791 MB uploadedwandb: | 0.791 MB of 0.791 MB uploadedwandb: / 0.791 MB of 0.791 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–‚â–â–ˆâ–„â–…â–†â–…â–…â–‚â–„â–‚â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.42854
wandb:                 train/loss 0.23855
wandb:   val/directional_accuracy 50.37134
wandb:                   val/loss 0.18996
wandb:                    val/mae 0.02628
wandb:                   val/mape 56538800.0
wandb:                    val/mse 0.00122
wandb:                     val/r2 -0.03146
wandb:                   val/rmse 0.03489
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/i64xe9fl
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_194809-i64xe9fl/logs
Completed: NVIDIA H=22

Training: Autoformer on NVIDIA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_195153-bx5i2a3c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/bx5i2a3c
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NVIDIA_H50Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/bx5i2a3c
>>>>>>>start training : long_term_forecast_Autoformer_NVIDIA_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.2879563 Vali Loss: 0.2083404 Test Loss: 0.5586437
Validation loss decreased (inf --> 0.208340).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.28795627098191867, 'val/loss': 0.2083403617143631, 'test/loss': 0.55864367634058, '_timestamp': 1762883542.3062832}).
Epoch: 2, Steps: 132 | Train Loss: 0.2601156 Vali Loss: 0.2106595 Test Loss: 0.5675513
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2536864 Vali Loss: 0.2035300 Test Loss: 0.5438859
Validation loss decreased (0.208340 --> 0.203530).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26011556951385556, 'val/loss': 0.21065946171681085, 'test/loss': 0.567551251500845, '_timestamp': 1762883553.0768166}).
Epoch: 4, Steps: 132 | Train Loss: 0.2513689 Vali Loss: 0.2056646 Test Loss: 0.5535706
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2497976 Vali Loss: 0.2063321 Test Loss: 0.5376996
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2483981 Vali Loss: 0.2066175 Test Loss: 0.5348490
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2477733 Vali Loss: 0.2064489 Test Loss: 0.5355300
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2475590 Vali Loss: 0.2075235 Test Loss: 0.5357059
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2477867 Vali Loss: 0.2070019 Test Loss: 0.5364071
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2476323 Vali Loss: 0.2077266 Test Loss: 0.5366193
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2474708 Vali Loss: 0.2076605 Test Loss: 0.5367390
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2471400 Vali Loss: 0.2076500 Test Loss: 0.5366459
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2468877 Vali Loss: 0.2077524 Test Loss: 0.5366202
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NVIDIA_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.0012080894084647298, mae:0.026512041687965393, rmse:0.0347575806081295, r2:-0.015208005905151367, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0265, RMSE: 0.0348, RÂ²: -0.0152, MAPE: 334660.78%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.632 MB of 0.634 MB uploadedwandb: \ 0.632 MB of 0.634 MB uploadedwandb: | 0.632 MB of 0.634 MB uploadedwandb: / 0.634 MB of 0.634 MB uploadedwandb: - 0.634 MB of 0.634 MB uploadedwandb: \ 0.634 MB of 0.634 MB uploadedwandb: | 0.634 MB of 0.838 MB uploadedwandb: / 0.634 MB of 0.838 MB uploadedwandb: - 0.838 MB of 0.838 MB uploadedwandb: \ 0.838 MB of 0.838 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–„â–ˆâ–‚â–â–â–â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–†â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–…â–†â–†â–†â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.53662
wandb:                 train/loss 0.24689
wandb:   val/directional_accuracy 51.28894
wandb:                   val/loss 0.20775
wandb:                    val/mae 0.02651
wandb:                   val/mape 33466078.125
wandb:                    val/mse 0.00121
wandb:                     val/r2 -0.01521
wandb:                   val/rmse 0.03476
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/bx5i2a3c
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_195153-bx5i2a3c/logs
Completed: NVIDIA H=50

Training: Autoformer on NVIDIA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_195653-1moe1g0r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/1moe1g0r
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NVIDIA_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/1moe1g0r
>>>>>>>start training : long_term_forecast_Autoformer_NVIDIA_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.3054019 Vali Loss: 0.2303090 Test Loss: 0.8291393
Validation loss decreased (inf --> 0.230309).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3054019002960278, 'val/loss': 0.2303089588880539, 'test/loss': 0.8291392743587493, '_timestamp': 1762883847.544739}).
Epoch: 2, Steps: 130 | Train Loss: 0.2802191 Vali Loss: 0.2287481 Test Loss: 0.8282072
Validation loss decreased (0.230309 --> 0.228748).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.2755358 Vali Loss: 0.2373758 Test Loss: 0.8075912
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2802191299887804, 'val/loss': 0.22874810695648193, 'test/loss': 0.8282072067260742, '_timestamp': 1762883858.0052357}).
Epoch: 4, Steps: 130 | Train Loss: 0.2734030 Vali Loss: 0.2284191 Test Loss: 0.8197074
Validation loss decreased (0.228748 --> 0.228419).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2721373 Vali Loss: 0.2356134 Test Loss: 0.8149082
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2715898 Vali Loss: 0.2368633 Test Loss: 0.8119760
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2718637 Vali Loss: 0.2423607 Test Loss: 0.8200042
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2715518 Vali Loss: 0.2385796 Test Loss: 0.8175335
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2715472 Vali Loss: 0.2333570 Test Loss: 0.8176840
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2712464 Vali Loss: 0.2371185 Test Loss: 0.8177166
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2713411 Vali Loss: 0.2408917 Test Loss: 0.8176158
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2715080 Vali Loss: 0.2374088 Test Loss: 0.8175800
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2714389 Vali Loss: 0.2391459 Test Loss: 0.8175742
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.2712570 Vali Loss: 0.2397069 Test Loss: 0.8175784
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NVIDIA_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.001306304708123207, mae:0.027484392747282982, rmse:0.03614283725619316, r2:-0.014672279357910156, dtw:Not calculated


VAL - MSE: 0.0013, MAE: 0.0275, RMSE: 0.0361, RÂ²: -0.0147, MAPE: 237254.47%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.682 MB of 0.687 MB uploadedwandb: \ 0.682 MB of 0.687 MB uploadedwandb: | 0.687 MB of 0.687 MB uploadedwandb: / 0.687 MB of 0.687 MB uploadedwandb: - 0.687 MB of 0.891 MB uploadedwandb: \ 0.891 MB of 0.891 MB uploadedwandb: | 0.891 MB of 0.891 MB uploadedwandb: / 0.891 MB of 0.891 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–…â–ƒâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:                 train/loss â–ˆâ–…â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–â–…â–…â–ˆâ–†â–ƒâ–…â–‡â–†â–†â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 13
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.81758
wandb:                 train/loss 0.27126
wandb:   val/directional_accuracy 49.74026
wandb:                   val/loss 0.23971
wandb:                    val/mae 0.02748
wandb:                   val/mape 23725446.875
wandb:                    val/mse 0.00131
wandb:                     val/r2 -0.01467
wandb:                   val/rmse 0.03614
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/1moe1g0r
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_195653-1moe1g0r/logs
Completed: NVIDIA H=100

Training: Autoformer on APPLE for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_200115-kd8t3y7y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/kd8t3y7y
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_APPLE_H3 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/kd8t3y7y
>>>>>>>start training : long_term_forecast_Autoformer_APPLE_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.2977457 Vali Loss: 0.0966607 Test Loss: 0.1430276
Validation loss decreased (inf --> 0.096661).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2977457491302849, 'val/loss': 0.0966606829315424, 'test/loss': 0.14302761480212212, '_timestamp': 1762884106.6118898}).
Epoch: 2, Steps: 133 | Train Loss: 0.2527218 Vali Loss: 0.1005876 Test Loss: 0.1432603
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2363716 Vali Loss: 0.0902098 Test Loss: 0.1339765
Validation loss decreased (0.096661 --> 0.090210).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25272182425610107, 'val/loss': 0.1005876213312149, 'test/loss': 0.1432603020220995, '_timestamp': 1762884117.8642097}).
Epoch: 4, Steps: 133 | Train Loss: 0.2303020 Vali Loss: 0.0896405 Test Loss: 0.1306847
Validation loss decreased (0.090210 --> 0.089641).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2255205 Vali Loss: 0.0880632 Test Loss: 0.1309959
Validation loss decreased (0.089641 --> 0.088063).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2247961 Vali Loss: 0.0837052 Test Loss: 0.1294240
Validation loss decreased (0.088063 --> 0.083705).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2238430 Vali Loss: 0.0860414 Test Loss: 0.1292977
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2240657 Vali Loss: 0.0869297 Test Loss: 0.1295048
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2233603 Vali Loss: 0.0866051 Test Loss: 0.1293593
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2229366 Vali Loss: 0.0851383 Test Loss: 0.1293495
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2234366 Vali Loss: 0.0861513 Test Loss: 0.1292893
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2235162 Vali Loss: 0.0909379 Test Loss: 0.1292605
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2226094 Vali Loss: 0.0874800 Test Loss: 0.1292653
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2231083 Vali Loss: 0.0893337 Test Loss: 0.1292577
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2228452 Vali Loss: 0.0906990 Test Loss: 0.1292552
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2225615 Vali Loss: 0.0839659 Test Loss: 0.1292544
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_APPLE_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.00020697522268164903, mae:0.010508128441870213, rmse:0.014386633411049843, r2:-0.035172462463378906, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0105, RMSE: 0.0144, RÂ²: -0.0352, MAPE: 455414.22%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.480 MB of 0.481 MB uploadedwandb: \ 0.480 MB of 0.481 MB uploadedwandb: | 0.480 MB of 0.481 MB uploadedwandb: / 0.481 MB of 0.481 MB uploadedwandb: - 0.481 MB of 0.481 MB uploadedwandb: \ 0.481 MB of 0.481 MB uploadedwandb: | 0.481 MB of 0.481 MB uploadedwandb: / 0.481 MB of 0.481 MB uploadedwandb: - 0.481 MB of 0.481 MB uploadedwandb: \ 0.611 MB of 0.815 MB uploaded (0.002 MB deduped)wandb: | 0.815 MB of 0.815 MB uploaded (0.002 MB deduped)wandb: / 0.815 MB of 0.815 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–„â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–‡â–…â–â–ƒâ–„â–„â–‚â–ƒâ–ˆâ–…â–†â–ˆâ–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.12925
wandb:                 train/loss 0.22256
wandb:   val/directional_accuracy 49.57806
wandb:                   val/loss 0.08397
wandb:                    val/mae 0.01051
wandb:                   val/mape 45541421.875
wandb:                    val/mse 0.00021
wandb:                     val/r2 -0.03517
wandb:                   val/rmse 0.01439
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/kd8t3y7y
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_200115-kd8t3y7y/logs
Completed: APPLE H=3

Training: Autoformer on APPLE for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_200512-iupo8xz1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/iupo8xz1
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_APPLE_H5 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/iupo8xz1
>>>>>>>start training : long_term_forecast_Autoformer_APPLE_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.2882580 Vali Loss: 0.0955469 Test Loss: 0.1452188
Validation loss decreased (inf --> 0.095547).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2882580162215054, 'val/loss': 0.09554694779217243, 'test/loss': 0.14521884080022573, '_timestamp': 1762884344.8023176}).
Epoch: 2, Steps: 133 | Train Loss: 0.2494178 Vali Loss: 0.0899835 Test Loss: 0.1440133
Validation loss decreased (0.095547 --> 0.089984).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2363362 Vali Loss: 0.0932653 Test Loss: 0.1430428
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24941776950556532, 'val/loss': 0.08998352754861116, 'test/loss': 0.14401331637054682, '_timestamp': 1762884355.4758399}).
Epoch: 4, Steps: 133 | Train Loss: 0.2306382 Vali Loss: 0.0888589 Test Loss: 0.1371198
Validation loss decreased (0.089984 --> 0.088859).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2295592 Vali Loss: 0.0920919 Test Loss: 0.1338665
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2258696 Vali Loss: 0.0874775 Test Loss: 0.1370886
Validation loss decreased (0.088859 --> 0.087478).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2266509 Vali Loss: 0.0867642 Test Loss: 0.1361817
Validation loss decreased (0.087478 --> 0.086764).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2267145 Vali Loss: 0.0891009 Test Loss: 0.1356826
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2270530 Vali Loss: 0.0872276 Test Loss: 0.1354894
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2263457 Vali Loss: 0.0861042 Test Loss: 0.1356370
Validation loss decreased (0.086764 --> 0.086104).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2258643 Vali Loss: 0.0861906 Test Loss: 0.1358380
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2265980 Vali Loss: 0.0865525 Test Loss: 0.1360357
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2258332 Vali Loss: 0.0866657 Test Loss: 0.1354746
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2261218 Vali Loss: 0.0861698 Test Loss: 0.1354886
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2262918 Vali Loss: 0.0867197 Test Loss: 0.1354772
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2261287 Vali Loss: 0.0855684 Test Loss: 0.1354773
Validation loss decreased (0.086104 --> 0.085568).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2255921 Vali Loss: 0.0872549 Test Loss: 0.1354764
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2247456 Vali Loss: 0.0861508 Test Loss: 0.1354768
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2261097 Vali Loss: 0.0869144 Test Loss: 0.1354771
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2256997 Vali Loss: 0.0897548 Test Loss: 0.1354769
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2273518 Vali Loss: 0.0862572 Test Loss: 0.1354766
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2256486 Vali Loss: 0.0897021 Test Loss: 0.1354769
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2260339 Vali Loss: 0.0889640 Test Loss: 0.1354772
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2259228 Vali Loss: 0.0855869 Test Loss: 0.1354769
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2259438 Vali Loss: 0.0878249 Test Loss: 0.1354766
EarlyStopping counter: 9 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2257343 Vali Loss: 0.0863005 Test Loss: 0.1354767
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_APPLE_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.00021236835164017975, mae:0.010581406764686108, rmse:0.014572863467037678, r2:-0.057297706604003906, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0106, RMSE: 0.0146, RÂ²: -0.0573, MAPE: 392687.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.517 MB of 0.518 MB uploadedwandb: \ 0.517 MB of 0.518 MB uploadedwandb: | 0.518 MB of 0.518 MB uploadedwandb: / 0.518 MB of 0.518 MB uploadedwandb: - 0.518 MB of 0.724 MB uploadedwandb: \ 0.686 MB of 0.724 MB uploadedwandb: | 0.686 MB of 0.724 MB uploadedwandb: / 0.724 MB of 0.724 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–‡â–ƒâ–‚â–„â–ƒâ–â–‚â–‚â–‚â–‚â–‚â–â–ƒâ–‚â–‚â–…â–‚â–…â–„â–â–ƒâ–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 25
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.13548
wandb:                 train/loss 0.22573
wandb:   val/directional_accuracy 44.14894
wandb:                   val/loss 0.0863
wandb:                    val/mae 0.01058
wandb:                   val/mape 39268775.0
wandb:                    val/mse 0.00021
wandb:                     val/r2 -0.0573
wandb:                   val/rmse 0.01457
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/iupo8xz1
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_200512-iupo8xz1/logs
Completed: APPLE H=5

Training: Autoformer on APPLE for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_201153-icp3i90u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/icp3i90u
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_APPLE_H10Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/icp3i90u
>>>>>>>start training : long_term_forecast_Autoformer_APPLE_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2774969 Vali Loss: 0.1027220 Test Loss: 0.1470549
Validation loss decreased (inf --> 0.102722).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.27749693886678023, 'val/loss': 0.10272204782813787, 'test/loss': 0.14705488085746765, '_timestamp': 1762884746.5070245}).
Epoch: 2, Steps: 133 | Train Loss: 0.2446197 Vali Loss: 0.0970834 Test Loss: 0.1401246
Validation loss decreased (0.102722 --> 0.097083).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24461971100111654, 'val/loss': 0.09708338603377342, 'test/loss': 0.14012464322149754, '_timestamp': 1762884759.616153}).
Epoch: 3, Steps: 133 | Train Loss: 0.2350076 Vali Loss: 0.0925861 Test Loss: 0.1435914
Validation loss decreased (0.097083 --> 0.092586).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2294169 Vali Loss: 0.0953062 Test Loss: 0.1393724
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2277939 Vali Loss: 0.0910663 Test Loss: 0.1403385
Validation loss decreased (0.092586 --> 0.091066).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2255517 Vali Loss: 0.0911517 Test Loss: 0.1392510
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2250117 Vali Loss: 0.0904685 Test Loss: 0.1387697
Validation loss decreased (0.091066 --> 0.090468).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2252416 Vali Loss: 0.0892012 Test Loss: 0.1391637
Validation loss decreased (0.090468 --> 0.089201).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2256409 Vali Loss: 0.0892749 Test Loss: 0.1385600
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2242912 Vali Loss: 0.0909778 Test Loss: 0.1383134
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2243101 Vali Loss: 0.0885367 Test Loss: 0.1382922
Validation loss decreased (0.089201 --> 0.088537).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2237665 Vali Loss: 0.0933569 Test Loss: 0.1383021
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2247478 Vali Loss: 0.0900580 Test Loss: 0.1382964
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2251468 Vali Loss: 0.0917075 Test Loss: 0.1383044
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2238496 Vali Loss: 0.0897892 Test Loss: 0.1383054
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2244497 Vali Loss: 0.0907523 Test Loss: 0.1383743
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2252091 Vali Loss: 0.0906148 Test Loss: 0.1383741
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2238671 Vali Loss: 0.0916837 Test Loss: 0.1383741
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2243030 Vali Loss: 0.0910610 Test Loss: 0.1383742
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2249002 Vali Loss: 0.0906280 Test Loss: 0.1383743
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2257333 Vali Loss: 0.0908577 Test Loss: 0.1383743
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_APPLE_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.00021889375057071447, mae:0.01074229832738638, rmse:0.014795058406889439, r2:-0.08113443851470947, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0107, RMSE: 0.0148, RÂ²: -0.0811, MAPE: 626186.44%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.581 MB of 0.582 MB uploadedwandb: \ 0.582 MB of 0.582 MB uploadedwandb: | 0.582 MB of 0.582 MB uploadedwandb: / 0.582 MB of 0.582 MB uploadedwandb: - 0.582 MB of 0.582 MB uploadedwandb: \ 0.582 MB of 0.787 MB uploadedwandb: | 0.582 MB of 0.787 MB uploadedwandb: / 0.787 MB of 0.787 MB uploadedwandb: - 0.787 MB of 0.787 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–„â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–â–â–â–‚â–‚â–â–â–‚â–â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–ˆâ–„â–„â–ƒâ–‚â–‚â–„â–â–†â–ƒâ–„â–‚â–ƒâ–ƒâ–„â–„â–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.13837
wandb:                 train/loss 0.22573
wandb:   val/directional_accuracy 50.09662
wandb:                   val/loss 0.09086
wandb:                    val/mae 0.01074
wandb:                   val/mape 62618643.75
wandb:                    val/mse 0.00022
wandb:                     val/r2 -0.08113
wandb:                   val/rmse 0.0148
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/icp3i90u
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_201153-icp3i90u/logs
Completed: APPLE H=10

Training: Autoformer on APPLE for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_201706-et8fjun1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/et8fjun1
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_APPLE_H22Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/et8fjun1
>>>>>>>start training : long_term_forecast_Autoformer_APPLE_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2778097 Vali Loss: 0.0946690 Test Loss: 0.1386249
Validation loss decreased (inf --> 0.094669).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2778097245503556, 'val/loss': 0.09466895780393056, 'test/loss': 0.13862492463418416, '_timestamp': 1762885058.201197}).
Epoch: 2, Steps: 132 | Train Loss: 0.2467059 Vali Loss: 0.0918060 Test Loss: 0.1357591
Validation loss decreased (0.094669 --> 0.091806).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2467059183752898, 'val/loss': 0.09180603069918496, 'test/loss': 0.13575906732252666, '_timestamp': 1762885068.8098292}).
Epoch: 3, Steps: 132 | Train Loss: 0.2389610 Vali Loss: 0.0913761 Test Loss: 0.1405974
Validation loss decreased (0.091806 --> 0.091376).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2352344 Vali Loss: 0.0917195 Test Loss: 0.1415535
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2329332 Vali Loss: 0.0902077 Test Loss: 0.1391933
Validation loss decreased (0.091376 --> 0.090208).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2318311 Vali Loss: 0.0898888 Test Loss: 0.1400923
Validation loss decreased (0.090208 --> 0.089889).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2312432 Vali Loss: 0.0900436 Test Loss: 0.1399058
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2307066 Vali Loss: 0.0908884 Test Loss: 0.1396600
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2308035 Vali Loss: 0.0907010 Test Loss: 0.1396247
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2303711 Vali Loss: 0.0909784 Test Loss: 0.1396611
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2307443 Vali Loss: 0.0906611 Test Loss: 0.1396939
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2303572 Vali Loss: 0.0909246 Test Loss: 0.1397329
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2302322 Vali Loss: 0.0909110 Test Loss: 0.1397373
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2306079 Vali Loss: 0.0906095 Test Loss: 0.1397335
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2307155 Vali Loss: 0.0905629 Test Loss: 0.1397365
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2304997 Vali Loss: 0.0906842 Test Loss: 0.1397385
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_APPLE_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.0002225355274276808, mae:0.01077259611338377, rmse:0.014917625114321709, r2:-0.07243716716766357, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0108, RMSE: 0.0149, RÂ²: -0.0724, MAPE: 567231.88%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.647 MB of 0.649 MB uploadedwandb: \ 0.647 MB of 0.649 MB uploadedwandb: | 0.649 MB of 0.649 MB uploadedwandb: / 0.649 MB of 0.649 MB uploadedwandb: - 0.649 MB of 0.649 MB uploadedwandb: \ 0.649 MB of 0.853 MB uploadedwandb: | 0.853 MB of 0.853 MB uploadedwandb: / 0.853 MB of 0.853 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–ˆâ–â–„â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–ˆâ–‚â–â–‚â–…â–„â–…â–„â–…â–…â–„â–„â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.13974
wandb:                 train/loss 0.2305
wandb:   val/directional_accuracy 48.86413
wandb:                   val/loss 0.09068
wandb:                    val/mae 0.01077
wandb:                   val/mape 56723187.5
wandb:                    val/mse 0.00022
wandb:                     val/r2 -0.07244
wandb:                   val/rmse 0.01492
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/et8fjun1
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_201706-et8fjun1/logs
Completed: APPLE H=22

Training: Autoformer on APPLE for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_202231-25pn9qh7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/25pn9qh7
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_APPLE_H50Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/25pn9qh7
>>>>>>>start training : long_term_forecast_Autoformer_APPLE_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.2839285 Vali Loss: 0.1002714 Test Loss: 0.1611205
Validation loss decreased (inf --> 0.100271).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2839284947875774, 'val/loss': 0.10027138143777847, 'test/loss': 0.16112049917380014, '_timestamp': 1762885385.166119}).
Epoch: 2, Steps: 132 | Train Loss: 0.2548238 Vali Loss: 0.0961141 Test Loss: 0.1555230
Validation loss decreased (0.100271 --> 0.096114).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2548238132713419, 'val/loss': 0.09611406301458676, 'test/loss': 0.15552299718062082, '_timestamp': 1762885395.341442}).
Epoch: 3, Steps: 132 | Train Loss: 0.2450044 Vali Loss: 0.0942670 Test Loss: 0.1545711
Validation loss decreased (0.096114 --> 0.094267).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2419354 Vali Loss: 0.0930888 Test Loss: 0.1561789
Validation loss decreased (0.094267 --> 0.093089).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2406017 Vali Loss: 0.0909054 Test Loss: 0.1534960
Validation loss decreased (0.093089 --> 0.090905).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2394408 Vali Loss: 0.0929156 Test Loss: 0.1557992
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2383848 Vali Loss: 0.0934848 Test Loss: 0.1565549
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2378539 Vali Loss: 0.0934860 Test Loss: 0.1563622
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2384112 Vali Loss: 0.0937711 Test Loss: 0.1567840
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2385875 Vali Loss: 0.0938110 Test Loss: 0.1568959
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2386374 Vali Loss: 0.0937532 Test Loss: 0.1567289
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2380863 Vali Loss: 0.0936249 Test Loss: 0.1566576
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2380385 Vali Loss: 0.0936495 Test Loss: 0.1566120
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2374199 Vali Loss: 0.0937257 Test Loss: 0.1566110
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2377335 Vali Loss: 0.0936546 Test Loss: 0.1566041
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_APPLE_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.00023597291146870703, mae:0.011133217252790928, rmse:0.01536140963435173, r2:-0.06353974342346191, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0111, RMSE: 0.0154, RÂ²: -0.0635, MAPE: 368510.34%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.730 MB of 0.732 MB uploadedwandb: \ 0.730 MB of 0.732 MB uploadedwandb: | 0.732 MB of 0.732 MB uploadedwandb: / 0.732 MB of 0.732 MB uploadedwandb: - 0.732 MB of 0.936 MB uploadedwandb: \ 0.936 MB of 0.936 MB uploadedwandb: | 0.936 MB of 0.936 MB uploadedwandb: / 0.936 MB of 0.936 MB uploadedwandb: - 0.936 MB of 0.936 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ƒâ–‡â–â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡
wandb:                 train/loss â–ˆâ–…â–„â–ƒâ–‚â–â–‚â–‚â–‚â–‚â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.1566
wandb:                 train/loss 0.23773
wandb:   val/directional_accuracy 48.23845
wandb:                   val/loss 0.09365
wandb:                    val/mae 0.01113
wandb:                   val/mape 36851034.375
wandb:                    val/mse 0.00024
wandb:                     val/r2 -0.06354
wandb:                   val/rmse 0.01536
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/25pn9qh7
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_202231-25pn9qh7/logs
Completed: APPLE H=50

Training: Autoformer on APPLE for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_202708-gxf5s2zh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/gxf5s2zh
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_APPLE_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/gxf5s2zh
>>>>>>>start training : long_term_forecast_Autoformer_APPLE_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.2972880 Vali Loss: 0.0995323 Test Loss: 0.1655733
Validation loss decreased (inf --> 0.099532).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2972879640184916, 'val/loss': 0.09953225702047348, 'test/loss': 0.16557333171367644, '_timestamp': 1762885659.953867}).
Epoch: 2, Steps: 130 | Train Loss: 0.2709301 Vali Loss: 0.0952680 Test Loss: 0.1651535
Validation loss decreased (0.099532 --> 0.095268).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.2642930 Vali Loss: 0.0998367 Test Loss: 0.1680555
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2709301267678921, 'val/loss': 0.09526801258325576, 'test/loss': 0.16515350639820098, '_timestamp': 1762885671.50382}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2709301267678921, 'val/loss': 0.09526801258325576, 'test/loss': 0.16515350639820098, '_timestamp': 1762885671.50382}).
Epoch: 4, Steps: 130 | Train Loss: 0.2614540 Vali Loss: 0.0970633 Test Loss: 0.1681230
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2599007 Vali Loss: 0.0973349 Test Loss: 0.1693089
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2595564 Vali Loss: 0.0976054 Test Loss: 0.1675964
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2591132 Vali Loss: 0.0981658 Test Loss: 0.1682635
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2583288 Vali Loss: 0.0983799 Test Loss: 0.1675837
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2582585 Vali Loss: 0.0975198 Test Loss: 0.1678395
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2580742 Vali Loss: 0.0978153 Test Loss: 0.1677080
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2583349 Vali Loss: 0.0978472 Test Loss: 0.1676751
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2585470 Vali Loss: 0.0979991 Test Loss: 0.1676931
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_APPLE_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.00024839400430209935, mae:0.011314795352518559, rmse:0.015760520473122597, r2:-0.05201864242553711, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0113, RMSE: 0.0158, RÂ²: -0.0520, MAPE: 977523.06%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.730 MB of 0.734 MB uploadedwandb: \ 0.730 MB of 0.734 MB uploadedwandb: | 0.734 MB of 0.734 MB uploadedwandb: / 0.734 MB of 0.734 MB uploadedwandb: - 0.734 MB of 0.938 MB uploadedwandb: \ 0.734 MB of 0.938 MB uploadedwandb: | 0.938 MB of 0.938 MB uploadedwandb: / 0.938 MB of 0.938 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ƒâ–ƒâ–ˆâ–â–„â–â–‚â–‚â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–‚â–‚â–„â–„â–‚â–ƒâ–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.16769
wandb:                 train/loss 0.25855
wandb:   val/directional_accuracy 49.00433
wandb:                   val/loss 0.098
wandb:                    val/mae 0.01131
wandb:                   val/mape 97752306.25
wandb:                    val/mse 0.00025
wandb:                     val/r2 -0.05202
wandb:                   val/rmse 0.01576
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/gxf5s2zh
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_202708-gxf5s2zh/logs
Completed: APPLE H=100

Training: Autoformer on SP500 for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_203053-6e1jxrp8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/6e1jxrp8
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SP500_H3 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/6e1jxrp8
>>>>>>>start training : long_term_forecast_Autoformer_SP500_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 3
============================================================
train 4241
val 237
test 238
Epoch: 1, Steps: 133 | Train Loss: 0.2429840 Vali Loss: 0.0952987 Test Loss: 0.1059295
Validation loss decreased (inf --> 0.095299).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2429840073437619, 'val/loss': 0.09529874008148909, 'test/loss': 0.10592947714030743, '_timestamp': 1762885882.7600377}).
Epoch: 2, Steps: 133 | Train Loss: 0.2058843 Vali Loss: 0.0757823 Test Loss: 0.0769343
Validation loss decreased (0.095299 --> 0.075782).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.1934485 Vali Loss: 0.0738519 Test Loss: 0.0782253
Validation loss decreased (0.075782 --> 0.073852).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.20588430375757075, 'val/loss': 0.0757823484018445, 'test/loss': 0.07693425519391894, '_timestamp': 1762885893.8905041}).
Epoch: 4, Steps: 133 | Train Loss: 0.1881480 Vali Loss: 0.0691017 Test Loss: 0.0757827
Validation loss decreased (0.073852 --> 0.069102).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1855264 Vali Loss: 0.0717240 Test Loss: 0.0773391
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1848672 Vali Loss: 0.0675821 Test Loss: 0.0729311
Validation loss decreased (0.069102 --> 0.067582).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1851550 Vali Loss: 0.0688562 Test Loss: 0.0737523
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1832456 Vali Loss: 0.0687383 Test Loss: 0.0731789
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1831798 Vali Loss: 0.0695925 Test Loss: 0.0733355
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1852079 Vali Loss: 0.0692767 Test Loss: 0.0733965
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1832429 Vali Loss: 0.0682930 Test Loss: 0.0732492
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1829074 Vali Loss: 0.0684960 Test Loss: 0.0732196
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1831388 Vali Loss: 0.0663656 Test Loss: 0.0731709
Validation loss decreased (0.067582 --> 0.066366).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1826054 Vali Loss: 0.0686182 Test Loss: 0.0731972
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1834701 Vali Loss: 0.0715094 Test Loss: 0.0731953
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1842901 Vali Loss: 0.0691230 Test Loss: 0.0731980
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1822726 Vali Loss: 0.0680469 Test Loss: 0.0731952
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1838703 Vali Loss: 0.0680008 Test Loss: 0.0731952
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1826143 Vali Loss: 0.0696332 Test Loss: 0.0731948
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1828220 Vali Loss: 0.0687337 Test Loss: 0.0731950
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1836572 Vali Loss: 0.0684554 Test Loss: 0.0731950
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1835182 Vali Loss: 0.0689481 Test Loss: 0.0731950
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1843618 Vali Loss: 0.0689892 Test Loss: 0.0731951
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SP500_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 238
test shape: (238, 3, 1) (238, 3, 1)
test shape: (238, 3, 1) (238, 3, 1)


	mse:6.583349750144407e-05, mae:0.006001248024404049, rmse:0.008113784715533257, r2:-0.012647628784179688, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0060, RMSE: 0.0081, RÂ²: -0.0126, MAPE: 2.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.486 MB of 0.486 MB uploadedwandb: \ 0.486 MB of 0.486 MB uploadedwandb: | 0.486 MB of 0.486 MB uploadedwandb: / 0.486 MB of 0.486 MB uploadedwandb: - 0.486 MB of 0.486 MB uploadedwandb: \ 0.486 MB of 0.486 MB uploadedwandb: | 0.486 MB of 0.486 MB uploadedwandb: / 0.486 MB of 0.486 MB uploadedwandb: - 0.486 MB of 0.486 MB uploadedwandb: \ 0.616 MB of 0.821 MB uploaded (0.002 MB deduped)wandb: | 0.821 MB of 0.821 MB uploaded (0.002 MB deduped)wandb: / 0.821 MB of 0.821 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–‡â–â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–â–‚â–â–‚â–‚â–â–‚â–â–â–‚â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–†â–‚â–ƒâ–ƒâ–„â–„â–ƒâ–ƒâ–â–ƒâ–†â–„â–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.0732
wandb:                 train/loss 0.18436
wandb:   val/directional_accuracy 51.05042
wandb:                   val/loss 0.06899
wandb:                    val/mae 0.006
wandb:                   val/mape 225.06881
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.01265
wandb:                   val/rmse 0.00811
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/6e1jxrp8
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_203053-6e1jxrp8/logs
Completed: SP500 H=3

Training: Autoformer on SP500 for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_203809-9qgx2rx9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/9qgx2rx9
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SP500_H5 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/9qgx2rx9
>>>>>>>start training : long_term_forecast_Autoformer_SP500_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 5
============================================================
train 4239
val 235
test 236
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
Epoch: 1, Steps: 133 | Train Loss: 0.2376392 Vali Loss: 0.0924943 Test Loss: 0.1064376
Validation loss decreased (inf --> 0.092494).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23763921615996755, 'val/loss': 0.09249428287148476, 'test/loss': 0.10643755830824375, '_timestamp': 1762886324.0136783}).
Epoch: 2, Steps: 133 | Train Loss: 0.1994992 Vali Loss: 0.0731171 Test Loss: 0.0828326
Validation loss decreased (0.092494 --> 0.073117).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.1994991686783339, 'val/loss': 0.07311712019145489, 'test/loss': 0.08283257996663451, '_timestamp': 1762886336.3695052}).
Epoch: 3, Steps: 133 | Train Loss: 0.1877805 Vali Loss: 0.0736655 Test Loss: 0.0802431
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1836687 Vali Loss: 0.0748106 Test Loss: 0.0808763
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1807139 Vali Loss: 0.0723633 Test Loss: 0.0812259
Validation loss decreased (0.073117 --> 0.072363).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1797733 Vali Loss: 0.0708374 Test Loss: 0.0802460
Validation loss decreased (0.072363 --> 0.070837).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1784275 Vali Loss: 0.0695781 Test Loss: 0.0786126
Validation loss decreased (0.070837 --> 0.069578).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1782790 Vali Loss: 0.0706193 Test Loss: 0.0785939
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1789425 Vali Loss: 0.0686850 Test Loss: 0.0787082
Validation loss decreased (0.069578 --> 0.068685).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1778646 Vali Loss: 0.0705151 Test Loss: 0.0786194
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1780040 Vali Loss: 0.0707980 Test Loss: 0.0785404
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1782154 Vali Loss: 0.0704346 Test Loss: 0.0786195
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1788337 Vali Loss: 0.0709445 Test Loss: 0.0785634
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1777228 Vali Loss: 0.0697211 Test Loss: 0.0785466
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1779984 Vali Loss: 0.0687839 Test Loss: 0.0785471
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1777688 Vali Loss: 0.0696346 Test Loss: 0.0785492
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1774586 Vali Loss: 0.0722440 Test Loss: 0.0785508
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1776807 Vali Loss: 0.0706365 Test Loss: 0.0785509
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1777378 Vali Loss: 0.0727384 Test Loss: 0.0785506
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SP500_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
test shape: (236, 5, 1) (236, 5, 1)
test shape: (236, 5, 1) (236, 5, 1)


	mse:6.933050462976098e-05, mae:0.006219218019396067, rmse:0.008326494134962559, r2:-0.06725871562957764, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0062, RMSE: 0.0083, RÂ²: -0.0673, MAPE: 2.51%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.545 MB of 0.545 MB uploadedwandb: \ 0.545 MB of 0.545 MB uploadedwandb: | 0.545 MB of 0.545 MB uploadedwandb: / 0.545 MB of 0.545 MB uploadedwandb: - 0.545 MB of 0.545 MB uploadedwandb: \ 0.545 MB of 0.750 MB uploadedwandb: | 0.750 MB of 0.750 MB uploadedwandb: / 0.750 MB of 0.750 MB uploadedwandb: - 0.750 MB of 0.750 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–‡â–ˆâ–…â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–‚â–‚â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–ˆâ–…â–ƒâ–‚â–ƒâ–â–ƒâ–ƒâ–ƒâ–„â–‚â–â–‚â–…â–ƒâ–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 18
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.07855
wandb:                 train/loss 0.17774
wandb:   val/directional_accuracy 44.49153
wandb:                   val/loss 0.07274
wandb:                    val/mae 0.00622
wandb:                   val/mape 251.30417
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.06726
wandb:                   val/rmse 0.00833
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/9qgx2rx9
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_203809-9qgx2rx9/logs
Completed: SP500 H=5

Training: Autoformer on SP500 for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_204240-fq5yokj9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/fq5yokj9
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SP500_H10Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/fq5yokj9
>>>>>>>start training : long_term_forecast_Autoformer_SP500_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 10
============================================================
train 4234
val 230
test 231
Epoch: 1, Steps: 133 | Train Loss: 0.2231413 Vali Loss: 0.0714276 Test Loss: 0.0812589
Validation loss decreased (inf --> 0.071428).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.22314129237617766, 'val/loss': 0.07142758090049028, 'test/loss': 0.08125894563272595, '_timestamp': 1762886590.9387374}).
Epoch: 2, Steps: 133 | Train Loss: 0.1919308 Vali Loss: 0.0771354 Test Loss: 0.0850700
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.1855648 Vali Loss: 0.0739172 Test Loss: 0.0794548
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.1919307667285876, 'val/loss': 0.07713543623685837, 'test/loss': 0.08506998419761658, '_timestamp': 1762886602.357951}).
Epoch: 4, Steps: 133 | Train Loss: 0.1811327 Vali Loss: 0.0738460 Test Loss: 0.0799458
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1805366 Vali Loss: 0.0716051 Test Loss: 0.0797020
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1788707 Vali Loss: 0.0714546 Test Loss: 0.0806095
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1790180 Vali Loss: 0.0728200 Test Loss: 0.0800017
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1789598 Vali Loss: 0.0693828 Test Loss: 0.0800715
Validation loss decreased (0.071428 --> 0.069383).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1782987 Vali Loss: 0.0694690 Test Loss: 0.0800420
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1784019 Vali Loss: 0.0697289 Test Loss: 0.0800986
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1783648 Vali Loss: 0.0712343 Test Loss: 0.0800999
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1795450 Vali Loss: 0.0698241 Test Loss: 0.0800205
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1788470 Vali Loss: 0.0720983 Test Loss: 0.0800207
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1781271 Vali Loss: 0.0705613 Test Loss: 0.0800224
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1782056 Vali Loss: 0.0693285 Test Loss: 0.0800216
Validation loss decreased (0.069383 --> 0.069329).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1785677 Vali Loss: 0.0694049 Test Loss: 0.0800215
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1817751 Vali Loss: 0.0707793 Test Loss: 0.0800215
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1786927 Vali Loss: 0.0705343 Test Loss: 0.0800217
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1783537 Vali Loss: 0.0733255 Test Loss: 0.0800216
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1793224 Vali Loss: 0.0722647 Test Loss: 0.0800217
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1784579 Vali Loss: 0.0746815 Test Loss: 0.0800218
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1788731 Vali Loss: 0.0717889 Test Loss: 0.0800217
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1783031 Vali Loss: 0.0695137 Test Loss: 0.0800216
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.1783865 Vali Loss: 0.0709187 Test Loss: 0.0800217
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.1804142 Vali Loss: 0.0699454 Test Loss: 0.0800217
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SP500_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 231
test shape: (231, 10, 1) (231, 10, 1)
test shape: (231, 10, 1) (231, 10, 1)


	mse:6.762619159417227e-05, mae:0.006116541102528572, rmse:0.008223515003919601, r2:-0.04035818576812744, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0082, RÂ²: -0.0404, MAPE: 2.90%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.556 MB of 0.557 MB uploadedwandb: \ 0.556 MB of 0.557 MB uploadedwandb: | 0.557 MB of 0.557 MB uploadedwandb: / 0.557 MB of 0.557 MB uploadedwandb: - 0.557 MB of 0.557 MB uploadedwandb: \ 0.557 MB of 0.763 MB uploadedwandb: | 0.763 MB of 0.763 MB uploadedwandb: / 0.763 MB of 0.763 MB uploadedwandb: - 0.763 MB of 0.763 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–„â–‚â–ˆâ–„â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–‚â–â–â–â–‚â–‚â–â–â–â–„â–‚â–â–‚â–â–‚â–â–â–ƒ
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–‡â–„â–„â–†â–â–â–‚â–ƒâ–‚â–…â–ƒâ–â–â–ƒâ–ƒâ–†â–…â–ˆâ–„â–â–ƒâ–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 24
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.08002
wandb:                 train/loss 0.18041
wandb:   val/directional_accuracy 48.29245
wandb:                   val/loss 0.06995
wandb:                    val/mae 0.00612
wandb:                   val/mape 290.01868
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.04036
wandb:                   val/rmse 0.00822
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/fq5yokj9
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_204240-fq5yokj9/logs
Completed: SP500 H=10

Training: Autoformer on SP500 for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_204847-ovtzjy6x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/ovtzjy6x
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SP500_H22Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/ovtzjy6x
>>>>>>>start training : long_term_forecast_Autoformer_SP500_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 22
============================================================
train 4222
val 218
test 219
Epoch: 1, Steps: 132 | Train Loss: 0.2191718 Vali Loss: 0.0727129 Test Loss: 0.0730087
Validation loss decreased (inf --> 0.072713).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.21917183781889352, 'val/loss': 0.07271288122449603, 'test/loss': 0.07300866980637823, '_timestamp': 1762886961.2227685}).
Epoch: 2, Steps: 132 | Train Loss: 0.1900998 Vali Loss: 0.0734882 Test Loss: 0.0730763
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.1843406 Vali Loss: 0.0735691 Test Loss: 0.0745880
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19009976596994835, 'val/loss': 0.07348816628967013, 'test/loss': 0.07307625774826322, '_timestamp': 1762886971.3255403}).
Epoch: 4, Steps: 132 | Train Loss: 0.1819812 Vali Loss: 0.0718769 Test Loss: 0.0720934
Validation loss decreased (0.072713 --> 0.071877).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.1806984 Vali Loss: 0.0719931 Test Loss: 0.0721385
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.1801927 Vali Loss: 0.0719496 Test Loss: 0.0729084
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.1799633 Vali Loss: 0.0723140 Test Loss: 0.0721490
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.1798260 Vali Loss: 0.0720808 Test Loss: 0.0724527
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.1794310 Vali Loss: 0.0721319 Test Loss: 0.0722868
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.1794154 Vali Loss: 0.0719072 Test Loss: 0.0723544
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.1793676 Vali Loss: 0.0720111 Test Loss: 0.0723896
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.1796022 Vali Loss: 0.0723178 Test Loss: 0.0723738
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.1794406 Vali Loss: 0.0719053 Test Loss: 0.0723656
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.1794722 Vali Loss: 0.0720396 Test Loss: 0.0723650
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SP500_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 219
test shape: (219, 22, 1) (219, 22, 1)
test shape: (219, 22, 1) (219, 22, 1)


	mse:6.637070327997208e-05, mae:0.006062008440494537, rmse:0.008146821521222591, r2:-0.03955268859863281, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0081, RÂ²: -0.0396, MAPE: 3.34%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.643 MB of 0.643 MB uploadedwandb: \ 0.643 MB of 0.643 MB uploadedwandb: | 0.643 MB of 0.643 MB uploadedwandb: / 0.643 MB of 0.643 MB uploadedwandb: - 0.643 MB of 0.643 MB uploadedwandb: \ 0.643 MB of 0.847 MB uploadedwandb: | 0.847 MB of 0.847 MB uploadedwandb: / 0.847 MB of 0.847 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–â–ƒâ–â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–â–â–ƒâ–‚â–‚â–â–‚â–ƒâ–â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 13
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.07237
wandb:                 train/loss 0.17947
wandb:   val/directional_accuracy 48.1409
wandb:                   val/loss 0.07204
wandb:                    val/mae 0.00606
wandb:                   val/mape 334.24406
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.03955
wandb:                   val/rmse 0.00815
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/ovtzjy6x
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_204847-ovtzjy6x/logs
Completed: SP500 H=22

Training: Autoformer on SP500 for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_205221-fbdaxii7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/fbdaxii7
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SP500_H50Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/fbdaxii7
>>>>>>>start training : long_term_forecast_Autoformer_SP500_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 50
============================================================
train 4194
val 190
test 191
Epoch: 1, Steps: 132 | Train Loss: 0.2199751 Vali Loss: 0.0776507 Test Loss: 0.0816604
Validation loss decreased (inf --> 0.077651).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.21997513661556173, 'val/loss': 0.07765069728096326, 'test/loss': 0.08166042839487393, '_timestamp': 1762887172.1824923}).
Epoch: 2, Steps: 132 | Train Loss: 0.1928875 Vali Loss: 0.0750438 Test Loss: 0.0794986
Validation loss decreased (0.077651 --> 0.075044).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.1880596 Vali Loss: 0.0750022 Test Loss: 0.0791592
Validation loss decreased (0.075044 --> 0.075002).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19288749151834936, 'val/loss': 0.07504378135005633, 'test/loss': 0.07949864491820335, '_timestamp': 1762887179.3355315}).
Epoch: 4, Steps: 132 | Train Loss: 0.1855799 Vali Loss: 0.0728142 Test Loss: 0.0752271
Validation loss decreased (0.075002 --> 0.072814).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.1908269 Vali Loss: 0.0731819 Test Loss: 0.0766683
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.1844231 Vali Loss: 0.0733102 Test Loss: 0.0770102
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.1843706 Vali Loss: 0.0728276 Test Loss: 0.0759822
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.1892465 Vali Loss: 0.0730412 Test Loss: 0.0764401
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.1853333 Vali Loss: 0.0731083 Test Loss: 0.0763134
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.1837702 Vali Loss: 0.0730826 Test Loss: 0.0762184
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.1836850 Vali Loss: 0.0730617 Test Loss: 0.0762096
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.1836801 Vali Loss: 0.0730712 Test Loss: 0.0761626
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.1840673 Vali Loss: 0.0730870 Test Loss: 0.0761440
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.1841103 Vali Loss: 0.0730383 Test Loss: 0.0761582
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SP500_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 191
test shape: (191, 50, 1) (191, 50, 1)
test shape: (191, 50, 1) (191, 50, 1)


	mse:6.618980842176825e-05, mae:0.006036877166479826, rmse:0.008135711774230003, r2:-0.017795324325561523, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0060, RMSE: 0.0081, RÂ²: -0.0178, MAPE: 3.18%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.713 MB of 0.715 MB uploadedwandb: \ 0.715 MB of 0.715 MB uploadedwandb: | 0.715 MB of 0.715 MB uploadedwandb: / 0.715 MB of 0.919 MB uploadedwandb: - 0.715 MB of 0.919 MB uploadedwandb: \ 0.919 MB of 0.919 MB uploadedwandb: | 0.919 MB of 0.919 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–„â–„â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–…â–ƒâ–ˆâ–‚â–‚â–†â–ƒâ–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–‚â–ƒâ–â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 13
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.07616
wandb:                 train/loss 0.18411
wandb:   val/directional_accuracy 51.45849
wandb:                   val/loss 0.07304
wandb:                    val/mae 0.00604
wandb:                   val/mape 318.1627
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.0178
wandb:                   val/rmse 0.00814
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/fbdaxii7
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_205221-fbdaxii7/logs
Completed: SP500 H=50

Training: Autoformer on SP500 for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_205540-9ajvuu90
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/9ajvuu90
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SP500_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/9ajvuu90
>>>>>>>start training : long_term_forecast_Autoformer_SP500_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 100
============================================================
train 4144
val 140
test 141
Epoch: 1, Steps: 130 | Train Loss: 0.2300266 Vali Loss: 0.0695385 Test Loss: 0.0839050
Validation loss decreased (inf --> 0.069539).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23002658079449947, 'val/loss': 0.06953851133584976, 'test/loss': 0.08390501290559768, '_timestamp': 1762887370.5979676}).
Epoch: 2, Steps: 130 | Train Loss: 0.2034308 Vali Loss: 0.0679010 Test Loss: 0.0838700
Validation loss decreased (0.069539 --> 0.067901).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.1985520 Vali Loss: 0.0697944 Test Loss: 0.0848323
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2034308443275782, 'val/loss': 0.06790096759796142, 'test/loss': 0.08387002646923065, '_timestamp': 1762887380.6134815}).
Epoch: 4, Steps: 130 | Train Loss: 0.1972439 Vali Loss: 0.0688249 Test Loss: 0.0840782
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.1959601 Vali Loss: 0.0694087 Test Loss: 0.0845656
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.1959655 Vali Loss: 0.0690615 Test Loss: 0.0842551
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.1969036 Vali Loss: 0.0702419 Test Loss: 0.0854878
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.1954551 Vali Loss: 0.0695525 Test Loss: 0.0847805
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.1958040 Vali Loss: 0.0694860 Test Loss: 0.0849374
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.1956295 Vali Loss: 0.0697101 Test Loss: 0.0850280
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.1958621 Vali Loss: 0.0698005 Test Loss: 0.0850706
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.1954100 Vali Loss: 0.0693224 Test Loss: 0.0850448
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SP500_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 141
test shape: (141, 100, 1) (141, 100, 1)
test shape: (141, 100, 1) (141, 100, 1)


	mse:6.914889672771096e-05, mae:0.0061399745754897594, rmse:0.00831558182835579, r2:-0.009350061416625977, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0083, RÂ²: -0.0094, MAPE: 3.41%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.791 MB of 0.796 MB uploadedwandb: \ 0.791 MB of 0.796 MB uploadedwandb: | 0.796 MB of 0.796 MB uploadedwandb: / 0.796 MB of 0.796 MB uploadedwandb: - 0.796 MB of 1.000 MB uploadedwandb: \ 0.958 MB of 1.000 MB uploadedwandb: | 1.000 MB of 1.000 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–â–ƒâ–‚â–ˆâ–„â–…â–†â–†â–†
wandb:                 train/loss â–ˆâ–…â–‚â–‚â–„â–â–‚â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–â–„â–‚â–ˆâ–…â–„â–…â–†â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.08504
wandb:                 train/loss 0.19541
wandb:   val/directional_accuracy 53.42073
wandb:                   val/loss 0.06932
wandb:                    val/mae 0.00614
wandb:                   val/mape 341.26139
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.00935
wandb:                   val/rmse 0.00832
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/9ajvuu90
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_205540-9ajvuu90/logs
Completed: SP500 H=100

Training: Autoformer on NASDAQ for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_205916-h6mzc2zd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/h6mzc2zd
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NASDAQ_H3Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/h6mzc2zd
>>>>>>>start training : long_term_forecast_Autoformer_NASDAQ_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.3219787 Vali Loss: 0.1678109 Test Loss: 0.1560228
Validation loss decreased (inf --> 0.167811).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3219787268934393, 'val/loss': 0.16781092062592506, 'test/loss': 0.15602276008576155, '_timestamp': 1762887583.7895694}).
Epoch: 2, Steps: 133 | Train Loss: 0.2746885 Vali Loss: 0.1444780 Test Loss: 0.1388958
Validation loss decreased (0.167811 --> 0.144478).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2586459 Vali Loss: 0.1438710 Test Loss: 0.1302423
Validation loss decreased (0.144478 --> 0.143871).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.27468847242513095, 'val/loss': 0.14447800815105438, 'test/loss': 0.13889578823000193, '_timestamp': 1762887594.2447033}).
Epoch: 4, Steps: 133 | Train Loss: 0.2505396 Vali Loss: 0.1385961 Test Loss: 0.1289386
Validation loss decreased (0.143871 --> 0.138596).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2453823 Vali Loss: 0.1364195 Test Loss: 0.1275535
Validation loss decreased (0.138596 --> 0.136420).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2434937 Vali Loss: 0.1393831 Test Loss: 0.1278878
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2413247 Vali Loss: 0.1363276 Test Loss: 0.1266847
Validation loss decreased (0.136420 --> 0.136328).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2403403 Vali Loss: 0.1336563 Test Loss: 0.1268074
Validation loss decreased (0.136328 --> 0.133656).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2404908 Vali Loss: 0.1320221 Test Loss: 0.1270547
Validation loss decreased (0.133656 --> 0.132022).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2406918 Vali Loss: 0.1348399 Test Loss: 0.1271650
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2392120 Vali Loss: 0.1328248 Test Loss: 0.1270813
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2407793 Vali Loss: 0.1352395 Test Loss: 0.1272172
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2396860 Vali Loss: 0.1379142 Test Loss: 0.1272238
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2399095 Vali Loss: 0.1353165 Test Loss: 0.1272977
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2394924 Vali Loss: 0.1366749 Test Loss: 0.1272988
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2400891 Vali Loss: 0.1350877 Test Loss: 0.1272977
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2407507 Vali Loss: 0.1351887 Test Loss: 0.1272975
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2398331 Vali Loss: 0.1350269 Test Loss: 0.1271834
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2395143 Vali Loss: 0.1373107 Test Loss: 0.1271829
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NASDAQ_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.00014109889161773026, mae:0.0087337801232934, rmse:0.01187850534915924, r2:-0.03666234016418457, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0087, RMSE: 0.0119, RÂ²: -0.0367, MAPE: 2553126.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.507 MB of 0.507 MB uploadedwandb: \ 0.507 MB of 0.507 MB uploadedwandb: | 0.507 MB of 0.507 MB uploadedwandb: / 0.507 MB of 0.507 MB uploadedwandb: - 0.507 MB of 0.507 MB uploadedwandb: \ 0.507 MB of 0.507 MB uploadedwandb: | 0.507 MB of 0.507 MB uploadedwandb: / 0.638 MB of 0.842 MB uploaded (0.002 MB deduped)wandb: - 0.726 MB of 0.842 MB uploaded (0.002 MB deduped)wandb: \ 0.842 MB of 0.842 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–ƒâ–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–â–â–‚â–â–‚â–â–â–â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–„â–…â–„â–‚â–â–ƒâ–â–ƒâ–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 18
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.12718
wandb:                 train/loss 0.23951
wandb:   val/directional_accuracy 50.42194
wandb:                   val/loss 0.13731
wandb:                    val/mae 0.00873
wandb:                   val/mape 255312675.0
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.03666
wandb:                   val/rmse 0.01188
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/h6mzc2zd
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_205916-h6mzc2zd/logs
Completed: NASDAQ H=3

Training: Autoformer on NASDAQ for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_210456-zqx6c2dg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/zqx6c2dg
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NASDAQ_H5Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/zqx6c2dg
>>>>>>>start training : long_term_forecast_Autoformer_NASDAQ_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.3078916 Vali Loss: 0.1532932 Test Loss: 0.1435572
Validation loss decreased (inf --> 0.153293).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3078916131105638, 'val/loss': 0.1532931923866272, 'test/loss': 0.14355716574937105, '_timestamp': 1762887926.3613803}).
Epoch: 2, Steps: 133 | Train Loss: 0.2698379 Vali Loss: 0.1565946 Test Loss: 0.1367894
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2538849 Vali Loss: 0.1497360 Test Loss: 0.1327810
Validation loss decreased (0.153293 --> 0.149736).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2698378655919455, 'val/loss': 0.1565945716574788, 'test/loss': 0.13678938429802656, '_timestamp': 1762887938.1868074}).
Epoch: 4, Steps: 133 | Train Loss: 0.2472312 Vali Loss: 0.1444567 Test Loss: 0.1330192
Validation loss decreased (0.149736 --> 0.144457).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2442792 Vali Loss: 0.1355346 Test Loss: 0.1318459
Validation loss decreased (0.144457 --> 0.135535).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2419997 Vali Loss: 0.1349696 Test Loss: 0.1312540
Validation loss decreased (0.135535 --> 0.134970).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2415017 Vali Loss: 0.1451731 Test Loss: 0.1322365
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2416798 Vali Loss: 0.1450179 Test Loss: 0.1320749
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2412464 Vali Loss: 0.1364012 Test Loss: 0.1316672
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2406246 Vali Loss: 0.1448258 Test Loss: 0.1318424
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2404724 Vali Loss: 0.1364046 Test Loss: 0.1316476
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2415108 Vali Loss: 0.1400626 Test Loss: 0.1316897
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2406186 Vali Loss: 0.1461448 Test Loss: 0.1317087
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2406347 Vali Loss: 0.1376616 Test Loss: 0.1317164
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2408773 Vali Loss: 0.1451558 Test Loss: 0.1317114
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2406568 Vali Loss: 0.1365434 Test Loss: 0.1317126
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NASDAQ_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.0001459568302379921, mae:0.00880919024348259, rmse:0.012081259861588478, r2:-0.06691324710845947, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0088, RMSE: 0.0121, RÂ²: -0.0669, MAPE: 6568466.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.531 MB of 0.531 MB uploadedwandb: \ 0.531 MB of 0.531 MB uploadedwandb: | 0.531 MB of 0.531 MB uploadedwandb: / 0.531 MB of 0.531 MB uploadedwandb: - 0.531 MB of 0.735 MB uploadedwandb: \ 0.735 MB of 0.735 MB uploadedwandb: | 0.735 MB of 0.735 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‡â–ˆâ–ƒâ–â–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–â–â–†â–†â–‚â–†â–‚â–ƒâ–†â–‚â–†â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.13171
wandb:                 train/loss 0.24066
wandb:   val/directional_accuracy 49.89362
wandb:                   val/loss 0.13654
wandb:                    val/mae 0.00881
wandb:                   val/mape 656846650.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.06691
wandb:                   val/rmse 0.01208
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/zqx6c2dg
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_210456-zqx6c2dg/logs
Completed: NASDAQ H=5

Training: Autoformer on NASDAQ for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_210845-t38hiof4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/t38hiof4
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NASDAQ_H10Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/t38hiof4
>>>>>>>start training : long_term_forecast_Autoformer_NASDAQ_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2956628 Vali Loss: 0.1606158 Test Loss: 0.1466452
Validation loss decreased (inf --> 0.160616).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2956628170900775, 'val/loss': 0.1606158409267664, 'test/loss': 0.1466452283784747, '_timestamp': 1762888157.388927}).
Epoch: 2, Steps: 133 | Train Loss: 0.2619472 Vali Loss: 0.1530792 Test Loss: 0.1410014
Validation loss decreased (0.160616 --> 0.153079).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2513859 Vali Loss: 0.1487252 Test Loss: 0.1411961
Validation loss decreased (0.153079 --> 0.148725).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2619472455261345, 'val/loss': 0.15307919308543205, 'test/loss': 0.1410013623535633, '_timestamp': 1762888168.6823337}).
Epoch: 4, Steps: 133 | Train Loss: 0.2461043 Vali Loss: 0.1457860 Test Loss: 0.1388307
Validation loss decreased (0.148725 --> 0.145786).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2447355 Vali Loss: 0.1436625 Test Loss: 0.1375783
Validation loss decreased (0.145786 --> 0.143663).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2425173 Vali Loss: 0.1441375 Test Loss: 0.1373065
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2411924 Vali Loss: 0.1591967 Test Loss: 0.1381821
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2425752 Vali Loss: 0.1466060 Test Loss: 0.1384144
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2423236 Vali Loss: 0.1484043 Test Loss: 0.1384613
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2405838 Vali Loss: 0.1432107 Test Loss: 0.1381824
Validation loss decreased (0.143663 --> 0.143211).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2401223 Vali Loss: 0.1541831 Test Loss: 0.1381726
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2413351 Vali Loss: 0.1469225 Test Loss: 0.1381302
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2410031 Vali Loss: 0.1483863 Test Loss: 0.1379700
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2421230 Vali Loss: 0.1471420 Test Loss: 0.1381446
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2412708 Vali Loss: 0.1464142 Test Loss: 0.1379708
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2409364 Vali Loss: 0.1456370 Test Loss: 0.1379731
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2422309 Vali Loss: 0.1478129 Test Loss: 0.1379746
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2401222 Vali Loss: 0.1484835 Test Loss: 0.1379747
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2411351 Vali Loss: 0.1582127 Test Loss: 0.1379722
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2414997 Vali Loss: 0.1590630 Test Loss: 0.1379720
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NASDAQ_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.00015043711755424738, mae:0.009070714935660362, rmse:0.012265280820429325, r2:-0.08676862716674805, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0091, RMSE: 0.0123, RÂ²: -0.0868, MAPE: 7536239.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.614 MB of 0.614 MB uploadedwandb: \ 0.614 MB of 0.614 MB uploadedwandb: | 0.614 MB of 0.614 MB uploadedwandb: / 0.614 MB of 0.614 MB uploadedwandb: - 0.614 MB of 0.614 MB uploadedwandb: \ 0.614 MB of 0.819 MB uploadedwandb: | 0.819 MB of 0.819 MB uploadedwandb: / 0.819 MB of 0.819 MB uploadedwandb: - 0.819 MB of 0.819 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–ƒâ–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–‚â–â–â–ˆâ–‚â–ƒâ–â–†â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–ˆâ–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.13797
wandb:                 train/loss 0.2415
wandb:   val/directional_accuracy 48.98551
wandb:                   val/loss 0.15906
wandb:                    val/mae 0.00907
wandb:                   val/mape 753623950.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.08677
wandb:                   val/rmse 0.01227
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/t38hiof4
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_210845-t38hiof4/logs
Completed: NASDAQ H=10

Training: Autoformer on NASDAQ for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_211349-5rk6b9n7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/5rk6b9n7
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NASDAQ_H22Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/5rk6b9n7
>>>>>>>start training : long_term_forecast_Autoformer_NASDAQ_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2919733 Vali Loss: 0.1573302 Test Loss: 0.1353273
Validation loss decreased (inf --> 0.157330).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2919732585097804, 'val/loss': 0.15733024903706141, 'test/loss': 0.13532734555857523, '_timestamp': 1762888461.6183183}).
Epoch: 2, Steps: 132 | Train Loss: 0.2597965 Vali Loss: 0.1545990 Test Loss: 0.1414045
Validation loss decreased (0.157330 --> 0.154599).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25979652696035127, 'val/loss': 0.15459899178573064, 'test/loss': 0.14140452391334943, '_timestamp': 1762888473.5671475}).
Epoch: 3, Steps: 132 | Train Loss: 0.2508647 Vali Loss: 0.1598080 Test Loss: 0.1390078
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2458400 Vali Loss: 0.1596980 Test Loss: 0.1383202
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2435695 Vali Loss: 0.1594282 Test Loss: 0.1389025
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2432022 Vali Loss: 0.1581973 Test Loss: 0.1395641
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2415296 Vali Loss: 0.1580885 Test Loss: 0.1385590
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2412079 Vali Loss: 0.1583620 Test Loss: 0.1393665
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2413684 Vali Loss: 0.1574970 Test Loss: 0.1391907
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2409847 Vali Loss: 0.1571305 Test Loss: 0.1390074
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2407917 Vali Loss: 0.1579264 Test Loss: 0.1391162
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2405029 Vali Loss: 0.1568311 Test Loss: 0.1392174
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NASDAQ_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.00015218065527733415, mae:0.009029615670442581, rmse:0.012336152605712414, r2:-0.08758354187011719, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0090, RMSE: 0.0123, RÂ²: -0.0876, MAPE: 4541956.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.657 MB of 0.658 MB uploadedwandb: \ 0.657 MB of 0.658 MB uploadedwandb: | 0.657 MB of 0.658 MB uploadedwandb: / 0.658 MB of 0.658 MB uploadedwandb: - 0.658 MB of 0.658 MB uploadedwandb: \ 0.658 MB of 0.658 MB uploadedwandb: | 0.658 MB of 0.861 MB uploadedwandb: / 0.861 MB of 0.861 MB uploadedwandb: - 0.861 MB of 0.861 MB uploadedwandb: \ 0.861 MB of 0.861 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–â–„â–ˆâ–‚â–‡â–†â–…â–…â–†
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ˆâ–‡â–„â–„â–…â–ƒâ–‚â–„â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.13922
wandb:                 train/loss 0.2405
wandb:   val/directional_accuracy 50.04369
wandb:                   val/loss 0.15683
wandb:                    val/mae 0.00903
wandb:                   val/mape 454195600.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.08758
wandb:                   val/rmse 0.01234
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/5rk6b9n7
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_211349-5rk6b9n7/logs
Completed: NASDAQ H=22

Training: Autoformer on NASDAQ for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_211731-c0bc4ant
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/c0bc4ant
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NASDAQ_H50Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/c0bc4ant
>>>>>>>start training : long_term_forecast_Autoformer_NASDAQ_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.2907473 Vali Loss: 0.1777454 Test Loss: 0.1392460
Validation loss decreased (inf --> 0.177745).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29074731265956705, 'val/loss': 0.17774543662865958, 'test/loss': 0.1392459919055303, '_timestamp': 1762888681.8583424}).
Epoch: 2, Steps: 132 | Train Loss: 0.2621662 Vali Loss: 0.1706581 Test Loss: 0.1452482
Validation loss decreased (0.177745 --> 0.170658).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2552009 Vali Loss: 0.1709430 Test Loss: 0.1393523
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26216615905815904, 'val/loss': 0.1706580569346746, 'test/loss': 0.14524816100796065, '_timestamp': 1762888692.8789575}).
Epoch: 4, Steps: 132 | Train Loss: 0.2514692 Vali Loss: 0.1705281 Test Loss: 0.1395673
Validation loss decreased (0.170658 --> 0.170528).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2506543 Vali Loss: 0.1698391 Test Loss: 0.1396567
Validation loss decreased (0.170528 --> 0.169839).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2484010 Vali Loss: 0.1695084 Test Loss: 0.1388717
Validation loss decreased (0.169839 --> 0.169508).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2472004 Vali Loss: 0.1702369 Test Loss: 0.1383569
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2468923 Vali Loss: 0.1703771 Test Loss: 0.1387372
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2474462 Vali Loss: 0.1705155 Test Loss: 0.1383466
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2477927 Vali Loss: 0.1707333 Test Loss: 0.1384329
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2474475 Vali Loss: 0.1705570 Test Loss: 0.1384802
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2474916 Vali Loss: 0.1704953 Test Loss: 0.1384816
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2469718 Vali Loss: 0.1704411 Test Loss: 0.1384764
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2465557 Vali Loss: 0.1704562 Test Loss: 0.1384875
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2467544 Vali Loss: 0.1704993 Test Loss: 0.1384922
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2475140 Vali Loss: 0.1702563 Test Loss: 0.1384905
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NASDAQ_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.000152952314238064, mae:0.008985587395727634, rmse:0.012367389164865017, r2:-0.05596768856048584, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0090, RMSE: 0.0124, RÂ²: -0.0560, MAPE: 6326474.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.721 MB of 0.723 MB uploadedwandb: \ 0.721 MB of 0.723 MB uploadedwandb: | 0.721 MB of 0.723 MB uploadedwandb: / 0.723 MB of 0.723 MB uploadedwandb: - 0.723 MB of 0.723 MB uploadedwandb: \ 0.723 MB of 0.928 MB uploadedwandb: | 0.928 MB of 0.928 MB uploadedwandb: / 0.928 MB of 0.928 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–†â–ˆâ–ˆâ–„â–â–ƒâ–â–â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–ƒâ–â–…â–…â–†â–‡â–†â–†â–†â–†â–†â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.13849
wandb:                 train/loss 0.24751
wandb:   val/directional_accuracy 53.12567
wandb:                   val/loss 0.17026
wandb:                    val/mae 0.00899
wandb:                   val/mape 632647400.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.05597
wandb:                   val/rmse 0.01237
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/c0bc4ant
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_211731-c0bc4ant/logs
Completed: NASDAQ H=50

Training: Autoformer on NASDAQ for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_212159-ga56sbqn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/ga56sbqn
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NASDAQ_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/ga56sbqn
>>>>>>>start training : long_term_forecast_Autoformer_NASDAQ_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.2994045 Vali Loss: 0.1910581 Test Loss: 0.1444537
Validation loss decreased (inf --> 0.191058).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29940445262652177, 'val/loss': 0.19105809330940246, 'test/loss': 0.144453701376915, '_timestamp': 1762888946.0154672}).
Epoch: 2, Steps: 130 | Train Loss: 0.2723751 Vali Loss: 0.1798800 Test Loss: 0.1481729
Validation loss decreased (0.191058 --> 0.179880).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.27237509993406445, 'val/loss': 0.1798799991607666, 'test/loss': 0.14817287027835846, '_timestamp': 1762888958.4617646}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.27237509993406445, 'val/loss': 0.1798799991607666, 'test/loss': 0.14817287027835846, '_timestamp': 1762888958.4617646}).
Epoch: 3, Steps: 130 | Train Loss: 0.2659622 Vali Loss: 0.1863930 Test Loss: 0.1468457
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2632423 Vali Loss: 0.1878581 Test Loss: 0.1460874
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2617061 Vali Loss: 0.1867631 Test Loss: 0.1456660
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2605027 Vali Loss: 0.1846607 Test Loss: 0.1463522
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2605258 Vali Loss: 0.1872802 Test Loss: 0.1463713
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2599840 Vali Loss: 0.1860120 Test Loss: 0.1465174
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2598645 Vali Loss: 0.1819851 Test Loss: 0.1464780
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2592713 Vali Loss: 0.1850437 Test Loss: 0.1465100
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2593646 Vali Loss: 0.1859467 Test Loss: 0.1465083
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2595325 Vali Loss: 0.1852770 Test Loss: 0.1464705
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NASDAQ_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0001578704104758799, mae:0.008816281333565712, rmse:0.012564648874104023, r2:-0.04085373878479004, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0088, RMSE: 0.0126, RÂ²: -0.0409, MAPE: 2920368.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.731 MB of 0.736 MB uploadedwandb: \ 0.731 MB of 0.736 MB uploadedwandb: | 0.736 MB of 0.736 MB uploadedwandb: / 0.736 MB of 0.736 MB uploadedwandb: - 0.736 MB of 0.736 MB uploadedwandb: \ 0.736 MB of 0.940 MB uploadedwandb: | 0.940 MB of 0.940 MB uploadedwandb: / 0.940 MB of 0.940 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–â–…â–…â–†â–†â–†â–†â–†
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–ˆâ–‡â–„â–‡â–†â–â–…â–†â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.14647
wandb:                 train/loss 0.25953
wandb:   val/directional_accuracy 51.77489
wandb:                   val/loss 0.18528
wandb:                    val/mae 0.00882
wandb:                   val/mape 292036875.0
wandb:                    val/mse 0.00016
wandb:                     val/r2 -0.04085
wandb:                   val/rmse 0.01256
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/ga56sbqn
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_212159-ga56sbqn/logs
Completed: NASDAQ H=100

Training: Autoformer on ABSA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_212519-8xnyrdc3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/8xnyrdc3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ABSA_H3  Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/8xnyrdc3
>>>>>>>start training : long_term_forecast_Autoformer_ABSA_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 238
Epoch: 1, Steps: 133 | Train Loss: 0.3692643 Vali Loss: 0.1924918 Test Loss: 0.1771251
Validation loss decreased (inf --> 0.192492).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.36926434754877163, 'val/loss': 0.19249176234006882, 'test/loss': 0.1771250870078802, '_timestamp': 1762889148.5843832}).
Epoch: 2, Steps: 133 | Train Loss: 0.3202258 Vali Loss: 0.1850783 Test Loss: 0.1756760
Validation loss decreased (0.192492 --> 0.185078).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.32022584000028165, 'val/loss': 0.18507827445864677, 'test/loss': 0.17567597329616547, '_timestamp': 1762889158.2452888}).
Epoch: 3, Steps: 133 | Train Loss: 0.2993330 Vali Loss: 0.1744473 Test Loss: 0.1627540
Validation loss decreased (0.185078 --> 0.174447).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2876564 Vali Loss: 0.1729021 Test Loss: 0.1589987
Validation loss decreased (0.174447 --> 0.172902).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2840223 Vali Loss: 0.1683771 Test Loss: 0.1582576
Validation loss decreased (0.172902 --> 0.168377).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2835796 Vali Loss: 0.1654573 Test Loss: 0.1567980
Validation loss decreased (0.168377 --> 0.165457).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2816383 Vali Loss: 0.1656781 Test Loss: 0.1557524
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2806329 Vali Loss: 0.1651454 Test Loss: 0.1558955
Validation loss decreased (0.165457 --> 0.165145).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2800866 Vali Loss: 0.1661360 Test Loss: 0.1554738
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2807382 Vali Loss: 0.1653752 Test Loss: 0.1556283
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2803147 Vali Loss: 0.1658370 Test Loss: 0.1555566
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2799070 Vali Loss: 0.1663989 Test Loss: 0.1555514
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2796047 Vali Loss: 0.1606284 Test Loss: 0.1555575
Validation loss decreased (0.165145 --> 0.160628).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2803766 Vali Loss: 0.1639178 Test Loss: 0.1555143
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2794342 Vali Loss: 0.1641879 Test Loss: 0.1555193
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2808507 Vali Loss: 0.1659014 Test Loss: 0.1555174
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2812598 Vali Loss: 0.1673444 Test Loss: 0.1555159
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2795721 Vali Loss: 0.1686600 Test Loss: 0.1555153
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2810926 Vali Loss: 0.1680827 Test Loss: 0.1555149
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2793575 Vali Loss: 0.1694734 Test Loss: 0.1555153
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2783252 Vali Loss: 0.1673505 Test Loss: 0.1555154
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2806789 Vali Loss: 0.1630828 Test Loss: 0.1555154
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2796467 Vali Loss: 0.1647460 Test Loss: 0.1555151
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ABSA_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 238
test shape: (238, 3, 1) (238, 3, 1)
test shape: (238, 3, 1) (238, 3, 1)


	mse:0.00047794042620807886, mae:0.016743740066885948, rmse:0.0218618493527174, r2:-0.04958963394165039, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0167, RMSE: 0.0219, RÂ²: -0.0496, MAPE: 1.72%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.501 MB of 0.501 MB uploadedwandb: \ 0.501 MB of 0.501 MB uploadedwandb: | 0.501 MB of 0.501 MB uploadedwandb: / 0.501 MB of 0.501 MB uploadedwandb: - 0.501 MB of 0.501 MB uploadedwandb: \ 0.501 MB of 0.501 MB uploadedwandb: | 0.501 MB of 0.501 MB uploadedwandb: / 0.632 MB of 0.837 MB uploaded (0.002 MB deduped)wandb: - 0.837 MB of 0.837 MB uploaded (0.002 MB deduped)wandb: \ 0.837 MB of 0.837 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–„â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–â–‚â–â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‡â–…â–ƒâ–„â–ƒâ–„â–ƒâ–„â–„â–â–ƒâ–ƒâ–„â–„â–…â–…â–…â–„â–‚â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.15552
wandb:                 train/loss 0.27965
wandb:   val/directional_accuracy 47.26891
wandb:                   val/loss 0.16475
wandb:                    val/mae 0.01674
wandb:                   val/mape 171.78218
wandb:                    val/mse 0.00048
wandb:                     val/r2 -0.04959
wandb:                   val/rmse 0.02186
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/8xnyrdc3
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_212519-8xnyrdc3/logs
Completed: ABSA H=3

Training: Autoformer on ABSA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_213004-1bb37p1q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/1bb37p1q
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ABSA_H5  Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/1bb37p1q
>>>>>>>start training : long_term_forecast_Autoformer_ABSA_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 236
Epoch: 1, Steps: 133 | Train Loss: 0.3656454 Vali Loss: 0.1884633 Test Loss: 0.1757487
Validation loss decreased (inf --> 0.188463).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.36564537994843677, 'val/loss': 0.18846328556537628, 'test/loss': 0.1757486704736948, '_timestamp': 1762889430.9303358}).
Epoch: 2, Steps: 133 | Train Loss: 0.3233173 Vali Loss: 0.1901136 Test Loss: 0.1792365
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.3050516 Vali Loss: 0.1724077 Test Loss: 0.1702716
Validation loss decreased (0.188463 --> 0.172408).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.32331734537182, 'val/loss': 0.1901135966181755, 'test/loss': 0.17923650238662958, '_timestamp': 1762889441.8208907}).
Epoch: 4, Steps: 133 | Train Loss: 0.2961564 Vali Loss: 0.1732388 Test Loss: 0.1651344
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2932963 Vali Loss: 0.1733295 Test Loss: 0.1665669
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2904777 Vali Loss: 0.1683463 Test Loss: 0.1637686
Validation loss decreased (0.172408 --> 0.168346).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2881039 Vali Loss: 0.1700534 Test Loss: 0.1638239
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2887669 Vali Loss: 0.1723350 Test Loss: 0.1635927
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2898035 Vali Loss: 0.1689563 Test Loss: 0.1629473
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2870339 Vali Loss: 0.1656018 Test Loss: 0.1634711
Validation loss decreased (0.168346 --> 0.165602).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2878383 Vali Loss: 0.1698042 Test Loss: 0.1634167
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2880400 Vali Loss: 0.1714252 Test Loss: 0.1634156
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2882704 Vali Loss: 0.1691617 Test Loss: 0.1634281
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2870364 Vali Loss: 0.1708719 Test Loss: 0.1634670
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2874409 Vali Loss: 0.1700291 Test Loss: 0.1634693
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2880770 Vali Loss: 0.1696876 Test Loss: 0.1634685
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2869289 Vali Loss: 0.1680875 Test Loss: 0.1634693
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2881452 Vali Loss: 0.1711211 Test Loss: 0.1634693
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2878082 Vali Loss: 0.1736792 Test Loss: 0.1634698
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2885954 Vali Loss: 0.1720753 Test Loss: 0.1634693
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ABSA_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
test shape: (236, 5, 1) (236, 5, 1)
test shape: (236, 5, 1) (236, 5, 1)


	mse:0.0004852841957472265, mae:0.016671566292643547, rmse:0.022029167041182518, r2:-0.059520721435546875, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0167, RMSE: 0.0220, RÂ²: -0.0595, MAPE: 1.61%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.531 MB of 0.531 MB uploadedwandb: \ 0.531 MB of 0.531 MB uploadedwandb: | 0.531 MB of 0.531 MB uploadedwandb: / 0.531 MB of 0.531 MB uploadedwandb: - 0.531 MB of 0.531 MB uploadedwandb: \ 0.531 MB of 0.736 MB uploadedwandb: | 0.730 MB of 0.736 MB uploadedwandb: / 0.736 MB of 0.736 MB uploadedwandb: - 0.736 MB of 0.736 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–„â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–ˆâ–ˆâ–ƒâ–…â–‡â–„â–â–…â–†â–„â–†â–…â–…â–ƒâ–†â–ˆâ–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.16347
wandb:                 train/loss 0.2886
wandb:   val/directional_accuracy 51.37712
wandb:                   val/loss 0.17208
wandb:                    val/mae 0.01667
wandb:                   val/mape 161.07076
wandb:                    val/mse 0.00049
wandb:                     val/r2 -0.05952
wandb:                   val/rmse 0.02203
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/1bb37p1q
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_213004-1bb37p1q/logs
Completed: ABSA H=5

Training: Autoformer on ABSA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_213447-pgh2exrb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/pgh2exrb
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ABSA_H10 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/pgh2exrb
>>>>>>>start training : long_term_forecast_Autoformer_ABSA_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 231
Epoch: 1, Steps: 133 | Train Loss: 0.3583407 Vali Loss: 0.1839477 Test Loss: 0.1734653
Validation loss decreased (inf --> 0.183948).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.35834069863746043, 'val/loss': 0.18394771590828896, 'test/loss': 0.17346534132957458, '_timestamp': 1762889716.1259987}).
Epoch: 2, Steps: 133 | Train Loss: 0.3208038 Vali Loss: 0.1969036 Test Loss: 0.1705642
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.3069653 Vali Loss: 0.1834958 Test Loss: 0.1688244
Validation loss decreased (0.183948 --> 0.183496).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.320803802936597, 'val/loss': 0.19690358452498913, 'test/loss': 0.1705641932785511, '_timestamp': 1762889727.6564765}).
Epoch: 4, Steps: 133 | Train Loss: 0.3003672 Vali Loss: 0.1802834 Test Loss: 0.1650429
Validation loss decreased (0.183496 --> 0.180283).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2981559 Vali Loss: 0.1774080 Test Loss: 0.1653257
Validation loss decreased (0.180283 --> 0.177408).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2945207 Vali Loss: 0.1806706 Test Loss: 0.1656432
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2938877 Vali Loss: 0.1825807 Test Loss: 0.1652715
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2942304 Vali Loss: 0.1765728 Test Loss: 0.1654569
Validation loss decreased (0.177408 --> 0.176573).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2945746 Vali Loss: 0.1813567 Test Loss: 0.1653318
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2927118 Vali Loss: 0.1791282 Test Loss: 0.1651392
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2929892 Vali Loss: 0.1777928 Test Loss: 0.1651914
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2921578 Vali Loss: 0.1761313 Test Loss: 0.1652167
Validation loss decreased (0.176573 --> 0.176131).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2936470 Vali Loss: 0.1820408 Test Loss: 0.1652053
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2946392 Vali Loss: 0.1801790 Test Loss: 0.1651864
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2928490 Vali Loss: 0.1764710 Test Loss: 0.1651892
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2914715 Vali Loss: 0.1749602 Test Loss: 0.1651932
Validation loss decreased (0.176131 --> 0.174960).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2954572 Vali Loss: 0.1813309 Test Loss: 0.1651927
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2930746 Vali Loss: 0.1796521 Test Loss: 0.1651927
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2933190 Vali Loss: 0.1803616 Test Loss: 0.1651927
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2918367 Vali Loss: 0.1839670 Test Loss: 0.1651927
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2930094 Vali Loss: 0.1885404 Test Loss: 0.1651925
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2928584 Vali Loss: 0.1834395 Test Loss: 0.1651923
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2957762 Vali Loss: 0.1778088 Test Loss: 0.1651927
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2964599 Vali Loss: 0.1783020 Test Loss: 0.1651923
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2934240 Vali Loss: 0.1770341 Test Loss: 0.1651924
EarlyStopping counter: 9 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2928082 Vali Loss: 0.1809751 Test Loss: 0.1651922
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ABSA_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 231
test shape: (231, 10, 1) (231, 10, 1)
test shape: (231, 10, 1) (231, 10, 1)


	mse:0.00048523754230700433, mae:0.016842661425471306, rmse:0.022028107196092606, r2:-0.05103492736816406, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0168, RMSE: 0.0220, RÂ²: -0.0510, MAPE: 1.68%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.579 MB of 0.579 MB uploadedwandb: \ 0.579 MB of 0.579 MB uploadedwandb: | 0.579 MB of 0.579 MB uploadedwandb: / 0.579 MB of 0.579 MB uploadedwandb: - 0.579 MB of 0.785 MB uploadedwandb: \ 0.581 MB of 0.785 MB uploadedwandb: | 0.785 MB of 0.785 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–ƒâ–‚â–‚â–â–‚â–‚â–ƒâ–ƒâ–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–„â–‚â–„â–…â–‚â–„â–ƒâ–‚â–‚â–…â–„â–‚â–â–„â–ƒâ–„â–†â–ˆâ–…â–‚â–ƒâ–‚â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 25
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.16519
wandb:                 train/loss 0.29281
wandb:   val/directional_accuracy 51.89995
wandb:                   val/loss 0.18098
wandb:                    val/mae 0.01684
wandb:                   val/mape 168.33416
wandb:                    val/mse 0.00049
wandb:                     val/r2 -0.05103
wandb:                   val/rmse 0.02203
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/pgh2exrb
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_213447-pgh2exrb/logs
Completed: ABSA H=10

Training: Autoformer on ABSA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_214026-u7gjc9dv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/u7gjc9dv
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ABSA_H22 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/u7gjc9dv
>>>>>>>start training : long_term_forecast_Autoformer_ABSA_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 219
Epoch: 1, Steps: 132 | Train Loss: 0.3592379 Vali Loss: 0.1831321 Test Loss: 0.1657671
Validation loss decreased (inf --> 0.183132).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.35923792523416603, 'val/loss': 0.18313210138252803, 'test/loss': 0.16576712472098215, '_timestamp': 1762890057.200552}).
Epoch: 2, Steps: 132 | Train Loss: 0.3257918 Vali Loss: 0.1830883 Test Loss: 0.1632757
Validation loss decreased (0.183132 --> 0.183088).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3257917595857924, 'val/loss': 0.18308831538472856, 'test/loss': 0.16327565801995142, '_timestamp': 1762890068.5309072}).
Epoch: 3, Steps: 132 | Train Loss: 0.3116960 Vali Loss: 0.1922199 Test Loss: 0.1682687
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.3044942 Vali Loss: 0.1883657 Test Loss: 0.1679605
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2993989 Vali Loss: 0.1892151 Test Loss: 0.1700429
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2970765 Vali Loss: 0.1890818 Test Loss: 0.1690470
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2959021 Vali Loss: 0.1891203 Test Loss: 0.1683323
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2963632 Vali Loss: 0.1892276 Test Loss: 0.1678391
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2951544 Vali Loss: 0.1886063 Test Loss: 0.1682112
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2958227 Vali Loss: 0.1885333 Test Loss: 0.1682176
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2949471 Vali Loss: 0.1887157 Test Loss: 0.1680855
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2954065 Vali Loss: 0.1890949 Test Loss: 0.1680866
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ABSA_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 219
test shape: (219, 22, 1) (219, 22, 1)
test shape: (219, 22, 1) (219, 22, 1)


	mse:0.0004866708186455071, mae:0.016927750781178474, rmse:0.022060617804527283, r2:-0.03873765468597412, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0169, RMSE: 0.0221, RÂ²: -0.0387, MAPE: 1.71%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.656 MB of 0.658 MB uploadedwandb: \ 0.658 MB of 0.658 MB uploadedwandb: | 0.658 MB of 0.658 MB uploadedwandb: / 0.658 MB of 0.861 MB uploadedwandb: - 0.861 MB of 0.861 MB uploadedwandb: \ 0.861 MB of 0.861 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‚â–â–ˆâ–…â–ƒâ–â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–ƒâ–‚â–‚â–ƒâ–â–â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.16809
wandb:                 train/loss 0.29541
wandb:   val/directional_accuracy 51.20678
wandb:                   val/loss 0.18909
wandb:                    val/mae 0.01693
wandb:                   val/mape 171.1514
wandb:                    val/mse 0.00049
wandb:                     val/r2 -0.03874
wandb:                   val/rmse 0.02206
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/u7gjc9dv
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_214026-u7gjc9dv/logs
Completed: ABSA H=22

Training: Autoformer on ABSA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_214351-ng1u1fga
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/ng1u1fga
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ABSA_H50 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/ng1u1fga
>>>>>>>start training : long_term_forecast_Autoformer_ABSA_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 191
Epoch: 1, Steps: 132 | Train Loss: 0.3697436 Vali Loss: 0.1875180 Test Loss: 0.1687949
Validation loss decreased (inf --> 0.187518).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.36974356549255777, 'val/loss': 0.18751799315214157, 'test/loss': 0.16879491756359735, '_timestamp': 1762890261.5861588}).
Epoch: 2, Steps: 132 | Train Loss: 0.3386027 Vali Loss: 0.1946720 Test Loss: 0.1704308
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.3270107 Vali Loss: 0.2002997 Test Loss: 0.1721473
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.338602669649955, 'val/loss': 0.1946720058719317, 'test/loss': 0.17043077945709229, '_timestamp': 1762890272.3317602}).
Epoch: 4, Steps: 132 | Train Loss: 0.3218910 Vali Loss: 0.2000905 Test Loss: 0.1738533
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.3179562 Vali Loss: 0.1993186 Test Loss: 0.1758097
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.3161259 Vali Loss: 0.2036841 Test Loss: 0.1735550
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.3148125 Vali Loss: 0.2027565 Test Loss: 0.1731400
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.3141919 Vali Loss: 0.2025367 Test Loss: 0.1739092
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.3146001 Vali Loss: 0.2023380 Test Loss: 0.1736605
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.3153137 Vali Loss: 0.2024081 Test Loss: 0.1736045
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.3152232 Vali Loss: 0.2025706 Test Loss: 0.1734782
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ABSA_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 191
test shape: (191, 50, 1) (191, 50, 1)
test shape: (191, 50, 1) (191, 50, 1)


	mse:0.0005200469167903066, mae:0.017735419794917107, rmse:0.022804537788033485, r2:-0.0689777135848999, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0177, RMSE: 0.0228, RÂ²: -0.0690, MAPE: 1.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.770 MB of 0.773 MB uploadedwandb: \ 0.770 MB of 0.773 MB uploadedwandb: | 0.773 MB of 0.773 MB uploadedwandb: / 0.773 MB of 0.773 MB uploadedwandb: - 0.773 MB of 0.773 MB uploadedwandb: \ 0.773 MB of 0.976 MB uploadedwandb: | 0.976 MB of 0.976 MB uploadedwandb: / 0.976 MB of 0.976 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–„â–ˆâ–„â–ƒâ–„â–„â–„â–„
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–‚â–â–ˆâ–‡â–†â–†â–†â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.17348
wandb:                 train/loss 0.31522
wandb:   val/directional_accuracy 50.79603
wandb:                   val/loss 0.20257
wandb:                    val/mae 0.01774
wandb:                   val/mape 149.87334
wandb:                    val/mse 0.00052
wandb:                     val/r2 -0.06898
wandb:                   val/rmse 0.0228
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/ng1u1fga
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_214351-ng1u1fga/logs
Completed: ABSA H=50

Training: Autoformer on ABSA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_214722-4il731w1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/4il731w1
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ABSA_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/4il731w1
>>>>>>>start training : long_term_forecast_Autoformer_ABSA_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 141
Epoch: 1, Steps: 130 | Train Loss: 0.4089614 Vali Loss: 0.1916940 Test Loss: 0.1700858
Validation loss decreased (inf --> 0.191694).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.40896143248448, 'val/loss': 0.19169395864009858, 'test/loss': 0.17008584439754487, '_timestamp': 1762890474.0983016}).
Epoch: 2, Steps: 130 | Train Loss: 0.3808139 Vali Loss: 0.2047860 Test Loss: 0.1674229
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.3723875 Vali Loss: 0.2028415 Test Loss: 0.1691207
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.3684006 Vali Loss: 0.2027970 Test Loss: 0.1681621
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.38081387075094075, 'val/loss': 0.20478597283363342, 'test/loss': 0.16742286086082458, '_timestamp': 1762890481.2122407}).
Epoch: 5, Steps: 130 | Train Loss: 0.3658989 Vali Loss: 0.2022626 Test Loss: 0.1682395
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.3646405 Vali Loss: 0.2040247 Test Loss: 0.1688101
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.3650507 Vali Loss: 0.2039803 Test Loss: 0.1689094
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.3638192 Vali Loss: 0.2030485 Test Loss: 0.1693022
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.3638033 Vali Loss: 0.2029842 Test Loss: 0.1693389
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.3630278 Vali Loss: 0.2021722 Test Loss: 0.1692964
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.3633779 Vali Loss: 0.2026060 Test Loss: 0.1692789
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ABSA_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 141
test shape: (141, 100, 1) (141, 100, 1)
test shape: (141, 100, 1) (141, 100, 1)


	mse:0.0005378838395699859, mae:0.017688442021608353, rmse:0.023192323744297028, r2:-0.042342305183410645, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0177, RMSE: 0.0232, RÂ²: -0.0423, MAPE: 1.24%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.806 MB of 0.811 MB uploadedwandb: \ 0.806 MB of 0.811 MB uploadedwandb: | 0.811 MB of 0.811 MB uploadedwandb: / 0.811 MB of 0.811 MB uploadedwandb: - 0.811 MB of 0.811 MB uploadedwandb: \ 0.811 MB of 1.014 MB uploadedwandb: | 1.014 MB of 1.014 MB uploadedwandb: / 1.014 MB of 1.014 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‡â–â–â–…â–…â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–ƒâ–‚â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–ƒâ–â–ˆâ–ˆâ–„â–„â–â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.16928
wandb:                 train/loss 0.36338
wandb:   val/directional_accuracy 49.0293
wandb:                   val/loss 0.20261
wandb:                    val/mae 0.01769
wandb:                   val/mape 123.85043
wandb:                    val/mse 0.00054
wandb:                     val/r2 -0.04234
wandb:                   val/rmse 0.02319
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/4il731w1
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_214722-4il731w1/logs
Completed: ABSA H=100

Training: Autoformer on SASOL for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_215058-w536o5iu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/w536o5iu
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SASOL_H3 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/w536o5iu
>>>>>>>start training : long_term_forecast_Autoformer_SASOL_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 3
============================================================
train 3772
val 211
test 212
Epoch: 1, Steps: 118 | Train Loss: 0.2944103 Vali Loss: 0.1201978 Test Loss: 0.1622150
Validation loss decreased (inf --> 0.120198).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29441034301357755, 'val/loss': 0.12019779320274081, 'test/loss': 0.1622150433915002, '_timestamp': 1762890689.1731098}).
Epoch: 2, Steps: 118 | Train Loss: 0.2368343 Vali Loss: 0.1210802 Test Loss: 0.1679776
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 118 | Train Loss: 0.2197150 Vali Loss: 0.1133486 Test Loss: 0.1536743
Validation loss decreased (0.120198 --> 0.113349).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23683425506292763, 'val/loss': 0.12108024848358971, 'test/loss': 0.16797757680927003, '_timestamp': 1762890699.1095243}).
Epoch: 4, Steps: 118 | Train Loss: 0.2121385 Vali Loss: 0.1145022 Test Loss: 0.1512798
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2104407 Vali Loss: 0.1087817 Test Loss: 0.1471983
Validation loss decreased (0.113349 --> 0.108782).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2067889 Vali Loss: 0.1099060 Test Loss: 0.1481800
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2063802 Vali Loss: 0.1129575 Test Loss: 0.1467884
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2055613 Vali Loss: 0.1140108 Test Loss: 0.1467556
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2053278 Vali Loss: 0.1101387 Test Loss: 0.1468319
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2037656 Vali Loss: 0.1108134 Test Loss: 0.1468466
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2051924 Vali Loss: 0.1108105 Test Loss: 0.1468856
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2054575 Vali Loss: 0.1080735 Test Loss: 0.1468769
Validation loss decreased (0.108782 --> 0.108073).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2045968 Vali Loss: 0.1102664 Test Loss: 0.1468588
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2039230 Vali Loss: 0.1099483 Test Loss: 0.1467762
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.2045025 Vali Loss: 0.1107751 Test Loss: 0.1467912
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.2048990 Vali Loss: 0.1081017 Test Loss: 0.1467903
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.2029972 Vali Loss: 0.1088712 Test Loss: 0.1467902
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.2045553 Vali Loss: 0.1103406 Test Loss: 0.1467900
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.2045605 Vali Loss: 0.1113062 Test Loss: 0.1467899
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.2042007 Vali Loss: 0.1110835 Test Loss: 0.1467896
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 118 | Train Loss: 0.2040315 Vali Loss: 0.1140169 Test Loss: 0.1467897
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 118 | Train Loss: 0.2048612 Vali Loss: 0.1110092 Test Loss: 0.1467904
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SASOL_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 212
test shape: (212, 3, 1) (212, 3, 1)
test shape: (212, 3, 1) (212, 3, 1)


	mse:0.002274603582918644, mae:0.03517119213938713, rmse:0.04769280552864075, r2:-0.0325009822845459, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0352, RMSE: 0.0477, RÂ²: -0.0325, MAPE: 8114290.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.458 MB of 0.458 MB uploadedwandb: \ 0.458 MB of 0.458 MB uploadedwandb: | 0.458 MB of 0.458 MB uploadedwandb: / 0.458 MB of 0.458 MB uploadedwandb: - 0.458 MB of 0.458 MB uploadedwandb: \ 0.458 MB of 0.458 MB uploadedwandb: | 0.458 MB of 0.458 MB uploadedwandb: / 0.588 MB of 0.793 MB uploaded (0.002 MB deduped)wandb: - 0.588 MB of 0.793 MB uploaded (0.002 MB deduped)wandb: \ 0.793 MB of 0.793 MB uploaded (0.002 MB deduped)wandb: | 0.793 MB of 0.793 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–†â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–ˆâ–‚â–ƒâ–†â–‡â–ƒâ–„â–„â–â–ƒâ–ƒâ–„â–â–‚â–ƒâ–…â–„â–‡â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.14679
wandb:                 train/loss 0.20486
wandb:   val/directional_accuracy 50.4717
wandb:                   val/loss 0.11101
wandb:                    val/mae 0.03517
wandb:                   val/mape 811429050.0
wandb:                    val/mse 0.00227
wandb:                     val/r2 -0.0325
wandb:                   val/rmse 0.04769
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/w536o5iu
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_215058-w536o5iu/logs
Completed: SASOL H=3

Training: Autoformer on SASOL for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_215630-viojd11g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/viojd11g
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SASOL_H5 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/viojd11g
>>>>>>>start training : long_term_forecast_Autoformer_SASOL_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 5
============================================================
train 3770
val 209
test 210
Epoch: 1, Steps: 118 | Train Loss: 0.2916864 Vali Loss: 0.1317956 Test Loss: 0.1683562
Validation loss decreased (inf --> 0.131796).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29168640582238214, 'val/loss': 0.1317956479532378, 'test/loss': 0.1683561589036669, '_timestamp': 1762891016.7075026}).
Epoch: 2, Steps: 118 | Train Loss: 0.2416709 Vali Loss: 0.1194298 Test Loss: 0.1725637
Validation loss decreased (0.131796 --> 0.119430).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 118 | Train Loss: 0.2214329 Vali Loss: 0.1117320 Test Loss: 0.1641388
Validation loss decreased (0.119430 --> 0.111732).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24167090553348347, 'val/loss': 0.11942982567208153, 'test/loss': 0.1725636880312647, '_timestamp': 1762891026.5810683}).
Epoch: 4, Steps: 118 | Train Loss: 0.2145555 Vali Loss: 0.1135268 Test Loss: 0.1582141
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2105360 Vali Loss: 0.1134799 Test Loss: 0.1581952
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2084630 Vali Loss: 0.1131793 Test Loss: 0.1584014
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2069968 Vali Loss: 0.1110740 Test Loss: 0.1571843
Validation loss decreased (0.111732 --> 0.111074).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2078153 Vali Loss: 0.1102184 Test Loss: 0.1569449
Validation loss decreased (0.111074 --> 0.110218).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2077345 Vali Loss: 0.1099524 Test Loss: 0.1573363
Validation loss decreased (0.110218 --> 0.109952).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2073632 Vali Loss: 0.1099148 Test Loss: 0.1575936
Validation loss decreased (0.109952 --> 0.109915).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2066045 Vali Loss: 0.1123573 Test Loss: 0.1577563
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2064340 Vali Loss: 0.1106248 Test Loss: 0.1575759
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2078389 Vali Loss: 0.1117765 Test Loss: 0.1575551
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2073692 Vali Loss: 0.1095499 Test Loss: 0.1575604
Validation loss decreased (0.109915 --> 0.109550).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.2070580 Vali Loss: 0.1095683 Test Loss: 0.1576317
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.2061416 Vali Loss: 0.1099697 Test Loss: 0.1576297
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.2069621 Vali Loss: 0.1102511 Test Loss: 0.1576288
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.2064932 Vali Loss: 0.1081717 Test Loss: 0.1576290
Validation loss decreased (0.109550 --> 0.108172).  Saving model ...
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.2063327 Vali Loss: 0.1100228 Test Loss: 0.1576289
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.2065175 Vali Loss: 0.1092413 Test Loss: 0.1576289
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 118 | Train Loss: 0.2060949 Vali Loss: 0.1105251 Test Loss: 0.1576294
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 118 | Train Loss: 0.2068584 Vali Loss: 0.1096401 Test Loss: 0.1576293
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 118 | Train Loss: 0.2063189 Vali Loss: 0.1092862 Test Loss: 0.1576288
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 118 | Train Loss: 0.2070439 Vali Loss: 0.1089882 Test Loss: 0.1576291
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 118 | Train Loss: 0.2067942 Vali Loss: 0.1089339 Test Loss: 0.1576290
EarlyStopping counter: 7 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 118 | Train Loss: 0.2072940 Vali Loss: 0.1120990 Test Loss: 0.1576288
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 118 | Train Loss: 0.2068115 Vali Loss: 0.1110114 Test Loss: 0.1576288
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 118 | Train Loss: 0.2065304 Vali Loss: 0.1119115 Test Loss: 0.1576288
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SASOL_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 210
test shape: (210, 5, 1) (210, 5, 1)
test shape: (210, 5, 1) (210, 5, 1)


	mse:0.0022732107900083065, mae:0.03530995547771454, rmse:0.04767819866538048, r2:-0.024245619773864746, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0353, RMSE: 0.0477, RÂ²: -0.0242, MAPE: 10524634.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.495 MB of 0.495 MB uploadedwandb: \ 0.495 MB of 0.495 MB uploadedwandb: | 0.495 MB of 0.495 MB uploadedwandb: / 0.495 MB of 0.495 MB uploadedwandb: - 0.495 MB of 0.495 MB uploadedwandb: \ 0.495 MB of 0.702 MB uploadedwandb: | 0.702 MB of 0.702 MB uploadedwandb: / 0.702 MB of 0.702 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–‚â–‚â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–‚â–‚â–‚â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–ˆâ–ˆâ–ˆâ–…â–„â–ƒâ–ƒâ–†â–„â–†â–ƒâ–ƒâ–ƒâ–„â–â–ƒâ–‚â–„â–ƒâ–‚â–‚â–‚â–†â–…â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 27
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.15763
wandb:                 train/loss 0.20653
wandb:   val/directional_accuracy 53.33333
wandb:                   val/loss 0.11191
wandb:                    val/mae 0.03531
wandb:                   val/mape 1052463400.0
wandb:                    val/mse 0.00227
wandb:                     val/r2 -0.02425
wandb:                   val/rmse 0.04768
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/viojd11g
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_215630-viojd11g/logs
Completed: SASOL H=5

Training: Autoformer on SASOL for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_220128-7sv145i4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/7sv145i4
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SASOL_H10Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/7sv145i4
>>>>>>>start training : long_term_forecast_Autoformer_SASOL_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 10
============================================================
train 3765
val 204
test 205
Epoch: 1, Steps: 118 | Train Loss: 0.2812425 Vali Loss: 0.1220123 Test Loss: 0.1640203
Validation loss decreased (inf --> 0.122012).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.281242479080871, 'val/loss': 0.12201228099209922, 'test/loss': 0.16402031587702887, '_timestamp': 1762891317.691808}).
Epoch: 2, Steps: 118 | Train Loss: 0.2374643 Vali Loss: 0.1228536 Test Loss: 0.1716446
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 118 | Train Loss: 0.2234784 Vali Loss: 0.1118730 Test Loss: 0.1631054
Validation loss decreased (0.122012 --> 0.111873).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2374643148001978, 'val/loss': 0.12285359842436654, 'test/loss': 0.1716445886663028, '_timestamp': 1762891328.52117}).
Epoch: 4, Steps: 118 | Train Loss: 0.2181046 Vali Loss: 0.1117926 Test Loss: 0.1613733
Validation loss decreased (0.111873 --> 0.111793).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2148523 Vali Loss: 0.1107739 Test Loss: 0.1625781
Validation loss decreased (0.111793 --> 0.110774).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2129757 Vali Loss: 0.1110341 Test Loss: 0.1621536
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2127131 Vali Loss: 0.1170083 Test Loss: 0.1617407
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2121700 Vali Loss: 0.1123005 Test Loss: 0.1612032
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2126839 Vali Loss: 0.1115083 Test Loss: 0.1610411
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2119989 Vali Loss: 0.1116671 Test Loss: 0.1614246
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2121436 Vali Loss: 0.1144792 Test Loss: 0.1613977
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2123813 Vali Loss: 0.1122521 Test Loss: 0.1614228
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2131438 Vali Loss: 0.1146622 Test Loss: 0.1613903
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2124333 Vali Loss: 0.1137643 Test Loss: 0.1613855
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.2123803 Vali Loss: 0.1161046 Test Loss: 0.1613901
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SASOL_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 205
test shape: (205, 10, 1) (205, 10, 1)
test shape: (205, 10, 1) (205, 10, 1)


	mse:0.002293179975822568, mae:0.03525584936141968, rmse:0.047887157648801804, r2:-0.033103108406066895, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0353, RMSE: 0.0479, RÂ²: -0.0331, MAPE: 11286466.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.549 MB of 0.550 MB uploadedwandb: \ 0.550 MB of 0.550 MB uploadedwandb: | 0.550 MB of 0.550 MB uploadedwandb: / 0.550 MB of 0.550 MB uploadedwandb: - 0.550 MB of 0.550 MB uploadedwandb: \ 0.550 MB of 0.753 MB uploadedwandb: | 0.556 MB of 0.753 MB uploadedwandb: / 0.753 MB of 0.753 MB uploadedwandb: - 0.753 MB of 0.753 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–†â–…â–ƒâ–‚â–â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–‚â–â–â–ˆâ–ƒâ–‚â–‚â–…â–ƒâ–…â–„â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.16139
wandb:                 train/loss 0.21238
wandb:   val/directional_accuracy 51.76152
wandb:                   val/loss 0.1161
wandb:                    val/mae 0.03526
wandb:                   val/mape 1128646600.0
wandb:                    val/mse 0.00229
wandb:                     val/r2 -0.0331
wandb:                   val/rmse 0.04789
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/7sv145i4
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_220128-7sv145i4/logs
Completed: SASOL H=10

Training: Autoformer on SASOL for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_220502-bc9o87lk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/bc9o87lk
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SASOL_H22Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/bc9o87lk
>>>>>>>start training : long_term_forecast_Autoformer_SASOL_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 22
============================================================
train 3753
val 192
test 193
Epoch: 1, Steps: 118 | Train Loss: 0.2886763 Vali Loss: 0.1130937 Test Loss: 0.1686455
Validation loss decreased (inf --> 0.113094).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.288676320369971, 'val/loss': 0.11309373751282692, 'test/loss': 0.16864554903336934, '_timestamp': 1762891531.3623827}).
Epoch: 2, Steps: 118 | Train Loss: 0.2529176 Vali Loss: 0.1222740 Test Loss: 0.1723071
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 118 | Train Loss: 0.2382328 Vali Loss: 0.1164547 Test Loss: 0.1676285
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2315068 Vali Loss: 0.1161113 Test Loss: 0.1749572
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2529176102351334, 'val/loss': 0.12227399523059528, 'test/loss': 0.1723070666193962, '_timestamp': 1762891540.0741632}).
Epoch: 5, Steps: 118 | Train Loss: 0.2274915 Vali Loss: 0.1158089 Test Loss: 0.1743074
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2262234 Vali Loss: 0.1161055 Test Loss: 0.1715285
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2238426 Vali Loss: 0.1167745 Test Loss: 0.1713695
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2227340 Vali Loss: 0.1171838 Test Loss: 0.1725657
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2232610 Vali Loss: 0.1169544 Test Loss: 0.1719198
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2237583 Vali Loss: 0.1170251 Test Loss: 0.1720259
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2232922 Vali Loss: 0.1169639 Test Loss: 0.1719964
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SASOL_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 193
test shape: (193, 22, 1) (193, 22, 1)
test shape: (193, 22, 1) (193, 22, 1)


	mse:0.0023865527473390102, mae:0.03577873110771179, rmse:0.048852358013391495, r2:-0.06346547603607178, dtw:Not calculated


VAL - MSE: 0.0024, MAE: 0.0358, RMSE: 0.0489, RÂ²: -0.0635, MAPE: 13317625.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.609 MB of 0.610 MB uploadedwandb: \ 0.609 MB of 0.610 MB uploadedwandb: | 0.610 MB of 0.610 MB uploadedwandb: / 0.610 MB of 0.610 MB uploadedwandb: - 0.610 MB of 0.813 MB uploadedwandb: \ 0.813 MB of 0.813 MB uploadedwandb: | 0.813 MB of 0.813 MB uploadedwandb: / 0.813 MB of 0.813 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–‡â–…â–…â–†â–…â–…â–…
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–ƒâ–â–ƒâ–†â–ˆâ–‡â–‡â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.172
wandb:                 train/loss 0.22329
wandb:   val/directional_accuracy 47.91512
wandb:                   val/loss 0.11696
wandb:                    val/mae 0.03578
wandb:                   val/mape 1331762500.0
wandb:                    val/mse 0.00239
wandb:                     val/r2 -0.06347
wandb:                   val/rmse 0.04885
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/bc9o87lk
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_220502-bc9o87lk/logs
Completed: SASOL H=22

Training: Autoformer on SASOL for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_220829-89o0gyy8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/89o0gyy8
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SASOL_H50Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/89o0gyy8
>>>>>>>start training : long_term_forecast_Autoformer_SASOL_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 50
============================================================
train 3725
val 164
test 165
Epoch: 1, Steps: 117 | Train Loss: 0.3166532 Vali Loss: 0.1080592 Test Loss: 0.1893606
Validation loss decreased (inf --> 0.108059).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3166531770147829, 'val/loss': 0.10805923119187355, 'test/loss': 0.18936057140429816, '_timestamp': 1762891743.0307372}).
Epoch: 2, Steps: 117 | Train Loss: 0.2692834 Vali Loss: 0.1186237 Test Loss: 0.2219066
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 117 | Train Loss: 0.2513075 Vali Loss: 0.1258975 Test Loss: 0.2180266
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 117 | Train Loss: 0.2425700 Vali Loss: 0.1157270 Test Loss: 0.2199880
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26928336752785575, 'val/loss': 0.11862370371818542, 'test/loss': 0.22190660734971365, '_timestamp': 1762891752.8500972}).
Epoch: 5, Steps: 117 | Train Loss: 0.2387427 Vali Loss: 0.1234227 Test Loss: 0.2190347
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 117 | Train Loss: 0.2372942 Vali Loss: 0.1231419 Test Loss: 0.2200483
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 117 | Train Loss: 0.2356016 Vali Loss: 0.1238171 Test Loss: 0.2190905
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 117 | Train Loss: 0.2354960 Vali Loss: 0.1191308 Test Loss: 0.2204811
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 117 | Train Loss: 0.2350740 Vali Loss: 0.1218466 Test Loss: 0.2213628
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 117 | Train Loss: 0.2350035 Vali Loss: 0.1218710 Test Loss: 0.2217977
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 117 | Train Loss: 0.2345114 Vali Loss: 0.1182026 Test Loss: 0.2215485
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SASOL_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 165
test shape: (165, 50, 1) (165, 50, 1)
test shape: (165, 50, 1) (165, 50, 1)


	mse:0.0021906804759055376, mae:0.03470456600189209, rmse:0.04680470749735832, r2:-0.06927013397216797, dtw:Not calculated


VAL - MSE: 0.0022, MAE: 0.0347, RMSE: 0.0468, RÂ²: -0.0693, MAPE: 12552229.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.650 MB of 0.652 MB uploadedwandb: \ 0.650 MB of 0.652 MB uploadedwandb: | 0.650 MB of 0.652 MB uploadedwandb: / 0.652 MB of 0.652 MB uploadedwandb: - 0.652 MB of 0.652 MB uploadedwandb: \ 0.652 MB of 0.855 MB uploadedwandb: | 0.855 MB of 0.855 MB uploadedwandb: / 0.855 MB of 0.855 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–…â–ƒâ–…â–ƒâ–†â–‡â–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–†â–†â–‡â–ƒâ–…â–…â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.22155
wandb:                 train/loss 0.23451
wandb:   val/directional_accuracy 47.92826
wandb:                   val/loss 0.1182
wandb:                    val/mae 0.0347
wandb:                   val/mape 1255222900.0
wandb:                    val/mse 0.00219
wandb:                     val/r2 -0.06927
wandb:                   val/rmse 0.0468
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/89o0gyy8
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_220829-89o0gyy8/logs
Completed: SASOL H=50

Training: Autoformer on SASOL for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_221224-ncdeh211
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/ncdeh211
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SASOL_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/ncdeh211
>>>>>>>start training : long_term_forecast_Autoformer_SASOL_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 100
============================================================
train 3675
val 114
test 115
Epoch: 1, Steps: 115 | Train Loss: 0.3811456 Vali Loss: 0.1240215 Test Loss: 0.1873611
Validation loss decreased (inf --> 0.124022).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.38114562669525975, 'val/loss': 0.12402153760194778, 'test/loss': 0.18736106902360916, '_timestamp': 1762891974.8114855}).
Epoch: 2, Steps: 115 | Train Loss: 0.3454182 Vali Loss: 0.1189029 Test Loss: 0.2084676
Validation loss decreased (0.124022 --> 0.118903).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 115 | Train Loss: 0.3303631 Vali Loss: 0.1258287 Test Loss: 0.2289691
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3454181988602099, 'val/loss': 0.11890291050076485, 'test/loss': 0.20846758782863617, '_timestamp': 1762891985.9659789}).
Epoch: 4, Steps: 115 | Train Loss: 0.3249524 Vali Loss: 0.1371382 Test Loss: 0.2375938
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 115 | Train Loss: 0.3223960 Vali Loss: 0.1355915 Test Loss: 0.2350633
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 115 | Train Loss: 0.3206162 Vali Loss: 0.1320652 Test Loss: 0.2304517
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 115 | Train Loss: 0.3195415 Vali Loss: 0.1348717 Test Loss: 0.2363428
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 115 | Train Loss: 0.3189995 Vali Loss: 0.1310803 Test Loss: 0.2307725
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 115 | Train Loss: 0.3189964 Vali Loss: 0.1315155 Test Loss: 0.2351136
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 115 | Train Loss: 0.3191951 Vali Loss: 0.1325829 Test Loss: 0.2339761
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 115 | Train Loss: 0.3186785 Vali Loss: 0.1313932 Test Loss: 0.2321568
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 115 | Train Loss: 0.3190964 Vali Loss: 0.1336667 Test Loss: 0.2326657
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SASOL_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 115
test shape: (115, 100, 1) (115, 100, 1)
test shape: (115, 100, 1) (115, 100, 1)


	mse:0.0020187932532280684, mae:0.03301682323217392, rmse:0.04493098333477974, r2:-0.01732182502746582, dtw:Not calculated


VAL - MSE: 0.0020, MAE: 0.0330, RMSE: 0.0449, RÂ²: -0.0173, MAPE: 7447348.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.652 MB of 0.656 MB uploadedwandb: \ 0.652 MB of 0.656 MB uploadedwandb: | 0.656 MB of 0.656 MB uploadedwandb: / 0.656 MB of 0.656 MB uploadedwandb: - 0.656 MB of 0.860 MB uploadedwandb: \ 0.860 MB of 0.860 MB uploadedwandb: | 0.860 MB of 0.860 MB uploadedwandb: / 0.860 MB of 0.860 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–†â–‚â–‡â–‚â–†â–…â–„â–„
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ˆâ–‡â–…â–‡â–„â–…â–…â–„â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.23267
wandb:                 train/loss 0.3191
wandb:   val/directional_accuracy 50.49627
wandb:                   val/loss 0.13367
wandb:                    val/mae 0.03302
wandb:                   val/mape 744734850.0
wandb:                    val/mse 0.00202
wandb:                     val/r2 -0.01732
wandb:                   val/rmse 0.04493
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/ncdeh211
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_221224-ncdeh211/logs
Completed: SASOL H=100

Training: Autoformer on DRD_GOLD for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_221559-izqwpe7h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/izqwpe7h
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_DRD_GOLD_H3Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/izqwpe7h
>>>>>>>start training : long_term_forecast_Autoformer_DRD_GOLD_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.3321156 Vali Loss: 0.1681531 Test Loss: 0.1762661
Validation loss decreased (inf --> 0.168153).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3321155630108109, 'val/loss': 0.1681530922651291, 'test/loss': 0.17626614309847355, '_timestamp': 1762892193.6587863}).
Epoch: 2, Steps: 133 | Train Loss: 0.2691959 Vali Loss: 0.1547632 Test Loss: 0.1644979
Validation loss decreased (0.168153 --> 0.154763).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2518325 Vali Loss: 0.1533274 Test Loss: 0.1500066
Validation loss decreased (0.154763 --> 0.153327).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2691959000395653, 'val/loss': 0.15476320032030344, 'test/loss': 0.1644978504627943, '_timestamp': 1762892204.8228247}).
Epoch: 4, Steps: 133 | Train Loss: 0.2430812 Vali Loss: 0.1476537 Test Loss: 0.1493227
Validation loss decreased (0.153327 --> 0.147654).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2385067 Vali Loss: 0.1439797 Test Loss: 0.1453791
Validation loss decreased (0.147654 --> 0.143980).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2349739 Vali Loss: 0.1442057 Test Loss: 0.1439805
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2337243 Vali Loss: 0.1456135 Test Loss: 0.1439413
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2326852 Vali Loss: 0.1434050 Test Loss: 0.1438239
Validation loss decreased (0.143980 --> 0.143405).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2328509 Vali Loss: 0.1427545 Test Loss: 0.1435109
Validation loss decreased (0.143405 --> 0.142754).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2322400 Vali Loss: 0.1436654 Test Loss: 0.1433549
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2318117 Vali Loss: 0.1420260 Test Loss: 0.1437328
Validation loss decreased (0.142754 --> 0.142026).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2323232 Vali Loss: 0.1437320 Test Loss: 0.1436923
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2320501 Vali Loss: 0.1458122 Test Loss: 0.1436740
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2320004 Vali Loss: 0.1443579 Test Loss: 0.1436723
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2318013 Vali Loss: 0.1489705 Test Loss: 0.1436661
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2329324 Vali Loss: 0.1421782 Test Loss: 0.1436665
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2323443 Vali Loss: 0.1409591 Test Loss: 0.1436650
Validation loss decreased (0.142026 --> 0.140959).  Saving model ...
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2314821 Vali Loss: 0.1421383 Test Loss: 0.1436642
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2328688 Vali Loss: 0.1426154 Test Loss: 0.1436649
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2318878 Vali Loss: 0.1528291 Test Loss: 0.1436645
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2329764 Vali Loss: 0.1446420 Test Loss: 0.1436644
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2327252 Vali Loss: 0.1390480 Test Loss: 0.1436646
Validation loss decreased (0.140959 --> 0.139048).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2314469 Vali Loss: 0.1467841 Test Loss: 0.1436641
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2316076 Vali Loss: 0.1423537 Test Loss: 0.1436644
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2316031 Vali Loss: 0.1463021 Test Loss: 0.1436643
EarlyStopping counter: 3 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2329120 Vali Loss: 0.1416818 Test Loss: 0.1436642
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.2315737 Vali Loss: 0.1440022 Test Loss: 0.1436645
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.2315321 Vali Loss: 0.1449063 Test Loss: 0.1436643
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.2310415 Vali Loss: 0.1425390 Test Loss: 0.1436643
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 133 | Train Loss: 0.2319710 Vali Loss: 0.1424832 Test Loss: 0.1436644
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 133 | Train Loss: 0.2333383 Vali Loss: 0.1462447 Test Loss: 0.1436643
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 133 | Train Loss: 0.2326885 Vali Loss: 0.1425240 Test Loss: 0.1436643
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_DRD_GOLD_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.0009850282222032547, mae:0.02245444990694523, rmse:0.031385160982608795, r2:-0.062169551849365234, dtw:Not calculated


VAL - MSE: 0.0010, MAE: 0.0225, RMSE: 0.0314, RÂ²: -0.0622, MAPE: 2778468.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.483 MB of 0.483 MB uploadedwandb: \ 0.483 MB of 0.483 MB uploadedwandb: | 0.483 MB of 0.483 MB uploadedwandb: / 0.483 MB of 0.483 MB uploadedwandb: - 0.483 MB of 0.483 MB uploadedwandb: \ 0.483 MB of 0.483 MB uploadedwandb: | 0.483 MB of 0.483 MB uploadedwandb: / 0.614 MB of 0.820 MB uploaded (0.002 MB deduped)wandb: - 0.614 MB of 0.820 MB uploaded (0.002 MB deduped)wandb: \ 0.820 MB of 0.820 MB uploaded (0.002 MB deduped)wandb: | 0.820 MB of 0.820 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‡â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–‚â–â–‚â–‚â–â–â–â–‚â–â–â–â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–ƒâ–„â–„â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–„â–„â–†â–ƒâ–‚â–ƒâ–ƒâ–ˆâ–„â–â–…â–ƒâ–…â–‚â–ƒâ–„â–ƒâ–ƒâ–…â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 31
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.14366
wandb:                 train/loss 0.23269
wandb:   val/directional_accuracy 51.26582
wandb:                   val/loss 0.14252
wandb:                    val/mae 0.02245
wandb:                   val/mape 277846875.0
wandb:                    val/mse 0.00099
wandb:                     val/r2 -0.06217
wandb:                   val/rmse 0.03139
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/izqwpe7h
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_221559-izqwpe7h/logs
Completed: DRD_GOLD H=3

Training: Autoformer on DRD_GOLD for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_222227-awbx6on3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/awbx6on3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_DRD_GOLD_H5Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/awbx6on3
>>>>>>>start training : long_term_forecast_Autoformer_DRD_GOLD_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.3154590 Vali Loss: 0.1845095 Test Loss: 0.1691451
Validation loss decreased (inf --> 0.184509).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3154589604390295, 'val/loss': 0.18450947105884552, 'test/loss': 0.1691451482474804, '_timestamp': 1762892578.7779338}).
Epoch: 2, Steps: 133 | Train Loss: 0.2685663 Vali Loss: 0.1660401 Test Loss: 0.1556516
Validation loss decreased (0.184509 --> 0.166040).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.268566332477376, 'val/loss': 0.16604010108858347, 'test/loss': 0.15565163269639015, '_timestamp': 1762892590.5437741}).
Epoch: 3, Steps: 133 | Train Loss: 0.2521437 Vali Loss: 0.1622438 Test Loss: 0.1503777
Validation loss decreased (0.166040 --> 0.162244).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2447922 Vali Loss: 0.1551905 Test Loss: 0.1466521
Validation loss decreased (0.162244 --> 0.155190).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2412434 Vali Loss: 0.1539795 Test Loss: 0.1442980
Validation loss decreased (0.155190 --> 0.153980).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2394230 Vali Loss: 0.1546667 Test Loss: 0.1434455
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2382804 Vali Loss: 0.1570770 Test Loss: 0.1437439
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2383602 Vali Loss: 0.1554168 Test Loss: 0.1434223
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2383020 Vali Loss: 0.1536876 Test Loss: 0.1430539
Validation loss decreased (0.153980 --> 0.153688).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2380166 Vali Loss: 0.1554911 Test Loss: 0.1438276
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2377062 Vali Loss: 0.1593593 Test Loss: 0.1437983
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2365412 Vali Loss: 0.1522315 Test Loss: 0.1438288
Validation loss decreased (0.153688 --> 0.152232).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2376381 Vali Loss: 0.1511221 Test Loss: 0.1438183
Validation loss decreased (0.152232 --> 0.151122).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2376107 Vali Loss: 0.1509865 Test Loss: 0.1438221
Validation loss decreased (0.151122 --> 0.150987).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2373077 Vali Loss: 0.1495146 Test Loss: 0.1438207
Validation loss decreased (0.150987 --> 0.149515).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2374642 Vali Loss: 0.1506332 Test Loss: 0.1438184
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2364928 Vali Loss: 0.1521762 Test Loss: 0.1438180
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2372032 Vali Loss: 0.1530275 Test Loss: 0.1438184
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2368337 Vali Loss: 0.1550373 Test Loss: 0.1438177
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2376887 Vali Loss: 0.1511996 Test Loss: 0.1438183
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2364297 Vali Loss: 0.1518849 Test Loss: 0.1438180
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2362481 Vali Loss: 0.1552500 Test Loss: 0.1438178
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2368573 Vali Loss: 0.1540833 Test Loss: 0.1438183
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2376138 Vali Loss: 0.1551090 Test Loss: 0.1438183
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2370404 Vali Loss: 0.1575779 Test Loss: 0.1438182
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_DRD_GOLD_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.0009829868795350194, mae:0.022476529702544212, rmse:0.03135262057185173, r2:-0.0526806116104126, dtw:Not calculated


VAL - MSE: 0.0010, MAE: 0.0225, RMSE: 0.0314, RÂ²: -0.0527, MAPE: 3136780.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.511 MB of 0.511 MB uploadedwandb: \ 0.511 MB of 0.511 MB uploadedwandb: | 0.511 MB of 0.511 MB uploadedwandb: / 0.511 MB of 0.511 MB uploadedwandb: - 0.511 MB of 0.717 MB uploadedwandb: \ 0.717 MB of 0.717 MB uploadedwandb: | 0.717 MB of 0.717 MB uploadedwandb: / 0.717 MB of 0.717 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–â–â–â–‚â–â–â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–ƒâ–„â–…â–„â–ƒâ–„â–†â–‚â–‚â–‚â–â–‚â–‚â–ƒâ–„â–‚â–‚â–„â–„â–„â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 24
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.14382
wandb:                 train/loss 0.23704
wandb:   val/directional_accuracy 55.85106
wandb:                   val/loss 0.15758
wandb:                    val/mae 0.02248
wandb:                   val/mape 313678075.0
wandb:                    val/mse 0.00098
wandb:                     val/r2 -0.05268
wandb:                   val/rmse 0.03135
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/awbx6on3
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_222227-awbx6on3/logs
Completed: DRD_GOLD H=5

Training: Autoformer on DRD_GOLD for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_222749-f5g9o1ob
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/f5g9o1ob
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_DRD_GOLD_H10Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/f5g9o1ob
>>>>>>>start training : long_term_forecast_Autoformer_DRD_GOLD_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.3218164 Vali Loss: 0.1731925 Test Loss: 0.1588405
Validation loss decreased (inf --> 0.173192).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.32181639162669506, 'val/loss': 0.17319249361753464, 'test/loss': 0.158840486779809, '_timestamp': 1762892898.725744}).
Epoch: 2, Steps: 133 | Train Loss: 0.2736987 Vali Loss: 0.1816832 Test Loss: 0.1574363
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2606239 Vali Loss: 0.1688941 Test Loss: 0.1541455
Validation loss decreased (0.173192 --> 0.168894).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.273698745141352, 'val/loss': 0.1816832348704338, 'test/loss': 0.1574363335967064, '_timestamp': 1762892908.1960952}).
Epoch: 4, Steps: 133 | Train Loss: 0.2534489 Vali Loss: 0.1645657 Test Loss: 0.1496445
Validation loss decreased (0.168894 --> 0.164566).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2512565 Vali Loss: 0.1649560 Test Loss: 0.1472073
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2501916 Vali Loss: 0.1749215 Test Loss: 0.1471343
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2499805 Vali Loss: 0.1782900 Test Loss: 0.1476119
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2496820 Vali Loss: 0.1640630 Test Loss: 0.1477895
Validation loss decreased (0.164566 --> 0.164063).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2509822 Vali Loss: 0.1656805 Test Loss: 0.1468953
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2486115 Vali Loss: 0.1654287 Test Loss: 0.1471567
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2497289 Vali Loss: 0.1660211 Test Loss: 0.1473960
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2481384 Vali Loss: 0.1638902 Test Loss: 0.1473981
Validation loss decreased (0.164063 --> 0.163890).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2499347 Vali Loss: 0.1724765 Test Loss: 0.1474880
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2497008 Vali Loss: 0.1624759 Test Loss: 0.1474986
Validation loss decreased (0.163890 --> 0.162476).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2493737 Vali Loss: 0.1666568 Test Loss: 0.1474978
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2489399 Vali Loss: 0.1616052 Test Loss: 0.1474984
Validation loss decreased (0.162476 --> 0.161605).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2490779 Vali Loss: 0.1632928 Test Loss: 0.1474979
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2489062 Vali Loss: 0.1715395 Test Loss: 0.1474984
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2484197 Vali Loss: 0.1711399 Test Loss: 0.1474981
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2484683 Vali Loss: 0.1771632 Test Loss: 0.1474984
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2477529 Vali Loss: 0.1729328 Test Loss: 0.1474983
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2481582 Vali Loss: 0.1634138 Test Loss: 0.1474984
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2491524 Vali Loss: 0.1721954 Test Loss: 0.1474981
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2490569 Vali Loss: 0.1686342 Test Loss: 0.1474982
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2507563 Vali Loss: 0.1625886 Test Loss: 0.1474980
EarlyStopping counter: 9 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2478049 Vali Loss: 0.1708315 Test Loss: 0.1474981
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_DRD_GOLD_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.0009881106670945883, mae:0.022341560572385788, rmse:0.03143422678112984, r2:-0.042022109031677246, dtw:Not calculated


VAL - MSE: 0.0010, MAE: 0.0223, RMSE: 0.0314, RÂ²: -0.0420, MAPE: 2145898.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.528 MB of 0.529 MB uploadedwandb: \ 0.528 MB of 0.529 MB uploadedwandb: | 0.529 MB of 0.529 MB uploadedwandb: / 0.529 MB of 0.529 MB uploadedwandb: - 0.529 MB of 0.735 MB uploadedwandb: \ 0.735 MB of 0.735 MB uploadedwandb: | 0.735 MB of 0.735 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–â–â–‚â–‚â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–‚â–ƒâ–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–‚â–ƒâ–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–‚â–‚â–‡â–ˆâ–‚â–ƒâ–ƒâ–ƒâ–‚â–†â–â–ƒâ–â–‚â–…â–…â–ˆâ–†â–‚â–…â–„â–â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 25
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.1475
wandb:                 train/loss 0.2478
wandb:   val/directional_accuracy 53.62319
wandb:                   val/loss 0.17083
wandb:                    val/mae 0.02234
wandb:                   val/mape 214589875.0
wandb:                    val/mse 0.00099
wandb:                     val/r2 -0.04202
wandb:                   val/rmse 0.03143
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/f5g9o1ob
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_222749-f5g9o1ob/logs
Completed: DRD_GOLD H=10

Training: Autoformer on DRD_GOLD for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_223346-ictk1ln6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/ictk1ln6
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_DRD_GOLD_H22Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/ictk1ln6
>>>>>>>start training : long_term_forecast_Autoformer_DRD_GOLD_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.3405514 Vali Loss: 0.1910903 Test Loss: 0.1617528
Validation loss decreased (inf --> 0.191090).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.34055140828995994, 'val/loss': 0.19109034964016505, 'test/loss': 0.16175280724252974, '_timestamp': 1762893257.94393}).
Epoch: 2, Steps: 132 | Train Loss: 0.2935158 Vali Loss: 0.1802527 Test Loss: 0.1618878
Validation loss decreased (0.191090 --> 0.180253).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2777026 Vali Loss: 0.1821083 Test Loss: 0.1649220
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.29351582434592827, 'val/loss': 0.180252662726811, 'test/loss': 0.16188783092158182, '_timestamp': 1762893268.599588}).
Epoch: 4, Steps: 132 | Train Loss: 0.2704203 Vali Loss: 0.1758711 Test Loss: 0.1595348
Validation loss decreased (0.180253 --> 0.175871).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2677015 Vali Loss: 0.1743991 Test Loss: 0.1554168
Validation loss decreased (0.175871 --> 0.174399).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2661801 Vali Loss: 0.1727404 Test Loss: 0.1564125
Validation loss decreased (0.174399 --> 0.172740).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2651126 Vali Loss: 0.1729298 Test Loss: 0.1562489
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2646839 Vali Loss: 0.1720319 Test Loss: 0.1565092
Validation loss decreased (0.172740 --> 0.172032).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2643182 Vali Loss: 0.1714848 Test Loss: 0.1568428
Validation loss decreased (0.172032 --> 0.171485).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2644131 Vali Loss: 0.1713350 Test Loss: 0.1567257
Validation loss decreased (0.171485 --> 0.171335).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2642808 Vali Loss: 0.1717452 Test Loss: 0.1567368
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2642507 Vali Loss: 0.1719560 Test Loss: 0.1567600
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2644283 Vali Loss: 0.1713786 Test Loss: 0.1567306
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2642029 Vali Loss: 0.1719334 Test Loss: 0.1567310
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2648756 Vali Loss: 0.1715282 Test Loss: 0.1567333
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2640873 Vali Loss: 0.1715930 Test Loss: 0.1567305
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2646866 Vali Loss: 0.1719027 Test Loss: 0.1567299
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2641691 Vali Loss: 0.1715078 Test Loss: 0.1567304
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2644090 Vali Loss: 0.1719854 Test Loss: 0.1567302
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2646059 Vali Loss: 0.1713723 Test Loss: 0.1567304
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_DRD_GOLD_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.0009848212357610464, mae:0.022458933293819427, rmse:0.03138186037540436, r2:-0.0341719388961792, dtw:Not calculated


VAL - MSE: 0.0010, MAE: 0.0225, RMSE: 0.0314, RÂ²: -0.0342, MAPE: 2050646.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.607 MB of 0.608 MB uploadedwandb: \ 0.608 MB of 0.608 MB uploadedwandb: | 0.608 MB of 0.608 MB uploadedwandb: / 0.608 MB of 0.813 MB uploadedwandb: - 0.813 MB of 0.813 MB uploadedwandb: \ 0.813 MB of 0.813 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.15673
wandb:                 train/loss 0.26461
wandb:   val/directional_accuracy 54.34688
wandb:                   val/loss 0.17137
wandb:                    val/mae 0.02246
wandb:                   val/mape 205064600.0
wandb:                    val/mse 0.00098
wandb:                     val/r2 -0.03417
wandb:                   val/rmse 0.03138
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/ictk1ln6
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_223346-ictk1ln6/logs
Completed: DRD_GOLD H=22

Training: Autoformer on DRD_GOLD for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_223856-nybr9li2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/nybr9li2
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_DRD_GOLD_H50Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/nybr9li2
>>>>>>>start training : long_term_forecast_Autoformer_DRD_GOLD_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.3776541 Vali Loss: 0.2316239 Test Loss: 0.1682702
Validation loss decreased (inf --> 0.231624).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.37765408860463084, 'val/loss': 0.23162385324637094, 'test/loss': 0.16827024271090826, '_timestamp': 1762893564.018364}).
Epoch: 2, Steps: 132 | Train Loss: 0.3334951 Vali Loss: 0.2205227 Test Loss: 0.1710035
Validation loss decreased (0.231624 --> 0.220523).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.3117130 Vali Loss: 0.2157671 Test Loss: 0.1854250
Validation loss decreased (0.220523 --> 0.215767).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.33349507889061264, 'val/loss': 0.22052266200383505, 'test/loss': 0.17100348075230917, '_timestamp': 1762893573.5998557}).
Epoch: 4, Steps: 132 | Train Loss: 0.2988086 Vali Loss: 0.2190545 Test Loss: 0.1895335
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2954039 Vali Loss: 0.2201181 Test Loss: 0.1869794
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2927833 Vali Loss: 0.2212686 Test Loss: 0.1894193
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2914853 Vali Loss: 0.2211881 Test Loss: 0.1886648
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2900809 Vali Loss: 0.2208456 Test Loss: 0.1876164
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2896797 Vali Loss: 0.2203197 Test Loss: 0.1871150
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2946504 Vali Loss: 0.2202594 Test Loss: 0.1870774
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2936470 Vali Loss: 0.2205289 Test Loss: 0.1875648
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2894180 Vali Loss: 0.2203519 Test Loss: 0.1874962
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2887802 Vali Loss: 0.2207718 Test Loss: 0.1875854
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_DRD_GOLD_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.0009687415440566838, mae:0.02218656614422798, rmse:0.03112461231648922, r2:-0.040334463119506836, dtw:Not calculated


VAL - MSE: 0.0010, MAE: 0.0222, RMSE: 0.0311, RÂ²: -0.0403, MAPE: 3077858.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.711 MB of 0.713 MB uploadedwandb: \ 0.711 MB of 0.713 MB uploadedwandb: | 0.711 MB of 0.713 MB uploadedwandb: / 0.713 MB of 0.713 MB uploadedwandb: - 0.713 MB of 0.713 MB uploadedwandb: \ 0.713 MB of 0.916 MB uploadedwandb: | 0.916 MB of 0.916 MB uploadedwandb: / 0.916 MB of 0.916 MB uploadedwandb: - 0.916 MB of 0.916 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–„â–ˆâ–‡â–…â–„â–„â–…â–…â–…
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–â–ƒâ–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–…â–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.18759
wandb:                 train/loss 0.28878
wandb:   val/directional_accuracy 54.77981
wandb:                   val/loss 0.22077
wandb:                    val/mae 0.02219
wandb:                   val/mape 307785850.0
wandb:                    val/mse 0.00097
wandb:                     val/r2 -0.04033
wandb:                   val/rmse 0.03112
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/nybr9li2
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_223856-nybr9li2/logs
Completed: DRD_GOLD H=50

Training: Autoformer on DRD_GOLD for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_224156-goe0dzut
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/goe0dzut
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_DRD_GOLD_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/goe0dzut
>>>>>>>start training : long_term_forecast_Autoformer_DRD_GOLD_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.4656748 Vali Loss: 0.2816244 Test Loss: 0.1832374
Validation loss decreased (inf --> 0.281624).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.4656748281075404, 'val/loss': 0.28162444829940797, 'test/loss': 0.1832374155521393, '_timestamp': 1762893741.0989916}).
Epoch: 2, Steps: 130 | Train Loss: 0.4334754 Vali Loss: 0.2520469 Test Loss: 0.1850724
Validation loss decreased (0.281624 --> 0.252047).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.4213911 Vali Loss: 0.2649061 Test Loss: 0.1906445
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.43347540956277114, 'val/loss': 0.2520469069480896, 'test/loss': 0.18507242500782012, '_timestamp': 1762893751.0377698}).
Epoch: 4, Steps: 130 | Train Loss: 0.4136620 Vali Loss: 0.2631065 Test Loss: 0.1884636
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.4093142 Vali Loss: 0.2595942 Test Loss: 0.1811378
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.4054800 Vali Loss: 0.2588691 Test Loss: 0.1858108
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.4037532 Vali Loss: 0.2605580 Test Loss: 0.1841151
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.4029430 Vali Loss: 0.2614531 Test Loss: 0.1846022
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.4026521 Vali Loss: 0.2547982 Test Loss: 0.1842036
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.4020845 Vali Loss: 0.2581156 Test Loss: 0.1844871
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.4022236 Vali Loss: 0.2591236 Test Loss: 0.1843852
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.4020910 Vali Loss: 0.2604032 Test Loss: 0.1842549
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_DRD_GOLD_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0009390420163981616, mae:0.021644767373800278, rmse:0.03064379282295704, r2:-0.03928530216217041, dtw:Not calculated


VAL - MSE: 0.0009, MAE: 0.0216, RMSE: 0.0306, RÂ²: -0.0393, MAPE: 1224440.38%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.782 MB of 0.787 MB uploadedwandb: \ 0.782 MB of 0.787 MB uploadedwandb: | 0.782 MB of 0.787 MB uploadedwandb: / 0.787 MB of 0.787 MB uploadedwandb: - 0.787 MB of 0.787 MB uploadedwandb: \ 0.787 MB of 1.001 MB uploadedwandb: | 1.001 MB of 1.001 MB uploadedwandb: / 1.001 MB of 1.001 MB uploadedwandb: - 1.001 MB of 1.001 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–†â–â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‡â–„â–„â–…â–†â–â–ƒâ–„â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.18425
wandb:                 train/loss 0.40209
wandb:   val/directional_accuracy 53.64358
wandb:                   val/loss 0.2604
wandb:                    val/mae 0.02164
wandb:                   val/mape 122444037.5
wandb:                    val/mse 0.00094
wandb:                     val/r2 -0.03929
wandb:                   val/rmse 0.03064
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/goe0dzut
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_224156-goe0dzut/logs
Completed: DRD_GOLD H=100

Training: Autoformer on ANGLO_AMERICAN for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_224439-8cb4nhqc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/8cb4nhqc
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ANGLO_AMERICAN_H3Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/8cb4nhqc
>>>>>>>start training : long_term_forecast_Autoformer_ANGLO_AMERICAN_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 3
============================================================
train 778
val 44
test 46
Epoch: 1, Steps: 25 | Train Loss: 0.4681385 Vali Loss: 0.6150561 Test Loss: 0.9068789
Validation loss decreased (inf --> 0.615056).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 25 | Train Loss: 0.3560646 Vali Loss: 0.6537049 Test Loss: 0.9248253
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 25 | Train Loss: 0.3160420 Vali Loss: 0.6567537 Test Loss: 0.9084240
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 25 | Train Loss: 0.3013970 Vali Loss: 0.6213720 Test Loss: 0.9279069
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.46813849449157713, 'val/loss': 0.6150561273097992, 'test/loss': 0.9068789184093475, '_timestamp': 1762893902.4994953}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3560646331310272, 'val/loss': 0.6537048518657684, 'test/loss': 0.9248252511024475, '_timestamp': 1762893909.1461587}).
Epoch: 5, Steps: 25 | Train Loss: 0.2942351 Vali Loss: 0.5974674 Test Loss: 0.9116931
Validation loss decreased (0.615056 --> 0.597467).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 25 | Train Loss: 0.2940030 Vali Loss: 0.5760679 Test Loss: 0.8965917
Validation loss decreased (0.597467 --> 0.576068).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 25 | Train Loss: 0.2872294 Vali Loss: 0.5942175 Test Loss: 0.9020552
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 25 | Train Loss: 0.2932172 Vali Loss: 0.6068251 Test Loss: 0.8801358
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 25 | Train Loss: 0.2926844 Vali Loss: 0.5662865 Test Loss: 0.8841152
Validation loss decreased (0.576068 --> 0.566287).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 25 | Train Loss: 0.2844537 Vali Loss: 0.5853445 Test Loss: 0.8822999
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 25 | Train Loss: 0.2928195 Vali Loss: 0.6222683 Test Loss: 0.8819529
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 25 | Train Loss: 0.2880368 Vali Loss: 0.5941890 Test Loss: 0.8818730
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 25 | Train Loss: 0.2907395 Vali Loss: 0.5493706 Test Loss: 0.8822569
Validation loss decreased (0.566287 --> 0.549371).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 25 | Train Loss: 0.2900807 Vali Loss: 0.5928221 Test Loss: 0.8822925
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 25 | Train Loss: 0.2965415 Vali Loss: 0.5841382 Test Loss: 0.8823022
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 25 | Train Loss: 0.2873634 Vali Loss: 0.6011688 Test Loss: 0.8823059
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 25 | Train Loss: 0.2926975 Vali Loss: 0.5446882 Test Loss: 0.8823116
Validation loss decreased (0.549371 --> 0.544688).  Saving model ...
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 25 | Train Loss: 0.2896109 Vali Loss: 0.6263651 Test Loss: 0.8823098
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 25 | Train Loss: 0.2891678 Vali Loss: 0.5928746 Test Loss: 0.8823117
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 25 | Train Loss: 0.2911820 Vali Loss: 0.5728599 Test Loss: 0.8823096
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 25 | Train Loss: 0.2937611 Vali Loss: 0.5682885 Test Loss: 0.8823078
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 25 | Train Loss: 0.2884781 Vali Loss: 0.5851145 Test Loss: 0.8823106
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 25 | Train Loss: 0.2879058 Vali Loss: 0.5764554 Test Loss: 0.8823100
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 25 | Train Loss: 0.2867849 Vali Loss: 0.5772081 Test Loss: 0.8823110
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 25 | Train Loss: 0.2844965 Vali Loss: 0.5825325 Test Loss: 0.8823114
EarlyStopping counter: 8 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 25 | Train Loss: 0.2913933 Vali Loss: 0.5666250 Test Loss: 0.8823104
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 25 | Train Loss: 0.2898064 Vali Loss: 0.6066402 Test Loss: 0.8823110
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ANGLO_AMERICAN_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 46
test shape: (46, 3, 1) (46, 3, 1)
test shape: (46, 3, 1) (46, 3, 1)


	mse:0.0071858810260891914, mae:0.05958414822816849, rmse:0.08476957678794861, r2:-0.06066405773162842, dtw:Not calculated


VAL - MSE: 0.0072, MAE: 0.0596, RMSE: 0.0848, RÂ²: -0.0607, MAPE: 52204936.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.434 MB of 0.434 MB uploadedwandb: \ 0.434 MB of 0.434 MB uploadedwandb: | 0.434 MB of 0.434 MB uploadedwandb: / 0.434 MB of 0.434 MB uploadedwandb: - 0.434 MB of 0.434 MB uploadedwandb: \ 0.434 MB of 0.434 MB uploadedwandb: | 0.434 MB of 0.434 MB uploadedwandb: / 0.434 MB of 0.434 MB uploadedwandb: - 0.434 MB of 0.434 MB uploadedwandb: \ 0.564 MB of 0.770 MB uploaded (0.002 MB deduped)wandb: | 0.770 MB of 0.770 MB uploaded (0.002 MB deduped)wandb: / 0.770 MB of 0.770 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–ˆâ–†â–ƒâ–„â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–â–ƒâ–‚â–‚â–‚â–„â–‚â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–â–ƒâ–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–„â–ƒâ–„â–…â–‚â–„â–†â–„â–â–„â–ƒâ–…â–â–†â–„â–ƒâ–‚â–„â–ƒâ–ƒâ–ƒâ–‚â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 26
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.88231
wandb:                 train/loss 0.28981
wandb:   val/directional_accuracy 41.30435
wandb:                   val/loss 0.60664
wandb:                    val/mae 0.05958
wandb:                   val/mape 5220493600.0
wandb:                    val/mse 0.00719
wandb:                     val/r2 -0.06066
wandb:                   val/rmse 0.08477
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/8cb4nhqc
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_224439-8cb4nhqc/logs
Completed: ANGLO_AMERICAN H=3

Training: Autoformer on ANGLO_AMERICAN for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_224730-6x350mbw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/6x350mbw
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ANGLO_AMERICAN_H5Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/6x350mbw
>>>>>>>start training : long_term_forecast_Autoformer_ANGLO_AMERICAN_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 5
============================================================
train 776
val 42
test 44
Epoch: 1, Steps: 25 | Train Loss: 0.4551733 Vali Loss: 0.5825976 Test Loss: 0.8836663
Validation loss decreased (inf --> 0.582598).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 25 | Train Loss: 0.3585338 Vali Loss: 0.6051321 Test Loss: 0.8716090
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 25 | Train Loss: 0.3222957 Vali Loss: 0.6328810 Test Loss: 0.9370223
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 25 | Train Loss: 0.3088642 Vali Loss: 0.6589003 Test Loss: 0.9655931
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 25 | Train Loss: 0.3024484 Vali Loss: 0.6636777 Test Loss: 0.9382237
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 25 | Train Loss: 0.3010845 Vali Loss: 0.6003533 Test Loss: 0.9386897
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.45517327189445494, 'val/loss': 0.582597553730011, 'test/loss': 0.8836663365364075, '_timestamp': 1762894073.5427985}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.35853378534317015, 'val/loss': 0.605132132768631, 'test/loss': 0.8716090321540833, '_timestamp': 1762894076.2798967}).
Epoch: 7, Steps: 25 | Train Loss: 0.2983263 Vali Loss: 0.5953590 Test Loss: 0.9451470
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 25 | Train Loss: 0.2974517 Vali Loss: 0.6512920 Test Loss: 0.9438314
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 25 | Train Loss: 0.2958259 Vali Loss: 0.6379369 Test Loss: 0.9431937
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 25 | Train Loss: 0.3045175 Vali Loss: 0.6616144 Test Loss: 0.9438749
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 25 | Train Loss: 0.2968657 Vali Loss: 0.6292605 Test Loss: 0.9436594
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ANGLO_AMERICAN_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 44
test shape: (44, 5, 1) (44, 5, 1)
test shape: (44, 5, 1) (44, 5, 1)


	mse:0.007467512972652912, mae:0.06085873767733574, rmse:0.08641476929187775, r2:-0.05893373489379883, dtw:Not calculated


VAL - MSE: 0.0075, MAE: 0.0609, RMSE: 0.0864, RÂ²: -0.0589, MAPE: 42547032.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.474 MB of 0.474 MB uploadedwandb: \ 0.474 MB of 0.474 MB uploadedwandb: | 0.474 MB of 0.474 MB uploadedwandb: / 0.474 MB of 0.474 MB uploadedwandb: - 0.474 MB of 0.474 MB uploadedwandb: \ 0.474 MB of 0.677 MB uploadedwandb: | 0.677 MB of 0.677 MB uploadedwandb: / 0.677 MB of 0.677 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–â–ƒâ–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–ˆâ–ˆâ–‚â–â–‡â–…â–ˆâ–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.94366
wandb:                 train/loss 0.29687
wandb:   val/directional_accuracy 47.72727
wandb:                   val/loss 0.62926
wandb:                    val/mae 0.06086
wandb:                   val/mape 4254703200.0
wandb:                    val/mse 0.00747
wandb:                     val/r2 -0.05893
wandb:                   val/rmse 0.08641
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/6x350mbw
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_224730-6x350mbw/logs
Completed: ANGLO_AMERICAN H=5

Training: Autoformer on ANGLO_AMERICAN for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_224913-wcq16v9q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/wcq16v9q
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ANGLO_AMERICAN_H10Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/wcq16v9q
>>>>>>>start training : long_term_forecast_Autoformer_ANGLO_AMERICAN_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 10
============================================================
train 771
val 37
test 39
Epoch: 1, Steps: 25 | Train Loss: 0.4638065 Vali Loss: 0.6813455 Test Loss: 0.9155214
Validation loss decreased (inf --> 0.681345).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 25 | Train Loss: 0.3723537 Vali Loss: 0.6381421 Test Loss: 0.9078754
Validation loss decreased (0.681345 --> 0.638142).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 25 | Train Loss: 0.3422785 Vali Loss: 0.6593092 Test Loss: 0.8752813
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 25 | Train Loss: 0.3328972 Vali Loss: 0.6848449 Test Loss: 0.9097456
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.4638065195083618, 'val/loss': 0.6813454627990723, 'test/loss': 0.9155214428901672, '_timestamp': 1762894179.5910387}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3723536825180054, 'val/loss': 0.6381421089172363, 'test/loss': 0.9078754484653473, '_timestamp': 1762894184.0025208}).
Epoch: 5, Steps: 25 | Train Loss: 0.3233373 Vali Loss: 0.6441216 Test Loss: 0.9129521
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 25 | Train Loss: 0.3141869 Vali Loss: 0.6741029 Test Loss: 0.9154869
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 25 | Train Loss: 0.3177746 Vali Loss: 0.6417396 Test Loss: 0.9201901
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 25 | Train Loss: 0.3086412 Vali Loss: 0.6894646 Test Loss: 0.9192154
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 25 | Train Loss: 0.3107362 Vali Loss: 0.6777322 Test Loss: 0.9209706
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 25 | Train Loss: 0.3208744 Vali Loss: 0.6511832 Test Loss: 0.9217297
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 25 | Train Loss: 0.3103448 Vali Loss: 0.6879447 Test Loss: 0.9216730
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 25 | Train Loss: 0.3128964 Vali Loss: 0.6820488 Test Loss: 0.9217350
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ANGLO_AMERICAN_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 39
test shape: (39, 10, 1) (39, 10, 1)
test shape: (39, 10, 1) (39, 10, 1)


	mse:0.008199394680559635, mae:0.06484968960285187, rmse:0.0905505120754242, r2:-0.05819058418273926, dtw:Not calculated


VAL - MSE: 0.0082, MAE: 0.0648, RMSE: 0.0906, RÂ²: -0.0582, MAPE: 37552040.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.510 MB of 0.510 MB uploadedwandb: \ 0.510 MB of 0.510 MB uploadedwandb: | 0.510 MB of 0.510 MB uploadedwandb: / 0.510 MB of 0.713 MB uploadedwandb: - 0.713 MB of 0.713 MB uploadedwandb: \ 0.713 MB of 0.713 MB uploadedwandb: | 0.713 MB of 0.713 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–†â–„â–‚â–ƒâ–â–â–„â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–‡â–â–†â–â–ˆâ–†â–‚â–ˆâ–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.92174
wandb:                 train/loss 0.3129
wandb:   val/directional_accuracy 29.05983
wandb:                   val/loss 0.68205
wandb:                    val/mae 0.06485
wandb:                   val/mape 3755204000.0
wandb:                    val/mse 0.0082
wandb:                     val/r2 -0.05819
wandb:                   val/rmse 0.09055
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/wcq16v9q
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_224913-wcq16v9q/logs
Completed: ANGLO_AMERICAN H=10

Training: Autoformer on ANGLO_AMERICAN for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_225057-v5jjykb8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/v5jjykb8
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ANGLO_AMERICAN_H22Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/v5jjykb8
>>>>>>>start training : long_term_forecast_Autoformer_ANGLO_AMERICAN_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 22
============================================================
train 759
val 25
test 27
Epoch: 1, Steps: 24 | Train Loss: 0.4784271 Vali Loss: 0.6809647 Test Loss: 1.2652354
Validation loss decreased (inf --> 0.680965).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 24 | Train Loss: 0.3820041 Vali Loss: 0.7283117 Test Loss: 1.2679377
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 24 | Train Loss: 0.3573556 Vali Loss: 0.7270364 Test Loss: 1.2504201
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 24 | Train Loss: 0.3442668 Vali Loss: 0.7468009 Test Loss: 1.2349076
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.47842705249786377, 'val/loss': 0.6809647083282471, 'test/loss': 1.265235424041748, '_timestamp': 1762894283.0075397}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3820040784776211, 'val/loss': 0.7283117175102234, 'test/loss': 1.2679376602172852, '_timestamp': 1762894288.6710014}).
Epoch: 5, Steps: 24 | Train Loss: 0.3399854 Vali Loss: 0.7597584 Test Loss: 1.2529881
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 24 | Train Loss: 0.3333768 Vali Loss: 0.7538469 Test Loss: 1.2486558
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 24 | Train Loss: 0.3307394 Vali Loss: 0.7578374 Test Loss: 1.2461001
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 24 | Train Loss: 0.3310477 Vali Loss: 0.7617484 Test Loss: 1.2443826
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 24 | Train Loss: 0.3306490 Vali Loss: 0.7626901 Test Loss: 1.2435795
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 24 | Train Loss: 0.3308685 Vali Loss: 0.7631697 Test Loss: 1.2461748
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 24 | Train Loss: 0.3293986 Vali Loss: 0.7632490 Test Loss: 1.2447302
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ANGLO_AMERICAN_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 27
test shape: (27, 22, 1) (27, 22, 1)
test shape: (27, 22, 1) (27, 22, 1)


	mse:0.008245430886745453, mae:0.06540393829345703, rmse:0.09080435335636139, r2:-0.05022001266479492, dtw:Not calculated


VAL - MSE: 0.0082, MAE: 0.0654, RMSE: 0.0908, RÂ²: -0.0502, MAPE: 20624402.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.490 MB of 0.491 MB uploadedwandb: \ 0.490 MB of 0.491 MB uploadedwandb: | 0.491 MB of 0.491 MB uploadedwandb: / 0.491 MB of 0.491 MB uploadedwandb: - 0.491 MB of 0.491 MB uploadedwandb: \ 0.491 MB of 0.695 MB uploadedwandb: | 0.695 MB of 0.695 MB uploadedwandb: / 0.695 MB of 0.695 MB uploadedwandb: - 0.695 MB of 0.695 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‡â–â–ˆâ–†â–…â–…â–„â–…â–…
wandb:                 train/loss â–ˆâ–…â–„â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–…â–‡â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 1.24473
wandb:                 train/loss 0.3294
wandb:   val/directional_accuracy 42.50441
wandb:                   val/loss 0.76325
wandb:                    val/mae 0.0654
wandb:                   val/mape 2062440200.0
wandb:                    val/mse 0.00825
wandb:                     val/r2 -0.05022
wandb:                   val/rmse 0.0908
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/v5jjykb8
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_225057-v5jjykb8/logs
Completed: ANGLO_AMERICAN H=22

Training: Autoformer on ANGLO_AMERICAN for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_225234-370xlyk3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/370xlyk3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ANGLO_AMERICAN_H50Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/370xlyk3
>>>>>>>start training : long_term_forecast_Autoformer_ANGLO_AMERICAN_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 50
============================================================
train 731
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.005 MB of 0.024 MB uploadedwandb: / 0.024 MB of 0.024 MB uploadedwandb: ðŸš€ View run Autoformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/370xlyk3
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_225234-370xlyk3/logs
Traceback (most recent call last):
  File "run.py", line 207, in <module>
    exp.train(setting)
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 93, in train
    vali_data, vali_loader = self._get_data(flag='val')
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 43, in _get_data
    data_set, data_loader = data_provider(self.args, flag)
  File "/home-mscluster/bkodze/research_project/forecast-research/data_provider/data_factory.py", line 79, in data_provider
    print(flag, len(data_set))
ValueError: __len__() should return >= 0
Completed: ANGLO_AMERICAN H=50

Training: Autoformer on ANGLO_AMERICAN for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_225307-2l308feb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/2l308feb
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ANGLO_AMERICAN_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/2l308feb
>>>>>>>start training : long_term_forecast_Autoformer_ANGLO_AMERICAN_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 100
============================================================
train 681
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.005 MB of 0.024 MB uploadedwandb: / 0.024 MB of 0.024 MB uploadedwandb: ðŸš€ View run Autoformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/2l308feb
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_225307-2l308feb/logs
Traceback (most recent call last):
  File "run.py", line 207, in <module>
    exp.train(setting)
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 93, in train
    vali_data, vali_loader = self._get_data(flag='val')
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 43, in _get_data
    data_set, data_loader = data_provider(self.args, flag)
  File "/home-mscluster/bkodze/research_project/forecast-research/data_provider/data_factory.py", line 79, in data_provider
    print(flag, len(data_set))
ValueError: __len__() should return >= 0
Completed: ANGLO_AMERICAN H=100

Autoformer training completed for all datasets!
