##############################################################################
# Training Mamba Model on All Datasets
##############################################################################
Training: Mamba on NVIDIA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_091419-ig95ylfx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/ig95ylfx
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H3     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/ig95ylfx
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.2398016 Vali Loss: 0.1788270 Test Loss: 0.3172470
Validation loss decreased (inf --> 0.178827).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2340902 Vali Loss: 0.1717735 Test Loss: 0.2828078
Validation loss decreased (0.178827 --> 0.171773).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2290828 Vali Loss: 0.1712544 Test Loss: 0.2625151
Validation loss decreased (0.171773 --> 0.171254).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2264387 Vali Loss: 0.1759266 Test Loss: 0.2518765
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2253185 Vali Loss: 0.1716171 Test Loss: 0.2501391
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2247877 Vali Loss: 0.1717962 Test Loss: 0.2484813
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2251847 Vali Loss: 0.1693215 Test Loss: 0.2476090
Validation loss decreased (0.171254 --> 0.169321).  Saving model ...
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23980155475157544, 'val/loss': 0.17882704362273216, 'test/loss': 0.31724696327000856, '_timestamp': 1762326869.062712}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2340902469884184, 'val/loss': 0.17177345510572195, 'test/loss': 0.2828078009188175, '_timestamp': 1762326871.0314407}).
Epoch: 8, Steps: 133 | Train Loss: 0.2247420 Vali Loss: 0.1769274 Test Loss: 0.2472675
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2251137 Vali Loss: 0.1815853 Test Loss: 0.2472048
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2247136 Vali Loss: 0.1705476 Test Loss: 0.2471542
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2252591 Vali Loss: 0.1676287 Test Loss: 0.2471129
Validation loss decreased (0.169321 --> 0.167629).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2247293 Vali Loss: 0.1677361 Test Loss: 0.2471004
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2250578 Vali Loss: 0.1710783 Test Loss: 0.2470911
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2246744 Vali Loss: 0.1748466 Test Loss: 0.2470886
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2249781 Vali Loss: 0.1721212 Test Loss: 0.2470872
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2251096 Vali Loss: 0.1717082 Test Loss: 0.2470870
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2249535 Vali Loss: 0.1699347 Test Loss: 0.2470868
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2246095 Vali Loss: 0.1766164 Test Loss: 0.2470868
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2249725 Vali Loss: 0.1710211 Test Loss: 0.2470868
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2249478 Vali Loss: 0.1887673 Test Loss: 0.2470868
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2251654 Vali Loss: 0.1919309 Test Loss: 0.2470868
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.0011408244026824832, mae:0.02526126615703106, rmse:0.033776093274354935, r2:-0.01697099208831787, dtw:Not calculated


VAL - MSE: 0.0011, MAE: 0.0253, RMSE: 0.0338, RÂ²: -0.0170, MAPE: 505264.03%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.469 MB of 0.469 MB uploadedwandb: \ 0.469 MB of 0.469 MB uploadedwandb: | 0.469 MB of 0.469 MB uploadedwandb: / 0.469 MB of 0.469 MB uploadedwandb: - 0.469 MB of 0.469 MB uploadedwandb: \ 0.469 MB of 0.469 MB uploadedwandb: | 0.469 MB of 0.469 MB uploadedwandb: / 0.518 MB of 0.590 MB uploadedwandb: - 0.590 MB of 0.590 MB uploadedwandb: \ 0.590 MB of 0.590 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–‚â–â–‚â–â–‚â–â–‚â–â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–ƒâ–‚â–‚â–â–„â–…â–‚â–â–â–‚â–ƒâ–‚â–‚â–‚â–„â–‚â–‡â–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35218
wandb:     model/trainable_params 35218
wandb:                  test/loss 0.24709
wandb:                 train/loss 0.22517
wandb:   val/directional_accuracy 48.94515
wandb:                   val/loss 0.19193
wandb:                    val/mae 0.02526
wandb:                   val/mape 50526403.125
wandb:                    val/mse 0.00114
wandb:                     val/r2 -0.01697
wandb:                   val/rmse 0.03378
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/ig95ylfx
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_091419-ig95ylfx/logs
Completed: NVIDIA H=3

Training: Mamba on NVIDIA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_091539-lpzt6194
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/lpzt6194
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H5     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/lpzt6194
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.2424326 Vali Loss: 0.1815101 Test Loss: 0.3337510
Validation loss decreased (inf --> 0.181510).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2366367 Vali Loss: 0.1743817 Test Loss: 0.3049187
Validation loss decreased (0.181510 --> 0.174382).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2347172 Vali Loss: 0.1725042 Test Loss: 0.2926634
Validation loss decreased (0.174382 --> 0.172504).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2320447 Vali Loss: 0.1878856 Test Loss: 0.2857704
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2318516 Vali Loss: 0.1769637 Test Loss: 0.2840420
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2312570 Vali Loss: 0.1784714 Test Loss: 0.2823206
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2313774 Vali Loss: 0.1706789 Test Loss: 0.2816602
Validation loss decreased (0.172504 --> 0.170679).  Saving model ...
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.24243259172242387, 'val/loss': 0.18151008803397417, 'test/loss': 0.33375100046396255, '_timestamp': 1762326948.8577764}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2366366553351395, 'val/loss': 0.1743817124515772, 'test/loss': 0.30491867382079363, '_timestamp': 1762326950.9897697}).
Epoch: 8, Steps: 133 | Train Loss: 0.2302592 Vali Loss: 0.1780427 Test Loss: 0.2811970
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2304898 Vali Loss: 0.1744161 Test Loss: 0.2810155
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2304819 Vali Loss: 0.1722462 Test Loss: 0.2809382
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2300859 Vali Loss: 0.1941548 Test Loss: 0.2808755
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2305348 Vali Loss: 0.1756295 Test Loss: 0.2808554
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2305901 Vali Loss: 0.1960458 Test Loss: 0.2808451
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2304982 Vali Loss: 0.1722645 Test Loss: 0.2808399
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2306097 Vali Loss: 0.1938578 Test Loss: 0.2808385
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2304665 Vali Loss: 0.1859047 Test Loss: 0.2808379
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2308348 Vali Loss: 0.1710094 Test Loss: 0.2808376
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.0011469967430457473, mae:0.025410864502191544, rmse:0.03386734053492546, r2:-0.015868306159973145, dtw:Not calculated


VAL - MSE: 0.0011, MAE: 0.0254, RMSE: 0.0339, RÂ²: -0.0159, MAPE: 603449.81%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.532 MB of 0.532 MB uploadedwandb: \ 0.532 MB of 0.532 MB uploadedwandb: | 0.532 MB of 0.532 MB uploadedwandb: / 0.532 MB of 0.604 MB uploadedwandb: - 0.532 MB of 0.604 MB uploadedwandb: \ 0.604 MB of 0.604 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–„â–ƒâ–ƒâ–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–†â–ƒâ–ƒâ–â–ƒâ–‚â–â–‡â–‚â–ˆâ–â–‡â–…â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35998
wandb:     model/trainable_params 35998
wandb:                  test/loss 0.28084
wandb:                 train/loss 0.23083
wandb:   val/directional_accuracy 50.31915
wandb:                   val/loss 0.17101
wandb:                    val/mae 0.02541
wandb:                   val/mape 60344981.25
wandb:                    val/mse 0.00115
wandb:                     val/r2 -0.01587
wandb:                   val/rmse 0.03387
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/lpzt6194
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_091539-lpzt6194/logs
Completed: NVIDIA H=5

Training: Mamba on NVIDIA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_091648-167vmz0v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/167vmz0v
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H10    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/167vmz0v
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2457423 Vali Loss: 0.2138988 Test Loss: 0.3566367
Validation loss decreased (inf --> 0.213899).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2416601 Vali Loss: 0.1772527 Test Loss: 0.3360414
Validation loss decreased (0.213899 --> 0.177253).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2391447 Vali Loss: 0.1828327 Test Loss: 0.3260374
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2384738 Vali Loss: 0.1785791 Test Loss: 0.3218473
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2386500 Vali Loss: 0.1921391 Test Loss: 0.3196857
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2381857 Vali Loss: 0.1808511 Test Loss: 0.3187064
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2371379 Vali Loss: 0.1813596 Test Loss: 0.3182844
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.245742300399264, 'val/loss': 0.2138987835496664, 'test/loss': 0.35663665644824505, '_timestamp': 1762327016.9217236}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24166012034380346, 'val/loss': 0.17725269217044115, 'test/loss': 0.33604142256081104, '_timestamp': 1762327018.9150772}).
Epoch: 8, Steps: 133 | Train Loss: 0.2375710 Vali Loss: 0.1792200 Test Loss: 0.3180966
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2379339 Vali Loss: 0.1772255 Test Loss: 0.3179489
Validation loss decreased (0.177253 --> 0.177226).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2368558 Vali Loss: 0.1753297 Test Loss: 0.3178935
Validation loss decreased (0.177226 --> 0.175330).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2374540 Vali Loss: 0.1730128 Test Loss: 0.3178691
Validation loss decreased (0.175330 --> 0.173013).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2382330 Vali Loss: 0.2002591 Test Loss: 0.3178570
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2369997 Vali Loss: 0.1940116 Test Loss: 0.3178498
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2369992 Vali Loss: 0.1938522 Test Loss: 0.3178466
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2370086 Vali Loss: 0.1771567 Test Loss: 0.3178446
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2373629 Vali Loss: 0.1752111 Test Loss: 0.3178442
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2373955 Vali Loss: 0.1733002 Test Loss: 0.3178440
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2373358 Vali Loss: 0.1965321 Test Loss: 0.3178440
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2372083 Vali Loss: 0.1978634 Test Loss: 0.3178440
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2371697 Vali Loss: 0.1805365 Test Loss: 0.3178440
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2375547 Vali Loss: 0.1826738 Test Loss: 0.3178440
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.0011708087986335158, mae:0.025738045573234558, rmse:0.03421708196401596, r2:-0.020646095275878906, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0257, RMSE: 0.0342, RÂ²: -0.0206, MAPE: 555485.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.563 MB of 0.563 MB uploadedwandb: \ 0.563 MB of 0.563 MB uploadedwandb: | 0.563 MB of 0.563 MB uploadedwandb: / 0.563 MB of 0.563 MB uploadedwandb: - 0.563 MB of 0.563 MB uploadedwandb: \ 0.563 MB of 0.563 MB uploadedwandb: | 0.563 MB of 0.636 MB uploadedwandb: / 0.636 MB of 0.636 MB uploadedwandb: - 0.636 MB of 0.636 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–†â–†â–…â–‚â–ƒâ–„â–â–ƒâ–…â–â–â–â–ƒâ–ƒâ–‚â–‚â–‚â–ƒ
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–‚â–†â–ƒâ–ƒâ–ƒâ–‚â–‚â–â–ˆâ–†â–†â–‚â–‚â–â–‡â–‡â–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 37948
wandb:     model/trainable_params 37948
wandb:                  test/loss 0.31784
wandb:                 train/loss 0.23755
wandb:   val/directional_accuracy 49.85507
wandb:                   val/loss 0.18267
wandb:                    val/mae 0.02574
wandb:                   val/mape 55548550.0
wandb:                    val/mse 0.00117
wandb:                     val/r2 -0.02065
wandb:                   val/rmse 0.03422
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/167vmz0v
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_091648-167vmz0v/logs
Completed: NVIDIA H=10

Training: Mamba on NVIDIA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_091805-q03l1trf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/q03l1trf
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H22    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/q03l1trf
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2501167 Vali Loss: 0.1871742 Test Loss: 0.4230973
Validation loss decreased (inf --> 0.187174).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2471064 Vali Loss: 0.1857132 Test Loss: 0.3981702
Validation loss decreased (0.187174 --> 0.185713).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2452490 Vali Loss: 0.1856090 Test Loss: 0.3879586
Validation loss decreased (0.185713 --> 0.185609).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2441747 Vali Loss: 0.1848800 Test Loss: 0.3816607
Validation loss decreased (0.185609 --> 0.184880).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2437045 Vali Loss: 0.1826753 Test Loss: 0.3793149
Validation loss decreased (0.184880 --> 0.182675).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2435026 Vali Loss: 0.1845022 Test Loss: 0.3779257
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2435205 Vali Loss: 0.1839220 Test Loss: 0.3774702
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2501167230533831, 'val/loss': 0.18717418823923385, 'test/loss': 0.4230973379952567, '_timestamp': 1762327092.6347938}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.247106359199141, 'val/loss': 0.1857132124049323, 'test/loss': 0.39817018806934357, '_timestamp': 1762327094.595258}).
Epoch: 8, Steps: 132 | Train Loss: 0.2433110 Vali Loss: 0.1841608 Test Loss: 0.3772473
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2433337 Vali Loss: 0.1841746 Test Loss: 0.3771422
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2434458 Vali Loss: 0.1848292 Test Loss: 0.3770866
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2433054 Vali Loss: 0.1837328 Test Loss: 0.3770610
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2432216 Vali Loss: 0.1833400 Test Loss: 0.3770456
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2431975 Vali Loss: 0.1830692 Test Loss: 0.3770384
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2432275 Vali Loss: 0.1840516 Test Loss: 0.3770351
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2432920 Vali Loss: 0.1846509 Test Loss: 0.3770336
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.0011992166982963681, mae:0.025938289240002632, rmse:0.034629710018634796, r2:-0.016409754753112793, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0259, RMSE: 0.0346, RÂ²: -0.0164, MAPE: 547151.19%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.567 MB of 0.569 MB uploadedwandb: \ 0.569 MB of 0.569 MB uploadedwandb: | 0.569 MB of 0.569 MB uploadedwandb: / 0.569 MB of 0.641 MB uploadedwandb: - 0.641 MB of 0.641 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–â–…â–„â–…â–…â–†â–„â–ƒâ–‚â–„â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 42628
wandb:     model/trainable_params 42628
wandb:                  test/loss 0.37703
wandb:                 train/loss 0.24329
wandb:   val/directional_accuracy 49.56313
wandb:                   val/loss 0.18465
wandb:                    val/mae 0.02594
wandb:                   val/mape 54715118.75
wandb:                    val/mse 0.0012
wandb:                     val/r2 -0.01641
wandb:                   val/rmse 0.03463
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/q03l1trf
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_091805-q03l1trf/logs
Completed: NVIDIA H=22

Training: Mamba on NVIDIA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_091904-ktinfwts
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/ktinfwts
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H50    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/ktinfwts
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.2609774 Vali Loss: 0.2055285 Test Loss: 0.5615093
Validation loss decreased (inf --> 0.205529).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2583421 Vali Loss: 0.2029993 Test Loss: 0.5265764
Validation loss decreased (0.205529 --> 0.202999).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2557309 Vali Loss: 0.2005865 Test Loss: 0.5118629
Validation loss decreased (0.202999 --> 0.200587).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2559916 Vali Loss: 0.1995617 Test Loss: 0.5072306
Validation loss decreased (0.200587 --> 0.199562).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2555839 Vali Loss: 0.1991070 Test Loss: 0.5048560
Validation loss decreased (0.199562 --> 0.199107).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2535598 Vali Loss: 0.1983880 Test Loss: 0.5033350
Validation loss decreased (0.199107 --> 0.198388).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2546222 Vali Loss: 0.1982716 Test Loss: 0.5027114
Validation loss decreased (0.198388 --> 0.198272).  Saving model ...
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.26097744189654337, 'val/loss': 0.20552852749824524, 'test/loss': 0.5615093012650808, '_timestamp': 1762327151.0154796}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2583421197804538, 'val/loss': 0.20299925158421198, 'test/loss': 0.526576429605484, '_timestamp': 1762327153.0224717}).
Epoch: 8, Steps: 132 | Train Loss: 0.2536155 Vali Loss: 0.1981789 Test Loss: 0.5023946
Validation loss decreased (0.198272 --> 0.198179).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2538505 Vali Loss: 0.1982814 Test Loss: 0.5023131
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2565145 Vali Loss: 0.1983837 Test Loss: 0.5022464
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2584966 Vali Loss: 0.1981944 Test Loss: 0.5022154
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2544651 Vali Loss: 0.1982273 Test Loss: 0.5021967
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2542696 Vali Loss: 0.1982298 Test Loss: 0.5021892
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2542662 Vali Loss: 0.1982615 Test Loss: 0.5021883
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2570956 Vali Loss: 0.1981046 Test Loss: 0.5021867
Validation loss decreased (0.198179 --> 0.198105).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2585314 Vali Loss: 0.1983968 Test Loss: 0.5021861
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2557014 Vali Loss: 0.1982756 Test Loss: 0.5021859
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2541440 Vali Loss: 0.1985174 Test Loss: 0.5021858
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2539374 Vali Loss: 0.1982229 Test Loss: 0.5021858
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2538533 Vali Loss: 0.1984642 Test Loss: 0.5021858
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2555684 Vali Loss: 0.1985563 Test Loss: 0.5021858
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2537539 Vali Loss: 0.1985420 Test Loss: 0.5021858
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.2533504 Vali Loss: 0.1980295 Test Loss: 0.5021858
Validation loss decreased (0.198105 --> 0.198030).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 132 | Train Loss: 0.2548678 Vali Loss: 0.1982521 Test Loss: 0.5021858
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 132 | Train Loss: 0.2541494 Vali Loss: 0.1984278 Test Loss: 0.5021858
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 132 | Train Loss: 0.2536177 Vali Loss: 0.1982958 Test Loss: 0.5021858
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 132 | Train Loss: 0.2537178 Vali Loss: 0.1988859 Test Loss: 0.5021858
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 132 | Train Loss: 0.2539799 Vali Loss: 0.1979333 Test Loss: 0.5021858
Validation loss decreased (0.198030 --> 0.197933).  Saving model ...
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 132 | Train Loss: 0.2559412 Vali Loss: 0.1983606 Test Loss: 0.5021858
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 132 | Train Loss: 0.2534573 Vali Loss: 0.1981071 Test Loss: 0.5021858
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 132 | Train Loss: 0.2553585 Vali Loss: 0.1983380 Test Loss: 0.5021858
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 132 | Train Loss: 0.2545096 Vali Loss: 0.1983388 Test Loss: 0.5021858
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 132 | Train Loss: 0.2544068 Vali Loss: 0.1981609 Test Loss: 0.5021858
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 132 | Train Loss: 0.2558657 Vali Loss: 0.1981020 Test Loss: 0.5021858
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.1641532182693482e-14
Epoch: 35, Steps: 132 | Train Loss: 0.2540222 Vali Loss: 0.1983757 Test Loss: 0.5021858
EarlyStopping counter: 7 out of 10
Updating learning rate to 5.820766091346741e-15
Epoch: 36, Steps: 132 | Train Loss: 0.2573964 Vali Loss: 0.1981125 Test Loss: 0.5021858
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.9103830456733705e-15
Epoch: 37, Steps: 132 | Train Loss: 0.2546385 Vali Loss: 0.1984919 Test Loss: 0.5021858
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.4551915228366853e-15
Epoch: 38, Steps: 132 | Train Loss: 0.2554921 Vali Loss: 0.1982513 Test Loss: 0.5021858
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.0012001310242339969, mae:0.02625271864235401, rmse:0.03464290872216225, r2:-0.008520245552062988, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0263, RMSE: 0.0346, RÂ²: -0.0085, MAPE: 418686.72%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.631 MB of 0.634 MB uploadedwandb: \ 0.631 MB of 0.634 MB uploadedwandb: | 0.631 MB of 0.634 MB uploadedwandb: / 0.634 MB of 0.634 MB uploadedwandb: - 0.634 MB of 0.710 MB uploadedwandb: \ 0.710 MB of 0.710 MB uploadedwandb: | 0.710 MB of 0.710 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–„â–…â–„â–â–ƒâ–â–‚â–…â–ˆâ–ƒâ–‚â–‚â–†â–ˆâ–„â–‚â–‚â–‚â–„â–‚â–â–ƒâ–‚â–â–â–‚â–…â–â–„â–ƒâ–‚â–„â–‚â–†â–ƒâ–„
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–â–‚â–‚â–‚â–„â–â–‚â–â–‚â–‚â–‚â–â–‚â–â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 37
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 53548
wandb:     model/trainable_params 53548
wandb:                  test/loss 0.50219
wandb:                 train/loss 0.25549
wandb:   val/directional_accuracy 49.69925
wandb:                   val/loss 0.19825
wandb:                    val/mae 0.02625
wandb:                   val/mape 41868671.875
wandb:                    val/mse 0.0012
wandb:                     val/r2 -0.00852
wandb:                   val/rmse 0.03464
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/ktinfwts
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_091904-ktinfwts/logs
Completed: NVIDIA H=50

Training: Mamba on NVIDIA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_092055-fe9pkzuq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/fe9pkzuq
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H100   Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/fe9pkzuq
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.2819586 Vali Loss: 0.2433210 Test Loss: 0.8356802
Validation loss decreased (inf --> 0.243321).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2791050 Vali Loss: 0.2421639 Test Loss: 0.7755031
Validation loss decreased (0.243321 --> 0.242164).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.2756288 Vali Loss: 0.2417649 Test Loss: 0.7484781
Validation loss decreased (0.242164 --> 0.241765).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2740870 Vali Loss: 0.2334515 Test Loss: 0.7389239
Validation loss decreased (0.241765 --> 0.233451).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2734116 Vali Loss: 0.2301229 Test Loss: 0.7343592
Validation loss decreased (0.233451 --> 0.230123).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2733760 Vali Loss: 0.2247877 Test Loss: 0.7322787
Validation loss decreased (0.230123 --> 0.224788).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2729584 Vali Loss: 0.2307942 Test Loss: 0.7311626
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.28195859789848327, 'val/loss': 0.24332099854946138, 'test/loss': 0.835680240392685, '_timestamp': 1762327263.413265}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2791050178500322, 'val/loss': 0.24216385781764985, 'test/loss': 0.7755030870437623, '_timestamp': 1762327265.384255}).
Epoch: 8, Steps: 130 | Train Loss: 0.2731243 Vali Loss: 0.2357922 Test Loss: 0.7308600
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2729324 Vali Loss: 0.2318946 Test Loss: 0.7306715
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2726480 Vali Loss: 0.2234940 Test Loss: 0.7305548
Validation loss decreased (0.224788 --> 0.223494).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2729899 Vali Loss: 0.2295666 Test Loss: 0.7305134
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2726425 Vali Loss: 0.2302296 Test Loss: 0.7304782
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2727210 Vali Loss: 0.2316004 Test Loss: 0.7304683
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.2728953 Vali Loss: 0.2299142 Test Loss: 0.7304628
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.2729657 Vali Loss: 0.2280555 Test Loss: 0.7304601
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 130 | Train Loss: 0.2728701 Vali Loss: 0.2294913 Test Loss: 0.7304589
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 130 | Train Loss: 0.2730381 Vali Loss: 0.2306316 Test Loss: 0.7304587
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 130 | Train Loss: 0.2728308 Vali Loss: 0.2310511 Test Loss: 0.7304586
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 130 | Train Loss: 0.2730787 Vali Loss: 0.2335847 Test Loss: 0.7304586
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 130 | Train Loss: 0.2727354 Vali Loss: 0.2344110 Test Loss: 0.7304586
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0012996223522350192, mae:0.0275521669536829, rmse:0.03605027496814728, r2:-0.00948178768157959, dtw:Not calculated


VAL - MSE: 0.0013, MAE: 0.0276, RMSE: 0.0361, RÂ²: -0.0095, MAPE: 261134.05%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.709 MB of 0.714 MB uploadedwandb: \ 0.709 MB of 0.714 MB uploadedwandb: | 0.714 MB of 0.714 MB uploadedwandb: / 0.714 MB of 0.714 MB uploadedwandb: - 0.714 MB of 0.787 MB uploadedwandb: \ 0.787 MB of 0.787 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–â–â–‚â–‚â–‚â–‚â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–„â–â–„â–†â–„â–â–ƒâ–„â–„â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 73048
wandb:     model/trainable_params 73048
wandb:                  test/loss 0.73046
wandb:                 train/loss 0.27274
wandb:   val/directional_accuracy 50.03608
wandb:                   val/loss 0.23441
wandb:                    val/mae 0.02755
wandb:                   val/mape 26113404.6875
wandb:                    val/mse 0.0013
wandb:                     val/r2 -0.00948
wandb:                   val/rmse 0.03605
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/fe9pkzuq
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_092055-fe9pkzuq/logs
Completed: NVIDIA H=100

Training: Mamba on APPLE for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_092205-1yyu7fg4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/1yyu7fg4
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H3      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/1yyu7fg4
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.2306321 Vali Loss: 0.0865742 Test Loss: 0.1276309
Validation loss decreased (inf --> 0.086574).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2244123 Vali Loss: 0.0848729 Test Loss: 0.1264926
Validation loss decreased (0.086574 --> 0.084873).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2178714 Vali Loss: 0.0832271 Test Loss: 0.1243467
Validation loss decreased (0.084873 --> 0.083227).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2159199 Vali Loss: 0.0825583 Test Loss: 0.1244670
Validation loss decreased (0.083227 --> 0.082558).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2150647 Vali Loss: 0.0856008 Test Loss: 0.1245723
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2139222 Vali Loss: 0.0845905 Test Loss: 0.1245845
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2136695 Vali Loss: 0.0834685 Test Loss: 0.1245466
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23063214918724576, 'val/loss': 0.08657418796792626, 'test/loss': 0.12763087870553136, '_timestamp': 1762327333.3154347}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.22441227913350986, 'val/loss': 0.08487292006611824, 'test/loss': 0.12649256037548184, '_timestamp': 1762327335.3307338}).
Epoch: 8, Steps: 133 | Train Loss: 0.2138680 Vali Loss: 0.0825934 Test Loss: 0.1245345
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2140596 Vali Loss: 0.0825582 Test Loss: 0.1245307
Validation loss decreased (0.082558 --> 0.082558).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2141282 Vali Loss: 0.0882854 Test Loss: 0.1245323
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2146020 Vali Loss: 0.0861095 Test Loss: 0.1245324
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2141011 Vali Loss: 0.0834767 Test Loss: 0.1245329
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2137223 Vali Loss: 0.0849476 Test Loss: 0.1245330
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2135679 Vali Loss: 0.0854076 Test Loss: 0.1245325
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2138018 Vali Loss: 0.0828035 Test Loss: 0.1245325
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2141291 Vali Loss: 0.0855946 Test Loss: 0.1245324
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2142324 Vali Loss: 0.0860878 Test Loss: 0.1245324
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2141182 Vali Loss: 0.0866261 Test Loss: 0.1245324
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2141335 Vali Loss: 0.0845877 Test Loss: 0.1245324
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.00020185259927529842, mae:0.010253838263452053, rmse:0.01420748420059681, r2:-0.009552001953125, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0103, RMSE: 0.0142, RÂ²: -0.0096, MAPE: 416669.56%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.457 MB of 0.457 MB uploadedwandb: \ 0.457 MB of 0.457 MB uploadedwandb: | 0.457 MB of 0.457 MB uploadedwandb: / 0.457 MB of 0.457 MB uploadedwandb: - 0.457 MB of 0.457 MB uploadedwandb: \ 0.457 MB of 0.457 MB uploadedwandb: | 0.457 MB of 0.457 MB uploadedwandb: / 0.457 MB of 0.457 MB uploadedwandb: - 0.506 MB of 0.578 MB uploadedwandb: \ 0.578 MB of 0.578 MB uploadedwandb: | 0.578 MB of 0.578 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–…â–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–â–‚â–‚â–ƒâ–‚â–â–â–â–‚â–‚â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–â–…â–ƒâ–‚â–â–â–ˆâ–…â–‚â–„â–„â–â–…â–…â–†â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 18
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35218
wandb:     model/trainable_params 35218
wandb:                  test/loss 0.12453
wandb:                 train/loss 0.21413
wandb:   val/directional_accuracy 45.99156
wandb:                   val/loss 0.08459
wandb:                    val/mae 0.01025
wandb:                   val/mape 41666956.25
wandb:                    val/mse 0.0002
wandb:                     val/r2 -0.00955
wandb:                   val/rmse 0.01421
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/1yyu7fg4
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_092205-1yyu7fg4/logs
Completed: APPLE H=3

Training: Mamba on APPLE for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_092319-uvare1rg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/uvare1rg
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H5      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/uvare1rg
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.2332394 Vali Loss: 0.0876458 Test Loss: 0.1290435
Validation loss decreased (inf --> 0.087646).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2262634 Vali Loss: 0.0895127 Test Loss: 0.1295921
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2232746 Vali Loss: 0.0881311 Test Loss: 0.1281856
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2201150 Vali Loss: 0.0859996 Test Loss: 0.1282271
Validation loss decreased (0.087646 --> 0.086000).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2193164 Vali Loss: 0.0850845 Test Loss: 0.1280628
Validation loss decreased (0.086000 --> 0.085084).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2184932 Vali Loss: 0.0868933 Test Loss: 0.1280244
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2184791 Vali Loss: 0.0856328 Test Loss: 0.1279776
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23323937645532133, 'val/loss': 0.08764575235545635, 'test/loss': 0.12904348596930504, '_timestamp': 1762327406.7025733}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2262634284290156, 'val/loss': 0.08951267413794994, 'test/loss': 0.12959213368594646, '_timestamp': 1762327408.755639}).
Epoch: 8, Steps: 133 | Train Loss: 0.2183375 Vali Loss: 0.0879256 Test Loss: 0.1279697
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2184246 Vali Loss: 0.0836315 Test Loss: 0.1279564
Validation loss decreased (0.085084 --> 0.083631).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2176737 Vali Loss: 0.0843063 Test Loss: 0.1279550
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2178978 Vali Loss: 0.0882346 Test Loss: 0.1279553
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2178139 Vali Loss: 0.0840760 Test Loss: 0.1279541
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2177517 Vali Loss: 0.0838510 Test Loss: 0.1279529
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2179621 Vali Loss: 0.0849664 Test Loss: 0.1279525
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2181831 Vali Loss: 0.0861977 Test Loss: 0.1279523
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2180874 Vali Loss: 0.0842490 Test Loss: 0.1279522
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2180789 Vali Loss: 0.0885176 Test Loss: 0.1279522
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2185116 Vali Loss: 0.0850095 Test Loss: 0.1279522
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2174872 Vali Loss: 0.0852745 Test Loss: 0.1279522
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.00020312986453063786, mae:0.01026515755802393, rmse:0.014252363704144955, r2:-0.011302828788757324, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0103, RMSE: 0.0143, RÂ²: -0.0113, MAPE: 250383.80%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.491 MB of 0.491 MB uploadedwandb: \ 0.491 MB of 0.491 MB uploadedwandb: | 0.491 MB of 0.491 MB uploadedwandb: / 0.491 MB of 0.563 MB uploadedwandb: - 0.563 MB of 0.563 MB uploadedwandb: \ 0.563 MB of 0.563 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‡â–ˆâ–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–„â–ƒâ–†â–„â–‡â–â–‚â–ˆâ–‚â–â–ƒâ–…â–‚â–ˆâ–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 18
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35998
wandb:     model/trainable_params 35998
wandb:                  test/loss 0.12795
wandb:                 train/loss 0.21749
wandb:   val/directional_accuracy 48.51064
wandb:                   val/loss 0.08527
wandb:                    val/mae 0.01027
wandb:                   val/mape 25038379.6875
wandb:                    val/mse 0.0002
wandb:                     val/r2 -0.0113
wandb:                   val/rmse 0.01425
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/uvare1rg
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_092319-uvare1rg/logs
Completed: APPLE H=5

Training: Mamba on APPLE for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_092426-4qfgurt8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/4qfgurt8
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H10     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/4qfgurt8
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2358432 Vali Loss: 0.0850431 Test Loss: 0.1311064
Validation loss decreased (inf --> 0.085043).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2314479 Vali Loss: 0.0908981 Test Loss: 0.1312727
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2276617 Vali Loss: 0.0890594 Test Loss: 0.1315967
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2267411 Vali Loss: 0.0869244 Test Loss: 0.1310316
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2252383 Vali Loss: 0.0884522 Test Loss: 0.1310597
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2260602 Vali Loss: 0.0938946 Test Loss: 0.1310123
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2249073 Vali Loss: 0.0908851 Test Loss: 0.1309889
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23584323415630742, 'val/loss': 0.08504310622811317, 'test/loss': 0.13110639061778784, '_timestamp': 1762327475.0378084}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23144791336884177, 'val/loss': 0.09089808352291584, 'test/loss': 0.1312727201730013, '_timestamp': 1762327477.0546024}).
Epoch: 8, Steps: 133 | Train Loss: 0.2247821 Vali Loss: 0.0893364 Test Loss: 0.1309645
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2256609 Vali Loss: 0.0886136 Test Loss: 0.1309586
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2250164 Vali Loss: 0.0888672 Test Loss: 0.1309525
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2246693 Vali Loss: 0.0891355 Test Loss: 0.1309511
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.00020767757087014616, mae:0.010369697585701942, rmse:0.014411022886633873, r2:-0.02573680877685547, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0104, RMSE: 0.0144, RÂ²: -0.0257, MAPE: 357095.38%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.567 MB of 0.568 MB uploadedwandb: \ 0.568 MB of 0.568 MB uploadedwandb: | 0.568 MB of 0.568 MB uploadedwandb: / 0.568 MB of 0.639 MB uploadedwandb: - 0.568 MB of 0.639 MB uploadedwandb: \ 0.639 MB of 0.639 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–
wandb:                 train/loss â–ˆâ–†â–‚â–„â–‚â–â–ƒâ–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–â–ƒâ–ˆâ–…â–ƒâ–ƒâ–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 37948
wandb:     model/trainable_params 37948
wandb:                  test/loss 0.13095
wandb:                 train/loss 0.22467
wandb:   val/directional_accuracy 49.37198
wandb:                   val/loss 0.08914
wandb:                    val/mae 0.01037
wandb:                   val/mape 35709537.5
wandb:                    val/mse 0.00021
wandb:                     val/r2 -0.02574
wandb:                   val/rmse 0.01441
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/4qfgurt8
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_092426-4qfgurt8/logs
Completed: APPLE H=10

Training: Mamba on APPLE for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_092517-mgy5tko6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/mgy5tko6
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H22     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/mgy5tko6
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2427358 Vali Loss: 0.0888050 Test Loss: 0.1336867
Validation loss decreased (inf --> 0.088805).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2378967 Vali Loss: 0.0896493 Test Loss: 0.1352285
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2343187 Vali Loss: 0.0893510 Test Loss: 0.1351604
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2328132 Vali Loss: 0.0895479 Test Loss: 0.1355640
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2321174 Vali Loss: 0.0891334 Test Loss: 0.1354729
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2317955 Vali Loss: 0.0890905 Test Loss: 0.1353644
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2316720 Vali Loss: 0.0891826 Test Loss: 0.1353413
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.24273581791556242, 'val/loss': 0.08880498153822762, 'test/loss': 0.13368669471570424, '_timestamp': 1762327524.1110685}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2378967127790957, 'val/loss': 0.08964925152914864, 'test/loss': 0.13522852531501225, '_timestamp': 1762327526.0982013}).
Epoch: 8, Steps: 132 | Train Loss: 0.2317506 Vali Loss: 0.0891290 Test Loss: 0.1353300
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2317673 Vali Loss: 0.0891110 Test Loss: 0.1353179
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2317733 Vali Loss: 0.0892190 Test Loss: 0.1353192
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2315469 Vali Loss: 0.0890250 Test Loss: 0.1353184
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.00021331568132154644, mae:0.010486758314073086, rmse:0.014605330303311348, r2:-0.028005123138427734, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0105, RMSE: 0.0146, RÂ²: -0.0280, MAPE: 344432.69%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.645 MB of 0.646 MB uploadedwandb: \ 0.645 MB of 0.646 MB uploadedwandb: | 0.645 MB of 0.646 MB uploadedwandb: / 0.646 MB of 0.646 MB uploadedwandb: - 0.646 MB of 0.717 MB uploadedwandb: \ 0.646 MB of 0.717 MB uploadedwandb: | 0.717 MB of 0.717 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–†â–…â–„â–„â–„â–„â–„
wandb:                 train/loss â–ˆâ–„â–‚â–‚â–â–‚â–‚â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–ˆâ–‚â–‚â–ƒâ–‚â–‚â–„â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 42628
wandb:     model/trainable_params 42628
wandb:                  test/loss 0.13532
wandb:                 train/loss 0.23155
wandb:   val/directional_accuracy 49.08257
wandb:                   val/loss 0.08903
wandb:                    val/mae 0.01049
wandb:                   val/mape 34443268.75
wandb:                    val/mse 0.00021
wandb:                     val/r2 -0.02801
wandb:                   val/rmse 0.01461
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/mgy5tko6
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_092517-mgy5tko6/logs
Completed: APPLE H=22

Training: Mamba on APPLE for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_092608-klrwppht
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/klrwppht
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H50     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/klrwppht
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.2553219 Vali Loss: 0.0915600 Test Loss: 0.1509035
Validation loss decreased (inf --> 0.091560).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2511482 Vali Loss: 0.0921618 Test Loss: 0.1547728
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2465104 Vali Loss: 0.0921926 Test Loss: 0.1564678
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2445080 Vali Loss: 0.0916750 Test Loss: 0.1554230
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2446846 Vali Loss: 0.0915324 Test Loss: 0.1555552
Validation loss decreased (0.091560 --> 0.091532).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2435390 Vali Loss: 0.0915143 Test Loss: 0.1556869
Validation loss decreased (0.091532 --> 0.091514).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2429801 Vali Loss: 0.0915053 Test Loss: 0.1558494
Validation loss decreased (0.091514 --> 0.091505).  Saving model ...
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2553218701910792, 'val/loss': 0.09156000862518947, 'test/loss': 0.1509035254518191, '_timestamp': 1762327577.370043}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25114819859013415, 'val/loss': 0.09216180567940076, 'test/loss': 0.15477279449502626, '_timestamp': 1762327579.3731947}).
Epoch: 8, Steps: 132 | Train Loss: 0.2426997 Vali Loss: 0.0914323 Test Loss: 0.1557836
Validation loss decreased (0.091505 --> 0.091432).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2431474 Vali Loss: 0.0914770 Test Loss: 0.1557750
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2444575 Vali Loss: 0.0915328 Test Loss: 0.1557706
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2521760 Vali Loss: 0.0914962 Test Loss: 0.1557677
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2428106 Vali Loss: 0.0915041 Test Loss: 0.1557660
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2441897 Vali Loss: 0.0914922 Test Loss: 0.1557663
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2436674 Vali Loss: 0.0914864 Test Loss: 0.1557656
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2443314 Vali Loss: 0.0914942 Test Loss: 0.1557653
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2473584 Vali Loss: 0.0914898 Test Loss: 0.1557653
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2429994 Vali Loss: 0.0914606 Test Loss: 0.1557653
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2429153 Vali Loss: 0.0914675 Test Loss: 0.1557653
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.00022734566300641745, mae:0.010877016931772232, rmse:0.01507798582315445, r2:-0.02465641498565674, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0109, RMSE: 0.0151, RÂ²: -0.0247, MAPE: 236704.92%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.692 MB of 0.694 MB uploadedwandb: \ 0.692 MB of 0.694 MB uploadedwandb: | 0.694 MB of 0.694 MB uploadedwandb: / 0.694 MB of 0.766 MB uploadedwandb: - 0.694 MB of 0.766 MB uploadedwandb: \ 0.766 MB of 0.766 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–‚â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–„â–‚â–‚â–‚â–â–â–â–‚â–ˆâ–â–‚â–‚â–‚â–„â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–‚â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 17
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 53548
wandb:     model/trainable_params 53548
wandb:                  test/loss 0.15577
wandb:                 train/loss 0.24292
wandb:   val/directional_accuracy 49.73147
wandb:                   val/loss 0.09147
wandb:                    val/mae 0.01088
wandb:                   val/mape 23670492.1875
wandb:                    val/mse 0.00023
wandb:                     val/r2 -0.02466
wandb:                   val/rmse 0.01508
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/klrwppht
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_092608-klrwppht/logs
Completed: APPLE H=50

Training: Mamba on APPLE for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_092716-ysjrg2k8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/ysjrg2k8
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H100    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/ysjrg2k8
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.2730915 Vali Loss: 0.0992124 Test Loss: 0.1673684
Validation loss decreased (inf --> 0.099212).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2675001 Vali Loss: 0.1015967 Test Loss: 0.1749921
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.2622638 Vali Loss: 0.1019466 Test Loss: 0.1774007
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2606317 Vali Loss: 0.1012554 Test Loss: 0.1765848
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2599002 Vali Loss: 0.1003493 Test Loss: 0.1771528
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2595520 Vali Loss: 0.1000575 Test Loss: 0.1773552
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2594822 Vali Loss: 0.1011277 Test Loss: 0.1773335
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2730914833453985, 'val/loss': 0.09921240955591201, 'test/loss': 0.1673683851957321, '_timestamp': 1762327644.075809}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2675001351879193, 'val/loss': 0.10159671902656556, 'test/loss': 0.17499208450317383, '_timestamp': 1762327646.05207}).
Epoch: 8, Steps: 130 | Train Loss: 0.2596374 Vali Loss: 0.1011331 Test Loss: 0.1773401
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2596852 Vali Loss: 0.1013497 Test Loss: 0.1773388
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2592885 Vali Loss: 0.0994727 Test Loss: 0.1773459
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2597082 Vali Loss: 0.1011307 Test Loss: 0.1773489
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.00024629756808280945, mae:0.011310148984193802, rmse:0.015693871304392815, r2:-0.04313957691192627, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0113, RMSE: 0.0157, RÂ²: -0.0431, MAPE: 674458.62%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.713 MB of 0.717 MB uploadedwandb: \ 0.713 MB of 0.717 MB uploadedwandb: | 0.713 MB of 0.717 MB uploadedwandb: / 0.717 MB of 0.717 MB uploadedwandb: - 0.717 MB of 0.717 MB uploadedwandb: \ 0.717 MB of 0.789 MB uploadedwandb: | 0.789 MB of 0.789 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–†â–ˆâ–‡â–‡â–‡â–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–„â–‚â–‚â–â–‚â–‚â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–ƒâ–ƒâ–†â–†â–†â–â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 73048
wandb:     model/trainable_params 73048
wandb:                  test/loss 0.17735
wandb:                 train/loss 0.25971
wandb:   val/directional_accuracy 49.55988
wandb:                   val/loss 0.10113
wandb:                    val/mae 0.01131
wandb:                   val/mape 67445862.5
wandb:                    val/mse 0.00025
wandb:                     val/r2 -0.04314
wandb:                   val/rmse 0.01569
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/ysjrg2k8
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_092716-ysjrg2k8/logs
Completed: APPLE H=100

Training: Mamba on SP500 for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_092806-yrdj5cgg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/yrdj5cgg
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H3      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/yrdj5cgg
>>>>>>>start training : long_term_forecast_Mamba_SP500_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 3
============================================================
train 4241
val 237
test 238
Epoch: 1, Steps: 133 | Train Loss: 0.1788524 Vali Loss: 0.0678778 Test Loss: 0.0751026
Validation loss decreased (inf --> 0.067878).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.1776037 Vali Loss: 0.0684688 Test Loss: 0.0742747
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.1769178 Vali Loss: 0.0667518 Test Loss: 0.0737335
Validation loss decreased (0.067878 --> 0.066752).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1743766 Vali Loss: 0.0673000 Test Loss: 0.0735021
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1744497 Vali Loss: 0.0666213 Test Loss: 0.0733418
Validation loss decreased (0.066752 --> 0.066621).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1743111 Vali Loss: 0.0678516 Test Loss: 0.0732678
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1742530 Vali Loss: 0.0656145 Test Loss: 0.0732280
Validation loss decreased (0.066621 --> 0.065614).  Saving model ...
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.17885241433417887, 'val/loss': 0.06787781929597259, 'test/loss': 0.0751025932841003, '_timestamp': 1762327693.4320037}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.17760370229195832, 'val/loss': 0.06846880493685603, 'test/loss': 0.07427474344149232, '_timestamp': 1762327695.5664868}).
Epoch: 8, Steps: 133 | Train Loss: 0.1739880 Vali Loss: 0.0682379 Test Loss: 0.0732114
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1743990 Vali Loss: 0.0679404 Test Loss: 0.0732028
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1750076 Vali Loss: 0.0668560 Test Loss: 0.0731982
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1741783 Vali Loss: 0.0643423 Test Loss: 0.0731958
Validation loss decreased (0.065614 --> 0.064342).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1741164 Vali Loss: 0.0674902 Test Loss: 0.0731946
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1736123 Vali Loss: 0.0678885 Test Loss: 0.0731941
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1741792 Vali Loss: 0.0675878 Test Loss: 0.0731938
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1742104 Vali Loss: 0.0678191 Test Loss: 0.0731937
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1734958 Vali Loss: 0.0664307 Test Loss: 0.0731936
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1736178 Vali Loss: 0.0662938 Test Loss: 0.0731936
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1737632 Vali Loss: 0.0676386 Test Loss: 0.0731936
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1736723 Vali Loss: 0.0680269 Test Loss: 0.0731936
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1743744 Vali Loss: 0.0676261 Test Loss: 0.0731936
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1740358 Vali Loss: 0.0671669 Test Loss: 0.0731936
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SP500_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 238
test shape: (238, 3, 1) (238, 3, 1)
test shape: (238, 3, 1) (238, 3, 1)


	mse:6.494968693004921e-05, mae:0.00594298867508769, rmse:0.00805913656949997, r2:0.000947117805480957, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0059, RMSE: 0.0081, RÂ²: 0.0009, MAPE: 1.82%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.472 MB of 0.472 MB uploadedwandb: \ 0.472 MB of 0.472 MB uploadedwandb: | 0.472 MB of 0.472 MB uploadedwandb: / 0.472 MB of 0.472 MB uploadedwandb: - 0.472 MB of 0.472 MB uploadedwandb: \ 0.472 MB of 0.472 MB uploadedwandb: | 0.472 MB of 0.472 MB uploadedwandb: / 0.472 MB of 0.472 MB uploadedwandb: - 0.521 MB of 0.594 MB uploadedwandb: \ 0.594 MB of 0.594 MB uploadedwandb: | 0.594 MB of 0.594 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–„â–‚â–‚â–â–‚â–‚â–â–â–‚â–â–ƒâ–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–†â–…â–‡â–ƒâ–ˆâ–‡â–†â–â–‡â–‡â–‡â–‡â–…â–…â–‡â–ˆâ–‡â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35218
wandb:     model/trainable_params 35218
wandb:                  test/loss 0.07319
wandb:                 train/loss 0.17404
wandb:   val/directional_accuracy 44.53782
wandb:                   val/loss 0.06717
wandb:                    val/mae 0.00594
wandb:                   val/mape 181.55607
wandb:                    val/mse 6e-05
wandb:                     val/r2 0.00095
wandb:                   val/rmse 0.00806
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/yrdj5cgg
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_092806-yrdj5cgg/logs
Completed: SP500 H=3

Training: Mamba on SP500 for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_092924-k7ebqy93
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/k7ebqy93
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H5      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/k7ebqy93
>>>>>>>start training : long_term_forecast_Mamba_SP500_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 5
============================================================
train 4239
val 235
test 236
Epoch: 1, Steps: 133 | Train Loss: 0.1801395 Vali Loss: 0.0713168 Test Loss: 0.0770384
Validation loss decreased (inf --> 0.071317).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.1785941 Vali Loss: 0.0672549 Test Loss: 0.0762159
Validation loss decreased (0.071317 --> 0.067255).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.1762495 Vali Loss: 0.0694855 Test Loss: 0.0756848
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1774779 Vali Loss: 0.0686490 Test Loss: 0.0753421
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1750272 Vali Loss: 0.0699714 Test Loss: 0.0751547
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1751934 Vali Loss: 0.0681550 Test Loss: 0.0750647
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1767405 Vali Loss: 0.0653954 Test Loss: 0.0750229
Validation loss decreased (0.067255 --> 0.065395).  Saving model ...
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.18013953770461835, 'val/loss': 0.07131676049903035, 'test/loss': 0.0770384231582284, '_timestamp': 1762327771.4221718}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.178594083238141, 'val/loss': 0.06725488184019923, 'test/loss': 0.07621585112065077, '_timestamp': 1762327773.4242406}).
Epoch: 8, Steps: 133 | Train Loss: 0.1751583 Vali Loss: 0.0676073 Test Loss: 0.0749996
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1752959 Vali Loss: 0.0676637 Test Loss: 0.0749887
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1748698 Vali Loss: 0.0696925 Test Loss: 0.0749834
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1759585 Vali Loss: 0.0701257 Test Loss: 0.0749805
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1753220 Vali Loss: 0.0686201 Test Loss: 0.0749792
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1756526 Vali Loss: 0.0684123 Test Loss: 0.0749785
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1771436 Vali Loss: 0.0687310 Test Loss: 0.0749782
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1752538 Vali Loss: 0.0688444 Test Loss: 0.0749780
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1754403 Vali Loss: 0.0686093 Test Loss: 0.0749780
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1749983 Vali Loss: 0.0676177 Test Loss: 0.0749779
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SP500_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
test shape: (236, 5, 1) (236, 5, 1)
test shape: (236, 5, 1) (236, 5, 1)


	mse:6.54492323519662e-05, mae:0.00596848176792264, rmse:0.008090070448815823, r2:-0.007511258125305176, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0060, RMSE: 0.0081, RÂ²: -0.0075, MAPE: 1.51%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.523 MB of 0.523 MB uploadedwandb: \ 0.523 MB of 0.523 MB uploadedwandb: | 0.523 MB of 0.523 MB uploadedwandb: / 0.523 MB of 0.595 MB uploadedwandb: - 0.523 MB of 0.595 MB uploadedwandb: \ 0.595 MB of 0.595 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–…â–ˆâ–â–‚â–†â–‚â–‚â–â–„â–‚â–ƒâ–‡â–‚â–ƒâ–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–†â–ˆâ–…â–â–„â–„â–‡â–ˆâ–†â–…â–†â–†â–†â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35998
wandb:     model/trainable_params 35998
wandb:                  test/loss 0.07498
wandb:                 train/loss 0.175
wandb:   val/directional_accuracy 49.89407
wandb:                   val/loss 0.06762
wandb:                    val/mae 0.00597
wandb:                   val/mape 150.76143
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.00751
wandb:                   val/rmse 0.00809
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/k7ebqy93
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_092924-k7ebqy93/logs
Completed: SP500 H=5

Training: Mamba on SP500 for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_093027-rj493smi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/rj493smi
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H10     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/rj493smi
>>>>>>>start training : long_term_forecast_Mamba_SP500_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 10
============================================================
train 4234
val 230
test 231
Epoch: 1, Steps: 133 | Train Loss: 0.1812602 Vali Loss: 0.0692117 Test Loss: 0.0800250
Validation loss decreased (inf --> 0.069212).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.1790277 Vali Loss: 0.0703385 Test Loss: 0.0792416
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.1782398 Vali Loss: 0.0707564 Test Loss: 0.0786521
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1772740 Vali Loss: 0.0707266 Test Loss: 0.0783108
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1766916 Vali Loss: 0.0685502 Test Loss: 0.0781182
Validation loss decreased (0.069212 --> 0.068550).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1764892 Vali Loss: 0.0679444 Test Loss: 0.0780245
Validation loss decreased (0.068550 --> 0.067944).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1768823 Vali Loss: 0.0685678 Test Loss: 0.0779713
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.18126020244413749, 'val/loss': 0.0692117097787559, 'test/loss': 0.08002498745918274, '_timestamp': 1762327836.2823923}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.17902769770165136, 'val/loss': 0.07033848809078336, 'test/loss': 0.07924160826951265, '_timestamp': 1762327838.3192747}).
Epoch: 8, Steps: 133 | Train Loss: 0.1767534 Vali Loss: 0.0711418 Test Loss: 0.0779477
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1768152 Vali Loss: 0.0672876 Test Loss: 0.0779361
Validation loss decreased (0.067944 --> 0.067288).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1766193 Vali Loss: 0.0706952 Test Loss: 0.0779305
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1763620 Vali Loss: 0.0689263 Test Loss: 0.0779274
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1765967 Vali Loss: 0.0710142 Test Loss: 0.0779259
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1767955 Vali Loss: 0.0700138 Test Loss: 0.0779252
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1765360 Vali Loss: 0.0723019 Test Loss: 0.0779248
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1762669 Vali Loss: 0.0682031 Test Loss: 0.0779246
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1768613 Vali Loss: 0.0673728 Test Loss: 0.0779246
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1762402 Vali Loss: 0.0687916 Test Loss: 0.0779245
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1763440 Vali Loss: 0.0673098 Test Loss: 0.0779245
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1762044 Vali Loss: 0.0689646 Test Loss: 0.0779245
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SP500_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 231
test shape: (231, 10, 1) (231, 10, 1)
test shape: (231, 10, 1) (231, 10, 1)


	mse:6.543735071318224e-05, mae:0.005971229635179043, rmse:0.008089335635304451, r2:-0.006685137748718262, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0060, RMSE: 0.0081, RÂ²: -0.0067, MAPE: 1.74%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.533 MB of 0.533 MB uploadedwandb: \ 0.533 MB of 0.533 MB uploadedwandb: | 0.533 MB of 0.533 MB uploadedwandb: / 0.533 MB of 0.533 MB uploadedwandb: - 0.533 MB of 0.606 MB uploadedwandb: \ 0.533 MB of 0.606 MB uploadedwandb: | 0.606 MB of 0.606 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–â–ƒâ–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–†â–ƒâ–‚â–ƒâ–†â–â–†â–ƒâ–†â–…â–ˆâ–‚â–â–ƒâ–â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 18
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 37948
wandb:     model/trainable_params 37948
wandb:                  test/loss 0.07792
wandb:                 train/loss 0.1762
wandb:   val/directional_accuracy 50.45695
wandb:                   val/loss 0.06896
wandb:                    val/mae 0.00597
wandb:                   val/mape 173.52263
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.00669
wandb:                   val/rmse 0.00809
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/rj493smi
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_093027-rj493smi/logs
Completed: SP500 H=10

Training: Mamba on SP500 for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_093137-qbu677n0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/qbu677n0
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H22     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/qbu677n0
>>>>>>>start training : long_term_forecast_Mamba_SP500_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 22
============================================================
train 4222
val 218
test 219
Epoch: 1, Steps: 132 | Train Loss: 0.1838718 Vali Loss: 0.0727697 Test Loss: 0.0741248
Validation loss decreased (inf --> 0.072770).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.1820881 Vali Loss: 0.0718392 Test Loss: 0.0732928
Validation loss decreased (0.072770 --> 0.071839).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.1812293 Vali Loss: 0.0712448 Test Loss: 0.0726151
Validation loss decreased (0.071839 --> 0.071245).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.1806712 Vali Loss: 0.0711438 Test Loss: 0.0722045
Validation loss decreased (0.071245 --> 0.071144).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.1800631 Vali Loss: 0.0709258 Test Loss: 0.0719719
Validation loss decreased (0.071144 --> 0.070926).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.1801269 Vali Loss: 0.0706116 Test Loss: 0.0718512
Validation loss decreased (0.070926 --> 0.070612).  Saving model ...
Updating learning rate to 3.125e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.18387176457679633, 'val/loss': 0.07276968551533562, 'test/loss': 0.07412483862468175, '_timestamp': 1762327905.9868717}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.18208812165892485, 'val/loss': 0.0718391654746873, 'test/loss': 0.07329283654689789, '_timestamp': 1762327908.0027933}).
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
Epoch: 7, Steps: 132 | Train Loss: 0.1798420 Vali Loss: 0.0705399 Test Loss: 0.0717912
Validation loss decreased (0.070612 --> 0.070540).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.1798233 Vali Loss: 0.0706261 Test Loss: 0.0717610
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.1795399 Vali Loss: 0.0707133 Test Loss: 0.0717464
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.1796737 Vali Loss: 0.0708524 Test Loss: 0.0717391
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.1797595 Vali Loss: 0.0704680 Test Loss: 0.0717356
Validation loss decreased (0.070540 --> 0.070468).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.1796197 Vali Loss: 0.0708428 Test Loss: 0.0717338
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.1796181 Vali Loss: 0.0706283 Test Loss: 0.0717329
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.1795456 Vali Loss: 0.0707461 Test Loss: 0.0717325
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.1797274 Vali Loss: 0.0709394 Test Loss: 0.0717322
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.1795886 Vali Loss: 0.0705833 Test Loss: 0.0717322
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.1798254 Vali Loss: 0.0703931 Test Loss: 0.0717321
Validation loss decreased (0.070468 --> 0.070393).  Saving model ...
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.1797048 Vali Loss: 0.0705817 Test Loss: 0.0717321
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.1796639 Vali Loss: 0.0706966 Test Loss: 0.0717321
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.1797819 Vali Loss: 0.0706321 Test Loss: 0.0717321
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.1796644 Vali Loss: 0.0706061 Test Loss: 0.0717321
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.1797142 Vali Loss: 0.0706213 Test Loss: 0.0717321
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.1796190 Vali Loss: 0.0705104 Test Loss: 0.0717321
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 132 | Train Loss: 0.1798180 Vali Loss: 0.0708201 Test Loss: 0.0717321
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 132 | Train Loss: 0.1796751 Vali Loss: 0.0708165 Test Loss: 0.0717321
EarlyStopping counter: 8 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 132 | Train Loss: 0.1796470 Vali Loss: 0.0706025 Test Loss: 0.0717321
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 132 | Train Loss: 0.1797789 Vali Loss: 0.0707732 Test Loss: 0.0717321
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SP500_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 219
test shape: (219, 22, 1) (219, 22, 1)
test shape: (219, 22, 1) (219, 22, 1)


	mse:6.433632370317355e-05, mae:0.005940946750342846, rmse:0.008020992390811443, r2:-0.007688641548156738, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0059, RMSE: 0.0080, RÂ²: -0.0077, MAPE: 1.88%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.619 MB of 0.620 MB uploadedwandb: \ 0.619 MB of 0.620 MB uploadedwandb: | 0.620 MB of 0.620 MB uploadedwandb: / 0.620 MB of 0.620 MB uploadedwandb: - 0.620 MB of 0.694 MB uploadedwandb: \ 0.694 MB of 0.694 MB uploadedwandb: | 0.694 MB of 0.694 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–†â–ƒâ–ƒâ–‚â–‚â–â–‚â–‚â–â–â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‡â–…â–ƒâ–‚â–ƒâ–„â–…â–‚â–…â–ƒâ–„â–…â–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–…â–„â–ƒâ–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 26
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 42628
wandb:     model/trainable_params 42628
wandb:                  test/loss 0.07173
wandb:                 train/loss 0.17978
wandb:   val/directional_accuracy 50.38052
wandb:                   val/loss 0.07077
wandb:                    val/mae 0.00594
wandb:                   val/mape 188.32815
wandb:                    val/mse 6e-05
wandb:                     val/r2 -0.00769
wandb:                   val/rmse 0.00802
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/qbu677n0
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_093137-qbu677n0/logs
Completed: SP500 H=22

Training: Mamba on SP500 for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_093304-fvwwx20e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/fvwwx20e
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H50     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/fvwwx20e
>>>>>>>start training : long_term_forecast_Mamba_SP500_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 50
============================================================
train 4194
val 190
test 191
Epoch: 1, Steps: 132 | Train Loss: 0.1882779 Vali Loss: 0.0732229 Test Loss: 0.0797900
Validation loss decreased (inf --> 0.073223).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.1866174 Vali Loss: 0.0724440 Test Loss: 0.0787599
Validation loss decreased (0.073223 --> 0.072444).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.1852844 Vali Loss: 0.0718302 Test Loss: 0.0776732
Validation loss decreased (0.072444 --> 0.071830).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.1848197 Vali Loss: 0.0714366 Test Loss: 0.0769436
Validation loss decreased (0.071830 --> 0.071437).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.1843428 Vali Loss: 0.0711600 Test Loss: 0.0765285
Validation loss decreased (0.071437 --> 0.071160).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.1841111 Vali Loss: 0.0710439 Test Loss: 0.0763178
Validation loss decreased (0.071160 --> 0.071044).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.1865495 Vali Loss: 0.0710157 Test Loss: 0.0762124
Validation loss decreased (0.071044 --> 0.071016).  Saving model ...
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.18827791269304175, 'val/loss': 0.07322291160623233, 'test/loss': 0.0797899601360162, '_timestamp': 1762327993.057348}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.18661736612292854, 'val/loss': 0.07244398320714633, 'test/loss': 0.07875986148913701, '_timestamp': 1762327995.102082}).
Epoch: 8, Steps: 132 | Train Loss: 0.1839381 Vali Loss: 0.0709337 Test Loss: 0.0761645
Validation loss decreased (0.071016 --> 0.070934).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.1835074 Vali Loss: 0.0709756 Test Loss: 0.0761399
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.1843002 Vali Loss: 0.0709289 Test Loss: 0.0761277
Validation loss decreased (0.070934 --> 0.070929).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.1838997 Vali Loss: 0.0709850 Test Loss: 0.0761218
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.1837931 Vali Loss: 0.0709315 Test Loss: 0.0761188
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.1832904 Vali Loss: 0.0709473 Test Loss: 0.0761173
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.1841660 Vali Loss: 0.0709817 Test Loss: 0.0761166
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.1890228 Vali Loss: 0.0709783 Test Loss: 0.0761163
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.1898655 Vali Loss: 0.0709671 Test Loss: 0.0761161
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.1834140 Vali Loss: 0.0709646 Test Loss: 0.0761161
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.1857521 Vali Loss: 0.0709510 Test Loss: 0.0761161
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.1836593 Vali Loss: 0.0709340 Test Loss: 0.0761161
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.1838441 Vali Loss: 0.0709383 Test Loss: 0.0761161
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SP500_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 191
test shape: (191, 50, 1) (191, 50, 1)
test shape: (191, 50, 1) (191, 50, 1)


	mse:6.568945536855608e-05, mae:0.0060290019027888775, rmse:0.008104903623461723, r2:-0.010101318359375, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0060, RMSE: 0.0081, RÂ²: -0.0101, MAPE: 2.08%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.696 MB of 0.698 MB uploadedwandb: \ 0.696 MB of 0.698 MB uploadedwandb: | 0.696 MB of 0.698 MB uploadedwandb: / 0.698 MB of 0.698 MB uploadedwandb: - 0.698 MB of 0.771 MB uploadedwandb: \ 0.771 MB of 0.771 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ƒâ–ƒâ–‚â–‚â–„â–‚â–â–‚â–‚â–‚â–â–‚â–‡â–ˆâ–â–„â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 53548
wandb:     model/trainable_params 53548
wandb:                  test/loss 0.07612
wandb:                 train/loss 0.18384
wandb:   val/directional_accuracy 50.13356
wandb:                   val/loss 0.07094
wandb:                    val/mae 0.00603
wandb:                   val/mape 207.81503
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.0101
wandb:                   val/rmse 0.0081
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/fvwwx20e
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_093304-fvwwx20e/logs
Completed: SP500 H=50

Training: Mamba on SP500 for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_093417-rxckssgz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/rxckssgz
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H100    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/rxckssgz
>>>>>>>start training : long_term_forecast_Mamba_SP500_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 100
============================================================
train 4144
val 140
test 141
Epoch: 1, Steps: 130 | Train Loss: 0.1993215 Vali Loss: 0.0750596 Test Loss: 0.0946196
Validation loss decreased (inf --> 0.075060).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.1977835 Vali Loss: 0.0742972 Test Loss: 0.0917063
Validation loss decreased (0.075060 --> 0.074297).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.1957595 Vali Loss: 0.0727847 Test Loss: 0.0872238
Validation loss decreased (0.074297 --> 0.072785).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.1933947 Vali Loss: 0.0719779 Test Loss: 0.0842303
Validation loss decreased (0.072785 --> 0.071978).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.1927549 Vali Loss: 0.0709927 Test Loss: 0.0830149
Validation loss decreased (0.071978 --> 0.070993).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.1923292 Vali Loss: 0.0706676 Test Loss: 0.0825093
Validation loss decreased (0.070993 --> 0.070668).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.1919552 Vali Loss: 0.0711565 Test Loss: 0.0822827
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.19932149178706682, 'val/loss': 0.07505956292152405, 'test/loss': 0.09461957663297653, '_timestamp': 1762328064.9448364}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.1977834514127328, 'val/loss': 0.07429716885089874, 'test/loss': 0.09170630127191544, '_timestamp': 1762328066.9050136}).
Epoch: 8, Steps: 130 | Train Loss: 0.1919829 Vali Loss: 0.0709879 Test Loss: 0.0821785
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.1928899 Vali Loss: 0.0713085 Test Loss: 0.0821289
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.1925838 Vali Loss: 0.0703712 Test Loss: 0.0821058
Validation loss decreased (0.070668 --> 0.070371).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.1920687 Vali Loss: 0.0708435 Test Loss: 0.0820940
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.1935131 Vali Loss: 0.0707716 Test Loss: 0.0820886
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.1926296 Vali Loss: 0.0712194 Test Loss: 0.0820859
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.1927257 Vali Loss: 0.0707402 Test Loss: 0.0820845
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.1921695 Vali Loss: 0.0708355 Test Loss: 0.0820838
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 130 | Train Loss: 0.1922373 Vali Loss: 0.0705928 Test Loss: 0.0820836
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 130 | Train Loss: 0.1919663 Vali Loss: 0.0708736 Test Loss: 0.0820835
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 130 | Train Loss: 0.1918621 Vali Loss: 0.0713177 Test Loss: 0.0820835
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 130 | Train Loss: 0.1918866 Vali Loss: 0.0711224 Test Loss: 0.0820835
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 130 | Train Loss: 0.1920840 Vali Loss: 0.0712145 Test Loss: 0.0820835
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SP500_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 141
test shape: (141, 100, 1) (141, 100, 1)
test shape: (141, 100, 1) (141, 100, 1)


	mse:6.90598099026829e-05, mae:0.006134733557701111, rmse:0.008310222998261452, r2:-0.008049726486206055, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0083, RÂ²: -0.0080, MAPE: 2.24%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.797 MB of 0.802 MB uploadedwandb: \ 0.797 MB of 0.802 MB uploadedwandb: | 0.802 MB of 0.802 MB uploadedwandb: / 0.802 MB of 0.802 MB uploadedwandb: - 0.802 MB of 0.875 MB uploadedwandb: \ 0.875 MB of 0.875 MB uploadedwandb: | 0.875 MB of 0.875 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–â–â–ƒâ–‚â–â–„â–‚â–ƒâ–‚â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–ƒâ–‚â–ƒâ–ƒâ–„â–â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–„â–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 73048
wandb:     model/trainable_params 73048
wandb:                  test/loss 0.08208
wandb:                 train/loss 0.19208
wandb:   val/directional_accuracy 50.17551
wandb:                   val/loss 0.07121
wandb:                    val/mae 0.00613
wandb:                   val/mape 224.09401
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.00805
wandb:                   val/rmse 0.00831
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/rxckssgz
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_093417-rxckssgz/logs
Completed: SP500 H=100

Training: Mamba on NASDAQ for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_093528-aubwca4d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/aubwca4d
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H3     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/aubwca4d
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.2458621 Vali Loss: 0.1350313 Test Loss: 0.1263915
Validation loss decreased (inf --> 0.135031).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2389316 Vali Loss: 0.1296194 Test Loss: 0.1208975
Validation loss decreased (0.135031 --> 0.129619).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2331022 Vali Loss: 0.1284978 Test Loss: 0.1181132
Validation loss decreased (0.129619 --> 0.128498).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2297127 Vali Loss: 0.1388153 Test Loss: 0.1169119
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2289219 Vali Loss: 0.1269001 Test Loss: 0.1164269
Validation loss decreased (0.128498 --> 0.126900).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2277003 Vali Loss: 0.1274907 Test Loss: 0.1162542
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2270339 Vali Loss: 0.1272393 Test Loss: 0.1161851
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.24586209341099388, 'val/loss': 0.13503132481127977, 'test/loss': 0.12639151699841022, '_timestamp': 1762328135.8366919}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23893158597157413, 'val/loss': 0.12961940374225378, 'test/loss': 0.12089754128828645, '_timestamp': 1762328137.822337}).
Epoch: 8, Steps: 133 | Train Loss: 0.2274241 Vali Loss: 0.1254459 Test Loss: 0.1161386
Validation loss decreased (0.126900 --> 0.125446).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2270592 Vali Loss: 0.1282535 Test Loss: 0.1161228
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2276746 Vali Loss: 0.1286615 Test Loss: 0.1161134
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2280247 Vali Loss: 0.1368231 Test Loss: 0.1161082
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2269093 Vali Loss: 0.1258267 Test Loss: 0.1161063
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2272345 Vali Loss: 0.1270531 Test Loss: 0.1161052
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2270070 Vali Loss: 0.1287774 Test Loss: 0.1161048
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2274405 Vali Loss: 0.1426429 Test Loss: 0.1161045
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2285639 Vali Loss: 0.1266334 Test Loss: 0.1161044
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2273099 Vali Loss: 0.1257312 Test Loss: 0.1161044
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2270799 Vali Loss: 0.1273573 Test Loss: 0.1161044
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.00013761621084995568, mae:0.008526433259248734, rmse:0.011730993166565895, r2:-0.01107490062713623, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0085, RMSE: 0.0117, RÂ²: -0.0111, MAPE: 1860077.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.488 MB of 0.488 MB uploadedwandb: \ 0.488 MB of 0.488 MB uploadedwandb: | 0.488 MB of 0.488 MB uploadedwandb: / 0.488 MB of 0.488 MB uploadedwandb: - 0.488 MB of 0.488 MB uploadedwandb: \ 0.488 MB of 0.488 MB uploadedwandb: | 0.537 MB of 0.608 MB uploadedwandb: / 0.537 MB of 0.608 MB uploadedwandb: - 0.608 MB of 0.608 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–â–‚â–â–‚â–‚â–â–â–â–‚â–ƒâ–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–†â–‚â–‚â–‚â–â–‚â–‚â–†â–â–‚â–‚â–ˆâ–â–â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 17
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35218
wandb:     model/trainable_params 35218
wandb:                  test/loss 0.1161
wandb:                 train/loss 0.22708
wandb:   val/directional_accuracy 52.32068
wandb:                   val/loss 0.12736
wandb:                    val/mae 0.00853
wandb:                   val/mape 186007725.0
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.01107
wandb:                   val/rmse 0.01173
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/aubwca4d
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_093528-aubwca4d/logs
Completed: NASDAQ H=3

Training: Mamba on NASDAQ for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_093638-iqqjsvwb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/iqqjsvwb
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H5     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/iqqjsvwb
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.2478552 Vali Loss: 0.1398998 Test Loss: 0.1303058
Validation loss decreased (inf --> 0.139900).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2428223 Vali Loss: 0.1462959 Test Loss: 0.1270275
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2398199 Vali Loss: 0.1356710 Test Loss: 0.1250118
Validation loss decreased (0.139900 --> 0.135671).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2369165 Vali Loss: 0.1326666 Test Loss: 0.1239373
Validation loss decreased (0.135671 --> 0.132667).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2351199 Vali Loss: 0.1359565 Test Loss: 0.1234447
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2345554 Vali Loss: 0.1301606 Test Loss: 0.1232350
Validation loss decreased (0.132667 --> 0.130161).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2347062 Vali Loss: 0.1317360 Test Loss: 0.1231353
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.24785515273872175, 'val/loss': 0.13989978097379208, 'test/loss': 0.13030576659366488, '_timestamp': 1762328206.1140056}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24282230672083402, 'val/loss': 0.1462958650663495, 'test/loss': 0.12702753441408277, '_timestamp': 1762328208.0968494}).
Epoch: 8, Steps: 133 | Train Loss: 0.2343217 Vali Loss: 0.1312940 Test Loss: 0.1230874
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2356780 Vali Loss: 0.1306391 Test Loss: 0.1230646
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2343812 Vali Loss: 0.1317401 Test Loss: 0.1230557
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2339061 Vali Loss: 0.1416795 Test Loss: 0.1230496
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2338154 Vali Loss: 0.1380549 Test Loss: 0.1230465
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2347230 Vali Loss: 0.1356406 Test Loss: 0.1230453
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2335757 Vali Loss: 0.1313571 Test Loss: 0.1230443
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2345264 Vali Loss: 0.1313152 Test Loss: 0.1230440
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2348076 Vali Loss: 0.1325225 Test Loss: 0.1230438
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.00013998738722875714, mae:0.00859509315341711, rmse:0.011831626296043396, r2:-0.023277878761291504, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0086, RMSE: 0.0118, RÂ²: -0.0233, MAPE: 2300979.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.518 MB of 0.518 MB uploadedwandb: \ 0.518 MB of 0.518 MB uploadedwandb: | 0.518 MB of 0.518 MB uploadedwandb: / 0.518 MB of 0.518 MB uploadedwandb: - 0.518 MB of 0.589 MB uploadedwandb: \ 0.589 MB of 0.589 MB uploadedwandb: | 0.589 MB of 0.589 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–â–â–‚â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–ƒâ–…â–â–‚â–‚â–â–‚â–ˆâ–†â–„â–‚â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35998
wandb:     model/trainable_params 35998
wandb:                  test/loss 0.12304
wandb:                 train/loss 0.23481
wandb:   val/directional_accuracy 51.80851
wandb:                   val/loss 0.13252
wandb:                    val/mae 0.0086
wandb:                   val/mape 230097950.0
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.02328
wandb:                   val/rmse 0.01183
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/iqqjsvwb
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_093638-iqqjsvwb/logs
Completed: NASDAQ H=5

Training: Mamba on NASDAQ for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_093741-onx44cbb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/onx44cbb
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H10    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/onx44cbb
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2510863 Vali Loss: 0.1557969 Test Loss: 0.1330072
Validation loss decreased (inf --> 0.155797).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2481563 Vali Loss: 0.1431543 Test Loss: 0.1318262
Validation loss decreased (0.155797 --> 0.143154).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2458481 Vali Loss: 0.1397991 Test Loss: 0.1310069
Validation loss decreased (0.143154 --> 0.139799).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2453734 Vali Loss: 0.1365723 Test Loss: 0.1304867
Validation loss decreased (0.139799 --> 0.136572).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2438168 Vali Loss: 0.1421161 Test Loss: 0.1301667
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2447364 Vali Loss: 0.1369689 Test Loss: 0.1300153
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2433778 Vali Loss: 0.1404324 Test Loss: 0.1299354
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.25108632994325536, 'val/loss': 0.1557969292625785, 'test/loss': 0.13300724048167467, '_timestamp': 1762328268.4278424}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24815633288003447, 'val/loss': 0.14315427280962467, 'test/loss': 0.13182620238512754, '_timestamp': 1762328270.476803}).
Epoch: 8, Steps: 133 | Train Loss: 0.2428096 Vali Loss: 0.1395540 Test Loss: 0.1299010
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2439006 Vali Loss: 0.1500483 Test Loss: 0.1298812
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2429639 Vali Loss: 0.1402433 Test Loss: 0.1298717
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2429978 Vali Loss: 0.1375659 Test Loss: 0.1298671
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2433817 Vali Loss: 0.1403224 Test Loss: 0.1298648
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2436838 Vali Loss: 0.1415152 Test Loss: 0.1298637
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2427569 Vali Loss: 0.1430254 Test Loss: 0.1298631
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.00014151899085845798, mae:0.008648729883134365, rmse:0.01189617533236742, r2:-0.02234339714050293, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0086, RMSE: 0.0119, RÂ²: -0.0223, MAPE: 3120336.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.575 MB of 0.575 MB uploadedwandb: \ 0.575 MB of 0.575 MB uploadedwandb: | 0.575 MB of 0.575 MB uploadedwandb: / 0.575 MB of 0.575 MB uploadedwandb: - 0.575 MB of 0.647 MB uploadedwandb: \ 0.647 MB of 0.647 MB uploadedwandb: | 0.647 MB of 0.647 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–‡â–ƒâ–…â–‚â–â–„â–â–‚â–‚â–ƒâ–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–â–„â–â–ƒâ–ƒâ–ˆâ–ƒâ–‚â–ƒâ–„â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 13
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 37948
wandb:     model/trainable_params 37948
wandb:                  test/loss 0.12986
wandb:                 train/loss 0.24276
wandb:   val/directional_accuracy 50.14493
wandb:                   val/loss 0.14303
wandb:                    val/mae 0.00865
wandb:                   val/mape 312033650.0
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.02234
wandb:                   val/rmse 0.0119
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/onx44cbb
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_093741-onx44cbb/logs
Exception in thread Exception in thread NetStatThrIntMsgThr:
:
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 268, in check_network_status
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 300, in check_internal_messages
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 848, in deliver_network_status
    return self._deliver_network_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 510, in _deliver_network_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
        local_handle = request()handle = mailbox._deliver_record(record, interface=self)

  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 856, in deliver_internal_messages
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    return self._deliver_internal_messages(internal_message)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 516, in _deliver_internal_messages
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: NASDAQ H=10

Training: Mamba on NASDAQ for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_093837-nac6c97n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/nac6c97n
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H22    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/nac6c97n
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2563020 Vali Loss: 0.1558620 Test Loss: 0.1346381
Validation loss decreased (inf --> 0.155862).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2543674 Vali Loss: 0.1557751 Test Loss: 0.1337934
Validation loss decreased (0.155862 --> 0.155775).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2530264 Vali Loss: 0.1550150 Test Loss: 0.1332167
Validation loss decreased (0.155775 --> 0.155015).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2520332 Vali Loss: 0.1544907 Test Loss: 0.1328998
Validation loss decreased (0.155015 --> 0.154491).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2515358 Vali Loss: 0.1537359 Test Loss: 0.1327009
Validation loss decreased (0.154491 --> 0.153736).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2513640 Vali Loss: 0.1550709 Test Loss: 0.1326076
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2514011 Vali Loss: 0.1538473 Test Loss: 0.1325596
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2563020463920001, 'val/loss': 0.15586199079241073, 'test/loss': 0.1346380774463926, '_timestamp': 1762328324.8655066}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2543674197160836, 'val/loss': 0.15577509573527745, 'test/loss': 0.13379337638616562, '_timestamp': 1762328326.84239}).
Epoch: 8, Steps: 132 | Train Loss: 0.2512388 Vali Loss: 0.1544327 Test Loss: 0.1325382
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2512583 Vali Loss: 0.1538791 Test Loss: 0.1325277
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2511768 Vali Loss: 0.1546133 Test Loss: 0.1325216
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2511310 Vali Loss: 0.1543489 Test Loss: 0.1325186
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2511022 Vali Loss: 0.1536543 Test Loss: 0.1325172
Validation loss decreased (0.153736 --> 0.153654).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2512009 Vali Loss: 0.1546342 Test Loss: 0.1325166
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2511068 Vali Loss: 0.1545709 Test Loss: 0.1325162
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2510871 Vali Loss: 0.1549676 Test Loss: 0.1325161
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2510399 Vali Loss: 0.1543680 Test Loss: 0.1325160
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2510683 Vali Loss: 0.1543520 Test Loss: 0.1325160
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2512153 Vali Loss: 0.1547643 Test Loss: 0.1325160
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2512075 Vali Loss: 0.1542513 Test Loss: 0.1325160
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2511316 Vali Loss: 0.1541747 Test Loss: 0.1325160
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2509509 Vali Loss: 0.1538635 Test Loss: 0.1325160
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2511469 Vali Loss: 0.1538452 Test Loss: 0.1325160
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.00014363547961693257, mae:0.00869136955589056, rmse:0.011984801851212978, r2:-0.026514172554016113, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0087, RMSE: 0.0120, RÂ²: -0.0265, MAPE: 3264882.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.629 MB of 0.630 MB uploadedwandb: \ 0.629 MB of 0.630 MB uploadedwandb: | 0.629 MB of 0.630 MB uploadedwandb: / 0.630 MB of 0.630 MB uploadedwandb: - 0.630 MB of 0.630 MB uploadedwandb: \ 0.630 MB of 0.703 MB uploadedwandb: | 0.703 MB of 0.703 MB uploadedwandb: / 0.703 MB of 0.703 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–‚â–‚â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–â–ˆâ–‚â–…â–‚â–†â–„â–â–†â–†â–‡â–…â–„â–†â–„â–„â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 42628
wandb:     model/trainable_params 42628
wandb:                  test/loss 0.13252
wandb:                 train/loss 0.25115
wandb:   val/directional_accuracy 50.39318
wandb:                   val/loss 0.15385
wandb:                    val/mae 0.00869
wandb:                   val/mape 326488250.0
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.02651
wandb:                   val/rmse 0.01198
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/nac6c97n
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_093837-nac6c97n/logs
Completed: NASDAQ H=22

Training: Mamba on NASDAQ for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_093955-5kgl383r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/5kgl383r
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H50    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/5kgl383r
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.2633448 Vali Loss: 0.1647384 Test Loss: 0.1453047
Validation loss decreased (inf --> 0.164738).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2617571 Vali Loss: 0.1641074 Test Loss: 0.1444465
Validation loss decreased (0.164738 --> 0.164107).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2603926 Vali Loss: 0.1637150 Test Loss: 0.1437629
Validation loss decreased (0.164107 --> 0.163715).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2607785 Vali Loss: 0.1638223 Test Loss: 0.1433320
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2621631 Vali Loss: 0.1637647 Test Loss: 0.1430936
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2594096 Vali Loss: 0.1633791 Test Loss: 0.1429730
Validation loss decreased (0.163715 --> 0.163379).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2584487 Vali Loss: 0.1635604 Test Loss: 0.1429129
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2633448236367919, 'val/loss': 0.16473844399054846, 'test/loss': 0.14530473202466965, '_timestamp': 1762328402.6028717}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2617571097657536, 'val/loss': 0.16410738229751587, 'test/loss': 0.1444464549422264, '_timestamp': 1762328404.6268587}).
Epoch: 8, Steps: 132 | Train Loss: 0.2586080 Vali Loss: 0.1633181 Test Loss: 0.1428834
Validation loss decreased (0.163379 --> 0.163318).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2588499 Vali Loss: 0.1634043 Test Loss: 0.1428684
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2656944 Vali Loss: 0.1635040 Test Loss: 0.1428610
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2690370 Vali Loss: 0.1634185 Test Loss: 0.1428575
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2607939 Vali Loss: 0.1635077 Test Loss: 0.1428558
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2595174 Vali Loss: 0.1634340 Test Loss: 0.1428549
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2597784 Vali Loss: 0.1635763 Test Loss: 0.1428545
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2650994 Vali Loss: 0.1633336 Test Loss: 0.1428543
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2700028 Vali Loss: 0.1635212 Test Loss: 0.1428542
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2589055 Vali Loss: 0.1636624 Test Loss: 0.1428542
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2599041 Vali Loss: 0.1637234 Test Loss: 0.1428542
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.00014797058247495443, mae:0.00870724767446518, rmse:0.012164316140115261, r2:-0.02157425880432129, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0087, RMSE: 0.0122, RÂ²: -0.0216, MAPE: 3436018.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.683 MB of 0.686 MB uploadedwandb: \ 0.683 MB of 0.686 MB uploadedwandb: | 0.686 MB of 0.686 MB uploadedwandb: / 0.686 MB of 0.686 MB uploadedwandb: - 0.686 MB of 0.758 MB uploadedwandb: \ 0.686 MB of 0.758 MB uploadedwandb: | 0.758 MB of 0.758 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–‚â–‚â–ƒâ–‚â–â–â–â–…â–‡â–‚â–‚â–‚â–…â–ˆâ–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–ˆâ–‡â–‚â–„â–â–‚â–„â–‚â–„â–ƒâ–…â–â–„â–†â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 17
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 53548
wandb:     model/trainable_params 53548
wandb:                  test/loss 0.14285
wandb:                 train/loss 0.2599
wandb:   val/directional_accuracy 50.09667
wandb:                   val/loss 0.16372
wandb:                    val/mae 0.00871
wandb:                   val/mape 343601825.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.02157
wandb:                   val/rmse 0.01216
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/5kgl383r
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_093955-5kgl383r/logs
Completed: NASDAQ H=50

Training: Mamba on NASDAQ for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_094101-dwe8maat
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/dwe8maat
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H100   Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/dwe8maat
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.2739329 Vali Loss: 0.1780014 Test Loss: 0.1580433
Validation loss decreased (inf --> 0.178001).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2737412 Vali Loss: 0.1801911 Test Loss: 0.1572926
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.2721675 Vali Loss: 0.1819681 Test Loss: 0.1567285
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2711011 Vali Loss: 0.1804123 Test Loss: 0.1563165
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2707598 Vali Loss: 0.1787556 Test Loss: 0.1560625
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2704709 Vali Loss: 0.1770764 Test Loss: 0.1559211
Validation loss decreased (0.178001 --> 0.177076).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2700779 Vali Loss: 0.1791614 Test Loss: 0.1558539
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.27393289311574054, 'val/loss': 0.17800138890743256, 'test/loss': 0.15804333984851837, '_timestamp': 1762328469.3645816}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2737411952935732, 'val/loss': 0.18019107282161712, 'test/loss': 0.15729255974292755, '_timestamp': 1762328471.3963501}).
Epoch: 8, Steps: 130 | Train Loss: 0.2704936 Vali Loss: 0.1808058 Test Loss: 0.1558173
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2706454 Vali Loss: 0.1805615 Test Loss: 0.1558010
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2699987 Vali Loss: 0.1759377 Test Loss: 0.1557926
Validation loss decreased (0.177076 --> 0.175938).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2704972 Vali Loss: 0.1805846 Test Loss: 0.1557882
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2700388 Vali Loss: 0.1790901 Test Loss: 0.1557861
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2701012 Vali Loss: 0.1805151 Test Loss: 0.1557850
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.2704551 Vali Loss: 0.1806993 Test Loss: 0.1557845
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.2702697 Vali Loss: 0.1784789 Test Loss: 0.1557843
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 130 | Train Loss: 0.2704255 Vali Loss: 0.1779687 Test Loss: 0.1557842
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 130 | Train Loss: 0.2701056 Vali Loss: 0.1788799 Test Loss: 0.1557841
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 130 | Train Loss: 0.2703304 Vali Loss: 0.1795608 Test Loss: 0.1557841
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 130 | Train Loss: 0.2705349 Vali Loss: 0.1803920 Test Loss: 0.1557841
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 130 | Train Loss: 0.2700562 Vali Loss: 0.1811233 Test Loss: 0.1557841
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.00015605593216605484, mae:0.008722483180463314, rmse:0.012492234818637371, r2:-0.02889072895050049, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0087, RMSE: 0.0125, RÂ²: -0.0289, MAPE: 1755704.12%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.747 MB of 0.752 MB uploadedwandb: \ 0.747 MB of 0.752 MB uploadedwandb: | 0.747 MB of 0.752 MB uploadedwandb: / 0.747 MB of 0.752 MB uploadedwandb: - 0.752 MB of 0.752 MB uploadedwandb: \ 0.752 MB of 0.752 MB uploadedwandb: | 0.752 MB of 0.825 MB uploadedwandb: / 0.825 MB of 0.825 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–â–ƒâ–ƒâ–â–ƒâ–â–â–‚â–‚â–‚â–â–‚â–ƒâ–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–„â–‚â–…â–‡â–†â–â–†â–…â–†â–‡â–„â–ƒâ–„â–…â–†â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 73048
wandb:     model/trainable_params 73048
wandb:                  test/loss 0.15578
wandb:                 train/loss 0.27006
wandb:   val/directional_accuracy 50.64935
wandb:                   val/loss 0.18112
wandb:                    val/mae 0.00872
wandb:                   val/mape 175570412.5
wandb:                    val/mse 0.00016
wandb:                     val/r2 -0.02889
wandb:                   val/rmse 0.01249
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/dwe8maat
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_094101-dwe8maat/logs
Completed: NASDAQ H=100

Training: Mamba on ABSA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_094218-t19eraqv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/t19eraqv
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H3       Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/t19eraqv
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 238
Epoch: 1, Steps: 133 | Train Loss: 0.3031713 Vali Loss: 0.1671447 Test Loss: 0.1571212
Validation loss decreased (inf --> 0.167145).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2948470 Vali Loss: 0.1612477 Test Loss: 0.1554512
Validation loss decreased (0.167145 --> 0.161248).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2863578 Vali Loss: 0.1635859 Test Loss: 0.1540404
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2818141 Vali Loss: 0.1635645 Test Loss: 0.1534657
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2794987 Vali Loss: 0.1605247 Test Loss: 0.1533132
Validation loss decreased (0.161248 --> 0.160525).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2781753 Vali Loss: 0.1659011 Test Loss: 0.1531356
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2778112 Vali Loss: 0.1676159 Test Loss: 0.1531009
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3031712922834812, 'val/loss': 0.16714470461010933, 'test/loss': 0.15712117217481136, '_timestamp': 1762328546.8587315}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.29484702501081883, 'val/loss': 0.16124766692519188, 'test/loss': 0.1554512046277523, '_timestamp': 1762328548.923497}).
Epoch: 8, Steps: 133 | Train Loss: 0.2776133 Vali Loss: 0.1687020 Test Loss: 0.1530809
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2771185 Vali Loss: 0.1700861 Test Loss: 0.1530631
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2798422 Vali Loss: 0.1659611 Test Loss: 0.1530558
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2787678 Vali Loss: 0.1630827 Test Loss: 0.1530504
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2774799 Vali Loss: 0.1666586 Test Loss: 0.1530494
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2773256 Vali Loss: 0.1658053 Test Loss: 0.1530482
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2775149 Vali Loss: 0.1656934 Test Loss: 0.1530476
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2778381 Vali Loss: 0.1665017 Test Loss: 0.1530474
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 238
test shape: (238, 3, 1) (238, 3, 1)
test shape: (238, 3, 1) (238, 3, 1)


	mse:0.0004496103210840374, mae:0.0161319300532341, rmse:0.021204017102718353, r2:0.01262521743774414, dtw:Not calculated


VAL - MSE: 0.0004, MAE: 0.0161, RMSE: 0.0212, RÂ²: 0.0126, MAPE: 1.34%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.492 MB of 0.492 MB uploadedwandb: \ 0.492 MB of 0.492 MB uploadedwandb: | 0.492 MB of 0.492 MB uploadedwandb: / 0.492 MB of 0.492 MB uploadedwandb: - 0.492 MB of 0.492 MB uploadedwandb: \ 0.541 MB of 0.613 MB uploadedwandb: | 0.541 MB of 0.613 MB uploadedwandb: / 0.613 MB of 0.613 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–ƒâ–‚â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–ƒâ–â–…â–†â–‡â–ˆâ–…â–ƒâ–…â–…â–…â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35218
wandb:     model/trainable_params 35218
wandb:                  test/loss 0.15305
wandb:                 train/loss 0.27784
wandb:   val/directional_accuracy 46.84874
wandb:                   val/loss 0.1665
wandb:                    val/mae 0.01613
wandb:                   val/mape 133.98541
wandb:                    val/mse 0.00045
wandb:                     val/r2 0.01263
wandb:                   val/rmse 0.0212
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/t19eraqv
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_094218-t19eraqv/logs
Completed: ABSA H=3

Training: Mamba on ABSA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_094323-ydiai8vo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/ydiai8vo
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H5       Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/ydiai8vo
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 236
Epoch: 1, Steps: 133 | Train Loss: 0.3088989 Vali Loss: 0.1685857 Test Loss: 0.1602641
Validation loss decreased (inf --> 0.168586).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.3012959 Vali Loss: 0.1665140 Test Loss: 0.1605071
Validation loss decreased (0.168586 --> 0.166514).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2950363 Vali Loss: 0.1635185 Test Loss: 0.1605325
Validation loss decreased (0.166514 --> 0.163518).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2898581 Vali Loss: 0.1612499 Test Loss: 0.1602611
Validation loss decreased (0.163518 --> 0.161250).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2878928 Vali Loss: 0.1753924 Test Loss: 0.1601379
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2867404 Vali Loss: 0.1723837 Test Loss: 0.1600261
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2859785 Vali Loss: 0.1658832 Test Loss: 0.1599716
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.30889892253212464, 'val/loss': 0.16858572140336037, 'test/loss': 0.16026407480239868, '_timestamp': 1762328610.5419822}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.30129594428646833, 'val/loss': 0.16651403345167637, 'test/loss': 0.160507139749825, '_timestamp': 1762328612.6269498}).
Epoch: 8, Steps: 133 | Train Loss: 0.2859113 Vali Loss: 0.1732481 Test Loss: 0.1599468
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2854085 Vali Loss: 0.1661213 Test Loss: 0.1599403
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2859459 Vali Loss: 0.1688849 Test Loss: 0.1599341
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2861650 Vali Loss: 0.1678732 Test Loss: 0.1599295
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2854408 Vali Loss: 0.1722674 Test Loss: 0.1599279
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2870865 Vali Loss: 0.1713215 Test Loss: 0.1599273
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2861529 Vali Loss: 0.1689958 Test Loss: 0.1599268
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
test shape: (236, 5, 1) (236, 5, 1)
test shape: (236, 5, 1) (236, 5, 1)


	mse:0.00045802516979165375, mae:0.0163111612200737, rmse:0.021401522681117058, r2:-6.079673767089844e-06, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0163, RMSE: 0.0214, RÂ²: -0.0000, MAPE: 1.27%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.527 MB of 0.527 MB uploadedwandb: \ 0.527 MB of 0.527 MB uploadedwandb: | 0.527 MB of 0.527 MB uploadedwandb: / 0.527 MB of 0.527 MB uploadedwandb: - 0.527 MB of 0.599 MB uploadedwandb: \ 0.599 MB of 0.599 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–â–â–â–â–‚â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–â–ˆâ–‡â–ƒâ–‡â–ƒâ–…â–„â–†â–†â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 13
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35998
wandb:     model/trainable_params 35998
wandb:                  test/loss 0.15993
wandb:                 train/loss 0.28615
wandb:   val/directional_accuracy 47.56356
wandb:                   val/loss 0.169
wandb:                    val/mae 0.01631
wandb:                   val/mape 126.52524
wandb:                    val/mse 0.00046
wandb:                     val/r2 -1e-05
wandb:                   val/rmse 0.0214
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/ydiai8vo
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_094323-ydiai8vo/logs
Exception in thread Exception in thread ChkStopThrNetStatThr:
:
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 286, in check_stop_status
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 268, in check_network_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 840, in deliver_stop_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 848, in deliver_network_status
    return self._deliver_stop_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 494, in _deliver_stop_status
    return self._deliver_network_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 510, in _deliver_network_status
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: ABSA H=5

Training: Mamba on ABSA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_094419-yxyutrnz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/yxyutrnz
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H10      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/yxyutrnz
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 231
Epoch: 1, Steps: 133 | Train Loss: 0.3146822 Vali Loss: 0.1780797 Test Loss: 0.1615114
Validation loss decreased (inf --> 0.178080).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.3112242 Vali Loss: 0.1799989 Test Loss: 0.1617071
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.3051082 Vali Loss: 0.1728824 Test Loss: 0.1616326
Validation loss decreased (0.178080 --> 0.172882).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.3045193 Vali Loss: 0.1671740 Test Loss: 0.1614908
Validation loss decreased (0.172882 --> 0.167174).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.3009685 Vali Loss: 0.1767034 Test Loss: 0.1614116
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.3015179 Vali Loss: 0.1673934 Test Loss: 0.1613463
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.3002735 Vali Loss: 0.1743524 Test Loss: 0.1613107
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3146821865461823, 'val/loss': 0.17807969078421593, 'test/loss': 0.16151143237948418, '_timestamp': 1762328666.595853}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3112242124358514, 'val/loss': 0.17999893985688686, 'test/loss': 0.16170713678002357, '_timestamp': 1762328668.622566}).
Epoch: 8, Steps: 133 | Train Loss: 0.3003307 Vali Loss: 0.1754874 Test Loss: 0.1612986
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.3006189 Vali Loss: 0.1691038 Test Loss: 0.1612864
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2995427 Vali Loss: 0.1700090 Test Loss: 0.1612818
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2992003 Vali Loss: 0.1676447 Test Loss: 0.1612811
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.3006350 Vali Loss: 0.1726306 Test Loss: 0.1612797
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.3003520 Vali Loss: 0.1728412 Test Loss: 0.1612792
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2992817 Vali Loss: 0.1759864 Test Loss: 0.1612790
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 231
test shape: (231, 10, 1) (231, 10, 1)
test shape: (231, 10, 1) (231, 10, 1)


	mse:0.00046365297748707235, mae:0.01639004983007908, rmse:0.021532602608203888, r2:-0.004282236099243164, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0164, RMSE: 0.0215, RÂ²: -0.0043, MAPE: 1.47%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.561 MB of 0.561 MB uploadedwandb: \ 0.561 MB of 0.561 MB uploadedwandb: | 0.561 MB of 0.561 MB uploadedwandb: / 0.561 MB of 0.561 MB uploadedwandb: - 0.561 MB of 0.633 MB uploadedwandb: \ 0.633 MB of 0.633 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–„â–‚â–‚â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–‡â–ƒâ–„â–‚â–‚â–ƒâ–â–â–ƒâ–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–â–ˆâ–â–†â–‡â–‚â–ƒâ–â–…â–…â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 13
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 37948
wandb:     model/trainable_params 37948
wandb:                  test/loss 0.16128
wandb:                 train/loss 0.29928
wandb:   val/directional_accuracy 49.25445
wandb:                   val/loss 0.17599
wandb:                    val/mae 0.01639
wandb:                   val/mape 147.29367
wandb:                    val/mse 0.00046
wandb:                     val/r2 -0.00428
wandb:                   val/rmse 0.02153
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/yxyutrnz
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_094419-yxyutrnz/logs
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread IntMsgThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 268, in check_network_status
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 300, in check_internal_messages
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 856, in deliver_internal_messages
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 848, in deliver_network_status
    return self._deliver_internal_messages(internal_message)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 516, in _deliver_internal_messages
    return self._deliver_network_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 510, in _deliver_network_status
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: ABSA H=10

Training: Mamba on ABSA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_094518-j7guyakt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/j7guyakt
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H22      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/j7guyakt
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 219
Epoch: 1, Steps: 132 | Train Loss: 0.3252476 Vali Loss: 0.1801121 Test Loss: 0.1564813
Validation loss decreased (inf --> 0.180112).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.3219973 Vali Loss: 0.1796400 Test Loss: 0.1565816
Validation loss decreased (0.180112 --> 0.179640).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.3193969 Vali Loss: 0.1791019 Test Loss: 0.1567026
Validation loss decreased (0.179640 --> 0.179102).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.3174250 Vali Loss: 0.1793249 Test Loss: 0.1567201
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.3162839 Vali Loss: 0.1791598 Test Loss: 0.1567312
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.3155492 Vali Loss: 0.1790310 Test Loss: 0.1566913
Validation loss decreased (0.179102 --> 0.179031).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.3153521 Vali Loss: 0.1792155 Test Loss: 0.1566958
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3252475552938201, 'val/loss': 0.18011213839054108, 'test/loss': 0.15648129475968225, '_timestamp': 1762328725.4518461}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.321997314578656, 'val/loss': 0.17963998019695282, 'test/loss': 0.15658155296530044, '_timestamp': 1762328727.487598}).
Epoch: 8, Steps: 132 | Train Loss: 0.3150440 Vali Loss: 0.1794222 Test Loss: 0.1566934
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.3149097 Vali Loss: 0.1791190 Test Loss: 0.1566946
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.3147866 Vali Loss: 0.1790102 Test Loss: 0.1566948
Validation loss decreased (0.179031 --> 0.179010).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.3146910 Vali Loss: 0.1792507 Test Loss: 0.1566945
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.3148347 Vali Loss: 0.1788808 Test Loss: 0.1566942
Validation loss decreased (0.179010 --> 0.178881).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.3148400 Vali Loss: 0.1790965 Test Loss: 0.1566944
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.3147164 Vali Loss: 0.1792499 Test Loss: 0.1566943
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.3148088 Vali Loss: 0.1796081 Test Loss: 0.1566943
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.3148285 Vali Loss: 0.1791950 Test Loss: 0.1566943
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.3148559 Vali Loss: 0.1790680 Test Loss: 0.1566943
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.3149928 Vali Loss: 0.1793392 Test Loss: 0.1566943
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.3148222 Vali Loss: 0.1794479 Test Loss: 0.1566943
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.3146033 Vali Loss: 0.1791983 Test Loss: 0.1566943
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.3148737 Vali Loss: 0.1789630 Test Loss: 0.1566943
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.3147468 Vali Loss: 0.1790222 Test Loss: 0.1566943
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 219
test shape: (219, 22, 1) (219, 22, 1)
test shape: (219, 22, 1) (219, 22, 1)


	mse:0.0004709883942268789, mae:0.016539130359888077, rmse:0.021702267229557037, r2:-0.005265474319458008, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0165, RMSE: 0.0217, RÂ²: -0.0053, MAPE: 1.36%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.628 MB of 0.629 MB uploadedwandb: \ 0.628 MB of 0.629 MB uploadedwandb: | 0.628 MB of 0.629 MB uploadedwandb: / 0.629 MB of 0.629 MB uploadedwandb: - 0.629 MB of 0.629 MB uploadedwandb: \ 0.629 MB of 0.702 MB uploadedwandb: | 0.702 MB of 0.702 MB uploadedwandb: / 0.702 MB of 0.702 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ƒâ–†â–ˆâ–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–…â–„â–‚â–„â–†â–ƒâ–‚â–…â–â–ƒâ–…â–ˆâ–„â–ƒâ–…â–†â–„â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 42628
wandb:     model/trainable_params 42628
wandb:                  test/loss 0.15669
wandb:                 train/loss 0.31475
wandb:   val/directional_accuracy 50.33703
wandb:                   val/loss 0.17902
wandb:                    val/mae 0.01654
wandb:                   val/mape 135.80569
wandb:                    val/mse 0.00047
wandb:                     val/r2 -0.00527
wandb:                   val/rmse 0.0217
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/j7guyakt
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_094518-j7guyakt/logs
Completed: ABSA H=22

Training: Mamba on ABSA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_094633-qtpz2laf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/qtpz2laf
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H50      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/qtpz2laf
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 191
Epoch: 1, Steps: 132 | Train Loss: 0.3457125 Vali Loss: 0.1805737 Test Loss: 0.1557199
Validation loss decreased (inf --> 0.180574).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.3429137 Vali Loss: 0.1806220 Test Loss: 0.1553494
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.3393749 Vali Loss: 0.1809102 Test Loss: 0.1551849
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.3411828 Vali Loss: 0.1810224 Test Loss: 0.1550996
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.3376583 Vali Loss: 0.1810180 Test Loss: 0.1550674
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.3361183 Vali Loss: 0.1810989 Test Loss: 0.1550497
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.3355236 Vali Loss: 0.1811335 Test Loss: 0.1550435
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3457124981690537, 'val/loss': 0.1805737391114235, 'test/loss': 0.15571987380584082, '_timestamp': 1762328800.8281562}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3429136646516395, 'val/loss': 0.180622028807799, 'test/loss': 0.1553493564327558, '_timestamp': 1762328802.8580725}).
Epoch: 8, Steps: 132 | Train Loss: 0.3358826 Vali Loss: 0.1811418 Test Loss: 0.1550396
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.3353287 Vali Loss: 0.1811416 Test Loss: 0.1550379
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.3479830 Vali Loss: 0.1811464 Test Loss: 0.1550369
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.3483835 Vali Loss: 0.1811540 Test Loss: 0.1550365
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 191
test shape: (191, 50, 1) (191, 50, 1)
test shape: (191, 50, 1) (191, 50, 1)


	mse:0.0004924027598462999, mae:0.016992047429084778, rmse:0.022190149873495102, r2:-0.012154102325439453, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0170, RMSE: 0.0222, RÂ²: -0.0122, MAPE: 1.21%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.737 MB of 0.740 MB uploadedwandb: \ 0.737 MB of 0.740 MB uploadedwandb: | 0.740 MB of 0.740 MB uploadedwandb: / 0.740 MB of 0.740 MB uploadedwandb: - 0.740 MB of 0.811 MB uploadedwandb: \ 0.811 MB of 0.811 MB uploadedwandb: | 0.811 MB of 0.811 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–
wandb:                 train/loss â–ƒâ–„â–‚â–â–â–â–â–ˆâ–ˆ
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–„â–„â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 53548
wandb:     model/trainable_params 53548
wandb:                  test/loss 0.15504
wandb:                 train/loss 0.34838
wandb:   val/directional_accuracy 49.58863
wandb:                   val/loss 0.18115
wandb:                    val/mae 0.01699
wandb:                   val/mape 121.13445
wandb:                    val/mse 0.00049
wandb:                     val/r2 -0.01215
wandb:                   val/rmse 0.02219
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/qtpz2laf
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_094633-qtpz2laf/logs
Completed: ABSA H=50

Training: Mamba on ABSA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_094722-ikpnd9k1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/ikpnd9k1
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H100     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/ikpnd9k1
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 141
Epoch: 1, Steps: 130 | Train Loss: 0.3825173 Vali Loss: 0.1949567 Test Loss: 0.1645957
Validation loss decreased (inf --> 0.194957).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.3819092 Vali Loss: 0.1939927 Test Loss: 0.1643275
Validation loss decreased (0.194957 --> 0.193993).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.3782293 Vali Loss: 0.1948356 Test Loss: 0.1642723
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.3758118 Vali Loss: 0.1965022 Test Loss: 0.1643067
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.3743647 Vali Loss: 0.1963244 Test Loss: 0.1643553
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.3741643 Vali Loss: 0.1970607 Test Loss: 0.1643677
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.3731485 Vali Loss: 0.1962737 Test Loss: 0.1643686
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3825172992853018, 'val/loss': 0.1949567049741745, 'test/loss': 0.16459567844867706, '_timestamp': 1762328849.6896312}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.38190917063217894, 'val/loss': 0.193992680311203, 'test/loss': 0.16432749927043916, '_timestamp': 1762328851.6770644}).
Epoch: 8, Steps: 130 | Train Loss: 0.3741647 Vali Loss: 0.1963744 Test Loss: 0.1643721
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.3737353 Vali Loss: 0.1965617 Test Loss: 0.1643744
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.3729689 Vali Loss: 0.1982321 Test Loss: 0.1643757
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.3735393 Vali Loss: 0.1960339 Test Loss: 0.1643760
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.3732674 Vali Loss: 0.1960779 Test Loss: 0.1643763
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 141
test shape: (141, 100, 1) (141, 100, 1)
test shape: (141, 100, 1) (141, 100, 1)


	mse:0.0005207785870879889, mae:0.01743066869676113, rmse:0.022820573300123215, r2:-0.00919485092163086, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0174, RMSE: 0.0228, RÂ²: -0.0092, MAPE: 1.15%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.816 MB of 0.821 MB uploadedwandb: \ 0.816 MB of 0.821 MB uploadedwandb: | 0.816 MB of 0.821 MB uploadedwandb: / 0.821 MB of 0.821 MB uploadedwandb: - 0.821 MB of 0.893 MB uploadedwandb: \ 0.893 MB of 0.893 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ƒâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–â–ƒâ–‚â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–„â–„â–†â–„â–„â–…â–ˆâ–ƒâ–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 73048
wandb:     model/trainable_params 73048
wandb:                  test/loss 0.16438
wandb:                 train/loss 0.37327
wandb:   val/directional_accuracy 49.58808
wandb:                   val/loss 0.19608
wandb:                    val/mae 0.01743
wandb:                   val/mape 114.9825
wandb:                    val/mse 0.00052
wandb:                     val/r2 -0.00919
wandb:                   val/rmse 0.02282
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/ikpnd9k1
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_094722-ikpnd9k1/logs
Completed: ABSA H=100

Training: Mamba on SASOL for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_094818-98n4xvhl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/98n4xvhl
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H3      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/98n4xvhl
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 3
============================================================
train 3772
val 211
test 212
Epoch: 1, Steps: 118 | Train Loss: 0.2304092 Vali Loss: 0.1088375 Test Loss: 0.1538957
Validation loss decreased (inf --> 0.108837).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2197946 Vali Loss: 0.1067985 Test Loss: 0.1492286
Validation loss decreased (0.108837 --> 0.106799).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 118 | Train Loss: 0.2100712 Vali Loss: 0.1081661 Test Loss: 0.1462707
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2049142 Vali Loss: 0.1034864 Test Loss: 0.1448475
Validation loss decreased (0.106799 --> 0.103486).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2024168 Vali Loss: 0.1050934 Test Loss: 0.1442229
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2013207 Vali Loss: 0.1034047 Test Loss: 0.1439393
Validation loss decreased (0.103486 --> 0.103405).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2005001 Vali Loss: 0.1034159 Test Loss: 0.1438064
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23040920672780377, 'val/loss': 0.10883748957089015, 'test/loss': 0.15389565591301238, '_timestamp': 1762328906.3035378}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.21979459627705106, 'val/loss': 0.10679853494678225, 'test/loss': 0.14922862499952316, '_timestamp': 1762328908.1196837}).
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
Epoch: 8, Steps: 118 | Train Loss: 0.2002206 Vali Loss: 0.1059102 Test Loss: 0.1437410
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2003113 Vali Loss: 0.1046886 Test Loss: 0.1437086
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.1995655 Vali Loss: 0.1039094 Test Loss: 0.1436933
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2004139 Vali Loss: 0.1074579 Test Loss: 0.1436858
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2001765 Vali Loss: 0.1050249 Test Loss: 0.1436818
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.1994653 Vali Loss: 0.1033353 Test Loss: 0.1436800
Validation loss decreased (0.103405 --> 0.103335).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.1994486 Vali Loss: 0.1042860 Test Loss: 0.1436791
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.1995559 Vali Loss: 0.1072709 Test Loss: 0.1436787
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.1997363 Vali Loss: 0.1017164 Test Loss: 0.1436785
Validation loss decreased (0.103335 --> 0.101716).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.2001258 Vali Loss: 0.1052229 Test Loss: 0.1436784
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.1992748 Vali Loss: 0.1028518 Test Loss: 0.1436784
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.2000331 Vali Loss: 0.1038810 Test Loss: 0.1436784
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.1997282 Vali Loss: 0.1060273 Test Loss: 0.1436784
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 118 | Train Loss: 0.1996574 Vali Loss: 0.1067942 Test Loss: 0.1436784
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 118 | Train Loss: 0.1998879 Vali Loss: 0.1059075 Test Loss: 0.1436784
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 118 | Train Loss: 0.2001428 Vali Loss: 0.1028865 Test Loss: 0.1436784
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 118 | Train Loss: 0.1998924 Vali Loss: 0.1026650 Test Loss: 0.1436784
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 118 | Train Loss: 0.1995938 Vali Loss: 0.1079619 Test Loss: 0.1436784
EarlyStopping counter: 9 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 118 | Train Loss: 0.1992440 Vali Loss: 0.1024503 Test Loss: 0.1436784
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 212
test shape: (212, 3, 1) (212, 3, 1)
test shape: (212, 3, 1) (212, 3, 1)


	mse:0.0022293718066066504, mae:0.03516839072108269, rmse:0.047216225415468216, r2:-0.011969208717346191, dtw:Not calculated


VAL - MSE: 0.0022, MAE: 0.0352, RMSE: 0.0472, RÂ²: -0.0120, MAPE: 11297043.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.462 MB of 0.462 MB uploadedwandb: \ 0.462 MB of 0.462 MB uploadedwandb: | 0.462 MB of 0.462 MB uploadedwandb: / 0.462 MB of 0.462 MB uploadedwandb: - 0.462 MB of 0.462 MB uploadedwandb: \ 0.462 MB of 0.462 MB uploadedwandb: | 0.511 MB of 0.584 MB uploadedwandb: / 0.511 MB of 0.584 MB uploadedwandb: - 0.584 MB of 0.584 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–‚â–â–‚â–â–â–â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–…â–ƒâ–ƒâ–†â–„â–ƒâ–‡â–…â–ƒâ–„â–‡â–â–…â–‚â–ƒâ–†â–‡â–†â–‚â–‚â–ˆâ–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 25
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35218
wandb:     model/trainable_params 35218
wandb:                  test/loss 0.14368
wandb:                 train/loss 0.19924
wandb:   val/directional_accuracy 53.30189
wandb:                   val/loss 0.10245
wandb:                    val/mae 0.03517
wandb:                   val/mape 1129704300.0
wandb:                    val/mse 0.00223
wandb:                     val/r2 -0.01197
wandb:                   val/rmse 0.04722
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/98n4xvhl
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_094818-98n4xvhl/logs
Completed: SASOL H=3

Training: Mamba on SASOL for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_094949-jahp7ac9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/jahp7ac9
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H5      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/jahp7ac9
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 5
============================================================
train 3770
val 209
test 210
Epoch: 1, Steps: 118 | Train Loss: 0.2333904 Vali Loss: 0.1103252 Test Loss: 0.1551043
Validation loss decreased (inf --> 0.110325).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2222215 Vali Loss: 0.1083017 Test Loss: 0.1496416
Validation loss decreased (0.110325 --> 0.108302).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 118 | Train Loss: 0.2107871 Vali Loss: 0.1052748 Test Loss: 0.1468132
Validation loss decreased (0.108302 --> 0.105275).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2060271 Vali Loss: 0.1060283 Test Loss: 0.1455563
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2031321 Vali Loss: 0.1074398 Test Loss: 0.1450451
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2019300 Vali Loss: 0.1046266 Test Loss: 0.1448327
Validation loss decreased (0.105275 --> 0.104627).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2014186 Vali Loss: 0.1071675 Test Loss: 0.1447373
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23339035103129127, 'val/loss': 0.11032524704933167, 'test/loss': 0.15510433592966624, '_timestamp': 1762328997.2187421}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.22222148740695694, 'val/loss': 0.10830173428569521, 'test/loss': 0.149641646870545, '_timestamp': 1762328999.0787978}).
Epoch: 8, Steps: 118 | Train Loss: 0.2007962 Vali Loss: 0.1061752 Test Loss: 0.1446935
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2013979 Vali Loss: 0.1059574 Test Loss: 0.1446705
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2007775 Vali Loss: 0.1071656 Test Loss: 0.1446600
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2013956 Vali Loss: 0.1052391 Test Loss: 0.1446543
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2010200 Vali Loss: 0.1069970 Test Loss: 0.1446516
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2009578 Vali Loss: 0.1055566 Test Loss: 0.1446502
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2015518 Vali Loss: 0.1029787 Test Loss: 0.1446495
Validation loss decreased (0.104627 --> 0.102979).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.2013543 Vali Loss: 0.1036263 Test Loss: 0.1446492
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.2012573 Vali Loss: 0.1043529 Test Loss: 0.1446491
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.2010570 Vali Loss: 0.1030791 Test Loss: 0.1446490
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.2009886 Vali Loss: 0.1032048 Test Loss: 0.1446490
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.2011293 Vali Loss: 0.1025418 Test Loss: 0.1446490
Validation loss decreased (0.102979 --> 0.102542).  Saving model ...
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.2012142 Vali Loss: 0.1054519 Test Loss: 0.1446490
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 118 | Train Loss: 0.2009836 Vali Loss: 0.1076001 Test Loss: 0.1446490
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 118 | Train Loss: 0.2009421 Vali Loss: 0.1059951 Test Loss: 0.1446490
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 118 | Train Loss: 0.2012936 Vali Loss: 0.1063864 Test Loss: 0.1446490
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 118 | Train Loss: 0.2014692 Vali Loss: 0.1033929 Test Loss: 0.1446490
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 118 | Train Loss: 0.2009177 Vali Loss: 0.1044855 Test Loss: 0.1446490
EarlyStopping counter: 6 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 118 | Train Loss: 0.2014363 Vali Loss: 0.1043715 Test Loss: 0.1446490
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 118 | Train Loss: 0.2010664 Vali Loss: 0.1058889 Test Loss: 0.1446490
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 118 | Train Loss: 0.2013669 Vali Loss: 0.1035854 Test Loss: 0.1446490
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 118 | Train Loss: 0.2008433 Vali Loss: 0.1064139 Test Loss: 0.1446490
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 210
test shape: (210, 5, 1) (210, 5, 1)
test shape: (210, 5, 1) (210, 5, 1)


	mse:0.0022311913780868053, mae:0.03523603454232216, rmse:0.04723548889160156, r2:-0.005312919616699219, dtw:Not calculated


VAL - MSE: 0.0022, MAE: 0.0352, RMSE: 0.0472, RÂ²: -0.0053, MAPE: 10999160.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.497 MB of 0.497 MB uploadedwandb: \ 0.497 MB of 0.497 MB uploadedwandb: | 0.497 MB of 0.571 MB uploadedwandb: / 0.571 MB of 0.571 MB uploadedwandb: - 0.571 MB of 0.571 MB uploadedwandb: \ 0.571 MB of 0.571 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–†â–ˆâ–„â–‡â–†â–†â–‡â–…â–‡â–…â–‚â–ƒâ–„â–‚â–‚â–â–…â–ˆâ–†â–†â–‚â–„â–„â–†â–‚â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 28
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35998
wandb:     model/trainable_params 35998
wandb:                  test/loss 0.14465
wandb:                 train/loss 0.20084
wandb:   val/directional_accuracy 51.19048
wandb:                   val/loss 0.10641
wandb:                    val/mae 0.03524
wandb:                   val/mape 1099916000.0
wandb:                    val/mse 0.00223
wandb:                     val/r2 -0.00531
wandb:                   val/rmse 0.04724
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/jahp7ac9
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_094949-jahp7ac9/logs
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread ChkStopThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 268, in check_network_status
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 286, in check_stop_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 840, in deliver_stop_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 848, in deliver_network_status
    return self._deliver_stop_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 494, in _deliver_stop_status
    return self._deliver_network_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 510, in _deliver_network_status
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: SASOL H=5

Training: Mamba on SASOL for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_095120-49wfgcw3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/49wfgcw3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H10     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/49wfgcw3
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 10
============================================================
train 3765
val 204
test 205
Epoch: 1, Steps: 118 | Train Loss: 0.2406788 Vali Loss: 0.1123103 Test Loss: 0.1581212
Validation loss decreased (inf --> 0.112310).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2331267 Vali Loss: 0.1101183 Test Loss: 0.1537539
Validation loss decreased (0.112310 --> 0.110118).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 118 | Train Loss: 0.2211032 Vali Loss: 0.1117146 Test Loss: 0.1496626
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2135842 Vali Loss: 0.1072034 Test Loss: 0.1478940
Validation loss decreased (0.110118 --> 0.107203).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2101773 Vali Loss: 0.1100801 Test Loss: 0.1472060
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2090008 Vali Loss: 0.1105404 Test Loss: 0.1469323
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2090440 Vali Loss: 0.1090248 Test Loss: 0.1468108
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2406788305458376, 'val/loss': 0.11231031481708799, 'test/loss': 0.1581212356686592, '_timestamp': 1762329088.381783}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23312674683787055, 'val/loss': 0.11011828907898494, 'test/loss': 0.15375390648841858, '_timestamp': 1762329090.3007164}).
Epoch: 8, Steps: 118 | Train Loss: 0.2087125 Vali Loss: 0.1125299 Test Loss: 0.1467541
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2092606 Vali Loss: 0.1102721 Test Loss: 0.1467260
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2084380 Vali Loss: 0.1077550 Test Loss: 0.1467126
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2081059 Vali Loss: 0.1084380 Test Loss: 0.1467060
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2087330 Vali Loss: 0.1092026 Test Loss: 0.1467027
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2084189 Vali Loss: 0.1113582 Test Loss: 0.1467011
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2081596 Vali Loss: 0.1113855 Test Loss: 0.1467003
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 205
test shape: (205, 10, 1) (205, 10, 1)
test shape: (205, 10, 1) (205, 10, 1)


	mse:0.002242519985884428, mae:0.03514242172241211, rmse:0.047355253249406815, r2:-0.010280251502990723, dtw:Not calculated


VAL - MSE: 0.0022, MAE: 0.0351, RMSE: 0.0474, RÂ²: -0.0103, MAPE: 9834387.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.535 MB of 0.535 MB uploadedwandb: \ 0.535 MB of 0.535 MB uploadedwandb: | 0.535 MB of 0.535 MB uploadedwandb: / 0.535 MB of 0.535 MB uploadedwandb: - 0.535 MB of 0.535 MB uploadedwandb: \ 0.535 MB of 0.607 MB uploadedwandb: | 0.607 MB of 0.607 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–‚â–â–‚â–â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–â–…â–…â–ƒâ–ˆâ–…â–‚â–ƒâ–„â–†â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 13
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 37948
wandb:     model/trainable_params 37948
wandb:                  test/loss 0.1467
wandb:                 train/loss 0.20816
wandb:   val/directional_accuracy 50.1897
wandb:                   val/loss 0.11139
wandb:                    val/mae 0.03514
wandb:                   val/mape 983438700.0
wandb:                    val/mse 0.00224
wandb:                     val/r2 -0.01028
wandb:                   val/rmse 0.04736
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/49wfgcw3
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_095120-49wfgcw3/logs
Completed: SASOL H=10

Training: Mamba on SASOL for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_095214-ctffvyno
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/ctffvyno
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H22     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/ctffvyno
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 22
============================================================
train 3753
val 192
test 193
Epoch: 1, Steps: 118 | Train Loss: 0.2556521 Vali Loss: 0.1134161 Test Loss: 0.1605874
Validation loss decreased (inf --> 0.113416).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2454627 Vali Loss: 0.1135860 Test Loss: 0.1558337
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 118 | Train Loss: 0.2312844 Vali Loss: 0.1133688 Test Loss: 0.1522444
Validation loss decreased (0.113416 --> 0.113369).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2238785 Vali Loss: 0.1123986 Test Loss: 0.1510098
Validation loss decreased (0.113369 --> 0.112399).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2213870 Vali Loss: 0.1119008 Test Loss: 0.1506809
Validation loss decreased (0.112399 --> 0.111901).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2209606 Vali Loss: 0.1116716 Test Loss: 0.1505292
Validation loss decreased (0.111901 --> 0.111672).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2222231 Vali Loss: 0.1115431 Test Loss: 0.1504657
Validation loss decreased (0.111672 --> 0.111543).  Saving model ...
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.25565207067687634, 'val/loss': 0.11341614897052447, 'test/loss': 0.16058742361409323, '_timestamp': 1762329141.3758562}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2454626627140126, 'val/loss': 0.11358603835105896, 'test/loss': 0.1558337498988424, '_timestamp': 1762329143.2452195}).
Epoch: 8, Steps: 118 | Train Loss: 0.2202505 Vali Loss: 0.1114933 Test Loss: 0.1504345
Validation loss decreased (0.111543 --> 0.111493).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2206099 Vali Loss: 0.1114681 Test Loss: 0.1504188
Validation loss decreased (0.111493 --> 0.111468).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2213211 Vali Loss: 0.1114554 Test Loss: 0.1504107
Validation loss decreased (0.111468 --> 0.111455).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2200246 Vali Loss: 0.1114490 Test Loss: 0.1504075
Validation loss decreased (0.111455 --> 0.111449).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2200071 Vali Loss: 0.1114459 Test Loss: 0.1504055
Validation loss decreased (0.111449 --> 0.111446).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2218701 Vali Loss: 0.1114442 Test Loss: 0.1504046
Validation loss decreased (0.111446 --> 0.111444).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2200917 Vali Loss: 0.1114434 Test Loss: 0.1504041
Validation loss decreased (0.111444 --> 0.111443).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.2205108 Vali Loss: 0.1114430 Test Loss: 0.1504039
Validation loss decreased (0.111443 --> 0.111443).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.2203431 Vali Loss: 0.1114428 Test Loss: 0.1504038
Validation loss decreased (0.111443 --> 0.111443).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.2197627 Vali Loss: 0.1114427 Test Loss: 0.1504038
Validation loss decreased (0.111443 --> 0.111443).  Saving model ...
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.2201483 Vali Loss: 0.1114427 Test Loss: 0.1504038
Validation loss decreased (0.111443 --> 0.111443).  Saving model ...
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.2203204 Vali Loss: 0.1114427 Test Loss: 0.1504038
Validation loss decreased (0.111443 --> 0.111443).  Saving model ...
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.2197809 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 118 | Train Loss: 0.2199068 Vali Loss: 0.1114427 Test Loss: 0.1504038
Validation loss decreased (0.111443 --> 0.111443).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 118 | Train Loss: 0.2197476 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 118 | Train Loss: 0.2211213 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 118 | Train Loss: 0.2211593 Vali Loss: 0.1114427 Test Loss: 0.1504038
Validation loss decreased (0.111443 --> 0.111443).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 118 | Train Loss: 0.2199434 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 118 | Train Loss: 0.2198541 Vali Loss: 0.1114427 Test Loss: 0.1504038
Validation loss decreased (0.111443 --> 0.111443).  Saving model ...
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 118 | Train Loss: 0.2203777 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 118 | Train Loss: 0.2208747 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 118 | Train Loss: 0.2205625 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 118 | Train Loss: 0.2200783 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 118 | Train Loss: 0.2200577 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 118 | Train Loss: 0.2204037 Vali Loss: 0.1114427 Test Loss: 0.1504038
Validation loss decreased (0.111443 --> 0.111443).  Saving model ...
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 118 | Train Loss: 0.2194825 Vali Loss: 0.1114427 Test Loss: 0.1504038
Validation loss decreased (0.111443 --> 0.111443).  Saving model ...
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 118 | Train Loss: 0.2195189 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.1641532182693482e-14
Epoch: 35, Steps: 118 | Train Loss: 0.2211314 Vali Loss: 0.1114427 Test Loss: 0.1504038
Validation loss decreased (0.111443 --> 0.111443).  Saving model ...
Updating learning rate to 5.820766091346741e-15
Epoch: 36, Steps: 118 | Train Loss: 0.2201101 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.9103830456733705e-15
Epoch: 37, Steps: 118 | Train Loss: 0.2199343 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.4551915228366853e-15
Epoch: 38, Steps: 118 | Train Loss: 0.2205444 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.275957614183426e-16
Epoch: 39, Steps: 118 | Train Loss: 0.2207301 Vali Loss: 0.1114427 Test Loss: 0.1504038
Validation loss decreased (0.111443 --> 0.111443).  Saving model ...
Updating learning rate to 3.637978807091713e-16
Epoch: 40, Steps: 118 | Train Loss: 0.2198356 Vali Loss: 0.1114427 Test Loss: 0.1504038
Validation loss decreased (0.111443 --> 0.111443).  Saving model ...
Updating learning rate to 1.8189894035458566e-16
Epoch: 41, Steps: 118 | Train Loss: 0.2197414 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.094947017729283e-17
Epoch: 42, Steps: 118 | Train Loss: 0.2198937 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.5474735088646414e-17
Epoch: 43, Steps: 118 | Train Loss: 0.2206122 Vali Loss: 0.1114427 Test Loss: 0.1504038
Validation loss decreased (0.111443 --> 0.111443).  Saving model ...
Updating learning rate to 2.2737367544323207e-17
Epoch: 44, Steps: 118 | Train Loss: 0.2204464 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.1368683772161604e-17
Epoch: 45, Steps: 118 | Train Loss: 0.2213022 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.684341886080802e-18
Epoch: 46, Steps: 118 | Train Loss: 0.2212659 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.842170943040401e-18
Epoch: 47, Steps: 118 | Train Loss: 0.2199207 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.4210854715202004e-18
Epoch: 48, Steps: 118 | Train Loss: 0.2201337 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.105427357601002e-19
Epoch: 49, Steps: 118 | Train Loss: 0.2207330 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.552713678800501e-19
Epoch: 50, Steps: 118 | Train Loss: 0.2199597 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.7763568394002505e-19
Epoch: 51, Steps: 118 | Train Loss: 0.2199504 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 8 out of 10
Updating learning rate to 8.881784197001253e-20
Epoch: 52, Steps: 118 | Train Loss: 0.2198265 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.4408920985006264e-20
Epoch: 53, Steps: 118 | Train Loss: 0.2198828 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 193
test shape: (193, 22, 1) (193, 22, 1)
test shape: (193, 22, 1) (193, 22, 1)


	mse:0.0022675301879644394, mae:0.035104043781757355, rmse:0.04761859029531479, r2:-0.010428190231323242, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0351, RMSE: 0.0476, RÂ²: -0.0104, MAPE: 8188427.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.591 MB of 0.592 MB uploadedwandb: \ 0.591 MB of 0.592 MB uploadedwandb: | 0.592 MB of 0.592 MB uploadedwandb: / 0.592 MB of 0.671 MB uploadedwandb: - 0.592 MB of 0.671 MB uploadedwandb: \ 0.592 MB of 0.671 MB uploadedwandb: | 0.671 MB of 0.671 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–‚â–‚â–â–‚â–‚â–â–‚â–â–‚â–â–â–â–â–â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–â–‚â–â–â–‚â–â–â–â–‚â–‚â–‚â–â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 52
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 42628
wandb:     model/trainable_params 42628
wandb:                  test/loss 0.1504
wandb:                 train/loss 0.21988
wandb:   val/directional_accuracy 50.16038
wandb:                   val/loss 0.11144
wandb:                    val/mae 0.0351
wandb:                   val/mape 818842700.0
wandb:                    val/mse 0.00227
wandb:                     val/r2 -0.01043
wandb:                   val/rmse 0.04762
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/ctffvyno
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_095214-ctffvyno/logs
Completed: SASOL H=22

Training: Mamba on SASOL for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_095432-qoy65crf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/qoy65crf
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H50     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/qoy65crf
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 50
============================================================
train 3725
val 164
test 165
Epoch: 1, Steps: 117 | Train Loss: 0.2879258 Vali Loss: 0.1196701 Test Loss: 0.1652908
Validation loss decreased (inf --> 0.119670).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 117 | Train Loss: 0.2798147 Vali Loss: 0.1224882 Test Loss: 0.1621795
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 117 | Train Loss: 0.2640119 Vali Loss: 0.1179545 Test Loss: 0.1613773
Validation loss decreased (0.119670 --> 0.117954).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 117 | Train Loss: 0.2564386 Vali Loss: 0.1154840 Test Loss: 0.1614184
Validation loss decreased (0.117954 --> 0.115484).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 117 | Train Loss: 0.2527200 Vali Loss: 0.1145394 Test Loss: 0.1615200
Validation loss decreased (0.115484 --> 0.114539).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 117 | Train Loss: 0.2517435 Vali Loss: 0.1145927 Test Loss: 0.1615821
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 117 | Train Loss: 0.2510648 Vali Loss: 0.1124983 Test Loss: 0.1616304
Validation loss decreased (0.114539 --> 0.112498).  Saving model ...
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2879257719231467, 'val/loss': 0.11967014148831367, 'test/loss': 0.1652907927831014, '_timestamp': 1762329280.53358}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2798147235925381, 'val/loss': 0.12248819693922997, 'test/loss': 0.1621795172492663, '_timestamp': 1762329282.5897796}).
Epoch: 8, Steps: 117 | Train Loss: 0.2512406 Vali Loss: 0.1091539 Test Loss: 0.1616403
Validation loss decreased (0.112498 --> 0.109154).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 117 | Train Loss: 0.2505070 Vali Loss: 0.1100802 Test Loss: 0.1616443
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 117 | Train Loss: 0.2512545 Vali Loss: 0.1162158 Test Loss: 0.1616491
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 117 | Train Loss: 0.2505639 Vali Loss: 0.1143381 Test Loss: 0.1616503
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 117 | Train Loss: 0.2507978 Vali Loss: 0.1123553 Test Loss: 0.1616519
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 117 | Train Loss: 0.2511379 Vali Loss: 0.1150365 Test Loss: 0.1616523
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 117 | Train Loss: 0.2505010 Vali Loss: 0.1092295 Test Loss: 0.1616525
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 117 | Train Loss: 0.2504112 Vali Loss: 0.1150372 Test Loss: 0.1616527
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 117 | Train Loss: 0.2509028 Vali Loss: 0.1190745 Test Loss: 0.1616527
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 117 | Train Loss: 0.2512469 Vali Loss: 0.1099135 Test Loss: 0.1616527
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 117 | Train Loss: 0.2521809 Vali Loss: 0.1174420 Test Loss: 0.1616527
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 165
test shape: (165, 50, 1) (165, 50, 1)
test shape: (165, 50, 1) (165, 50, 1)


	mse:0.002066132379695773, mae:0.03380599990487099, rmse:0.0454547293484211, r2:-0.008478164672851562, dtw:Not calculated


VAL - MSE: 0.0021, MAE: 0.0338, RMSE: 0.0455, RÂ²: -0.0085, MAPE: 7459519.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.630 MB of 0.632 MB uploadedwandb: \ 0.630 MB of 0.632 MB uploadedwandb: | 0.632 MB of 0.632 MB uploadedwandb: / 0.632 MB of 0.705 MB uploadedwandb: - 0.632 MB of 0.705 MB uploadedwandb: \ 0.705 MB of 0.705 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–‚â–…â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–…â–…â–…â–ƒâ–â–‚â–†â–…â–ƒâ–…â–â–…â–ˆâ–‚â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 17
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 53548
wandb:     model/trainable_params 53548
wandb:                  test/loss 0.16165
wandb:                 train/loss 0.25218
wandb:   val/directional_accuracy 49.40012
wandb:                   val/loss 0.11744
wandb:                    val/mae 0.03381
wandb:                   val/mape 745951950.0
wandb:                    val/mse 0.00207
wandb:                     val/r2 -0.00848
wandb:                   val/rmse 0.04545
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/qoy65crf
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_095432-qoy65crf/logs
Completed: SASOL H=50

Training: Mamba on SASOL for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_095538-2zbk8hyb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/2zbk8hyb
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H100    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/2zbk8hyb
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 100
============================================================
train 3675
val 114
test 115
Epoch: 1, Steps: 115 | Train Loss: 0.3502283 Vali Loss: 0.1349269 Test Loss: 0.1729450
Validation loss decreased (inf --> 0.134927).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 115 | Train Loss: 0.3393322 Vali Loss: 0.1358073 Test Loss: 0.1727349
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 115 | Train Loss: 0.3184477 Vali Loss: 0.1313805 Test Loss: 0.1744700
Validation loss decreased (0.134927 --> 0.131381).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 115 | Train Loss: 0.3117521 Vali Loss: 0.1297756 Test Loss: 0.1752041
Validation loss decreased (0.131381 --> 0.129776).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 115 | Train Loss: 0.3094026 Vali Loss: 0.1293053 Test Loss: 0.1753702
Validation loss decreased (0.129776 --> 0.129305).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 115 | Train Loss: 0.3081623 Vali Loss: 0.1288759 Test Loss: 0.1754912
Validation loss decreased (0.129305 --> 0.128876).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 115 | Train Loss: 0.3090302 Vali Loss: 0.1281108 Test Loss: 0.1755394
Validation loss decreased (0.128876 --> 0.128111).  Saving model ...
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.35022834383923074, 'val/loss': 0.13492687046527863, 'test/loss': 0.172945037484169, '_timestamp': 1762329347.1139357}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.33933224859444994, 'val/loss': 0.135807316750288, 'test/loss': 0.17273494973778725, '_timestamp': 1762329348.9034717}).
Epoch: 8, Steps: 115 | Train Loss: 0.3082744 Vali Loss: 0.1276743 Test Loss: 0.1755531
Validation loss decreased (0.128111 --> 0.127674).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 115 | Train Loss: 0.3082769 Vali Loss: 0.1288245 Test Loss: 0.1755688
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 115 | Train Loss: 0.3081176 Vali Loss: 0.1277128 Test Loss: 0.1755725
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 115 | Train Loss: 0.3083311 Vali Loss: 0.1261496 Test Loss: 0.1755763
Validation loss decreased (0.127674 --> 0.126150).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 115 | Train Loss: 0.3089729 Vali Loss: 0.1277878 Test Loss: 0.1755775
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 115 | Train Loss: 0.3084225 Vali Loss: 0.1284459 Test Loss: 0.1755785
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 115 | Train Loss: 0.3082564 Vali Loss: 0.1284188 Test Loss: 0.1755788
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 115 | Train Loss: 0.3080096 Vali Loss: 0.1278930 Test Loss: 0.1755790
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 115 | Train Loss: 0.3078651 Vali Loss: 0.1293649 Test Loss: 0.1755791
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 115 | Train Loss: 0.3078618 Vali Loss: 0.1270288 Test Loss: 0.1755791
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 115 | Train Loss: 0.3081300 Vali Loss: 0.1289014 Test Loss: 0.1755791
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 115 | Train Loss: 0.3083811 Vali Loss: 0.1286056 Test Loss: 0.1755791
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 115 | Train Loss: 0.3081079 Vali Loss: 0.1292680 Test Loss: 0.1755791
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 115 | Train Loss: 0.3076080 Vali Loss: 0.1292471 Test Loss: 0.1755791
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 115
test shape: (115, 100, 1) (115, 100, 1)
test shape: (115, 100, 1) (115, 100, 1)


	mse:0.002001367509365082, mae:0.03322107344865799, rmse:0.04473664611577988, r2:-0.008540511131286621, dtw:Not calculated


VAL - MSE: 0.0020, MAE: 0.0332, RMSE: 0.0447, RÂ²: -0.0085, MAPE: 7615237.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.674 MB of 0.679 MB uploadedwandb: \ 0.679 MB of 0.679 MB uploadedwandb: | 0.679 MB of 0.679 MB uploadedwandb: / 0.679 MB of 0.752 MB uploadedwandb: - 0.752 MB of 0.752 MB uploadedwandb: \ 0.752 MB of 0.752 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–„â–‚â–â–‚â–â–â–â–â–‚â–‚â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–…â–…â–„â–ƒâ–…â–ƒâ–â–ƒâ–„â–„â–ƒâ–…â–‚â–…â–„â–…â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 73048
wandb:     model/trainable_params 73048
wandb:                  test/loss 0.17558
wandb:                 train/loss 0.30761
wandb:   val/directional_accuracy 49.39833
wandb:                   val/loss 0.12925
wandb:                    val/mae 0.03322
wandb:                   val/mape 761523750.0
wandb:                    val/mse 0.002
wandb:                     val/r2 -0.00854
wandb:                   val/rmse 0.04474
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/2zbk8hyb
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_095538-2zbk8hyb/logs
Completed: SASOL H=100

Mamba training completed for all datasets!
