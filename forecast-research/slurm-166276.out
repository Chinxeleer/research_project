##############################################################################
# Training Mamba Model on All Datasets
##############################################################################
Training: Mamba on NVIDIA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_233431-i22ghab0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/i22ghab0
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H3     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/i22ghab0
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 3
============================================================
train 2201
val 123
test 125
Epoch: 1, Steps: 69 | Train Loss: 0.3915596 Vali Loss: 0.1551287 Test Loss: 0.3049048
Validation loss decreased (inf --> 0.155129).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 69 | Train Loss: 0.3614848 Vali Loss: 0.1456772 Test Loss: 0.2506334
Validation loss decreased (0.155129 --> 0.145677).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 69 | Train Loss: 0.3213995 Vali Loss: 0.1409805 Test Loss: 0.2292196
Validation loss decreased (0.145677 --> 0.140980).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 69 | Train Loss: 0.3012729 Vali Loss: 0.1425635 Test Loss: 0.2245457
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 69 | Train Loss: 0.2912284 Vali Loss: 0.1393999 Test Loss: 0.2230783
Validation loss decreased (0.140980 --> 0.139400).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 69 | Train Loss: 0.2856933 Vali Loss: 0.1396365 Test Loss: 0.2226011
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 69 | Train Loss: 0.2852532 Vali Loss: 0.1409996 Test Loss: 0.2222660
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.39155956217344257, 'val/loss': 0.15512867830693722, 'test/loss': 0.304904792457819, '_timestamp': 1762292092.3128405}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3614848359771397, 'val/loss': 0.14567718468606472, 'test/loss': 0.2506333887577057, '_timestamp': 1762292094.0267694}).
Epoch: 8, Steps: 69 | Train Loss: 0.2830093 Vali Loss: 0.1389199 Test Loss: 0.2220833
Validation loss decreased (0.139400 --> 0.138920).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 69 | Train Loss: 0.2824868 Vali Loss: 0.1398565 Test Loss: 0.2220208
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 69 | Train Loss: 0.2835992 Vali Loss: 0.1385152 Test Loss: 0.2219915
Validation loss decreased (0.138920 --> 0.138515).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 69 | Train Loss: 0.2825380 Vali Loss: 0.1384714 Test Loss: 0.2219726
Validation loss decreased (0.138515 --> 0.138471).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 69 | Train Loss: 0.2833303 Vali Loss: 0.1387146 Test Loss: 0.2219650
EarlyStopping counter: 1 out of 5
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 69 | Train Loss: 0.2814457 Vali Loss: 0.1408165 Test Loss: 0.2219601
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 69 | Train Loss: 0.2829669 Vali Loss: 0.1420185 Test Loss: 0.2219582
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 69 | Train Loss: 0.2833819 Vali Loss: 0.1390656 Test Loss: 0.2219570
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 69 | Train Loss: 0.2822472 Vali Loss: 0.1385913 Test Loss: 0.2219565
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 125
test shape: (125, 3, 1) (125, 3, 1)
test shape: (125, 3, 1) (125, 3, 1)


	mse:0.0006606000824831426, mae:0.016366364434361458, rmse:0.025702141225337982, r2:-0.01511383056640625, dtw:Not calculated


VAL - MSE: 0.0007, MAE: 0.0164, RMSE: 0.0257, RÂ²: -0.0151, MAPE: 28992042.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.454 MB of 0.455 MB uploadedwandb: \ 0.454 MB of 0.455 MB uploadedwandb: | 0.454 MB of 0.455 MB uploadedwandb: / 0.455 MB of 0.455 MB uploadedwandb: - 0.455 MB of 0.455 MB uploadedwandb: \ 0.455 MB of 0.455 MB uploadedwandb: | 0.455 MB of 0.455 MB uploadedwandb: / 0.455 MB of 0.455 MB uploadedwandb: - 0.503 MB of 0.575 MB uploadedwandb: \ 0.503 MB of 0.575 MB uploadedwandb: | 0.575 MB of 0.575 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–ˆâ–ƒâ–ƒâ–…â–‚â–ƒâ–â–â–â–…â–‡â–‚â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35218
wandb:     model/trainable_params 35218
wandb:                  test/loss 0.22196
wandb:                 train/loss 0.28225
wandb:   val/directional_accuracy 45.2
wandb:                   val/loss 0.13859
wandb:                    val/mae 0.01637
wandb:                   val/mape 2899204200.0
wandb:                    val/mse 0.00066
wandb:                     val/r2 -0.01511
wandb:                   val/rmse 0.0257
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/i22ghab0
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_233431-i22ghab0/logs
Completed: NVIDIA H=3

Training: Mamba on NVIDIA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_233550-thcb9y14
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/thcb9y14
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H5     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/thcb9y14
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 5
============================================================
train 2199
val 121
test 123
Epoch: 1, Steps: 69 | Train Loss: 0.3986459 Vali Loss: 0.1593339 Test Loss: 0.3155991
Validation loss decreased (inf --> 0.159334).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 69 | Train Loss: 0.3696409 Vali Loss: 0.1505073 Test Loss: 0.2668871
Validation loss decreased (0.159334 --> 0.150507).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 69 | Train Loss: 0.3188341 Vali Loss: 0.1482664 Test Loss: 0.2392975
Validation loss decreased (0.150507 --> 0.148266).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 69 | Train Loss: 0.2945070 Vali Loss: 0.1453826 Test Loss: 0.2332862
Validation loss decreased (0.148266 --> 0.145383).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 69 | Train Loss: 0.2901300 Vali Loss: 0.1440437 Test Loss: 0.2317450
Validation loss decreased (0.145383 --> 0.144044).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 69 | Train Loss: 0.2873554 Vali Loss: 0.1451035 Test Loss: 0.2309392
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 69 | Train Loss: 0.2867915 Vali Loss: 0.1475360 Test Loss: 0.2306777
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 69 | Train Loss: 0.2868436 Vali Loss: 0.1451222 Test Loss: 0.2305190
EarlyStopping counter: 3 out of 5
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 69 | Train Loss: 0.2857580 Vali Loss: 0.1472241 Test Loss: 0.2304497
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 69 | Train Loss: 0.2864267 Vali Loss: 0.1454708 Test Loss: 0.2304101
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 123
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.39864592621291894, 'val/loss': 0.15933385863900185, 'test/loss': 0.31559906527400017, '_timestamp': 1762292158.81091}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.36964094811591547, 'val/loss': 0.15050730481743813, 'test/loss': 0.2668870538473129, '_timestamp': 1762292160.0698452}).
test shape: (123, 5, 1) (123, 5, 1)
test shape: (123, 5, 1) (123, 5, 1)


	mse:0.0006800285773351789, mae:0.016498493030667305, rmse:0.026077358052134514, r2:-0.03395402431488037, dtw:Not calculated


VAL - MSE: 0.0007, MAE: 0.0165, RMSE: 0.0261, RÂ²: -0.0340, MAPE: 29332982.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.475 MB of 0.476 MB uploadedwandb: \ 0.476 MB of 0.476 MB uploadedwandb: | 0.476 MB of 0.546 MB uploadedwandb: / 0.546 MB of 0.546 MB uploadedwandb: - 0.546 MB of 0.546 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–â–â–â–â–
wandb:                 train/loss â–ˆâ–ƒâ–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–â–ƒâ–‡â–ƒâ–†â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 9
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35998
wandb:     model/trainable_params 35998
wandb:                  test/loss 0.23041
wandb:                 train/loss 0.28643
wandb:   val/directional_accuracy 40.65041
wandb:                   val/loss 0.14547
wandb:                    val/mae 0.0165
wandb:                   val/mape 2933298200.0
wandb:                    val/mse 0.00068
wandb:                     val/r2 -0.03395
wandb:                   val/rmse 0.02608
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/thcb9y14
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_233550-thcb9y14/logs
Completed: NVIDIA H=5

Training: Mamba on NVIDIA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_233633-f5bo1qun
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/f5bo1qun
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H10    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/f5bo1qun
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 10
============================================================
train 2194
val 116
test 118
Epoch: 1, Steps: 69 | Train Loss: 0.4182519 Vali Loss: 0.1672099 Test Loss: 0.3357513
Validation loss decreased (inf --> 0.167210).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 69 | Train Loss: 0.3964292 Vali Loss: 0.1596930 Test Loss: 0.2925321
Validation loss decreased (0.167210 --> 0.159693).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 69 | Train Loss: 0.3541043 Vali Loss: 0.1563409 Test Loss: 0.2549032
Validation loss decreased (0.159693 --> 0.156341).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 69 | Train Loss: 0.3236219 Vali Loss: 0.1585849 Test Loss: 0.2411051
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 69 | Train Loss: 0.3149931 Vali Loss: 0.1557106 Test Loss: 0.2373557
Validation loss decreased (0.156341 --> 0.155711).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 69 | Train Loss: 0.3123693 Vali Loss: 0.1567703 Test Loss: 0.2357625
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 69 | Train Loss: 0.3108973 Vali Loss: 0.1554540 Test Loss: 0.2352131
Validation loss decreased (0.155711 --> 0.155454).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 69 | Train Loss: 0.3106347 Vali Loss: 0.1530129 Test Loss: 0.2348551
Validation loss decreased (0.155454 --> 0.153013).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 69 | Train Loss: 0.3101277 Vali Loss: 0.1553183 Test Loss: 0.2347228
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 69 | Train Loss: 0.3097406 Vali Loss: 0.1557329 Test Loss: 0.2346456
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 69 | Train Loss: 0.3093824 Vali Loss: 0.1534600 Test Loss: 0.2346050
EarlyStopping counter: 3 out of 5
Updating learning rate to 9.765625e-08
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.41825188415637915, 'val/loss': 0.16720985621213913, 'test/loss': 0.335751298815012, '_timestamp': 1762292199.313917}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3964291646860648, 'val/loss': 0.15969295799732208, 'test/loss': 0.2925320528447628, '_timestamp': 1762292200.5836496}).
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3964291646860648, 'val/loss': 0.15969295799732208, 'test/loss': 0.2925320528447628, '_timestamp': 1762292200.5836496}).
Epoch: 12, Steps: 69 | Train Loss: 0.3096609 Vali Loss: 0.1553137 Test Loss: 0.2345915
EarlyStopping counter: 4 out of 5
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 69 | Train Loss: 0.3095608 Vali Loss: 0.1538698 Test Loss: 0.2345812
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 118
test shape: (118, 10, 1) (118, 10, 1)
test shape: (118, 10, 1) (118, 10, 1)


	mse:0.000702237302903086, mae:0.016886817291378975, rmse:0.026499761268496513, r2:-0.03624391555786133, dtw:Not calculated


VAL - MSE: 0.0007, MAE: 0.0169, RMSE: 0.0265, RÂ²: -0.0362, MAPE: 32753830.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.518 MB of 0.519 MB uploadedwandb: \ 0.519 MB of 0.519 MB uploadedwandb: | 0.519 MB of 0.519 MB uploadedwandb: / 0.519 MB of 0.590 MB uploadedwandb: - 0.590 MB of 0.590 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–ˆâ–„â–†â–„â–â–„â–„â–‚â–„â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 37948
wandb:     model/trainable_params 37948
wandb:                  test/loss 0.23458
wandb:                 train/loss 0.30956
wandb:   val/directional_accuracy 41.52542
wandb:                   val/loss 0.15387
wandb:                    val/mae 0.01689
wandb:                   val/mape 3275383000.0
wandb:                    val/mse 0.0007
wandb:                     val/r2 -0.03624
wandb:                   val/rmse 0.0265
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/f5bo1qun
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_233633-f5bo1qun/logs
Completed: NVIDIA H=10

Training: Mamba on NVIDIA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_233718-gzmd1zs1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/gzmd1zs1
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H22    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/gzmd1zs1
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 22
============================================================
train 2182
val 104
test 106
Epoch: 1, Steps: 69 | Train Loss: 0.4589792 Vali Loss: 0.1674203 Test Loss: 0.4069257
Validation loss decreased (inf --> 0.167420).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 69 | Train Loss: 0.4388712 Vali Loss: 0.1645038 Test Loss: 0.3545117
Validation loss decreased (0.167420 --> 0.164504).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 69 | Train Loss: 0.3883676 Vali Loss: 0.1624484 Test Loss: 0.2994977
Validation loss decreased (0.164504 --> 0.162448).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 69 | Train Loss: 0.3657834 Vali Loss: 0.1691868 Test Loss: 0.2856253
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 69 | Train Loss: 0.3561355 Vali Loss: 0.1645092 Test Loss: 0.2796283
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 69 | Train Loss: 0.3522758 Vali Loss: 0.1628408 Test Loss: 0.2778438
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 69 | Train Loss: 0.3532235 Vali Loss: 0.1652569 Test Loss: 0.2769658
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 69 | Train Loss: 0.3524376 Vali Loss: 0.1672456 Test Loss: 0.2765074
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 106
test shape: (106, 22, 1) (106, 22, 1)
test shape: (106, 22, 1) (106, 22, 1)


	mse:0.0007701427675783634, mae:0.017802368849515915, rmse:0.027751445770263672, r2:-0.05841970443725586, dtw:Not calculated


VAL - MSE: 0.0008, MAE: 0.0178, RMSE: 0.0278, RÂ²: -0.0584, MAPE: 42168116.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.597 MB of 0.598 MB uploadedwandb: \ 0.598 MB of 0.598 MB uploadedwandb: | 0.598 MB of 0.598 MB uploadedwandb: / 0.598 MB of 0.668 MB uploadedwandb: - 0.668 MB of 0.668 MB uploadedwandb: \ 0.668 MB of 0.668 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–„â–…â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–â–â–
wandb:                 train/loss â–ˆâ–„â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ˆâ–ƒâ–â–„â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 7
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 42628
wandb:     model/trainable_params 42628
wandb:                  test/loss 0.27651
wandb:                 train/loss 0.35244
wandb:   val/directional_accuracy 41.55436
wandb:                   val/loss 0.16725
wandb:                    val/mae 0.0178
wandb:                   val/mape 4216811600.0
wandb:                    val/mse 0.00077
wandb:                     val/r2 -0.05842
wandb:                   val/rmse 0.02775
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/gzmd1zs1
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_233718-gzmd1zs1/logs
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.458979196306588, 'val/loss': 0.16742025315761566, 'test/loss': 0.40692566335201263, '_timestamp': 1762292246.0082948}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.43887121910634247, 'val/loss': 0.1645037680864334, 'test/loss': 0.35451171174645424, '_timestamp': 1762292247.2724187}).
Completed: NVIDIA H=22

Training: Mamba on NVIDIA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_233758-s6ik9tyx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/s6ik9tyx
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H50    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/s6ik9tyx
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 50
============================================================
train 2154
val 76
test 78
Epoch: 1, Steps: 68 | Train Loss: 0.5654320 Vali Loss: 0.2110892 Test Loss: 0.5368099
Validation loss decreased (inf --> 0.211089).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 68 | Train Loss: 0.5484232 Vali Loss: 0.2070793 Test Loss: 0.5029487
Validation loss decreased (0.211089 --> 0.207079).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 68 | Train Loss: 0.5069739 Vali Loss: 0.1998213 Test Loss: 0.4560032
Validation loss decreased (0.207079 --> 0.199821).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 68 | Train Loss: 0.4759678 Vali Loss: 0.2028596 Test Loss: 0.4350239
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 68 | Train Loss: 0.4691086 Vali Loss: 0.1975135 Test Loss: 0.4261818
Validation loss decreased (0.199821 --> 0.197514).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 68 | Train Loss: 0.4618538 Vali Loss: 0.1961062 Test Loss: 0.4231313
Validation loss decreased (0.197514 --> 0.196106).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 68 | Train Loss: 0.4600108 Vali Loss: 0.1961610 Test Loss: 0.4214607
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 68 | Train Loss: 0.4597251 Vali Loss: 0.2031036 Test Loss: 0.4209495
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 68 | Train Loss: 0.4595511 Vali Loss: 0.1976084 Test Loss: 0.4204995
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 68 | Train Loss: 0.4608914 Vali Loss: 0.1982474 Test Loss: 0.4203036
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 68 | Train Loss: 0.4579913 Vali Loss: 0.1954785 Test Loss: 0.4202050
Validation loss decreased (0.196106 --> 0.195478).  Saving model ...
Updating learning rate to 9.765625e-08
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.565432026544038, 'val/loss': 0.2110892434914907, 'test/loss': 0.5368098715941111, '_timestamp': 1762292284.7401764}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.5484232166234184, 'val/loss': 0.2070792963107427, 'test/loss': 0.5029487311840057, '_timestamp': 1762292285.967006}).
Epoch: 12, Steps: 68 | Train Loss: 0.4608287 Vali Loss: 0.1993713 Test Loss: 0.4201585
EarlyStopping counter: 1 out of 5
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 68 | Train Loss: 0.4593144 Vali Loss: 0.2034988 Test Loss: 0.4201364
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 68 | Train Loss: 0.4591157 Vali Loss: 0.2013726 Test Loss: 0.4201263
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 68 | Train Loss: 0.4593569 Vali Loss: 0.1980970 Test Loss: 0.4201216
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 68 | Train Loss: 0.4587914 Vali Loss: 0.2014915 Test Loss: 0.4201191
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 78
test shape: (78, 50, 1) (78, 50, 1)
test shape: (78, 50, 1) (78, 50, 1)


	mse:0.0007845552172511816, mae:0.01834080182015896, rmse:0.028009911999106407, r2:-0.06972789764404297, dtw:Not calculated


VAL - MSE: 0.0008, MAE: 0.0183, RMSE: 0.0280, RÂ²: -0.0697, MAPE: 36366044.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.631 MB of 0.633 MB uploadedwandb: \ 0.633 MB of 0.633 MB uploadedwandb: | 0.633 MB of 0.633 MB uploadedwandb: / 0.633 MB of 0.633 MB uploadedwandb: - 0.633 MB of 0.633 MB uploadedwandb: \ 0.633 MB of 0.705 MB uploadedwandb: | 0.705 MB of 0.705 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–‡â–ƒâ–‚â–‚â–ˆâ–ƒâ–ƒâ–â–„â–ˆâ–†â–ƒâ–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 53548
wandb:     model/trainable_params 53548
wandb:                  test/loss 0.42012
wandb:                 train/loss 0.45879
wandb:   val/directional_accuracy 43.11879
wandb:                   val/loss 0.20149
wandb:                    val/mae 0.01834
wandb:                   val/mape 3636604400.0
wandb:                    val/mse 0.00078
wandb:                     val/r2 -0.06973
wandb:                   val/rmse 0.02801
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/s6ik9tyx
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_233758-s6ik9tyx/logs
Completed: NVIDIA H=50

Training: Mamba on NVIDIA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_233847-akz0y2ap
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/akz0y2ap
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H100   Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/akz0y2ap
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 100
============================================================
train 2104
val 26
test 28
Epoch: 1, Steps: 66 | Train Loss: 0.7582810 Vali Loss: 0.2411183 Test Loss: 0.5821422
Validation loss decreased (inf --> 0.241118).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 66 | Train Loss: 0.7430849 Vali Loss: 0.2404626 Test Loss: 0.6147137
Validation loss decreased (0.241118 --> 0.240463).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 66 | Train Loss: 0.7011803 Vali Loss: 0.2390808 Test Loss: 0.6657757
Validation loss decreased (0.240463 --> 0.239081).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 66 | Train Loss: 0.6692102 Vali Loss: 0.2381652 Test Loss: 0.6868383
Validation loss decreased (0.239081 --> 0.238165).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 66 | Train Loss: 0.6578297 Vali Loss: 0.2377281 Test Loss: 0.6932794
Validation loss decreased (0.238165 --> 0.237728).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 66 | Train Loss: 0.6523195 Vali Loss: 0.2375737 Test Loss: 0.6971088
Validation loss decreased (0.237728 --> 0.237574).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 66 | Train Loss: 0.6512775 Vali Loss: 0.2374935 Test Loss: 0.6984143
Validation loss decreased (0.237574 --> 0.237493).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 66 | Train Loss: 0.6499008 Vali Loss: 0.2374517 Test Loss: 0.6989567
Validation loss decreased (0.237493 --> 0.237452).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 66 | Train Loss: 0.6508377 Vali Loss: 0.2374332 Test Loss: 0.6991556
Validation loss decreased (0.237452 --> 0.237433).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 66 | Train Loss: 0.6493548 Vali Loss: 0.2374267 Test Loss: 0.6993675
Validation loss decreased (0.237433 --> 0.237427).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 66 | Train Loss: 0.6508012 Vali Loss: 0.2374221 Test Loss: 0.6994389
Validation loss decreased (0.237427 --> 0.237422).  Saving model ...
Updating learning rate to 9.765625e-08
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.758280975800572, 'val/loss': 0.24111831188201904, 'test/loss': 0.582142174243927, '_timestamp': 1762292334.5264742}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.7430849427526648, 'val/loss': 0.24046257138252258, 'test/loss': 0.6147136688232422, '_timestamp': 1762292335.762026}).
Epoch: 12, Steps: 66 | Train Loss: 0.6497072 Vali Loss: 0.2374202 Test Loss: 0.6994789
Validation loss decreased (0.237422 --> 0.237420).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 66 | Train Loss: 0.6514317 Vali Loss: 0.2374191 Test Loss: 0.6994996
Validation loss decreased (0.237420 --> 0.237419).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 66 | Train Loss: 0.6490210 Vali Loss: 0.2374187 Test Loss: 0.6995084
Validation loss decreased (0.237419 --> 0.237419).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 66 | Train Loss: 0.6502200 Vali Loss: 0.2374185 Test Loss: 0.6995124
Validation loss decreased (0.237419 --> 0.237418).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 66 | Train Loss: 0.6504829 Vali Loss: 0.2374184 Test Loss: 0.6995142
Validation loss decreased (0.237418 --> 0.237418).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 66 | Train Loss: 0.6485342 Vali Loss: 0.2374184 Test Loss: 0.6995150
Validation loss decreased (0.237418 --> 0.237418).  Saving model ...
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 66 | Train Loss: 0.6501714 Vali Loss: 0.2374184 Test Loss: 0.6995151
Validation loss decreased (0.237418 --> 0.237418).  Saving model ...
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 66 | Train Loss: 0.6494773 Vali Loss: 0.2374184 Test Loss: 0.6995151
Validation loss decreased (0.237418 --> 0.237418).  Saving model ...
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 66 | Train Loss: 0.6503691 Vali Loss: 0.2374184 Test Loss: 0.6995151
Validation loss decreased (0.237418 --> 0.237418).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 66 | Train Loss: 0.6505827 Vali Loss: 0.2374184 Test Loss: 0.6995151
Validation loss decreased (0.237418 --> 0.237418).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 66 | Train Loss: 0.6497151 Vali Loss: 0.2374184 Test Loss: 0.6995151
Validation loss decreased (0.237418 --> 0.237418).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 66 | Train Loss: 0.6499125 Vali Loss: 0.2374184 Test Loss: 0.6995151
Validation loss decreased (0.237418 --> 0.237418).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 66 | Train Loss: 0.6511906 Vali Loss: 0.2374184 Test Loss: 0.6995151
Validation loss decreased (0.237418 --> 0.237418).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 66 | Train Loss: 0.6511658 Vali Loss: 0.2374184 Test Loss: 0.6995151
EarlyStopping counter: 1 out of 5
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 66 | Train Loss: 0.6486397 Vali Loss: 0.2374184 Test Loss: 0.6995151
Validation loss decreased (0.237418 --> 0.237418).  Saving model ...
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 66 | Train Loss: 0.6490346 Vali Loss: 0.2374184 Test Loss: 0.6995151
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 66 | Train Loss: 0.6510268 Vali Loss: 0.2374184 Test Loss: 0.6995151
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 66 | Train Loss: 0.6501684 Vali Loss: 0.2374184 Test Loss: 0.6995151
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 66 | Train Loss: 0.6494934 Vali Loss: 0.2374184 Test Loss: 0.6995151
Validation loss decreased (0.237418 --> 0.237418).  Saving model ...
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 66 | Train Loss: 0.6511804 Vali Loss: 0.2374184 Test Loss: 0.6995151
Validation loss decreased (0.237418 --> 0.237418).  Saving model ...
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 66 | Train Loss: 0.6482212 Vali Loss: 0.2374184 Test Loss: 0.6995151
Validation loss decreased (0.237418 --> 0.237418).  Saving model ...
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 66 | Train Loss: 0.6486194 Vali Loss: 0.2374184 Test Loss: 0.6995151
Validation loss decreased (0.237418 --> 0.237418).  Saving model ...
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 66 | Train Loss: 0.6488348 Vali Loss: 0.2374184 Test Loss: 0.6995151
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.1641532182693482e-14
Epoch: 35, Steps: 66 | Train Loss: 0.6495782 Vali Loss: 0.2374184 Test Loss: 0.6995151
Validation loss decreased (0.237418 --> 0.237418).  Saving model ...
Updating learning rate to 5.820766091346741e-15
Epoch: 36, Steps: 66 | Train Loss: 0.6506763 Vali Loss: 0.2374184 Test Loss: 0.6995151
Validation loss decreased (0.237418 --> 0.237418).  Saving model ...
Updating learning rate to 2.9103830456733705e-15
Epoch: 37, Steps: 66 | Train Loss: 0.6494879 Vali Loss: 0.2374184 Test Loss: 0.6995151
Validation loss decreased (0.237418 --> 0.237418).  Saving model ...
Updating learning rate to 1.4551915228366853e-15
Epoch: 38, Steps: 66 | Train Loss: 0.6505107 Vali Loss: 0.2374184 Test Loss: 0.6995151
Validation loss decreased (0.237418 --> 0.237418).  Saving model ...
Updating learning rate to 7.275957614183426e-16
Epoch: 39, Steps: 66 | Train Loss: 0.6503397 Vali Loss: 0.2374184 Test Loss: 0.6995151
Validation loss decreased (0.237418 --> 0.237418).  Saving model ...
Updating learning rate to 3.637978807091713e-16
Epoch: 40, Steps: 66 | Train Loss: 0.6501931 Vali Loss: 0.2374184 Test Loss: 0.6995151
Validation loss decreased (0.237418 --> 0.237418).  Saving model ...
Updating learning rate to 1.8189894035458566e-16
Epoch: 41, Steps: 66 | Train Loss: 0.6511789 Vali Loss: 0.2374184 Test Loss: 0.6995151
Validation loss decreased (0.237418 --> 0.237418).  Saving model ...
Updating learning rate to 9.094947017729283e-17
Epoch: 42, Steps: 66 | Train Loss: 0.6516138 Vali Loss: 0.2374184 Test Loss: 0.6995151
EarlyStopping counter: 1 out of 5
Updating learning rate to 4.5474735088646414e-17
Epoch: 43, Steps: 66 | Train Loss: 0.6505879 Vali Loss: 0.2374184 Test Loss: 0.6995151
Validation loss decreased (0.237418 --> 0.237418).  Saving model ...
Updating learning rate to 2.2737367544323207e-17
Epoch: 44, Steps: 66 | Train Loss: 0.6521065 Vali Loss: 0.2374184 Test Loss: 0.6995151
Validation loss decreased (0.237418 --> 0.237418).  Saving model ...
Updating learning rate to 1.1368683772161604e-17
Epoch: 45, Steps: 66 | Train Loss: 0.6476545 Vali Loss: 0.2374184 Test Loss: 0.6995151
Validation loss decreased (0.237418 --> 0.237418).  Saving model ...
Updating learning rate to 5.684341886080802e-18
Epoch: 46, Steps: 66 | Train Loss: 0.6491286 Vali Loss: 0.2374184 Test Loss: 0.6995151
Validation loss decreased (0.237418 --> 0.237418).  Saving model ...
Updating learning rate to 2.842170943040401e-18
Epoch: 47, Steps: 66 | Train Loss: 0.6511794 Vali Loss: 0.2374184 Test Loss: 0.6995151
Validation loss decreased (0.237418 --> 0.237418).  Saving model ...
Updating learning rate to 1.4210854715202004e-18
Epoch: 48, Steps: 66 | Train Loss: 0.6494597 Vali Loss: 0.2374184 Test Loss: 0.6995151
Validation loss decreased (0.237418 --> 0.237418).  Saving model ...
Updating learning rate to 7.105427357601002e-19
Epoch: 49, Steps: 66 | Train Loss: 0.6489396 Vali Loss: 0.2374184 Test Loss: 0.6995151
Validation loss decreased (0.237418 --> 0.237418).  Saving model ...
Updating learning rate to 3.552713678800501e-19
Epoch: 50, Steps: 66 | Train Loss: 0.6492888 Vali Loss: 0.2374184 Test Loss: 0.6995151
Validation loss decreased (0.237418 --> 0.237418).  Saving model ...
Updating learning rate to 1.7763568394002505e-19
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 28
test shape: (28, 100, 1) (28, 100, 1)
test shape: (28, 100, 1) (28, 100, 1)


	mse:0.0007825800566934049, mae:0.018082134425640106, rmse:0.027974631637334824, r2:-0.03957712650299072, dtw:Not calculated


VAL - MSE: 0.0008, MAE: 0.0181, RMSE: 0.0280, RÂ²: -0.0396, MAPE: 40844524.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.664 MB of 0.669 MB uploadedwandb: \ 0.664 MB of 0.669 MB uploadedwandb: | 0.664 MB of 0.669 MB uploadedwandb: / 0.669 MB of 0.669 MB uploadedwandb: - 0.669 MB of 0.747 MB uploadedwandb: \ 0.669 MB of 0.747 MB uploadedwandb: | 0.747 MB of 0.747 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–…â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 49
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 73048
wandb:     model/trainable_params 73048
wandb:                  test/loss 0.69952
wandb:                 train/loss 0.64929
wandb:   val/directional_accuracy 42.82107
wandb:                   val/loss 0.23742
wandb:                    val/mae 0.01808
wandb:                   val/mape 4084452400.0
wandb:                    val/mse 0.00078
wandb:                     val/r2 -0.03958
wandb:                   val/rmse 0.02797
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/akz0y2ap
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_233847-akz0y2ap/logs
Completed: NVIDIA H=100

Training: Mamba on APPLE for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_234028-obvlfk5g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/obvlfk5g
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H3      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/obvlfk5g
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 3
============================================================
train 2201
val 123
test 125
Epoch: 1, Steps: 69 | Train Loss: 0.2708463 Vali Loss: 0.1342792 Test Loss: 0.2184760
Validation loss decreased (inf --> 0.134279).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 69 | Train Loss: 0.2631593 Vali Loss: 0.1339636 Test Loss: 0.2171019
Validation loss decreased (0.134279 --> 0.133964).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 69 | Train Loss: 0.2564597 Vali Loss: 0.1337043 Test Loss: 0.2157162
Validation loss decreased (0.133964 --> 0.133704).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 69 | Train Loss: 0.2526900 Vali Loss: 0.1333483 Test Loss: 0.2145752
Validation loss decreased (0.133704 --> 0.133348).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 69 | Train Loss: 0.2518111 Vali Loss: 0.1340585 Test Loss: 0.2140331
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 69 | Train Loss: 0.2507078 Vali Loss: 0.1329367 Test Loss: 0.2137333
Validation loss decreased (0.133348 --> 0.132937).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 69 | Train Loss: 0.2499896 Vali Loss: 0.1315501 Test Loss: 0.2135996
Validation loss decreased (0.132937 --> 0.131550).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 69 | Train Loss: 0.2498264 Vali Loss: 0.1330564 Test Loss: 0.2135246
EarlyStopping counter: 1 out of 5
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 69 | Train Loss: 0.2498687 Vali Loss: 0.1322925 Test Loss: 0.2134892
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 69 | Train Loss: 0.2506081 Vali Loss: 0.1311554 Test Loss: 0.2134737
Validation loss decreased (0.131550 --> 0.131155).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 69 | Train Loss: 0.2497083 Vali Loss: 0.1316089 Test Loss: 0.2134641
EarlyStopping counter: 1 out of 5
Updating learning rate to 9.765625e-08
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.27084627294022107, 'val/loss': 0.13427924364805222, 'test/loss': 0.21847601607441902, '_timestamp': 1762292438.674165}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2631592906039694, 'val/loss': 0.13396361283957958, 'test/loss': 0.21710187569260597, '_timestamp': 1762292439.9292066}).
Epoch: 12, Steps: 69 | Train Loss: 0.2500847 Vali Loss: 0.1315509 Test Loss: 0.2134603
EarlyStopping counter: 2 out of 5
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 69 | Train Loss: 0.2500712 Vali Loss: 0.1318156 Test Loss: 0.2134581
EarlyStopping counter: 3 out of 5
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 69 | Train Loss: 0.2496856 Vali Loss: 0.1337970 Test Loss: 0.2134572
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 69 | Train Loss: 0.2501371 Vali Loss: 0.1311473 Test Loss: 0.2134567
Validation loss decreased (0.131155 --> 0.131147).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 69 | Train Loss: 0.2498938 Vali Loss: 0.1314114 Test Loss: 0.2134565
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 69 | Train Loss: 0.2499698 Vali Loss: 0.1314874 Test Loss: 0.2134564
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 69 | Train Loss: 0.2500121 Vali Loss: 0.1303851 Test Loss: 0.2134564
Validation loss decreased (0.131147 --> 0.130385).  Saving model ...
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 69 | Train Loss: 0.2499388 Vali Loss: 0.1325390 Test Loss: 0.2134564
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 69 | Train Loss: 0.2494886 Vali Loss: 0.1318184 Test Loss: 0.2134564
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 69 | Train Loss: 0.2497295 Vali Loss: 0.1313062 Test Loss: 0.2134564
EarlyStopping counter: 3 out of 5
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 69 | Train Loss: 0.2499526 Vali Loss: 0.1308405 Test Loss: 0.2134564
EarlyStopping counter: 4 out of 5
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 69 | Train Loss: 0.2493953 Vali Loss: 0.1317300 Test Loss: 0.2134564
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 125
test shape: (125, 3, 1) (125, 3, 1)
test shape: (125, 3, 1) (125, 3, 1)


	mse:0.000357694982085377, mae:0.014253522269427776, rmse:0.018912825733423233, r2:-0.02015841007232666, dtw:Not calculated


VAL - MSE: 0.0004, MAE: 0.0143, RMSE: 0.0189, RÂ²: -0.0202, MAPE: 423788.88%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.444 MB of 0.444 MB uploadedwandb: \ 0.444 MB of 0.444 MB uploadedwandb: | 0.444 MB of 0.444 MB uploadedwandb: / 0.444 MB of 0.444 MB uploadedwandb: - 0.444 MB of 0.444 MB uploadedwandb: \ 0.444 MB of 0.444 MB uploadedwandb: | 0.444 MB of 0.444 MB uploadedwandb: / 0.492 MB of 0.565 MB uploadedwandb: - 0.492 MB of 0.565 MB uploadedwandb: \ 0.565 MB of 0.565 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–â–‚â–â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–‡â–ˆâ–†â–ƒâ–†â–…â–‚â–ƒâ–ƒâ–„â–ˆâ–‚â–ƒâ–ƒâ–â–…â–„â–ƒâ–‚â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35218
wandb:     model/trainable_params 35218
wandb:                  test/loss 0.21346
wandb:                 train/loss 0.2494
wandb:   val/directional_accuracy 51.6
wandb:                   val/loss 0.13173
wandb:                    val/mae 0.01425
wandb:                   val/mape 42378887.5
wandb:                    val/mse 0.00036
wandb:                     val/r2 -0.02016
wandb:                   val/rmse 0.01891
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/obvlfk5g
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_234028-obvlfk5g/logs
Completed: APPLE H=3

Training: Mamba on APPLE for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_234139-o66mym5h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/o66mym5h
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H5      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/o66mym5h
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 5
============================================================
train 2199
val 121
test 123
Epoch: 1, Steps: 69 | Train Loss: 0.2758713 Vali Loss: 0.1336933 Test Loss: 0.2199036
Validation loss decreased (inf --> 0.133693).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 69 | Train Loss: 0.2680373 Vali Loss: 0.1356038 Test Loss: 0.2204223
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
Epoch: 3, Steps: 69 | Train Loss: 0.2619143 Vali Loss: 0.1335230 Test Loss: 0.2204639
Validation loss decreased (0.133693 --> 0.133523).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 69 | Train Loss: 0.2587053 Vali Loss: 0.1329979 Test Loss: 0.2199848
Validation loss decreased (0.133523 --> 0.132998).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 69 | Train Loss: 0.2579770 Vali Loss: 0.1345175 Test Loss: 0.2197827
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 69 | Train Loss: 0.2570744 Vali Loss: 0.1361330 Test Loss: 0.2197101
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 69 | Train Loss: 0.2566440 Vali Loss: 0.1339291 Test Loss: 0.2196510
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 69 | Train Loss: 0.2567576 Vali Loss: 0.1357606 Test Loss: 0.2196287
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 69 | Train Loss: 0.2559912 Vali Loss: 0.1341845 Test Loss: 0.2196187
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 123
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2758713314930598, 'val/loss': 0.13369329273700714, 'test/loss': 0.21990357711911201, '_timestamp': 1762292506.2839952}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2680373206950616, 'val/loss': 0.1356037575751543, 'test/loss': 0.22042232751846313, '_timestamp': 1762292507.5452552}).
test shape: (123, 5, 1) (123, 5, 1)
test shape: (123, 5, 1) (123, 5, 1)


	mse:0.0003614150919020176, mae:0.014378850348293781, rmse:0.019010920077562332, r2:-0.01838827133178711, dtw:Not calculated


VAL - MSE: 0.0004, MAE: 0.0144, RMSE: 0.0190, RÂ²: -0.0184, MAPE: 412472.38%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.473 MB of 0.474 MB uploadedwandb: \ 0.474 MB of 0.474 MB uploadedwandb: | 0.474 MB of 0.474 MB uploadedwandb: / 0.474 MB of 0.544 MB uploadedwandb: - 0.544 MB of 0.544 MB uploadedwandb: \ 0.544 MB of 0.544 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–â–„â–ˆâ–ƒâ–‡â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 8
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35998
wandb:     model/trainable_params 35998
wandb:                  test/loss 0.21962
wandb:                 train/loss 0.25599
wandb:   val/directional_accuracy 47.96748
wandb:                   val/loss 0.13418
wandb:                    val/mae 0.01438
wandb:                   val/mape 41247237.5
wandb:                    val/mse 0.00036
wandb:                     val/r2 -0.01839
wandb:                   val/rmse 0.01901
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/o66mym5h
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_234139-o66mym5h/logs
Completed: APPLE H=5

Training: Mamba on APPLE for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_234216-76s6sf8k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/76s6sf8k
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H10     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/76s6sf8k
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 10
============================================================
train 2194
val 116
test 118
Epoch: 1, Steps: 69 | Train Loss: 0.2816669 Vali Loss: 0.1379564 Test Loss: 0.2234088
Validation loss decreased (inf --> 0.137956).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 69 | Train Loss: 0.2755246 Vali Loss: 0.1404244 Test Loss: 0.2239956
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
Epoch: 3, Steps: 69 | Train Loss: 0.2711202 Vali Loss: 0.1370868 Test Loss: 0.2245248
Validation loss decreased (0.137956 --> 0.137087).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 69 | Train Loss: 0.2685897 Vali Loss: 0.1383512 Test Loss: 0.2245200
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 69 | Train Loss: 0.2669248 Vali Loss: 0.1369185 Test Loss: 0.2245281
Validation loss decreased (0.137087 --> 0.136919).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 69 | Train Loss: 0.2660850 Vali Loss: 0.1359330 Test Loss: 0.2245117
Validation loss decreased (0.136919 --> 0.135933).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 69 | Train Loss: 0.2659673 Vali Loss: 0.1375019 Test Loss: 0.2245007
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 69 | Train Loss: 0.2659126 Vali Loss: 0.1391348 Test Loss: 0.2244859
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 69 | Train Loss: 0.2657439 Vali Loss: 0.1393300 Test Loss: 0.2244794
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 69 | Train Loss: 0.2657040 Vali Loss: 0.1373098 Test Loss: 0.2244773
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 69 | Train Loss: 0.2651589 Vali Loss: 0.1382909 Test Loss: 0.2244765
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 118
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2816669111666472, 'val/loss': 0.13795642368495464, 'test/loss': 0.2234087660908699, '_timestamp': 1762292542.5346725}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.27552464669165405, 'val/loss': 0.14042437076568604, 'test/loss': 0.22399558126926422, '_timestamp': 1762292543.8012145}).
test shape: (118, 10, 1) (118, 10, 1)
test shape: (118, 10, 1) (118, 10, 1)


	mse:0.00036677191383205354, mae:0.014460088685154915, rmse:0.019151290878653526, r2:-0.019606590270996094, dtw:Not calculated


VAL - MSE: 0.0004, MAE: 0.0145, RMSE: 0.0192, RÂ²: -0.0196, MAPE: 678378.56%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.508 MB of 0.509 MB uploadedwandb: \ 0.508 MB of 0.509 MB uploadedwandb: | 0.508 MB of 0.509 MB uploadedwandb: / 0.509 MB of 0.509 MB uploadedwandb: - 0.509 MB of 0.580 MB uploadedwandb: \ 0.509 MB of 0.580 MB uploadedwandb: | 0.580 MB of 0.580 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‡â–ˆâ–†â–„â–‚â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–†â–ƒâ–â–„â–ˆâ–ˆâ–„â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 37948
wandb:     model/trainable_params 37948
wandb:                  test/loss 0.22448
wandb:                 train/loss 0.26516
wandb:   val/directional_accuracy 51.03578
wandb:                   val/loss 0.13829
wandb:                    val/mae 0.01446
wandb:                   val/mape 67837856.25
wandb:                    val/mse 0.00037
wandb:                     val/r2 -0.01961
wandb:                   val/rmse 0.01915
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/76s6sf8k
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_234216-76s6sf8k/logs
Exception in thread IntMsgThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread ChkStopThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 286, in check_stop_status
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 300, in check_internal_messages
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 856, in deliver_internal_messages
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 840, in deliver_stop_status
    return self._deliver_stop_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 494, in _deliver_stop_status
    return self._deliver_internal_messages(internal_message)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 516, in _deliver_internal_messages
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: APPLE H=10

Training: Mamba on APPLE for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_234300-7xxyft26
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/7xxyft26
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H22     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/7xxyft26
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 22
============================================================
train 2182
val 104
test 106
Epoch: 1, Steps: 69 | Train Loss: 0.2944271 Vali Loss: 0.1449834 Test Loss: 0.2325897
Validation loss decreased (inf --> 0.144983).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 69 | Train Loss: 0.2893223 Vali Loss: 0.1425628 Test Loss: 0.2341856
Validation loss decreased (0.144983 --> 0.142563).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 69 | Train Loss: 0.2822944 Vali Loss: 0.1453010 Test Loss: 0.2351359
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 69 | Train Loss: 0.2831630 Vali Loss: 0.1497479 Test Loss: 0.2350171
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 69 | Train Loss: 0.2777964 Vali Loss: 0.1445915 Test Loss: 0.2350876
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 69 | Train Loss: 0.2777211 Vali Loss: 0.1467084 Test Loss: 0.2350548
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 69 | Train Loss: 0.2789582 Vali Loss: 0.1480626 Test Loss: 0.2350327
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 106
test shape: (106, 22, 1) (106, 22, 1)
test shape: (106, 22, 1) (106, 22, 1)


	mse:0.0003765209694392979, mae:0.014515168033540249, rmse:0.019404148682951927, r2:-0.018398284912109375, dtw:Not calculated


VAL - MSE: 0.0004, MAE: 0.0145, RMSE: 0.0194, RÂ²: -0.0184, MAPE: 854685.44%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.591 MB of 0.592 MB uploadedwandb: \ 0.592 MB of 0.592 MB uploadedwandb: | 0.592 MB of 0.592 MB uploadedwandb: / 0.592 MB of 0.662 MB uploadedwandb: - 0.662 MB of 0.662 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–ƒâ–…â–†â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–…â–ƒâ–‚
wandb:                 train/loss â–‡â–ˆâ–â–â–ƒ
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–ˆâ–â–„â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 6
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 42628
wandb:     model/trainable_params 42628
wandb:                  test/loss 0.23503
wandb:                 train/loss 0.27896
wandb:   val/directional_accuracy 51.12309
wandb:                   val/loss 0.14806
wandb:                    val/mae 0.01452
wandb:                   val/mape 85468543.75
wandb:                    val/mse 0.00038
wandb:                     val/r2 -0.0184
wandb:                   val/rmse 0.0194
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/7xxyft26
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_234300-7xxyft26/logs
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29442707179249195, 'val/loss': 0.1449834257364273, 'test/loss': 0.2325896993279457, '_timestamp': 1762292587.651688}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.28932226568028546, 'val/loss': 0.14256278052926064, 'test/loss': 0.23418563231825829, '_timestamp': 1762292588.9179}).
Completed: APPLE H=22

Training: Mamba on APPLE for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_234335-9jg3r6gt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/9jg3r6gt
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H50     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/9jg3r6gt
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 50
============================================================
train 2154
val 76
test 78
Epoch: 1, Steps: 68 | Train Loss: 0.3271783 Vali Loss: 0.1451529 Test Loss: 0.2293995
Validation loss decreased (inf --> 0.145153).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 68 | Train Loss: 0.3196890 Vali Loss: 0.1499346 Test Loss: 0.2286071
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
Epoch: 3, Steps: 68 | Train Loss: 0.3149413 Vali Loss: 0.1563481 Test Loss: 0.2281967
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 68 | Train Loss: 0.3105214 Vali Loss: 0.1587698 Test Loss: 0.2280242
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 68 | Train Loss: 0.3084472 Vali Loss: 0.1591032 Test Loss: 0.2279322
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 68 | Train Loss: 0.3080752 Vali Loss: 0.1585142 Test Loss: 0.2278787
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 78
test shape: (78, 50, 1) (78, 50, 1)
test shape: (78, 50, 1) (78, 50, 1)


	mse:0.0003797185781877488, mae:0.014607208780944347, rmse:0.01948636956512928, r2:-0.01810455322265625, dtw:Not calculated


VAL - MSE: 0.0004, MAE: 0.0146, RMSE: 0.0195, RÂ²: -0.0181, MAPE: 1324516.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.649 MB of 0.651 MB uploadedwandb: \ 0.649 MB of 0.651 MB uploadedwandb: | 0.651 MB of 0.651 MB uploadedwandb: / 0.651 MB of 0.651 MB uploadedwandb: - 0.651 MB of 0.721 MB uploadedwandb: \ 0.721 MB of 0.721 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–ƒâ–†â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–
wandb:                 train/loss â–ˆâ–ƒâ–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–‡â–ˆâ–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 5
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 53548
wandb:     model/trainable_params 53548
wandb:                  test/loss 0.22788
wandb:                 train/loss 0.30808
wandb:   val/directional_accuracy 49.34589
wandb:                   val/loss 0.15851
wandb:                    val/mae 0.01461
wandb:                   val/mape 132451625.0
wandb:                    val/mse 0.00038
wandb:                     val/r2 -0.0181
wandb:                   val/rmse 0.01949
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/9jg3r6gt
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_234335-9jg3r6gt/logs
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.32717832243617845, 'val/loss': 0.14515288174152374, 'test/loss': 0.22939953207969666, '_timestamp': 1762292621.2627397}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.31968896967523236, 'val/loss': 0.14993455012639365, 'test/loss': 0.2286070634921392, '_timestamp': 1762292622.5491648}).
Completed: APPLE H=50

Training: Mamba on APPLE for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_234412-b9zh3y5h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/b9zh3y5h
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H100    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/b9zh3y5h
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 100
============================================================
train 2104
val 26
test 28
Epoch: 1, Steps: 66 | Train Loss: 0.3744274 Vali Loss: 0.1476687 Test Loss: 0.2330561
Validation loss decreased (inf --> 0.147669).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 66 | Train Loss: 0.3699019 Vali Loss: 0.1489659 Test Loss: 0.2330985
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
Epoch: 3, Steps: 66 | Train Loss: 0.3640394 Vali Loss: 0.1511470 Test Loss: 0.2335259
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 66 | Train Loss: 0.3595101 Vali Loss: 0.1522438 Test Loss: 0.2338791
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 66 | Train Loss: 0.3576150 Vali Loss: 0.1526207 Test Loss: 0.2341044
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 66 | Train Loss: 0.3569960 Vali Loss: 0.1527478 Test Loss: 0.2342071
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 28
test shape: (28, 100, 1) (28, 100, 1)
test shape: (28, 100, 1) (28, 100, 1)


	mse:0.0003797116514761001, mae:0.014557045884430408, rmse:0.019486190751194954, r2:-0.004344344139099121, dtw:Not calculated


VAL - MSE: 0.0004, MAE: 0.0146, RMSE: 0.0195, RÂ²: -0.0043, MAPE: 519084.97%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.679 MB of 0.684 MB uploadedwandb: \ 0.684 MB of 0.684 MB uploadedwandb: | 0.684 MB of 0.684 MB uploadedwandb: / 0.684 MB of 0.754 MB uploadedwandb: - 0.754 MB of 0.754 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–ƒâ–†â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–…â–‡â–ˆ
wandb:                 train/loss â–ˆâ–ƒâ–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–†â–‡â–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 5
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 73048
wandb:     model/trainable_params 73048
wandb:                  test/loss 0.23421
wandb:                 train/loss 0.357
wandb:   val/directional_accuracy 50.10823
wandb:                   val/loss 0.15275
wandb:                    val/mae 0.01456
wandb:                   val/mape 51908496.875
wandb:                    val/mse 0.00038
wandb:                     val/r2 -0.00434
wandb:                   val/rmse 0.01949
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/b9zh3y5h
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_234412-b9zh3y5h/logs
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.37442742107492505, 'val/loss': 0.14766867458820343, 'test/loss': 0.23305612802505493, '_timestamp': 1762292659.671778}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.36990185262578906, 'val/loss': 0.14896589517593384, 'test/loss': 0.23309853672981262, '_timestamp': 1762292660.907563}).
Completed: APPLE H=100

Training: Mamba on SP500 for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_234446-fvy3mdfd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/fvy3mdfd
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H3      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/fvy3mdfd
>>>>>>>start training : long_term_forecast_Mamba_SP500_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 3
============================================================
train 2201
val 123
test 125
Epoch: 1, Steps: 69 | Train Loss: 0.2146358 Vali Loss: 0.0595080 Test Loss: 0.1381142
Validation loss decreased (inf --> 0.059508).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 69 | Train Loss: 0.2112148 Vali Loss: 0.0587707 Test Loss: 0.1368075
Validation loss decreased (0.059508 --> 0.058771).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 69 | Train Loss: 0.2077109 Vali Loss: 0.0581258 Test Loss: 0.1357064
Validation loss decreased (0.058771 --> 0.058126).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 69 | Train Loss: 0.2048099 Vali Loss: 0.0581999 Test Loss: 0.1350620
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 69 | Train Loss: 0.2032141 Vali Loss: 0.0578863 Test Loss: 0.1346968
Validation loss decreased (0.058126 --> 0.057886).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 69 | Train Loss: 0.2020993 Vali Loss: 0.0579430 Test Loss: 0.1344853
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 69 | Train Loss: 0.2018473 Vali Loss: 0.0578268 Test Loss: 0.1344013
Validation loss decreased (0.057886 --> 0.057827).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 69 | Train Loss: 0.2014404 Vali Loss: 0.0573215 Test Loss: 0.1343551
Validation loss decreased (0.057827 --> 0.057321).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 69 | Train Loss: 0.2010360 Vali Loss: 0.0581650 Test Loss: 0.1343314
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 69 | Train Loss: 0.2012402 Vali Loss: 0.0574379 Test Loss: 0.1343218
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 69 | Train Loss: 0.2009344 Vali Loss: 0.0580359 Test Loss: 0.1343168
EarlyStopping counter: 3 out of 5
Updating learning rate to 9.765625e-08
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.21463582373183707, 'val/loss': 0.059507972560822964, 'test/loss': 0.13811416923999786, '_timestamp': 1762292693.9736242}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2112148221636164, 'val/loss': 0.05877069476991892, 'test/loss': 0.13680746406316757, '_timestamp': 1762292695.1737452}).
Epoch: 12, Steps: 69 | Train Loss: 0.2007973 Vali Loss: 0.0574941 Test Loss: 0.1343140
EarlyStopping counter: 4 out of 5
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 69 | Train Loss: 0.2014066 Vali Loss: 0.0577500 Test Loss: 0.1343127
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_SP500_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 125
test shape: (125, 3, 1) (125, 3, 1)
test shape: (125, 3, 1) (125, 3, 1)


	mse:0.00012879513087682426, mae:0.008074888028204441, rmse:0.011348794214427471, r2:-0.009225010871887207, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0081, RMSE: 0.0113, RÂ²: -0.0092, MAPE: 1.19%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.469 MB of 0.470 MB uploadedwandb: \ 0.470 MB of 0.470 MB uploadedwandb: | 0.470 MB of 0.470 MB uploadedwandb: / 0.470 MB of 0.470 MB uploadedwandb: - 0.470 MB of 0.470 MB uploadedwandb: \ 0.470 MB of 0.470 MB uploadedwandb: | 0.470 MB of 0.470 MB uploadedwandb: / 0.518 MB of 0.590 MB uploadedwandb: - 0.590 MB of 0.590 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–ˆâ–†â–†â–…â–â–ˆâ–‚â–‡â–‚â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35218
wandb:     model/trainable_params 35218
wandb:                  test/loss 0.13431
wandb:                 train/loss 0.20141
wandb:   val/directional_accuracy 48.4
wandb:                   val/loss 0.05775
wandb:                    val/mae 0.00807
wandb:                   val/mape 118.60522
wandb:                    val/mse 0.00013
wandb:                     val/r2 -0.00922
wandb:                   val/rmse 0.01135
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/fvy3mdfd
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_234446-fvy3mdfd/logs
Completed: SP500 H=3

Training: Mamba on SP500 for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_234536-jjnumfqm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/jjnumfqm
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H5      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/jjnumfqm
>>>>>>>start training : long_term_forecast_Mamba_SP500_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 5
============================================================
train 2199
val 121
test 123
Epoch: 1, Steps: 69 | Train Loss: 0.2185746 Vali Loss: 0.0592565 Test Loss: 0.1391170
Validation loss decreased (inf --> 0.059256).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 69 | Train Loss: 0.2140648 Vali Loss: 0.0580532 Test Loss: 0.1371216
Validation loss decreased (0.059256 --> 0.058053).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 69 | Train Loss: 0.2089301 Vali Loss: 0.0579416 Test Loss: 0.1356661
Validation loss decreased (0.058053 --> 0.057942).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 69 | Train Loss: 0.2063766 Vali Loss: 0.0577212 Test Loss: 0.1348162
Validation loss decreased (0.057942 --> 0.057721).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 69 | Train Loss: 0.2050604 Vali Loss: 0.0570703 Test Loss: 0.1343557
Validation loss decreased (0.057721 --> 0.057070).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 69 | Train Loss: 0.2034914 Vali Loss: 0.0569071 Test Loss: 0.1341285
Validation loss decreased (0.057070 --> 0.056907).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 69 | Train Loss: 0.2024851 Vali Loss: 0.0568114 Test Loss: 0.1340025
Validation loss decreased (0.056907 --> 0.056811).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 69 | Train Loss: 0.2028549 Vali Loss: 0.0578692 Test Loss: 0.1339415
EarlyStopping counter: 1 out of 5
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 69 | Train Loss: 0.2021868 Vali Loss: 0.0573904 Test Loss: 0.1339132
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 69 | Train Loss: 0.2029243 Vali Loss: 0.0570870 Test Loss: 0.1338994
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 69 | Train Loss: 0.2026993 Vali Loss: 0.0573049 Test Loss: 0.1338925
EarlyStopping counter: 4 out of 5
Updating learning rate to 9.765625e-08
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.21857459076504776, 'val/loss': 0.0592564856633544, 'test/loss': 0.1391170397400856, '_timestamp': 1762292742.4423237}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2140648410372112, 'val/loss': 0.058053224347531796, 'test/loss': 0.13712162990123034, '_timestamp': 1762292743.7245235}).
Epoch: 12, Steps: 69 | Train Loss: 0.2029197 Vali Loss: 0.0575149 Test Loss: 0.1338891
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_SP500_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 123
test shape: (123, 5, 1) (123, 5, 1)
test shape: (123, 5, 1) (123, 5, 1)


	mse:0.00012828066246584058, mae:0.008042274974286556, rmse:0.011326105333864689, r2:-0.00042617321014404297, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0080, RMSE: 0.0113, RÂ²: -0.0004, MAPE: 1.16%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.530 MB of 0.530 MB uploadedwandb: \ 0.530 MB of 0.530 MB uploadedwandb: | 0.530 MB of 0.530 MB uploadedwandb: / 0.530 MB of 0.530 MB uploadedwandb: - 0.530 MB of 0.601 MB uploadedwandb: \ 0.601 MB of 0.601 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–„â–‚â–â–‚â–â–‚â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‡â–ƒâ–‚â–â–ˆâ–…â–ƒâ–„â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35998
wandb:     model/trainable_params 35998
wandb:                  test/loss 0.13389
wandb:                 train/loss 0.20292
wandb:   val/directional_accuracy 50.20325
wandb:                   val/loss 0.05751
wandb:                    val/mae 0.00804
wandb:                   val/mape 116.20417
wandb:                    val/mse 0.00013
wandb:                     val/r2 -0.00043
wandb:                   val/rmse 0.01133
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/jjnumfqm
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_234536-jjnumfqm/logs
Completed: SP500 H=5

Training: Mamba on SP500 for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_234617-td3idezi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/td3idezi
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H10     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/td3idezi
>>>>>>>start training : long_term_forecast_Mamba_SP500_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 10
============================================================
train 2194
val 116
test 118
Epoch: 1, Steps: 69 | Train Loss: 0.2235155 Vali Loss: 0.0590849 Test Loss: 0.1419390
Validation loss decreased (inf --> 0.059085).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 69 | Train Loss: 0.2204117 Vali Loss: 0.0591095 Test Loss: 0.1406771
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
Epoch: 3, Steps: 69 | Train Loss: 0.2175508 Vali Loss: 0.0588965 Test Loss: 0.1397180
Validation loss decreased (0.059085 --> 0.058897).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 69 | Train Loss: 0.2159278 Vali Loss: 0.0584674 Test Loss: 0.1390675
Validation loss decreased (0.058897 --> 0.058467).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 69 | Train Loss: 0.2132520 Vali Loss: 0.0583547 Test Loss: 0.1386880
Validation loss decreased (0.058467 --> 0.058355).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 69 | Train Loss: 0.2139699 Vali Loss: 0.0584681 Test Loss: 0.1384922
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 69 | Train Loss: 0.2132830 Vali Loss: 0.0566672 Test Loss: 0.1383903
Validation loss decreased (0.058355 --> 0.056667).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 69 | Train Loss: 0.2132473 Vali Loss: 0.0586846 Test Loss: 0.1383409
EarlyStopping counter: 1 out of 5
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 69 | Train Loss: 0.2124564 Vali Loss: 0.0574575 Test Loss: 0.1383166
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 69 | Train Loss: 0.2122321 Vali Loss: 0.0575182 Test Loss: 0.1383044
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 69 | Train Loss: 0.2114499 Vali Loss: 0.0573951 Test Loss: 0.1382982
EarlyStopping counter: 4 out of 5
Updating learning rate to 9.765625e-08
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.22351553982150726, 'val/loss': 0.059084867127239704, 'test/loss': 0.14193897880613804, '_timestamp': 1762292783.523511}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.22041169154471246, 'val/loss': 0.0591095145791769, 'test/loss': 0.14067711681127548, '_timestamp': 1762292784.8096051}).
Epoch: 12, Steps: 69 | Train Loss: 0.2117593 Vali Loss: 0.0580819 Test Loss: 0.1382952
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_SP500_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 118
test shape: (118, 10, 1) (118, 10, 1)
test shape: (118, 10, 1) (118, 10, 1)


	mse:0.0001310782099608332, mae:0.008123026229441166, rmse:0.011448939330875874, r2:-0.008370637893676758, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0081, RMSE: 0.0114, RÂ²: -0.0084, MAPE: 1.14%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.540 MB of 0.541 MB uploadedwandb: \ 0.540 MB of 0.541 MB uploadedwandb: | 0.541 MB of 0.541 MB uploadedwandb: / 0.541 MB of 0.541 MB uploadedwandb: - 0.541 MB of 0.612 MB uploadedwandb: \ 0.612 MB of 0.612 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–†â–ƒâ–„â–ƒâ–ƒâ–‚â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‡â–†â–‡â–â–‡â–ƒâ–„â–ƒâ–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 37948
wandb:     model/trainable_params 37948
wandb:                  test/loss 0.1383
wandb:                 train/loss 0.21176
wandb:   val/directional_accuracy 50.18832
wandb:                   val/loss 0.05808
wandb:                    val/mae 0.00812
wandb:                   val/mape 113.92311
wandb:                    val/mse 0.00013
wandb:                     val/r2 -0.00837
wandb:                   val/rmse 0.01145
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/td3idezi
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_234617-td3idezi/logs
Completed: SP500 H=10

Training: Mamba on SP500 for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_234707-siqd7cam
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/siqd7cam
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H22     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/siqd7cam
>>>>>>>start training : long_term_forecast_Mamba_SP500_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 22
============================================================
train 2182
val 104
test 106
Epoch: 1, Steps: 69 | Train Loss: 0.2354382 Vali Loss: 0.0568026 Test Loss: 0.1461737
Validation loss decreased (inf --> 0.056803).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 69 | Train Loss: 0.2319804 Vali Loss: 0.0591248 Test Loss: 0.1450788
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
Epoch: 3, Steps: 69 | Train Loss: 0.2285552 Vali Loss: 0.0576990 Test Loss: 0.1438894
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 69 | Train Loss: 0.2291062 Vali Loss: 0.0608728 Test Loss: 0.1429173
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 69 | Train Loss: 0.2243621 Vali Loss: 0.0571657 Test Loss: 0.1423364
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 69 | Train Loss: 0.2232293 Vali Loss: 0.0549834 Test Loss: 0.1420342
Validation loss decreased (0.056803 --> 0.054983).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 69 | Train Loss: 0.2237513 Vali Loss: 0.0576021 Test Loss: 0.1418876
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 69 | Train Loss: 0.2234124 Vali Loss: 0.0579798 Test Loss: 0.1418159
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 69 | Train Loss: 0.2217853 Vali Loss: 0.0597584 Test Loss: 0.1417811
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 69 | Train Loss: 0.2309727 Vali Loss: 0.0566161 Test Loss: 0.1417639
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 69 | Train Loss: 0.2236133 Vali Loss: 0.0555057 Test Loss: 0.1417556
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_SP500_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 106
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23543821771939596, 'val/loss': 0.05680261366069317, 'test/loss': 0.14617368765175343, '_timestamp': 1762292833.6594877}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23198037551365036, 'val/loss': 0.05912476405501366, 'test/loss': 0.14507884345948696, '_timestamp': 1762292834.9532266}).
test shape: (106, 22, 1) (106, 22, 1)
test shape: (106, 22, 1) (106, 22, 1)


	mse:0.00013643861166201532, mae:0.00825403816998005, rmse:0.011680694296956062, r2:-0.01498723030090332, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0083, RMSE: 0.0117, RÂ²: -0.0150, MAPE: 1.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.613 MB of 0.614 MB uploadedwandb: \ 0.613 MB of 0.614 MB uploadedwandb: | 0.613 MB of 0.614 MB uploadedwandb: / 0.614 MB of 0.614 MB uploadedwandb: - 0.614 MB of 0.685 MB uploadedwandb: \ 0.685 MB of 0.685 MB uploadedwandb: | 0.685 MB of 0.685 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–
wandb:                 train/loss â–†â–‡â–ƒâ–‚â–‚â–‚â–â–ˆâ–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–ˆâ–„â–â–„â–…â–‡â–ƒâ–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 42628
wandb:     model/trainable_params 42628
wandb:                  test/loss 0.14176
wandb:                 train/loss 0.22361
wandb:   val/directional_accuracy 49.50584
wandb:                   val/loss 0.05551
wandb:                    val/mae 0.00825
wandb:                   val/mape 125.39923
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.01499
wandb:                   val/rmse 0.01168
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/siqd7cam
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_234707-siqd7cam/logs
Completed: SP500 H=22

Training: Mamba on SP500 for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_234754-zjtjiihh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/zjtjiihh
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H50     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/zjtjiihh
>>>>>>>start training : long_term_forecast_Mamba_SP500_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 50
============================================================
train 2154
val 76
test 78
Epoch: 1, Steps: 68 | Train Loss: 0.2711336 Vali Loss: 0.0604396 Test Loss: 0.1676025
Validation loss decreased (inf --> 0.060440).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 68 | Train Loss: 0.2665774 Vali Loss: 0.0599555 Test Loss: 0.1670447
Validation loss decreased (0.060440 --> 0.059956).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 68 | Train Loss: 0.2642381 Vali Loss: 0.0603557 Test Loss: 0.1667342
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 68 | Train Loss: 0.2604776 Vali Loss: 0.0593587 Test Loss: 0.1665610
Validation loss decreased (0.059956 --> 0.059359).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 68 | Train Loss: 0.2565111 Vali Loss: 0.0584094 Test Loss: 0.1664854
Validation loss decreased (0.059359 --> 0.058409).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 68 | Train Loss: 0.2548361 Vali Loss: 0.0581635 Test Loss: 0.1664536
Validation loss decreased (0.058409 --> 0.058164).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 68 | Train Loss: 0.2531634 Vali Loss: 0.0601496 Test Loss: 0.1664413
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 68 | Train Loss: 0.2541049 Vali Loss: 0.0599853 Test Loss: 0.1664326
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 68 | Train Loss: 0.2531300 Vali Loss: 0.0593678 Test Loss: 0.1664295
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 68 | Train Loss: 0.2538133 Vali Loss: 0.0584690 Test Loss: 0.1664285
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 68 | Train Loss: 0.2526408 Vali Loss: 0.0583211 Test Loss: 0.1664277
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_SP500_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 78
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2711335668230758, 'val/loss': 0.06043958539764086, 'test/loss': 0.16760251919428507, '_timestamp': 1762292880.2033148}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2665774351095452, 'val/loss': 0.059955522418022156, 'test/loss': 0.16704469919204712, '_timestamp': 1762292881.4501786}).
test shape: (78, 50, 1) (78, 50, 1)
test shape: (78, 50, 1) (78, 50, 1)


	mse:0.0001591230829944834, mae:0.008974344469606876, rmse:0.012614400126039982, r2:-0.012210369110107422, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0090, RMSE: 0.0126, RÂ²: -0.0122, MAPE: 1.27%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.589 MB of 0.592 MB uploadedwandb: \ 0.592 MB of 0.592 MB uploadedwandb: | 0.592 MB of 0.592 MB uploadedwandb: / 0.592 MB of 0.663 MB uploadedwandb: - 0.663 MB of 0.663 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–
wandb:                 train/loss â–ˆâ–†â–ƒâ–‚â–â–‚â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–‚â–â–‡â–‡â–…â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 53548
wandb:     model/trainable_params 53548
wandb:                  test/loss 0.16643
wandb:                 train/loss 0.25264
wandb:   val/directional_accuracy 49.18891
wandb:                   val/loss 0.05832
wandb:                    val/mae 0.00897
wandb:                   val/mape 126.85777
wandb:                    val/mse 0.00016
wandb:                     val/r2 -0.01221
wandb:                   val/rmse 0.01261
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/zjtjiihh
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_234754-zjtjiihh/logs
Completed: SP500 H=50

Training: Mamba on SP500 for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_234835-nuzepmzs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/nuzepmzs
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H100    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/nuzepmzs
>>>>>>>start training : long_term_forecast_Mamba_SP500_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 100
============================================================
train 2104
val 26
test 28
Epoch: 1, Steps: 66 | Train Loss: 0.3435880 Vali Loss: 0.0728078 Test Loss: 0.1627616
Validation loss decreased (inf --> 0.072808).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 66 | Train Loss: 0.3400095 Vali Loss: 0.0710986 Test Loss: 0.1628937
Validation loss decreased (0.072808 --> 0.071099).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 66 | Train Loss: 0.3327435 Vali Loss: 0.0681135 Test Loss: 0.1635100
Validation loss decreased (0.071099 --> 0.068113).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 66 | Train Loss: 0.3216072 Vali Loss: 0.0662092 Test Loss: 0.1641030
Validation loss decreased (0.068113 --> 0.066209).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 66 | Train Loss: 0.3156144 Vali Loss: 0.0655193 Test Loss: 0.1643462
Validation loss decreased (0.066209 --> 0.065519).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 66 | Train Loss: 0.3139398 Vali Loss: 0.0652354 Test Loss: 0.1644519
Validation loss decreased (0.065519 --> 0.065235).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 66 | Train Loss: 0.3124049 Vali Loss: 0.0651125 Test Loss: 0.1644930
Validation loss decreased (0.065235 --> 0.065113).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 66 | Train Loss: 0.3110379 Vali Loss: 0.0650564 Test Loss: 0.1645121
Validation loss decreased (0.065113 --> 0.065056).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 66 | Train Loss: 0.3112893 Vali Loss: 0.0650302 Test Loss: 0.1645205
Validation loss decreased (0.065056 --> 0.065030).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 66 | Train Loss: 0.3107073 Vali Loss: 0.0650174 Test Loss: 0.1645248
Validation loss decreased (0.065030 --> 0.065017).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 66 | Train Loss: 0.3113722 Vali Loss: 0.0650109 Test Loss: 0.1645274
Validation loss decreased (0.065017 --> 0.065011).  Saving model ...
Updating learning rate to 9.765625e-08
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.34358795303286926, 'val/loss': 0.07280778884887695, 'test/loss': 0.16276156902313232, '_timestamp': 1762292921.0250862}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.34000946220123407, 'val/loss': 0.07109856605529785, 'test/loss': 0.16289374232292175, '_timestamp': 1762292922.226333}).
Epoch: 12, Steps: 66 | Train Loss: 0.3108238 Vali Loss: 0.0650079 Test Loss: 0.1645285
Validation loss decreased (0.065011 --> 0.065008).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 66 | Train Loss: 0.3114406 Vali Loss: 0.0650064 Test Loss: 0.1645290
Validation loss decreased (0.065008 --> 0.065006).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 66 | Train Loss: 0.3107561 Vali Loss: 0.0650057 Test Loss: 0.1645293
Validation loss decreased (0.065006 --> 0.065006).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 66 | Train Loss: 0.3115699 Vali Loss: 0.0650053 Test Loss: 0.1645294
Validation loss decreased (0.065006 --> 0.065005).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 66 | Train Loss: 0.3110496 Vali Loss: 0.0650051 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 66 | Train Loss: 0.3109422 Vali Loss: 0.0650051 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 66 | Train Loss: 0.3118657 Vali Loss: 0.0650051 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 66 | Train Loss: 0.3114646 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 66 | Train Loss: 0.3120430 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 66 | Train Loss: 0.3114584 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 66 | Train Loss: 0.3109691 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 66 | Train Loss: 0.3115370 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 66 | Train Loss: 0.3120149 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 66 | Train Loss: 0.3104764 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 66 | Train Loss: 0.3115584 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 66 | Train Loss: 0.3104392 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 66 | Train Loss: 0.3111110 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 66 | Train Loss: 0.3112842 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 66 | Train Loss: 0.3109373 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 66 | Train Loss: 0.3120136 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 66 | Train Loss: 0.3114938 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 66 | Train Loss: 0.3110667 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 66 | Train Loss: 0.3107613 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 1.1641532182693482e-14
Epoch: 35, Steps: 66 | Train Loss: 0.3108838 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 5.820766091346741e-15
Epoch: 36, Steps: 66 | Train Loss: 0.3111575 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 2.9103830456733705e-15
Epoch: 37, Steps: 66 | Train Loss: 0.3107272 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 1.4551915228366853e-15
Epoch: 38, Steps: 66 | Train Loss: 0.3109332 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 7.275957614183426e-16
Epoch: 39, Steps: 66 | Train Loss: 0.3114972 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 3.637978807091713e-16
Epoch: 40, Steps: 66 | Train Loss: 0.3110232 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 1.8189894035458566e-16
Epoch: 41, Steps: 66 | Train Loss: 0.3116509 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 9.094947017729283e-17
Epoch: 42, Steps: 66 | Train Loss: 0.3119688 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 4.5474735088646414e-17
Epoch: 43, Steps: 66 | Train Loss: 0.3114676 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 2.2737367544323207e-17
Epoch: 44, Steps: 66 | Train Loss: 0.3117658 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 1.1368683772161604e-17
Epoch: 45, Steps: 66 | Train Loss: 0.3110144 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 5.684341886080802e-18
Epoch: 46, Steps: 66 | Train Loss: 0.3118491 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 2.842170943040401e-18
Epoch: 47, Steps: 66 | Train Loss: 0.3130255 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 1.4210854715202004e-18
Epoch: 48, Steps: 66 | Train Loss: 0.3108702 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 7.105427357601002e-19
Epoch: 49, Steps: 66 | Train Loss: 0.3111928 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 3.552713678800501e-19
Epoch: 50, Steps: 66 | Train Loss: 0.3112966 Vali Loss: 0.0650050 Test Loss: 0.1645295
Validation loss decreased (0.065005 --> 0.065005).  Saving model ...
Updating learning rate to 1.7763568394002505e-19
>>>>>>>testing : long_term_forecast_Mamba_SP500_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 28
test shape: (28, 100, 1) (28, 100, 1)
test shape: (28, 100, 1) (28, 100, 1)


	mse:0.00013865948130842298, mae:0.00831439159810543, rmse:0.01177537627518177, r2:-0.0041921138763427734, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0083, RMSE: 0.0118, RÂ²: -0.0042, MAPE: 1.22%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.633 MB of 0.638 MB uploadedwandb: \ 0.633 MB of 0.638 MB uploadedwandb: | 0.638 MB of 0.638 MB uploadedwandb: / 0.638 MB of 0.717 MB uploadedwandb: - 0.717 MB of 0.717 MB uploadedwandb: \ 0.717 MB of 0.717 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–…â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 49
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 73048
wandb:     model/trainable_params 73048
wandb:                  test/loss 0.16453
wandb:                 train/loss 0.3113
wandb:   val/directional_accuracy 50.61328
wandb:                   val/loss 0.06501
wandb:                    val/mae 0.00831
wandb:                   val/mape 122.0955
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.00419
wandb:                   val/rmse 0.01178
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/nuzepmzs
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_234835-nuzepmzs/logs
Completed: SP500 H=100

Training: Mamba on NASDAQ for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_235005-t62d68f5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/t62d68f5
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H3     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/t62d68f5
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 3
============================================================
train 2201
val 123
test 125
Epoch: 1, Steps: 69 | Train Loss: 0.3614955 Vali Loss: 0.1064765 Test Loss: 0.1762899
Validation loss decreased (inf --> 0.106476).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 69 | Train Loss: 0.3448247 Vali Loss: 0.0994569 Test Loss: 0.1667923
Validation loss decreased (0.106476 --> 0.099457).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 69 | Train Loss: 0.3245423 Vali Loss: 0.0952979 Test Loss: 0.1595381
Validation loss decreased (0.099457 --> 0.095298).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 69 | Train Loss: 0.3122385 Vali Loss: 0.0935829 Test Loss: 0.1567578
Validation loss decreased (0.095298 --> 0.093583).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 69 | Train Loss: 0.3090906 Vali Loss: 0.0935898 Test Loss: 0.1559384
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 69 | Train Loss: 0.3056901 Vali Loss: 0.0932926 Test Loss: 0.1556144
Validation loss decreased (0.093583 --> 0.093293).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 69 | Train Loss: 0.3034530 Vali Loss: 0.0931162 Test Loss: 0.1554171
Validation loss decreased (0.093293 --> 0.093116).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 69 | Train Loss: 0.3035071 Vali Loss: 0.0929074 Test Loss: 0.1553472
Validation loss decreased (0.093116 --> 0.092907).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 69 | Train Loss: 0.3029732 Vali Loss: 0.0925284 Test Loss: 0.1553072
Validation loss decreased (0.092907 --> 0.092528).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 69 | Train Loss: 0.3036194 Vali Loss: 0.0925560 Test Loss: 0.1552887
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 69 | Train Loss: 0.3031126 Vali Loss: 0.0928312 Test Loss: 0.1552794
EarlyStopping counter: 2 out of 5
Updating learning rate to 9.765625e-08
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.36149546115294745, 'val/loss': 0.10647648014128208, 'test/loss': 0.17628993093967438, '_timestamp': 1762293011.4752324}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3448246501494145, 'val/loss': 0.09945687092840672, 'test/loss': 0.16679233126342297, '_timestamp': 1762293012.8204405}).
Epoch: 12, Steps: 69 | Train Loss: 0.3031552 Vali Loss: 0.0919627 Test Loss: 0.1552738
Validation loss decreased (0.092528 --> 0.091963).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 69 | Train Loss: 0.3034646 Vali Loss: 0.0921521 Test Loss: 0.1552712
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 69 | Train Loss: 0.3036160 Vali Loss: 0.0927589 Test Loss: 0.1552704
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 69 | Train Loss: 0.3031201 Vali Loss: 0.0927689 Test Loss: 0.1552698
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 69 | Train Loss: 0.3023671 Vali Loss: 0.0925910 Test Loss: 0.1552696
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 69 | Train Loss: 0.3028253 Vali Loss: 0.0931728 Test Loss: 0.1552696
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 125
test shape: (125, 3, 1) (125, 3, 1)
test shape: (125, 3, 1) (125, 3, 1)


	mse:0.000247924035647884, mae:0.011765183880925179, rmse:0.015745604410767555, r2:-0.020588278770446777, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0118, RMSE: 0.0157, RÂ²: -0.0206, MAPE: 389266.06%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.479 MB of 0.479 MB uploadedwandb: \ 0.479 MB of 0.479 MB uploadedwandb: | 0.479 MB of 0.479 MB uploadedwandb: / 0.479 MB of 0.479 MB uploadedwandb: - 0.479 MB of 0.479 MB uploadedwandb: \ 0.479 MB of 0.479 MB uploadedwandb: | 0.479 MB of 0.479 MB uploadedwandb: / 0.528 MB of 0.599 MB uploadedwandb: - 0.528 MB of 0.599 MB uploadedwandb: \ 0.599 MB of 0.599 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–ƒâ–â–â–ƒâ–ƒâ–‚â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35218
wandb:     model/trainable_params 35218
wandb:                  test/loss 0.15527
wandb:                 train/loss 0.30283
wandb:   val/directional_accuracy 49.2
wandb:                   val/loss 0.09317
wandb:                    val/mae 0.01177
wandb:                   val/mape 38926606.25
wandb:                    val/mse 0.00025
wandb:                     val/r2 -0.02059
wandb:                   val/rmse 0.01575
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/t62d68f5
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_235005-t62d68f5/logs
Completed: NASDAQ H=3

Training: Mamba on NASDAQ for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_235056-ebcwn9tg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/ebcwn9tg
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H5     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/ebcwn9tg
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 5
============================================================
train 2199
val 121
test 123
Epoch: 1, Steps: 69 | Train Loss: 0.3671945 Vali Loss: 0.1070669 Test Loss: 0.1807117
Validation loss decreased (inf --> 0.107067).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 69 | Train Loss: 0.3533108 Vali Loss: 0.1022389 Test Loss: 0.1734668
Validation loss decreased (0.107067 --> 0.102239).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 69 | Train Loss: 0.3336014 Vali Loss: 0.0995515 Test Loss: 0.1675191
Validation loss decreased (0.102239 --> 0.099551).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 69 | Train Loss: 0.3196116 Vali Loss: 0.0969263 Test Loss: 0.1653452
Validation loss decreased (0.099551 --> 0.096926).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 69 | Train Loss: 0.3144443 Vali Loss: 0.0970415 Test Loss: 0.1645620
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 69 | Train Loss: 0.3116967 Vali Loss: 0.0980619 Test Loss: 0.1642269
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 69 | Train Loss: 0.3103527 Vali Loss: 0.0966578 Test Loss: 0.1640750
Validation loss decreased (0.096926 --> 0.096658).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 69 | Train Loss: 0.3093921 Vali Loss: 0.0978260 Test Loss: 0.1640012
EarlyStopping counter: 1 out of 5
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 69 | Train Loss: 0.3090077 Vali Loss: 0.0965185 Test Loss: 0.1639683
Validation loss decreased (0.096658 --> 0.096518).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 69 | Train Loss: 0.3088472 Vali Loss: 0.0980263 Test Loss: 0.1639542
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 69 | Train Loss: 0.3084438 Vali Loss: 0.0971118 Test Loss: 0.1639445
EarlyStopping counter: 2 out of 5
Updating learning rate to 9.765625e-08
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3671944523635118, 'val/loss': 0.10706692188978195, 'test/loss': 0.1807116810232401, '_timestamp': 1762293062.383016}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.35331079558185907, 'val/loss': 0.10223886370658875, 'test/loss': 0.17346680723130703, '_timestamp': 1762293063.665407}).
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
Epoch: 12, Steps: 69 | Train Loss: 0.3091719 Vali Loss: 0.0971414 Test Loss: 0.1639405
EarlyStopping counter: 3 out of 5
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 69 | Train Loss: 0.3098268 Vali Loss: 0.0961148 Test Loss: 0.1639385
Validation loss decreased (0.096518 --> 0.096115).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 69 | Train Loss: 0.3091675 Vali Loss: 0.0969356 Test Loss: 0.1639375
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 69 | Train Loss: 0.3096015 Vali Loss: 0.0968849 Test Loss: 0.1639370
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 69 | Train Loss: 0.3083937 Vali Loss: 0.0974898 Test Loss: 0.1639369
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 69 | Train Loss: 0.3081626 Vali Loss: 0.0972857 Test Loss: 0.1639368
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 69 | Train Loss: 0.3087486 Vali Loss: 0.0963747 Test Loss: 0.1639368
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 123
test shape: (123, 5, 1) (123, 5, 1)
test shape: (123, 5, 1) (123, 5, 1)


	mse:0.0002523571893107146, mae:0.011908004991710186, rmse:0.01588575355708599, r2:-0.028469562530517578, dtw:Not calculated


VAL - MSE: 0.0003, MAE: 0.0119, RMSE: 0.0159, RÂ²: -0.0285, MAPE: 504531.16%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.484 MB of 0.484 MB uploadedwandb: \ 0.484 MB of 0.484 MB uploadedwandb: | 0.484 MB of 0.484 MB uploadedwandb: / 0.484 MB of 0.484 MB uploadedwandb: - 0.484 MB of 0.556 MB uploadedwandb: \ 0.556 MB of 0.556 MB uploadedwandb: | 0.556 MB of 0.556 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–ƒâ–…â–‚â–„â–‚â–…â–ƒâ–ƒâ–â–ƒâ–ƒâ–„â–ƒâ–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 17
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35998
wandb:     model/trainable_params 35998
wandb:                  test/loss 0.16394
wandb:                 train/loss 0.30875
wandb:   val/directional_accuracy 47.96748
wandb:                   val/loss 0.09637
wandb:                    val/mae 0.01191
wandb:                   val/mape 50453115.625
wandb:                    val/mse 0.00025
wandb:                     val/r2 -0.02847
wandb:                   val/rmse 0.01589
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/ebcwn9tg
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_235056-ebcwn9tg/logs
Completed: NASDAQ H=5

Training: Mamba on NASDAQ for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_235146-r9gau05a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/r9gau05a
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H10    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/r9gau05a
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 10
============================================================
train 2194
val 116
test 118
Epoch: 1, Steps: 69 | Train Loss: 0.3831483 Vali Loss: 0.1082525 Test Loss: 0.1862703
Validation loss decreased (inf --> 0.108252).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 69 | Train Loss: 0.3722766 Vali Loss: 0.1045788 Test Loss: 0.1825123
Validation loss decreased (0.108252 --> 0.104579).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 69 | Train Loss: 0.3581025 Vali Loss: 0.1020409 Test Loss: 0.1787798
Validation loss decreased (0.104579 --> 0.102041).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 69 | Train Loss: 0.3480431 Vali Loss: 0.1005672 Test Loss: 0.1759851
Validation loss decreased (0.102041 --> 0.100567).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 69 | Train Loss: 0.3415525 Vali Loss: 0.1015466 Test Loss: 0.1748121
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 69 | Train Loss: 0.3387877 Vali Loss: 0.1008381 Test Loss: 0.1742702
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 69 | Train Loss: 0.3370334 Vali Loss: 0.1012574 Test Loss: 0.1740342
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 69 | Train Loss: 0.3368833 Vali Loss: 0.0994432 Test Loss: 0.1739225
Validation loss decreased (0.100567 --> 0.099443).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 69 | Train Loss: 0.3359664 Vali Loss: 0.0994031 Test Loss: 0.1738740
Validation loss decreased (0.099443 --> 0.099403).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 69 | Train Loss: 0.3361545 Vali Loss: 0.1013519 Test Loss: 0.1738456
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 69 | Train Loss: 0.3350375 Vali Loss: 0.1014891 Test Loss: 0.1738320
EarlyStopping counter: 2 out of 5
Updating learning rate to 9.765625e-08
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3831483462582464, 'val/loss': 0.10825249552726746, 'test/loss': 0.18627029284834862, '_timestamp': 1762293113.9752662}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3722765696221504, 'val/loss': 0.10457876697182655, 'test/loss': 0.1825122982263565, '_timestamp': 1762293115.2635024}).
Epoch: 12, Steps: 69 | Train Loss: 0.3357416 Vali Loss: 0.1002425 Test Loss: 0.1738255
EarlyStopping counter: 3 out of 5
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 69 | Train Loss: 0.3355798 Vali Loss: 0.1008440 Test Loss: 0.1738227
EarlyStopping counter: 4 out of 5
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 69 | Train Loss: 0.3358172 Vali Loss: 0.1001050 Test Loss: 0.1738209
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 118
test shape: (118, 10, 1) (118, 10, 1)
test shape: (118, 10, 1) (118, 10, 1)


	mse:0.00025964222732000053, mae:0.012104099616408348, rmse:0.016113417223095894, r2:-0.056427001953125, dtw:Not calculated


VAL - MSE: 0.0003, MAE: 0.0121, RMSE: 0.0161, RÂ²: -0.0564, MAPE: 307581.84%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.523 MB of 0.523 MB uploadedwandb: \ 0.523 MB of 0.523 MB uploadedwandb: | 0.523 MB of 0.523 MB uploadedwandb: / 0.523 MB of 0.523 MB uploadedwandb: - 0.523 MB of 0.595 MB uploadedwandb: \ 0.523 MB of 0.595 MB uploadedwandb: | 0.595 MB of 0.595 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–‡â–…â–†â–â–â–†â–‡â–ƒâ–…â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 13
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 37948
wandb:     model/trainable_params 37948
wandb:                  test/loss 0.17382
wandb:                 train/loss 0.33582
wandb:   val/directional_accuracy 49.81168
wandb:                   val/loss 0.10011
wandb:                    val/mae 0.0121
wandb:                   val/mape 30758184.375
wandb:                    val/mse 0.00026
wandb:                     val/r2 -0.05643
wandb:                   val/rmse 0.01611
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/r9gau05a
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_235146-r9gau05a/logs
Completed: NASDAQ H=10

Training: Mamba on NASDAQ for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_235232-k8dosflj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/k8dosflj
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H22    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/k8dosflj
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 22
============================================================
train 2182
val 104
test 106
Epoch: 1, Steps: 69 | Train Loss: 0.4070979 Vali Loss: 0.1113340 Test Loss: 0.1994809
Validation loss decreased (inf --> 0.111334).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 69 | Train Loss: 0.4007814 Vali Loss: 0.1076531 Test Loss: 0.1956231
Validation loss decreased (0.111334 --> 0.107653).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 69 | Train Loss: 0.3867228 Vali Loss: 0.1039892 Test Loss: 0.1908081
Validation loss decreased (0.107653 --> 0.103989).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 69 | Train Loss: 0.3763753 Vali Loss: 0.1020160 Test Loss: 0.1879803
Validation loss decreased (0.103989 --> 0.102016).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 69 | Train Loss: 0.3657775 Vali Loss: 0.1012245 Test Loss: 0.1869283
Validation loss decreased (0.102016 --> 0.101224).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 69 | Train Loss: 0.3639256 Vali Loss: 0.1016319 Test Loss: 0.1863854
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 69 | Train Loss: 0.3636927 Vali Loss: 0.1024369 Test Loss: 0.1861354
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 69 | Train Loss: 0.3621579 Vali Loss: 0.0988419 Test Loss: 0.1860124
Validation loss decreased (0.101224 --> 0.098842).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 69 | Train Loss: 0.3614730 Vali Loss: 0.1028297 Test Loss: 0.1859559
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 69 | Train Loss: 0.3712027 Vali Loss: 0.1002478 Test Loss: 0.1859334
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 69 | Train Loss: 0.3622757 Vali Loss: 0.1015603 Test Loss: 0.1859197
EarlyStopping counter: 3 out of 5
Updating learning rate to 9.765625e-08
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.4070979311414387, 'val/loss': 0.11133404448628426, 'test/loss': 0.19948094710707664, '_timestamp': 1762293157.7893693}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.40078136022540106, 'val/loss': 0.1076530646532774, 'test/loss': 0.19562313705682755, '_timestamp': 1762293159.0505946}).
Epoch: 12, Steps: 69 | Train Loss: 0.3617756 Vali Loss: 0.1036033 Test Loss: 0.1859137
EarlyStopping counter: 4 out of 5
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 69 | Train Loss: 0.3637740 Vali Loss: 0.0995623 Test Loss: 0.1859108
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 106
test shape: (106, 22, 1) (106, 22, 1)
test shape: (106, 22, 1) (106, 22, 1)


	mse:0.00026325954240746796, mae:0.012190462090075016, rmse:0.01622527465224266, r2:-0.04742252826690674, dtw:Not calculated


VAL - MSE: 0.0003, MAE: 0.0122, RMSE: 0.0162, RÂ²: -0.0474, MAPE: 429449.94%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.586 MB of 0.587 MB uploadedwandb: \ 0.586 MB of 0.587 MB uploadedwandb: | 0.586 MB of 0.587 MB uploadedwandb: / 0.587 MB of 0.587 MB uploadedwandb: - 0.587 MB of 0.659 MB uploadedwandb: \ 0.587 MB of 0.659 MB uploadedwandb: | 0.659 MB of 0.659 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–‚â–‚â–‚â–â–â–„â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–„â–…â–†â–â–†â–ƒâ–…â–‡â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 42628
wandb:     model/trainable_params 42628
wandb:                  test/loss 0.18591
wandb:                 train/loss 0.36377
wandb:   val/directional_accuracy 48.96676
wandb:                   val/loss 0.09956
wandb:                    val/mae 0.01219
wandb:                   val/mape 42944993.75
wandb:                    val/mse 0.00026
wandb:                     val/r2 -0.04742
wandb:                   val/rmse 0.01623
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/k8dosflj
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_235232-k8dosflj/logs
Completed: NASDAQ H=22

Training: Mamba on NASDAQ for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_235319-09uaajzk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/09uaajzk
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H50    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/09uaajzk
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 50
============================================================
train 2154
val 76
test 78
Epoch: 1, Steps: 68 | Train Loss: 0.4644997 Vali Loss: 0.1289423 Test Loss: 0.2232135
Validation loss decreased (inf --> 0.128942).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 68 | Train Loss: 0.4590559 Vali Loss: 0.1261349 Test Loss: 0.2214643
Validation loss decreased (0.128942 --> 0.126135).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 68 | Train Loss: 0.4489142 Vali Loss: 0.1224550 Test Loss: 0.2187656
Validation loss decreased (0.126135 --> 0.122455).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 68 | Train Loss: 0.4394010 Vali Loss: 0.1199714 Test Loss: 0.2167755
Validation loss decreased (0.122455 --> 0.119971).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 68 | Train Loss: 0.4331578 Vali Loss: 0.1181590 Test Loss: 0.2157936
Validation loss decreased (0.119971 --> 0.118159).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 68 | Train Loss: 0.4295831 Vali Loss: 0.1176090 Test Loss: 0.2153806
Validation loss decreased (0.118159 --> 0.117609).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 68 | Train Loss: 0.4283384 Vali Loss: 0.1187676 Test Loss: 0.2151829
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 68 | Train Loss: 0.4274767 Vali Loss: 0.1185557 Test Loss: 0.2150834
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 68 | Train Loss: 0.4268480 Vali Loss: 0.1184251 Test Loss: 0.2150394
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 68 | Train Loss: 0.4288227 Vali Loss: 0.1173941 Test Loss: 0.2150160
Validation loss decreased (0.117609 --> 0.117394).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 68 | Train Loss: 0.4261014 Vali Loss: 0.1175489 Test Loss: 0.2150053
EarlyStopping counter: 1 out of 5
Updating learning rate to 9.765625e-08
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.46449971461997314, 'val/loss': 0.12894231577714285, 'test/loss': 0.22321350375811258, '_timestamp': 1762293205.5695837}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.45905590232680826, 'val/loss': 0.1261348749200503, 'test/loss': 0.22146427631378174, '_timestamp': 1762293206.8114479}).
Epoch: 12, Steps: 68 | Train Loss: 0.4307872 Vali Loss: 0.1171712 Test Loss: 0.2150002
Validation loss decreased (0.117394 --> 0.117171).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 68 | Train Loss: 0.4277393 Vali Loss: 0.1174063 Test Loss: 0.2149977
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 68 | Train Loss: 0.4273461 Vali Loss: 0.1187337 Test Loss: 0.2149964
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 68 | Train Loss: 0.4258462 Vali Loss: 0.1175382 Test Loss: 0.2149957
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 68 | Train Loss: 0.4261987 Vali Loss: 0.1183105 Test Loss: 0.2149954
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 68 | Train Loss: 0.4288980 Vali Loss: 0.1167440 Test Loss: 0.2149954
Validation loss decreased (0.117171 --> 0.116744).  Saving model ...
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 68 | Train Loss: 0.4271576 Vali Loss: 0.1171536 Test Loss: 0.2149953
EarlyStopping counter: 1 out of 5
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 68 | Train Loss: 0.4268926 Vali Loss: 0.1183792 Test Loss: 0.2149953
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 68 | Train Loss: 0.4275626 Vali Loss: 0.1169401 Test Loss: 0.2149953
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 68 | Train Loss: 0.4284938 Vali Loss: 0.1172920 Test Loss: 0.2149953
EarlyStopping counter: 4 out of 5
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 68 | Train Loss: 0.4280806 Vali Loss: 0.1167393 Test Loss: 0.2149953
Validation loss decreased (0.116744 --> 0.116739).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 68 | Train Loss: 0.4286370 Vali Loss: 0.1176196 Test Loss: 0.2149953
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 68 | Train Loss: 0.4276035 Vali Loss: 0.1168077 Test Loss: 0.2149953
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 68 | Train Loss: 0.4289021 Vali Loss: 0.1188166 Test Loss: 0.2149953
EarlyStopping counter: 3 out of 5
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 68 | Train Loss: 0.4277006 Vali Loss: 0.1174205 Test Loss: 0.2149953
EarlyStopping counter: 4 out of 5
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 68 | Train Loss: 0.4293719 Vali Loss: 0.1171519 Test Loss: 0.2149953
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 78
test shape: (78, 50, 1) (78, 50, 1)
test shape: (78, 50, 1) (78, 50, 1)


	mse:0.0002799981157295406, mae:0.012569992803037167, rmse:0.016733143478631973, r2:-0.02729356288909912, dtw:Not calculated


VAL - MSE: 0.0003, MAE: 0.0126, RMSE: 0.0167, RÂ²: -0.0273, MAPE: 746650.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.662 MB of 0.664 MB uploadedwandb: \ 0.664 MB of 0.664 MB uploadedwandb: | 0.664 MB of 0.664 MB uploadedwandb: / 0.664 MB of 0.738 MB uploadedwandb: - 0.738 MB of 0.738 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–‚â–â–‚â–‚â–â–â–â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–â–‚â–ƒâ–â–‚â–â–‚â–â–„â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 26
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 53548
wandb:     model/trainable_params 53548
wandb:                  test/loss 0.215
wandb:                 train/loss 0.42937
wandb:   val/directional_accuracy 50.41863
wandb:                   val/loss 0.11715
wandb:                    val/mae 0.01257
wandb:                   val/mape 74665025.0
wandb:                    val/mse 0.00028
wandb:                     val/r2 -0.02729
wandb:                   val/rmse 0.01673
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/09uaajzk
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_235319-09uaajzk/logs
Completed: NASDAQ H=50

Training: Mamba on NASDAQ for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_235421-3gz4ybi8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/3gz4ybi8
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H100   Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/3gz4ybi8
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 100
============================================================
train 2104
val 26
test 28
Epoch: 1, Steps: 66 | Train Loss: 0.5418265 Vali Loss: 0.1815362 Test Loss: 0.2538367
Validation loss decreased (inf --> 0.181536).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 66 | Train Loss: 0.5381205 Vali Loss: 0.1782781 Test Loss: 0.2533026
Validation loss decreased (0.181536 --> 0.178278).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 66 | Train Loss: 0.5302320 Vali Loss: 0.1726549 Test Loss: 0.2526051
Validation loss decreased (0.178278 --> 0.172655).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 66 | Train Loss: 0.5213128 Vali Loss: 0.1681929 Test Loss: 0.2522436
Validation loss decreased (0.172655 --> 0.168193).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 66 | Train Loss: 0.5157760 Vali Loss: 0.1657980 Test Loss: 0.2520697
Validation loss decreased (0.168193 --> 0.165798).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 66 | Train Loss: 0.5132345 Vali Loss: 0.1647381 Test Loss: 0.2520154
Validation loss decreased (0.165798 --> 0.164738).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 66 | Train Loss: 0.5119208 Vali Loss: 0.1642587 Test Loss: 0.2520008
Validation loss decreased (0.164738 --> 0.164259).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 66 | Train Loss: 0.5109486 Vali Loss: 0.1640145 Test Loss: 0.2519912
Validation loss decreased (0.164259 --> 0.164014).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 66 | Train Loss: 0.5108426 Vali Loss: 0.1638885 Test Loss: 0.2519841
Validation loss decreased (0.164014 --> 0.163888).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 66 | Train Loss: 0.5110154 Vali Loss: 0.1638381 Test Loss: 0.2519832
Validation loss decreased (0.163888 --> 0.163838).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 66 | Train Loss: 0.5109194 Vali Loss: 0.1638104 Test Loss: 0.2519824
Validation loss decreased (0.163838 --> 0.163810).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 66 | Train Loss: 0.5103730 Vali Loss: 0.1637983 Test Loss: 0.2519825
Validation loss decreased (0.163810 --> 0.163798).  Saving model ...
Updating learning rate to 4.8828125e-08
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.5418264838782224, 'val/loss': 0.18153615295886993, 'test/loss': 0.25383666157722473, '_timestamp': 1762293268.430512}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.5381204801978487, 'val/loss': 0.1782781481742859, 'test/loss': 0.2533026337623596, '_timestamp': 1762293269.682668}).
Epoch: 13, Steps: 66 | Train Loss: 0.5109655 Vali Loss: 0.1637920 Test Loss: 0.2519822
Validation loss decreased (0.163798 --> 0.163792).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 66 | Train Loss: 0.5108381 Vali Loss: 0.1637886 Test Loss: 0.2519821
Validation loss decreased (0.163792 --> 0.163789).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 66 | Train Loss: 0.5106421 Vali Loss: 0.1637872 Test Loss: 0.2519821
Validation loss decreased (0.163789 --> 0.163787).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 66 | Train Loss: 0.5111796 Vali Loss: 0.1637864 Test Loss: 0.2519821
Validation loss decreased (0.163787 --> 0.163786).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 66 | Train Loss: 0.5109893 Vali Loss: 0.1637862 Test Loss: 0.2519821
Validation loss decreased (0.163786 --> 0.163786).  Saving model ...
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 66 | Train Loss: 0.5116696 Vali Loss: 0.1637862 Test Loss: 0.2519821
Validation loss decreased (0.163786 --> 0.163786).  Saving model ...
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 66 | Train Loss: 0.5110175 Vali Loss: 0.1637862 Test Loss: 0.2519820
Validation loss decreased (0.163786 --> 0.163786).  Saving model ...
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 66 | Train Loss: 0.5114918 Vali Loss: 0.1637862 Test Loss: 0.2519820
Validation loss decreased (0.163786 --> 0.163786).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 66 | Train Loss: 0.5112646 Vali Loss: 0.1637861 Test Loss: 0.2519820
Validation loss decreased (0.163786 --> 0.163786).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 66 | Train Loss: 0.5107392 Vali Loss: 0.1637861 Test Loss: 0.2519820
Validation loss decreased (0.163786 --> 0.163786).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 66 | Train Loss: 0.5110889 Vali Loss: 0.1637861 Test Loss: 0.2519820
Validation loss decreased (0.163786 --> 0.163786).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 66 | Train Loss: 0.5109031 Vali Loss: 0.1637861 Test Loss: 0.2519820
Validation loss decreased (0.163786 --> 0.163786).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 66 | Train Loss: 0.5108224 Vali Loss: 0.1637861 Test Loss: 0.2519820
Validation loss decreased (0.163786 --> 0.163786).  Saving model ...
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 66 | Train Loss: 0.5115467 Vali Loss: 0.1637862 Test Loss: 0.2519820
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 66 | Train Loss: 0.5102300 Vali Loss: 0.1637862 Test Loss: 0.2519820
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 66 | Train Loss: 0.5112308 Vali Loss: 0.1637861 Test Loss: 0.2519820
Validation loss decreased (0.163786 --> 0.163786).  Saving model ...
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 66 | Train Loss: 0.5109480 Vali Loss: 0.1637862 Test Loss: 0.2519820
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 66 | Train Loss: 0.5111370 Vali Loss: 0.1637862 Test Loss: 0.2519820
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 66 | Train Loss: 0.5116719 Vali Loss: 0.1637862 Test Loss: 0.2519820
EarlyStopping counter: 3 out of 5
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 66 | Train Loss: 0.5105727 Vali Loss: 0.1637862 Test Loss: 0.2519820
EarlyStopping counter: 4 out of 5
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 66 | Train Loss: 0.5105736 Vali Loss: 0.1637862 Test Loss: 0.2519820
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 28
test shape: (28, 100, 1) (28, 100, 1)
test shape: (28, 100, 1) (28, 100, 1)


	mse:0.000258584157563746, mae:0.012129325419664383, rmse:0.016080552712082863, r2:-0.00966191291809082, dtw:Not calculated


VAL - MSE: 0.0003, MAE: 0.0121, RMSE: 0.0161, RÂ²: -0.0097, MAPE: 470016.72%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.664 MB of 0.669 MB uploadedwandb: \ 0.664 MB of 0.669 MB uploadedwandb: | 0.669 MB of 0.669 MB uploadedwandb: / 0.669 MB of 0.669 MB uploadedwandb: - 0.669 MB of 0.745 MB uploadedwandb: \ 0.745 MB of 0.745 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 32
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 73048
wandb:     model/trainable_params 73048
wandb:                  test/loss 0.25198
wandb:                 train/loss 0.51057
wandb:   val/directional_accuracy 50.39683
wandb:                   val/loss 0.16379
wandb:                    val/mae 0.01213
wandb:                   val/mape 47001671.875
wandb:                    val/mse 0.00026
wandb:                     val/r2 -0.00966
wandb:                   val/rmse 0.01608
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/3gz4ybi8
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_235421-3gz4ybi8/logs
Completed: NASDAQ H=100

Training: Mamba on ABSA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_235538-5rg2azok
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/5rg2azok
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H3       Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/5rg2azok
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 3
============================================================
train 2201
val 123
test 125
Epoch: 1, Steps: 69 | Train Loss: 0.3436875 Vali Loss: 0.0762479 Test Loss: 0.1758453
Validation loss decreased (inf --> 0.076248).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 69 | Train Loss: 0.3342773 Vali Loss: 0.0765594 Test Loss: 0.1726981
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
Epoch: 3, Steps: 69 | Train Loss: 0.3275231 Vali Loss: 0.0763127 Test Loss: 0.1715318
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 69 | Train Loss: 0.3223424 Vali Loss: 0.0754392 Test Loss: 0.1714103
Validation loss decreased (0.076248 --> 0.075439).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 69 | Train Loss: 0.3210350 Vali Loss: 0.0756130 Test Loss: 0.1712789
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 69 | Train Loss: 0.3174690 Vali Loss: 0.0753147 Test Loss: 0.1712530
Validation loss decreased (0.075439 --> 0.075315).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 69 | Train Loss: 0.3168234 Vali Loss: 0.0752684 Test Loss: 0.1712550
Validation loss decreased (0.075315 --> 0.075268).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 69 | Train Loss: 0.3154168 Vali Loss: 0.0747644 Test Loss: 0.1712524
Validation loss decreased (0.075268 --> 0.074764).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 69 | Train Loss: 0.3156140 Vali Loss: 0.0750929 Test Loss: 0.1712484
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 69 | Train Loss: 0.3150024 Vali Loss: 0.0746782 Test Loss: 0.1712481
Validation loss decreased (0.074764 --> 0.074678).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 69 | Train Loss: 0.3148454 Vali Loss: 0.0751779 Test Loss: 0.1712481
EarlyStopping counter: 1 out of 5
Updating learning rate to 9.765625e-08
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3436874736478363, 'val/loss': 0.076247937977314, 'test/loss': 0.17584528028964996, '_timestamp': 1762293345.0905035}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.33427734424670535, 'val/loss': 0.0765593945980072, 'test/loss': 0.17269810661673546, '_timestamp': 1762293346.3349798}).
Epoch: 12, Steps: 69 | Train Loss: 0.3164925 Vali Loss: 0.0746157 Test Loss: 0.1712480
Validation loss decreased (0.074678 --> 0.074616).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 69 | Train Loss: 0.3150285 Vali Loss: 0.0753353 Test Loss: 0.1712478
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 69 | Train Loss: 0.3161339 Vali Loss: 0.0750588 Test Loss: 0.1712478
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 69 | Train Loss: 0.3149963 Vali Loss: 0.0749132 Test Loss: 0.1712478
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 69 | Train Loss: 0.3167421 Vali Loss: 0.0749491 Test Loss: 0.1712477
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 69 | Train Loss: 0.3159816 Vali Loss: 0.0750365 Test Loss: 0.1712477
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 125
test shape: (125, 3, 1) (125, 3, 1)
test shape: (125, 3, 1) (125, 3, 1)


	mse:0.000492267485242337, mae:0.016441792249679565, rmse:0.02218710258603096, r2:-0.024085164070129395, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0164, RMSE: 0.0222, RÂ²: -0.0241, MAPE: 1.15%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.481 MB of 0.482 MB uploadedwandb: \ 0.481 MB of 0.482 MB uploadedwandb: | 0.481 MB of 0.482 MB uploadedwandb: / 0.482 MB of 0.482 MB uploadedwandb: - 0.482 MB of 0.482 MB uploadedwandb: \ 0.482 MB of 0.482 MB uploadedwandb: | 0.482 MB of 0.482 MB uploadedwandb: / 0.482 MB of 0.482 MB uploadedwandb: - 0.530 MB of 0.602 MB uploadedwandb: \ 0.602 MB of 0.602 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–â–â–â–â–‚â–â–‚â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–…â–„â–„â–‚â–ƒâ–â–ƒâ–â–„â–ƒâ–‚â–‚â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35218
wandb:     model/trainable_params 35218
wandb:                  test/loss 0.17125
wandb:                 train/loss 0.31598
wandb:   val/directional_accuracy 42.4
wandb:                   val/loss 0.07504
wandb:                    val/mae 0.01644
wandb:                   val/mape 115.49261
wandb:                    val/mse 0.00049
wandb:                     val/r2 -0.02409
wandb:                   val/rmse 0.02219
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/5rg2azok
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_235538-5rg2azok/logs
Completed: ABSA H=3

Training: Mamba on ABSA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_235632-6omqjicf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/6omqjicf
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H5       Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/6omqjicf
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 5
============================================================
train 2199
val 121
test 123
Epoch: 1, Steps: 69 | Train Loss: 0.3506740 Vali Loss: 0.0763546 Test Loss: 0.1791413
Validation loss decreased (inf --> 0.076355).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 69 | Train Loss: 0.3428839 Vali Loss: 0.0766969 Test Loss: 0.1769851
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
Epoch: 3, Steps: 69 | Train Loss: 0.3359216 Vali Loss: 0.0772930 Test Loss: 0.1761482
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 69 | Train Loss: 0.3310863 Vali Loss: 0.0772802 Test Loss: 0.1758160
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 69 | Train Loss: 0.3304682 Vali Loss: 0.0778539 Test Loss: 0.1757099
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 69 | Train Loss: 0.3279964 Vali Loss: 0.0770884 Test Loss: 0.1756767
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 123
test shape: (123, 5, 1) (123, 5, 1)
test shape: (123, 5, 1) (123, 5, 1)


	mse:0.0004962827078998089, mae:0.016737747937440872, rmse:0.022277403622865677, r2:-0.019280552864074707, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0167, RMSE: 0.0223, RÂ²: -0.0193, MAPE: 1.30%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.514 MB of 0.515 MB uploadedwandb: \ 0.514 MB of 0.515 MB uploadedwandb: | 0.515 MB of 0.515 MB uploadedwandb: / 0.515 MB of 0.515 MB uploadedwandb: - 0.515 MB of 0.584 MB uploadedwandb: \ 0.584 MB of 0.584 MB uploadedwandb: | 0.584 MB of 0.584 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–ƒâ–†â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–ƒâ–ˆâ–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 5
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35998
wandb:     model/trainable_params 35998
wandb:                  test/loss 0.17568
wandb:                 train/loss 0.328
wandb:   val/directional_accuracy 48.37398
wandb:                   val/loss 0.07709
wandb:                    val/mae 0.01674
wandb:                   val/mape 129.85862
wandb:                    val/mse 0.0005
wandb:                     val/r2 -0.01928
wandb:                   val/rmse 0.02228
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/6omqjicf
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_235632-6omqjicf/logs
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3506740374841552, 'val/loss': 0.07635461166501045, 'test/loss': 0.17914128303527832, '_timestamp': 1762293399.0341506}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.34288387216519617, 'val/loss': 0.07669693790376186, 'test/loss': 0.17698509618639946, '_timestamp': 1762293400.3300338}).
Completed: ABSA H=5

Training: Mamba on ABSA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_235708-qs8i1bz5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/qs8i1bz5
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H10      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/qs8i1bz5
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 10
============================================================
train 2194
val 116
test 118
Epoch: 1, Steps: 69 | Train Loss: 0.3614107 Vali Loss: 0.0775093 Test Loss: 0.1822332
Validation loss decreased (inf --> 0.077509).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 69 | Train Loss: 0.3547322 Vali Loss: 0.0774620 Test Loss: 0.1817255
Validation loss decreased (0.077509 --> 0.077462).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 69 | Train Loss: 0.3512786 Vali Loss: 0.0780297 Test Loss: 0.1812910
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 69 | Train Loss: 0.3510560 Vali Loss: 0.0772493 Test Loss: 0.1811276
Validation loss decreased (0.077462 --> 0.077249).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 69 | Train Loss: 0.3464479 Vali Loss: 0.0778666 Test Loss: 0.1810534
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 69 | Train Loss: 0.3475731 Vali Loss: 0.0773892 Test Loss: 0.1810149
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 69 | Train Loss: 0.3479689 Vali Loss: 0.0773809 Test Loss: 0.1809955
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 69 | Train Loss: 0.3468403 Vali Loss: 0.0788978 Test Loss: 0.1809852
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 69 | Train Loss: 0.3453514 Vali Loss: 0.0780067 Test Loss: 0.1809820
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 118
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.36141069773314655, 'val/loss': 0.07750931195914745, 'test/loss': 0.18223319947719574, '_timestamp': 1762293434.4636245}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3547321633584257, 'val/loss': 0.07746203243732452, 'test/loss': 0.18172552064061165, '_timestamp': 1762293435.7491908}).
test shape: (118, 10, 1) (118, 10, 1)
test shape: (118, 10, 1) (118, 10, 1)


	mse:0.0005066202720627189, mae:0.016736747696995735, rmse:0.022508226335048676, r2:-0.01871788501739502, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0167, RMSE: 0.0225, RÂ²: -0.0187, MAPE: 1.23%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.544 MB of 0.544 MB uploadedwandb: \ 0.544 MB of 0.544 MB uploadedwandb: | 0.544 MB of 0.544 MB uploadedwandb: / 0.544 MB of 0.544 MB uploadedwandb: - 0.544 MB of 0.544 MB uploadedwandb: \ 0.544 MB of 0.615 MB uploadedwandb: | 0.615 MB of 0.615 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–ƒâ–‚â–â–â–
wandb:                 train/loss â–ˆâ–ˆâ–‚â–„â–„â–ƒâ–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–â–„â–‚â–‚â–ˆâ–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 8
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 37948
wandb:     model/trainable_params 37948
wandb:                  test/loss 0.18098
wandb:                 train/loss 0.34535
wandb:   val/directional_accuracy 48.39925
wandb:                   val/loss 0.07801
wandb:                    val/mae 0.01674
wandb:                   val/mape 123.30767
wandb:                    val/mse 0.00051
wandb:                     val/r2 -0.01872
wandb:                   val/rmse 0.02251
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/qs8i1bz5
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_235708-qs8i1bz5/logs
Completed: ABSA H=10

Training: Mamba on ABSA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_235749-w9636ey9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/w9636ey9
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H22      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/w9636ey9
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 22
============================================================
train 2182
val 104
test 106
Epoch: 1, Steps: 69 | Train Loss: 0.3782769 Vali Loss: 0.0800836 Test Loss: 0.1856408
Validation loss decreased (inf --> 0.080084).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 69 | Train Loss: 0.3737915 Vali Loss: 0.0833780 Test Loss: 0.1857534
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
Epoch: 3, Steps: 69 | Train Loss: 0.3711630 Vali Loss: 0.0832117 Test Loss: 0.1858058
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 69 | Train Loss: 0.3707738 Vali Loss: 0.0844512 Test Loss: 0.1859538
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 69 | Train Loss: 0.3667005 Vali Loss: 0.0829544 Test Loss: 0.1860647
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 69 | Train Loss: 0.3679442 Vali Loss: 0.0824800 Test Loss: 0.1861399
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 106
test shape: (106, 22, 1) (106, 22, 1)
test shape: (106, 22, 1) (106, 22, 1)


	mse:0.0005196523852646351, mae:0.01691429503262043, rmse:0.022795885801315308, r2:-0.015055179595947266, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0169, RMSE: 0.0228, RÂ²: -0.0151, MAPE: 1.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.592 MB of 0.593 MB uploadedwandb: \ 0.592 MB of 0.593 MB uploadedwandb: | 0.593 MB of 0.593 MB uploadedwandb: / 0.593 MB of 0.593 MB uploadedwandb: - 0.593 MB of 0.663 MB uploadedwandb: \ 0.663 MB of 0.663 MB uploadedwandb: | 0.663 MB of 0.663 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–ƒâ–†â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–„â–†â–ˆ
wandb:                 train/loss â–ˆâ–‡â–â–ƒ
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–ˆâ–ƒâ–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 5
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 42628
wandb:     model/trainable_params 42628
wandb:                  test/loss 0.18614
wandb:                 train/loss 0.36794
wandb:   val/directional_accuracy 50.08985
wandb:                   val/loss 0.08248
wandb:                    val/mae 0.01691
wandb:                   val/mape 125.03316
wandb:                    val/mse 0.00052
wandb:                     val/r2 -0.01506
wandb:                   val/rmse 0.0228
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/w9636ey9
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_235749-w9636ey9/logs
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.37827688196431036, 'val/loss': 0.08008359000086784, 'test/loss': 0.1856408454477787, '_timestamp': 1762293475.796563}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3737915469252545, 'val/loss': 0.0833780150860548, 'test/loss': 0.18575341254472733, '_timestamp': 1762293477.0566661}).
Completed: ABSA H=22

Training: Mamba on ABSA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_235826-50yhvka6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/50yhvka6
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H50      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/50yhvka6
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 50
============================================================
train 2154
val 76
test 78
Epoch: 1, Steps: 68 | Train Loss: 0.4251026 Vali Loss: 0.0896252 Test Loss: 0.1903663
Validation loss decreased (inf --> 0.089625).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 68 | Train Loss: 0.4185927 Vali Loss: 0.0911116 Test Loss: 0.1898967
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
Epoch: 3, Steps: 68 | Train Loss: 0.4180626 Vali Loss: 0.0915602 Test Loss: 0.1896895
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 68 | Train Loss: 0.4171110 Vali Loss: 0.0915875 Test Loss: 0.1895540
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 68 | Train Loss: 0.4139550 Vali Loss: 0.0925242 Test Loss: 0.1894915
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 68 | Train Loss: 0.4127262 Vali Loss: 0.0923905 Test Loss: 0.1894701
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 78
test shape: (78, 50, 1) (78, 50, 1)
test shape: (78, 50, 1) (78, 50, 1)


	mse:0.0005464903078973293, mae:0.016933349892497063, rmse:0.023377131670713425, r2:-0.00796961784362793, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0169, RMSE: 0.0234, RÂ²: -0.0080, MAPE: 1.31%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.662 MB of 0.665 MB uploadedwandb: \ 0.662 MB of 0.665 MB uploadedwandb: | 0.665 MB of 0.665 MB uploadedwandb: / 0.665 MB of 0.665 MB uploadedwandb: - 0.665 MB of 0.735 MB uploadedwandb: \ 0.735 MB of 0.735 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–ƒâ–†â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–
wandb:                 train/loss â–ˆâ–‡â–ƒâ–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–â–ˆâ–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 5
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 53548
wandb:     model/trainable_params 53548
wandb:                  test/loss 0.18947
wandb:                 train/loss 0.41273
wandb:   val/directional_accuracy 49.86918
wandb:                   val/loss 0.09239
wandb:                    val/mae 0.01693
wandb:                   val/mape 130.53868
wandb:                    val/mse 0.00055
wandb:                     val/r2 -0.00797
wandb:                   val/rmse 0.02338
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/50yhvka6
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_235826-50yhvka6/logs
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.42510263577980156, 'val/loss': 0.08962523688872655, 'test/loss': 0.19036632776260376, '_timestamp': 1762293512.0587308}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.4185926675358239, 'val/loss': 0.09111155072848003, 'test/loss': 0.18989665309588113, '_timestamp': 1762293513.286489}).
Completed: ABSA H=50

Training: Mamba on ABSA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_235900-66qnf8wn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/66qnf8wn
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H100     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/66qnf8wn
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 100
============================================================
train 2104
val 26
test 28
Epoch: 1, Steps: 66 | Train Loss: 0.5061914 Vali Loss: 0.1134927 Test Loss: 0.2015319
Validation loss decreased (inf --> 0.113493).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 66 | Train Loss: 0.5032443 Vali Loss: 0.1144529 Test Loss: 0.1999209
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
Epoch: 3, Steps: 66 | Train Loss: 0.5013720 Vali Loss: 0.1154713 Test Loss: 0.1985652
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 66 | Train Loss: 0.4978120 Vali Loss: 0.1161364 Test Loss: 0.1977068
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 66 | Train Loss: 0.4954465 Vali Loss: 0.1164843 Test Loss: 0.1972873
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 66 | Train Loss: 0.4955910 Vali Loss: 0.1166519 Test Loss: 0.1970946
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 28
test shape: (28, 100, 1) (28, 100, 1)
test shape: (28, 100, 1) (28, 100, 1)


	mse:0.000527188356500119, mae:0.016779376193881035, rmse:0.022960582748055458, r2:-0.01679837703704834, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0168, RMSE: 0.0230, RÂ²: -0.0168, MAPE: 1.20%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.656 MB of 0.661 MB uploadedwandb: \ 0.661 MB of 0.661 MB uploadedwandb: | 0.661 MB of 0.661 MB uploadedwandb: / 0.661 MB of 0.731 MB uploadedwandb: - 0.680 MB of 0.731 MB uploadedwandb: \ 0.731 MB of 0.731 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–ƒâ–†â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–
wandb:                 train/loss â–ˆâ–„â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–…â–‡â–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 5
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 73048
wandb:     model/trainable_params 73048
wandb:                  test/loss 0.19709
wandb:                 train/loss 0.49559
wandb:   val/directional_accuracy 50.64935
wandb:                   val/loss 0.11665
wandb:                    val/mae 0.01678
wandb:                   val/mape 120.4067
wandb:                    val/mse 0.00053
wandb:                     val/r2 -0.0168
wandb:                   val/rmse 0.02296
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/66qnf8wn
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_235900-66qnf8wn/logs
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.5061913692589962, 'val/loss': 0.11349274218082428, 'test/loss': 0.20153185725212097, '_timestamp': 1762293545.7766244}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.5032443151329503, 'val/loss': 0.11445288360118866, 'test/loss': 0.1999209076166153, '_timestamp': 1762293546.9933648}).
Completed: ABSA H=100

Training: Mamba on SASOL for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251104_235935-s6a661g2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/s6a661g2
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H3      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/s6a661g2
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 3
============================================================
train 2201
val 123
test 125
Epoch: 1, Steps: 69 | Train Loss: 0.2911541 Vali Loss: 0.2254851 Test Loss: 0.1880001
Validation loss decreased (inf --> 0.225485).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 69 | Train Loss: 0.2784783 Vali Loss: 0.2216498 Test Loss: 0.1858445
Validation loss decreased (0.225485 --> 0.221650).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 69 | Train Loss: 0.2657987 Vali Loss: 0.2130625 Test Loss: 0.1857904
Validation loss decreased (0.221650 --> 0.213062).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 69 | Train Loss: 0.2592146 Vali Loss: 0.2077588 Test Loss: 0.1847363
Validation loss decreased (0.213062 --> 0.207759).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 69 | Train Loss: 0.2540656 Vali Loss: 0.2057249 Test Loss: 0.1844464
Validation loss decreased (0.207759 --> 0.205725).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 69 | Train Loss: 0.2520523 Vali Loss: 0.2021860 Test Loss: 0.1841882
Validation loss decreased (0.205725 --> 0.202186).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 69 | Train Loss: 0.2500247 Vali Loss: 0.2023800 Test Loss: 0.1840891
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 69 | Train Loss: 0.2501361 Vali Loss: 0.2006414 Test Loss: 0.1840381
Validation loss decreased (0.202186 --> 0.200641).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 69 | Train Loss: 0.2488096 Vali Loss: 0.2016507 Test Loss: 0.1840129
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 69 | Train Loss: 0.2485907 Vali Loss: 0.1999366 Test Loss: 0.1840067
Validation loss decreased (0.200641 --> 0.199937).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 69 | Train Loss: 0.2492334 Vali Loss: 0.2018526 Test Loss: 0.1839990
EarlyStopping counter: 1 out of 5
Updating learning rate to 9.765625e-08
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2911540503087251, 'val/loss': 0.22548511624336243, 'test/loss': 0.18800008669495583, '_timestamp': 1762293582.7549596}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2784782974184423, 'val/loss': 0.22164975479245186, 'test/loss': 0.18584450893104076, '_timestamp': 1762293584.0297558}).
Epoch: 12, Steps: 69 | Train Loss: 0.2494102 Vali Loss: 0.2010010 Test Loss: 0.1839964
EarlyStopping counter: 2 out of 5
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 69 | Train Loss: 0.2488961 Vali Loss: 0.2017685 Test Loss: 0.1839940
EarlyStopping counter: 3 out of 5
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 69 | Train Loss: 0.2491163 Vali Loss: 0.2028374 Test Loss: 0.1839930
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 69 | Train Loss: 0.2493848 Vali Loss: 0.1992901 Test Loss: 0.1839927
Validation loss decreased (0.199937 --> 0.199290).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 69 | Train Loss: 0.2492417 Vali Loss: 0.2001832 Test Loss: 0.1839926
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 69 | Train Loss: 0.2485366 Vali Loss: 0.2000075 Test Loss: 0.1839925
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 69 | Train Loss: 0.2484648 Vali Loss: 0.1991115 Test Loss: 0.1839925
Validation loss decreased (0.199290 --> 0.199111).  Saving model ...
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 69 | Train Loss: 0.2494148 Vali Loss: 0.2013655 Test Loss: 0.1839925
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 69 | Train Loss: 0.2491187 Vali Loss: 0.2008205 Test Loss: 0.1839925
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 69 | Train Loss: 0.2504095 Vali Loss: 0.2008865 Test Loss: 0.1839925
EarlyStopping counter: 3 out of 5
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 69 | Train Loss: 0.2489954 Vali Loss: 0.2000241 Test Loss: 0.1839925
EarlyStopping counter: 4 out of 5
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 69 | Train Loss: 0.2484734 Vali Loss: 0.2010248 Test Loss: 0.1839925
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 125
test shape: (125, 3, 1) (125, 3, 1)
test shape: (125, 3, 1) (125, 3, 1)


	mse:0.0014861339004710317, mae:0.02786470577120781, rmse:0.03855040669441223, r2:-0.009676933288574219, dtw:Not calculated


VAL - MSE: 0.0015, MAE: 0.0279, RMSE: 0.0386, RÂ²: -0.0097, MAPE: 5514415.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.456 MB of 0.456 MB uploadedwandb: \ 0.456 MB of 0.456 MB uploadedwandb: | 0.456 MB of 0.456 MB uploadedwandb: / 0.456 MB of 0.456 MB uploadedwandb: - 0.456 MB of 0.456 MB uploadedwandb: \ 0.456 MB of 0.456 MB uploadedwandb: | 0.505 MB of 0.578 MB uploadedwandb: / 0.505 MB of 0.578 MB uploadedwandb: - 0.578 MB of 0.578 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–â–‚â–‚â–‚â–ƒâ–â–‚â–â–â–‚â–‚â–‚â–â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35218
wandb:     model/trainable_params 35218
wandb:                  test/loss 0.18399
wandb:                 train/loss 0.24847
wandb:   val/directional_accuracy 42.4
wandb:                   val/loss 0.20102
wandb:                    val/mae 0.02786
wandb:                   val/mape 551441550.0
wandb:                    val/mse 0.00149
wandb:                     val/r2 -0.00968
wandb:                   val/rmse 0.03855
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/s6a661g2
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251104_235935-s6a661g2/logs
Completed: SASOL H=3

Training: Mamba on SASOL for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_000037-mdlubr0t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/mdlubr0t
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H5      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/mdlubr0t
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 5
============================================================
train 2199
val 121
test 123
Epoch: 1, Steps: 69 | Train Loss: 0.2975287 Vali Loss: 0.2329236 Test Loss: 0.1928131
Validation loss decreased (inf --> 0.232924).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 69 | Train Loss: 0.2839307 Vali Loss: 0.2251250 Test Loss: 0.1879000
Validation loss decreased (0.232924 --> 0.225125).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 69 | Train Loss: 0.2703070 Vali Loss: 0.2132438 Test Loss: 0.1879739
Validation loss decreased (0.225125 --> 0.213244).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 69 | Train Loss: 0.2608305 Vali Loss: 0.2091488 Test Loss: 0.1880266
Validation loss decreased (0.213244 --> 0.209149).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 69 | Train Loss: 0.2564127 Vali Loss: 0.2057570 Test Loss: 0.1880850
Validation loss decreased (0.209149 --> 0.205757).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 69 | Train Loss: 0.2527107 Vali Loss: 0.2042383 Test Loss: 0.1878761
Validation loss decreased (0.205757 --> 0.204238).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 69 | Train Loss: 0.2521391 Vali Loss: 0.2035450 Test Loss: 0.1878375
Validation loss decreased (0.204238 --> 0.203545).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 69 | Train Loss: 0.2512330 Vali Loss: 0.2053263 Test Loss: 0.1878019
EarlyStopping counter: 1 out of 5
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 69 | Train Loss: 0.2502736 Vali Loss: 0.2039222 Test Loss: 0.1877846
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 69 | Train Loss: 0.2506999 Vali Loss: 0.2030504 Test Loss: 0.1877828
Validation loss decreased (0.203545 --> 0.203050).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 69 | Train Loss: 0.2507586 Vali Loss: 0.2043330 Test Loss: 0.1877791
EarlyStopping counter: 1 out of 5
Updating learning rate to 9.765625e-08
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2975287407204725, 'val/loss': 0.23292364180088043, 'test/loss': 0.1928130891174078, '_timestamp': 1762293644.2041714}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2839306700920713, 'val/loss': 0.2251250259578228, 'test/loss': 0.18790003284811974, '_timestamp': 1762293645.4989586}).
Epoch: 12, Steps: 69 | Train Loss: 0.2507152 Vali Loss: 0.2030926 Test Loss: 0.1877763
EarlyStopping counter: 2 out of 5
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 69 | Train Loss: 0.2513342 Vali Loss: 0.2042104 Test Loss: 0.1877743
EarlyStopping counter: 3 out of 5
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 69 | Train Loss: 0.2519252 Vali Loss: 0.2030916 Test Loss: 0.1877736
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 69 | Train Loss: 0.2501570 Vali Loss: 0.2031698 Test Loss: 0.1877733
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 123
test shape: (123, 5, 1) (123, 5, 1)
test shape: (123, 5, 1) (123, 5, 1)


	mse:0.0014826684491708875, mae:0.027471370995044708, rmse:0.0385054349899292, r2:-0.004598140716552734, dtw:Not calculated


VAL - MSE: 0.0015, MAE: 0.0275, RMSE: 0.0385, RÂ²: -0.0046, MAPE: 4463699.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.507 MB of 0.507 MB uploadedwandb: \ 0.507 MB of 0.507 MB uploadedwandb: | 0.507 MB of 0.507 MB uploadedwandb: / 0.507 MB of 0.507 MB uploadedwandb: - 0.507 MB of 0.507 MB uploadedwandb: \ 0.507 MB of 0.579 MB uploadedwandb: | 0.579 MB of 0.579 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–†â–‡â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–ƒâ–‚â–â–ƒâ–‚â–â–‚â–â–‚â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35998
wandb:     model/trainable_params 35998
wandb:                  test/loss 0.18777
wandb:                 train/loss 0.25016
wandb:   val/directional_accuracy 48.17073
wandb:                   val/loss 0.20317
wandb:                    val/mae 0.02747
wandb:                   val/mape 446369950.0
wandb:                    val/mse 0.00148
wandb:                     val/r2 -0.0046
wandb:                   val/rmse 0.03851
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/mdlubr0t
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_000037-mdlubr0t/logs
Completed: SASOL H=5

Training: Mamba on SASOL for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_000127-s815x0a2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/s815x0a2
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H10     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/s815x0a2
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 10
============================================================
train 2194
val 116
test 118
Epoch: 1, Steps: 69 | Train Loss: 0.3088569 Vali Loss: 0.2595279 Test Loss: 0.1975818
Validation loss decreased (inf --> 0.259528).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 69 | Train Loss: 0.2997915 Vali Loss: 0.2523092 Test Loss: 0.1917843
Validation loss decreased (0.259528 --> 0.252309).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 69 | Train Loss: 0.2876681 Vali Loss: 0.2322805 Test Loss: 0.1906563
Validation loss decreased (0.252309 --> 0.232281).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 69 | Train Loss: 0.2783540 Vali Loss: 0.2222374 Test Loss: 0.1911063
Validation loss decreased (0.232281 --> 0.222237).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 69 | Train Loss: 0.2714536 Vali Loss: 0.2148870 Test Loss: 0.1913334
Validation loss decreased (0.222237 --> 0.214887).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 69 | Train Loss: 0.2689134 Vali Loss: 0.2101320 Test Loss: 0.1915087
Validation loss decreased (0.214887 --> 0.210132).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 69 | Train Loss: 0.2684945 Vali Loss: 0.2094753 Test Loss: 0.1915203
Validation loss decreased (0.210132 --> 0.209475).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 69 | Train Loss: 0.2672187 Vali Loss: 0.2161697 Test Loss: 0.1915328
EarlyStopping counter: 1 out of 5
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 69 | Train Loss: 0.2673785 Vali Loss: 0.2121450 Test Loss: 0.1915177
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 69 | Train Loss: 0.2676726 Vali Loss: 0.2091731 Test Loss: 0.1915174
Validation loss decreased (0.209475 --> 0.209173).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 69 | Train Loss: 0.2665572 Vali Loss: 0.2070099 Test Loss: 0.1915191
Validation loss decreased (0.209173 --> 0.207010).  Saving model ...
Updating learning rate to 9.765625e-08
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3088568585074466, 'val/loss': 0.25952790305018425, 'test/loss': 0.19758178479969501, '_timestamp': 1762293692.541635}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2997914600199547, 'val/loss': 0.2523092031478882, 'test/loss': 0.19178426824510098, '_timestamp': 1762293693.805685}).
Epoch: 12, Steps: 69 | Train Loss: 0.2677675 Vali Loss: 0.2114198 Test Loss: 0.1915197
EarlyStopping counter: 1 out of 5
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 69 | Train Loss: 0.2663623 Vali Loss: 0.2132159 Test Loss: 0.1915201
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 69 | Train Loss: 0.2668908 Vali Loss: 0.2109780 Test Loss: 0.1915198
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 69 | Train Loss: 0.2664890 Vali Loss: 0.2080197 Test Loss: 0.1915197
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 69 | Train Loss: 0.2675469 Vali Loss: 0.2075821 Test Loss: 0.1915196
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 118
test shape: (118, 10, 1) (118, 10, 1)
test shape: (118, 10, 1) (118, 10, 1)


	mse:0.0015186931705102324, mae:0.0276484414935112, rmse:0.03897041454911232, r2:-0.00892019271850586, dtw:Not calculated


VAL - MSE: 0.0015, MAE: 0.0276, RMSE: 0.0390, RÂ²: -0.0089, MAPE: 5478644.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.567 MB of 0.567 MB uploadedwandb: \ 0.567 MB of 0.567 MB uploadedwandb: | 0.567 MB of 0.567 MB uploadedwandb: / 0.567 MB of 0.567 MB uploadedwandb: - 0.567 MB of 0.567 MB uploadedwandb: \ 0.567 MB of 0.639 MB uploadedwandb: | 0.639 MB of 0.639 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–…â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–ƒâ–‚â–‚â–„â–‚â–‚â–â–‚â–ƒâ–‚â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 37948
wandb:     model/trainable_params 37948
wandb:                  test/loss 0.19152
wandb:                 train/loss 0.26755
wandb:   val/directional_accuracy 49.05838
wandb:                   val/loss 0.20758
wandb:                    val/mae 0.02765
wandb:                   val/mape 547864450.0
wandb:                    val/mse 0.00152
wandb:                     val/r2 -0.00892
wandb:                   val/rmse 0.03897
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/s815x0a2
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_000127-s815x0a2/logs
Completed: SASOL H=10

Training: Mamba on SASOL for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_000215-ojegf8i8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/ojegf8i8
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H22     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/ojegf8i8
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 22
============================================================
train 2182
val 104
test 106
Epoch: 1, Steps: 69 | Train Loss: 0.3333493 Vali Loss: 0.3045451 Test Loss: 0.2060489
Validation loss decreased (inf --> 0.304545).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 69 | Train Loss: 0.3214864 Vali Loss: 0.2862514 Test Loss: 0.2017226
Validation loss decreased (0.304545 --> 0.286251).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 69 | Train Loss: 0.3068525 Vali Loss: 0.2489255 Test Loss: 0.2023548
Validation loss decreased (0.286251 --> 0.248925).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 69 | Train Loss: 0.2975946 Vali Loss: 0.2712712 Test Loss: 0.2026673
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 69 | Train Loss: 0.2906983 Vali Loss: 0.2343660 Test Loss: 0.2026165
Validation loss decreased (0.248925 --> 0.234366).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 69 | Train Loss: 0.2877593 Vali Loss: 0.2332845 Test Loss: 0.2024818
Validation loss decreased (0.234366 --> 0.233285).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 69 | Train Loss: 0.2866362 Vali Loss: 0.2565945 Test Loss: 0.2024647
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 69 | Train Loss: 0.2860307 Vali Loss: 0.2451542 Test Loss: 0.2024099
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 69 | Train Loss: 0.2864242 Vali Loss: 0.2375939 Test Loss: 0.2023955
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 69 | Train Loss: 0.2888887 Vali Loss: 0.2434303 Test Loss: 0.2023933
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 69 | Train Loss: 0.2864033 Vali Loss: 0.2295036 Test Loss: 0.2023908
Validation loss decreased (0.233285 --> 0.229504).  Saving model ...
Updating learning rate to 9.765625e-08
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.33334927304067474, 'val/loss': 0.304545059800148, 'test/loss': 0.20604892261326313, '_timestamp': 1762293741.6791008}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3214863603529723, 'val/loss': 0.2862514331936836, 'test/loss': 0.20172260142862797, '_timestamp': 1762293742.990438}).
Epoch: 12, Steps: 69 | Train Loss: 0.2853362 Vali Loss: 0.2278239 Test Loss: 0.2023891
Validation loss decreased (0.229504 --> 0.227824).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 69 | Train Loss: 0.2929917 Vali Loss: 0.2367570 Test Loss: 0.2023877
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 69 | Train Loss: 0.2863840 Vali Loss: 0.2466596 Test Loss: 0.2023874
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 69 | Train Loss: 0.2871678 Vali Loss: 0.2513427 Test Loss: 0.2023872
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 69 | Train Loss: 0.2860204 Vali Loss: 0.2291610 Test Loss: 0.2023869
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 69 | Train Loss: 0.2867212 Vali Loss: 0.2468189 Test Loss: 0.2023869
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 106
test shape: (106, 22, 1) (106, 22, 1)
test shape: (106, 22, 1) (106, 22, 1)


	mse:0.0015372477937489748, mae:0.02762870118021965, rmse:0.03920775279402733, r2:-0.013973355293273926, dtw:Not calculated


VAL - MSE: 0.0015, MAE: 0.0276, RMSE: 0.0392, RÂ²: -0.0140, MAPE: 6759642.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.619 MB of 0.620 MB uploadedwandb: \ 0.619 MB of 0.620 MB uploadedwandb: | 0.619 MB of 0.620 MB uploadedwandb: / 0.620 MB of 0.620 MB uploadedwandb: - 0.620 MB of 0.620 MB uploadedwandb: \ 0.620 MB of 0.693 MB uploadedwandb: | 0.693 MB of 0.693 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–‚â–â–â–ƒâ–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–ˆâ–‚â–‚â–†â–„â–ƒâ–„â–â–â–‚â–„â–…â–â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 42628
wandb:     model/trainable_params 42628
wandb:                  test/loss 0.20239
wandb:                 train/loss 0.28672
wandb:   val/directional_accuracy 49.95508
wandb:                   val/loss 0.24682
wandb:                    val/mae 0.02763
wandb:                   val/mape 675964200.0
wandb:                    val/mse 0.00154
wandb:                     val/r2 -0.01397
wandb:                   val/rmse 0.03921
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/ojegf8i8
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_000215-ojegf8i8/logs
Completed: SASOL H=22

Training: Mamba on SASOL for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_000310-djzrazkf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/djzrazkf
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H50     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/djzrazkf
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 50
============================================================
train 2154
val 76
test 78
Epoch: 1, Steps: 68 | Train Loss: 0.3842850 Vali Loss: 0.4571402 Test Loss: 0.2837576
Validation loss decreased (inf --> 0.457140).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 68 | Train Loss: 0.3775232 Vali Loss: 0.4269490 Test Loss: 0.2831979
Validation loss decreased (0.457140 --> 0.426949).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 68 | Train Loss: 0.3629800 Vali Loss: 0.3797593 Test Loss: 0.2868991
Validation loss decreased (0.426949 --> 0.379759).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 68 | Train Loss: 0.3520731 Vali Loss: 0.3559407 Test Loss: 0.2888554
Validation loss decreased (0.379759 --> 0.355941).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 68 | Train Loss: 0.3466256 Vali Loss: 0.3452836 Test Loss: 0.2896432
Validation loss decreased (0.355941 --> 0.345284).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 68 | Train Loss: 0.3428970 Vali Loss: 0.3417830 Test Loss: 0.2898153
Validation loss decreased (0.345284 --> 0.341783).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 68 | Train Loss: 0.3421727 Vali Loss: 0.3539059 Test Loss: 0.2899646
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 68 | Train Loss: 0.3434129 Vali Loss: 0.3553371 Test Loss: 0.2899424
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 68 | Train Loss: 0.3409731 Vali Loss: 0.3444487 Test Loss: 0.2899455
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 68 | Train Loss: 0.3400116 Vali Loss: 0.3449549 Test Loss: 0.2899443
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 68 | Train Loss: 0.3419842 Vali Loss: 0.3438105 Test Loss: 0.2899497
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 78
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.38428497270626183, 'val/loss': 0.4571402172247569, 'test/loss': 0.28375764687856037, '_timestamp': 1762293798.1252182}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.37752318426090126, 'val/loss': 0.42694904406865436, 'test/loss': 0.28319793442885083, '_timestamp': 1762293799.4131472}).
test shape: (78, 50, 1) (78, 50, 1)
test shape: (78, 50, 1) (78, 50, 1)


	mse:0.0016368554206565022, mae:0.028393836691975594, rmse:0.0404580682516098, r2:-0.01361083984375, dtw:Not calculated


VAL - MSE: 0.0016, MAE: 0.0284, RMSE: 0.0405, RÂ²: -0.0136, MAPE: 7233640.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.683 MB of 0.685 MB uploadedwandb: \ 0.683 MB of 0.685 MB uploadedwandb: | 0.683 MB of 0.685 MB uploadedwandb: / 0.685 MB of 0.685 MB uploadedwandb: - 0.685 MB of 0.756 MB uploadedwandb: \ 0.756 MB of 0.756 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–…â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–‚â–â–ƒâ–ƒâ–â–‚â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 53548
wandb:     model/trainable_params 53548
wandb:                  test/loss 0.28995
wandb:                 train/loss 0.34198
wandb:   val/directional_accuracy 50.62794
wandb:                   val/loss 0.34381
wandb:                    val/mae 0.02839
wandb:                   val/mape 723364050.0
wandb:                    val/mse 0.00164
wandb:                     val/r2 -0.01361
wandb:                   val/rmse 0.04046
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/djzrazkf
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_000310-djzrazkf/logs
Completed: SASOL H=50

Training: Mamba on SASOL for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_000354-xpfgaqtw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/xpfgaqtw
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H100    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/xpfgaqtw
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 100
============================================================
train 2104
val 26
test 28
Epoch: 1, Steps: 66 | Train Loss: 0.4853259 Vali Loss: 0.8935979 Test Loss: 0.2796466
Validation loss decreased (inf --> 0.893598).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 66 | Train Loss: 0.4761027 Vali Loss: 0.8208033 Test Loss: 0.2809303
Validation loss decreased (0.893598 --> 0.820803).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 66 | Train Loss: 0.4580625 Vali Loss: 0.7420661 Test Loss: 0.2818850
Validation loss decreased (0.820803 --> 0.742066).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 66 | Train Loss: 0.4445873 Vali Loss: 0.7058831 Test Loss: 0.2821879
Validation loss decreased (0.742066 --> 0.705883).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 66 | Train Loss: 0.4384130 Vali Loss: 0.6942729 Test Loss: 0.2822115
Validation loss decreased (0.705883 --> 0.694273).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 66 | Train Loss: 0.4357233 Vali Loss: 0.6897808 Test Loss: 0.2822100
Validation loss decreased (0.694273 --> 0.689781).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 66 | Train Loss: 0.4336522 Vali Loss: 0.6870072 Test Loss: 0.2822179
Validation loss decreased (0.689781 --> 0.687007).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 66 | Train Loss: 0.4332514 Vali Loss: 0.6859552 Test Loss: 0.2822186
Validation loss decreased (0.687007 --> 0.685955).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 66 | Train Loss: 0.4333666 Vali Loss: 0.6854671 Test Loss: 0.2822183
Validation loss decreased (0.685955 --> 0.685467).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 66 | Train Loss: 0.4331249 Vali Loss: 0.6851794 Test Loss: 0.2822179
Validation loss decreased (0.685467 --> 0.685179).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 66 | Train Loss: 0.4337747 Vali Loss: 0.6850497 Test Loss: 0.2822186
Validation loss decreased (0.685179 --> 0.685050).  Saving model ...
Updating learning rate to 9.765625e-08
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.48532594514615607, 'val/loss': 0.8935979008674622, 'test/loss': 0.2796465754508972, '_timestamp': 1762293841.4373803}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.4761026731946252, 'val/loss': 0.8208033442497253, 'test/loss': 0.2809302806854248, '_timestamp': 1762293842.6499496}).
Epoch: 12, Steps: 66 | Train Loss: 0.4333639 Vali Loss: 0.6850001 Test Loss: 0.2822182
Validation loss decreased (0.685050 --> 0.685000).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 66 | Train Loss: 0.4332612 Vali Loss: 0.6849662 Test Loss: 0.2822182
Validation loss decreased (0.685000 --> 0.684966).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 66 | Train Loss: 0.4339034 Vali Loss: 0.6849560 Test Loss: 0.2822181
Validation loss decreased (0.684966 --> 0.684956).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 66 | Train Loss: 0.4329634 Vali Loss: 0.6849473 Test Loss: 0.2822181
Validation loss decreased (0.684956 --> 0.684947).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 66 | Train Loss: 0.4341321 Vali Loss: 0.6849445 Test Loss: 0.2822181
Validation loss decreased (0.684947 --> 0.684945).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 66 | Train Loss: 0.4332793 Vali Loss: 0.6849440 Test Loss: 0.2822181
Validation loss decreased (0.684945 --> 0.684944).  Saving model ...
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 66 | Train Loss: 0.4329166 Vali Loss: 0.6849437 Test Loss: 0.2822181
Validation loss decreased (0.684944 --> 0.684944).  Saving model ...
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 66 | Train Loss: 0.4326406 Vali Loss: 0.6849437 Test Loss: 0.2822181
Validation loss decreased (0.684944 --> 0.684944).  Saving model ...
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 66 | Train Loss: 0.4329050 Vali Loss: 0.6849437 Test Loss: 0.2822181
Validation loss decreased (0.684944 --> 0.684944).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 66 | Train Loss: 0.4333985 Vali Loss: 0.6849437 Test Loss: 0.2822181
Validation loss decreased (0.684944 --> 0.684944).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 66 | Train Loss: 0.4337704 Vali Loss: 0.6849437 Test Loss: 0.2822181
Validation loss decreased (0.684944 --> 0.684944).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 66 | Train Loss: 0.4337089 Vali Loss: 0.6849437 Test Loss: 0.2822181
Validation loss decreased (0.684944 --> 0.684944).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 66 | Train Loss: 0.4330604 Vali Loss: 0.6849437 Test Loss: 0.2822181
Validation loss decreased (0.684944 --> 0.684944).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 66 | Train Loss: 0.4337988 Vali Loss: 0.6849437 Test Loss: 0.2822181
Validation loss decreased (0.684944 --> 0.684944).  Saving model ...
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 66 | Train Loss: 0.4334293 Vali Loss: 0.6849436 Test Loss: 0.2822181
Validation loss decreased (0.684944 --> 0.684944).  Saving model ...
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 66 | Train Loss: 0.4331589 Vali Loss: 0.6849437 Test Loss: 0.2822181
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 66 | Train Loss: 0.4333203 Vali Loss: 0.6849437 Test Loss: 0.2822181
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 66 | Train Loss: 0.4330126 Vali Loss: 0.6849436 Test Loss: 0.2822181
Validation loss decreased (0.684944 --> 0.684944).  Saving model ...
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 66 | Train Loss: 0.4333428 Vali Loss: 0.6849437 Test Loss: 0.2822181
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 66 | Train Loss: 0.4339020 Vali Loss: 0.6849437 Test Loss: 0.2822181
EarlyStopping counter: 2 out of 5
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 66 | Train Loss: 0.4334246 Vali Loss: 0.6849437 Test Loss: 0.2822181
EarlyStopping counter: 3 out of 5
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 66 | Train Loss: 0.4329784 Vali Loss: 0.6849437 Test Loss: 0.2822181
EarlyStopping counter: 4 out of 5
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 66 | Train Loss: 0.4341924 Vali Loss: 0.6849437 Test Loss: 0.2822181
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 28
test shape: (28, 100, 1) (28, 100, 1)
test shape: (28, 100, 1) (28, 100, 1)


	mse:0.0015564258210361004, mae:0.02768738567829132, rmse:0.039451561868190765, r2:-0.00918567180633545, dtw:Not calculated


VAL - MSE: 0.0016, MAE: 0.0277, RMSE: 0.0395, RÂ²: -0.0092, MAPE: 3494507.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.632 MB of 0.637 MB uploadedwandb: \ 0.637 MB of 0.637 MB uploadedwandb: | 0.637 MB of 0.637 MB uploadedwandb: / 0.637 MB of 0.713 MB uploadedwandb: - 0.713 MB of 0.713 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 33
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 73048
wandb:     model/trainable_params 73048
wandb:                  test/loss 0.28222
wandb:                 train/loss 0.43419
wandb:   val/directional_accuracy 49.02597
wandb:                   val/loss 0.68494
wandb:                    val/mae 0.02769
wandb:                   val/mape 349450750.0
wandb:                    val/mse 0.00156
wandb:                     val/r2 -0.00919
wandb:                   val/rmse 0.03945
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/xpfgaqtw
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_000354-xpfgaqtw/logs
Completed: SASOL H=100

Mamba training completed for all datasets!
