##############################################################################
# Training Mamba Model on All Datasets
##############################################################################
Training: Mamba on NVIDIA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_193143-d636l920
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/d636l920
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H3     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/d636l920
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.2361398 Vali Loss: 0.1791503 Test Loss: 0.2631287
Validation loss decreased (inf --> 0.179150).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2255464 Vali Loss: 0.1690252 Test Loss: 0.2350518
Validation loss decreased (0.179150 --> 0.169025).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2208438 Vali Loss: 0.1719107 Test Loss: 0.2318857
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2194696 Vali Loss: 0.1666308 Test Loss: 0.2351695
Validation loss decreased (0.169025 --> 0.166631).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2183632 Vali Loss: 0.1665643 Test Loss: 0.2313927
Validation loss decreased (0.166631 --> 0.166564).  Saving model ...
Updating learning rate to 6.25e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23613984143375455, 'val/loss': 0.17915029078722, 'test/loss': 0.26312870997935534, '_timestamp': 1762882334.860856}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.22554640068595572, 'val/loss': 0.16902522929012775, 'test/loss': 0.2350518312305212, '_timestamp': 1762882337.3571877}).
Epoch: 6, Steps: 133 | Train Loss: 0.2183275 Vali Loss: 0.1697295 Test Loss: 0.2312341
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2173983 Vali Loss: 0.1864424 Test Loss: 0.2309344
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2172630 Vali Loss: 0.1711943 Test Loss: 0.2310575
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2173782 Vali Loss: 0.1757276 Test Loss: 0.2310450
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2171822 Vali Loss: 0.1687508 Test Loss: 0.2310621
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2168127 Vali Loss: 0.1673612 Test Loss: 0.2310262
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2180918 Vali Loss: 0.1740278 Test Loss: 0.2310275
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2169406 Vali Loss: 0.1691029 Test Loss: 0.2310240
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2170377 Vali Loss: 0.1862201 Test Loss: 0.2310238
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2172948 Vali Loss: 0.1686043 Test Loss: 0.2310225
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.0011230934178456664, mae:0.02513883076608181, rmse:0.03351258486509323, r2:-0.0011650323867797852, dtw:Not calculated


VAL - MSE: 0.0011, MAE: 0.0251, RMSE: 0.0335, RÂ²: -0.0012, MAPE: 459355.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.464 MB of 0.464 MB uploadedwandb: \ 0.464 MB of 0.464 MB uploadedwandb: | 0.464 MB of 0.464 MB uploadedwandb: / 0.464 MB of 0.464 MB uploadedwandb: - 0.464 MB of 0.464 MB uploadedwandb: \ 0.464 MB of 0.464 MB uploadedwandb: | 0.464 MB of 0.464 MB uploadedwandb: / 0.464 MB of 0.464 MB uploadedwandb: - 0.464 MB of 0.464 MB uploadedwandb: \ 0.464 MB of 0.464 MB uploadedwandb: | 0.513 MB of 0.574 MB uploaded (0.002 MB deduped)wandb: / 0.574 MB of 0.574 MB uploaded (0.002 MB deduped)wandb: - 0.574 MB of 0.574 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ƒâ–ˆâ–‚â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–†â–„â–„â–‚â–‚â–‚â–‚â–â–ƒâ–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–â–â–‚â–ˆâ–ƒâ–„â–‚â–â–„â–‚â–ˆâ–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 207634
wandb:     model/trainable_params 207634
wandb:                  test/loss 0.23102
wandb:                 train/loss 0.21729
wandb:   val/directional_accuracy 51.89873
wandb:                   val/loss 0.1686
wandb:                    val/mae 0.02514
wandb:                   val/mape 45935550.0
wandb:                    val/mse 0.00112
wandb:                     val/r2 -0.00117
wandb:                   val/rmse 0.03351
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/d636l920
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_193143-d636l920/logs
Completed: NVIDIA H=3

Training: Mamba on NVIDIA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_193444-106xxj60
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/106xxj60
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H5     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/106xxj60
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.2382808 Vali Loss: 0.1896746 Test Loss: 0.2922712
Validation loss decreased (inf --> 0.189675).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2295230 Vali Loss: 0.1711052 Test Loss: 0.2512104
Validation loss decreased (0.189675 --> 0.171105).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2252733 Vali Loss: 0.1741564 Test Loss: 0.2493982
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2247436 Vali Loss: 0.1716265 Test Loss: 0.2494003
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2237532 Vali Loss: 0.1765741 Test Loss: 0.2473630
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2224912 Vali Loss: 0.1657302 Test Loss: 0.2474246
Validation loss decreased (0.171105 --> 0.165730).  Saving model ...
Updating learning rate to 3.125e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23828077181837612, 'val/loss': 0.18967459630221128, 'test/loss': 0.29227121267467737, '_timestamp': 1762882505.6059945}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.22952302069144143, 'val/loss': 0.1711051883175969, 'test/loss': 0.2512103905901313, '_timestamp': 1762882508.8028176}).
Epoch: 7, Steps: 133 | Train Loss: 0.2223725 Vali Loss: 0.1833329 Test Loss: 0.2474416
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2223379 Vali Loss: 0.1735142 Test Loss: 0.2473700
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2229329 Vali Loss: 0.1663809 Test Loss: 0.2473707
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2220661 Vali Loss: 0.1839526 Test Loss: 0.2473148
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2219012 Vali Loss: 0.1686129 Test Loss: 0.2472755
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2217274 Vali Loss: 0.1708848 Test Loss: 0.2472673
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2223501 Vali Loss: 0.1824752 Test Loss: 0.2472653
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2215969 Vali Loss: 0.1690779 Test Loss: 0.2472634
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2220922 Vali Loss: 0.1679425 Test Loss: 0.2472619
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2218972 Vali Loss: 0.1909942 Test Loss: 0.2472617
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.0011394076282158494, mae:0.025345604866743088, rmse:0.033755112439394, r2:-0.009146809577941895, dtw:Not calculated


VAL - MSE: 0.0011, MAE: 0.0253, RMSE: 0.0338, RÂ²: -0.0091, MAPE: 526802.12%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.515 MB of 0.515 MB uploadedwandb: \ 0.515 MB of 0.515 MB uploadedwandb: | 0.515 MB of 0.515 MB uploadedwandb: / 0.515 MB of 0.515 MB uploadedwandb: - 0.515 MB of 0.515 MB uploadedwandb: \ 0.515 MB of 0.576 MB uploadedwandb: | 0.572 MB of 0.576 MB uploadedwandb: / 0.576 MB of 0.576 MB uploadedwandb: - 0.576 MB of 0.576 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ˆâ–â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–‡â–…â–ƒâ–‚â–‚â–„â–‚â–‚â–â–‚â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–ƒâ–„â–â–†â–ƒâ–â–†â–‚â–‚â–†â–‚â–‚â–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 209182
wandb:     model/trainable_params 209182
wandb:                  test/loss 0.24726
wandb:                 train/loss 0.2219
wandb:   val/directional_accuracy 52.44681
wandb:                   val/loss 0.19099
wandb:                    val/mae 0.02535
wandb:                   val/mape 52680212.5
wandb:                    val/mse 0.00114
wandb:                     val/r2 -0.00915
wandb:                   val/rmse 0.03376
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/106xxj60
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_193444-106xxj60/logs
Completed: NVIDIA H=5

Training: Mamba on NVIDIA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_193643-mib030py
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/mib030py
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H10    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/mib030py
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2413782 Vali Loss: 0.1765070 Test Loss: 0.3163385
Validation loss decreased (inf --> 0.176507).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2351265 Vali Loss: 0.1770485 Test Loss: 0.2804406
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2315267 Vali Loss: 0.1702975 Test Loss: 0.2726924
Validation loss decreased (0.176507 --> 0.170297).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2298759 Vali Loss: 0.1702012 Test Loss: 0.2711676
Validation loss decreased (0.170297 --> 0.170201).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2286080 Vali Loss: 0.1714846 Test Loss: 0.2731277
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2288027 Vali Loss: 0.1792096 Test Loss: 0.2708544
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.24137820874838004, 'val/loss': 0.17650700639933348, 'test/loss': 0.31633853912353516, '_timestamp': 1762882623.8839796}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2351264818046326, 'val/loss': 0.17704851739108562, 'test/loss': 0.28044060803949833, '_timestamp': 1762882626.5854673}).
Epoch: 7, Steps: 133 | Train Loss: 0.2289692 Vali Loss: 0.2096574 Test Loss: 0.2703552
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2289753 Vali Loss: 0.1730709 Test Loss: 0.2705621
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2286312 Vali Loss: 0.1727376 Test Loss: 0.2704432
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2282440 Vali Loss: 0.1715589 Test Loss: 0.2703653
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2289841 Vali Loss: 0.1696707 Test Loss: 0.2703409
Validation loss decreased (0.170201 --> 0.169671).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2279672 Vali Loss: 0.1898322 Test Loss: 0.2703386
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2285957 Vali Loss: 0.1869022 Test Loss: 0.2703382
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2283494 Vali Loss: 0.1699906 Test Loss: 0.2703383
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2283813 Vali Loss: 0.1740833 Test Loss: 0.2703365
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2284153 Vali Loss: 0.1785333 Test Loss: 0.2703358
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2283865 Vali Loss: 0.1906619 Test Loss: 0.2703356
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2284232 Vali Loss: 0.1769931 Test Loss: 0.2703355
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2281513 Vali Loss: 0.1742098 Test Loss: 0.2703355
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2282540 Vali Loss: 0.1764265 Test Loss: 0.2703355
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2290697 Vali Loss: 0.1684865 Test Loss: 0.2703355
Validation loss decreased (0.169671 --> 0.168486).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2286332 Vali Loss: 0.1805460 Test Loss: 0.2703355
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2286190 Vali Loss: 0.1705584 Test Loss: 0.2703355
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2284447 Vali Loss: 0.1927827 Test Loss: 0.2703355
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2284316 Vali Loss: 0.1931543 Test Loss: 0.2703355
EarlyStopping counter: 4 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2284190 Vali Loss: 0.1699550 Test Loss: 0.2703355
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.2280202 Vali Loss: 0.1695255 Test Loss: 0.2703355
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.2282812 Vali Loss: 0.1764717 Test Loss: 0.2703355
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.2282189 Vali Loss: 0.1697464 Test Loss: 0.2703355
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 133 | Train Loss: 0.2294343 Vali Loss: 0.1741583 Test Loss: 0.2703355
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 133 | Train Loss: 0.2283741 Vali Loss: 0.1691262 Test Loss: 0.2703355
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.0011636008275672793, mae:0.025678355246782303, rmse:0.034111592918634415, r2:-0.014362692832946777, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0257, RMSE: 0.0341, RÂ²: -0.0144, MAPE: 460520.97%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.553 MB of 0.554 MB uploadedwandb: \ 0.553 MB of 0.554 MB uploadedwandb: | 0.554 MB of 0.554 MB uploadedwandb: / 0.554 MB of 0.554 MB uploadedwandb: - 0.554 MB of 0.617 MB uploadedwandb: \ 0.554 MB of 0.617 MB uploadedwandb: | 0.617 MB of 0.617 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‡â–ƒâ–ˆâ–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–„â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–â–‚â–ƒâ–ˆâ–‚â–‚â–‚â–â–…â–„â–â–‚â–ƒâ–…â–‚â–‚â–‚â–â–ƒâ–â–…â–…â–â–â–‚â–â–‚â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 30
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 213052
wandb:     model/trainable_params 213052
wandb:                  test/loss 0.27034
wandb:                 train/loss 0.22837
wandb:   val/directional_accuracy 50.24155
wandb:                   val/loss 0.16913
wandb:                    val/mae 0.02568
wandb:                   val/mape 46052096.875
wandb:                    val/mse 0.00116
wandb:                     val/r2 -0.01436
wandb:                   val/rmse 0.03411
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/mib030py
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_193643-mib030py/logs
Completed: NVIDIA H=10

Training: Mamba on NVIDIA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_193923-4385e2bs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/4385e2bs
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H22    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/4385e2bs
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2473756 Vali Loss: 0.1858695 Test Loss: 0.3934751
Validation loss decreased (inf --> 0.185869).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2415642 Vali Loss: 0.1807446 Test Loss: 0.3657895
Validation loss decreased (0.185869 --> 0.180745).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2379836 Vali Loss: 0.1793725 Test Loss: 0.3470777
Validation loss decreased (0.180745 --> 0.179372).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2363490 Vali Loss: 0.1789902 Test Loss: 0.3436184
Validation loss decreased (0.179372 --> 0.178990).  Saving model ...
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.24737562532677795, 'val/loss': 0.18586946598121099, 'test/loss': 0.3934750769819532, '_timestamp': 1762882788.5359907}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24156415473782655, 'val/loss': 0.18074460008314677, 'test/loss': 0.3657895156315395, '_timestamp': 1762882791.776723}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24156415473782655, 'val/loss': 0.18074460008314677, 'test/loss': 0.3657895156315395, '_timestamp': 1762882791.776723}).
Epoch: 5, Steps: 132 | Train Loss: 0.2356587 Vali Loss: 0.1783812 Test Loss: 0.3399322
Validation loss decreased (0.178990 --> 0.178381).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2354091 Vali Loss: 0.1798190 Test Loss: 0.3389105
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2354454 Vali Loss: 0.1788347 Test Loss: 0.3390987
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2352950 Vali Loss: 0.1771784 Test Loss: 0.3392371
Validation loss decreased (0.178381 --> 0.177178).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2353651 Vali Loss: 0.1802479 Test Loss: 0.3392162
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2353019 Vali Loss: 0.1783091 Test Loss: 0.3392633
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2352438 Vali Loss: 0.1787433 Test Loss: 0.3392312
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2351492 Vali Loss: 0.1778284 Test Loss: 0.3392420
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2352489 Vali Loss: 0.1772179 Test Loss: 0.3392464
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2354369 Vali Loss: 0.1778058 Test Loss: 0.3392444
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2352341 Vali Loss: 0.1775245 Test Loss: 0.3392433
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2351575 Vali Loss: 0.1788145 Test Loss: 0.3392430
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2352282 Vali Loss: 0.1786821 Test Loss: 0.3392426
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2350919 Vali Loss: 0.1773846 Test Loss: 0.3392426
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.001193239470012486, mae:0.025860920548439026, rmse:0.034543298184871674, r2:-0.011343598365783691, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0259, RMSE: 0.0345, RÂ²: -0.0113, MAPE: 433803.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.570 MB of 0.571 MB uploadedwandb: \ 0.570 MB of 0.571 MB uploadedwandb: | 0.571 MB of 0.571 MB uploadedwandb: / 0.571 MB of 0.571 MB uploadedwandb: - 0.571 MB of 0.632 MB uploadedwandb: \ 0.632 MB of 0.632 MB uploadedwandb: | 0.632 MB of 0.632 MB uploadedwandb: / 0.632 MB of 0.632 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–…â–„â–‡â–…â–â–ˆâ–„â–…â–‚â–â–‚â–‚â–…â–„â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 17
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 222340
wandb:     model/trainable_params 222340
wandb:                  test/loss 0.33924
wandb:                 train/loss 0.23509
wandb:   val/directional_accuracy 50.13106
wandb:                   val/loss 0.17738
wandb:                    val/mae 0.02586
wandb:                   val/mape 43380325.0
wandb:                    val/mse 0.00119
wandb:                     val/r2 -0.01134
wandb:                   val/rmse 0.03454
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/4385e2bs
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_193923-4385e2bs/logs
Completed: NVIDIA H=22

Training: Mamba on NVIDIA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_194242-9gtz2zt9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/9gtz2zt9
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H50    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/9gtz2zt9
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.2615273 Vali Loss: 0.2024794 Test Loss: 0.5251706
Validation loss decreased (inf --> 0.202479).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2531823 Vali Loss: 0.1944127 Test Loss: 0.4734809
Validation loss decreased (0.202479 --> 0.194413).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2491189 Vali Loss: 0.1903174 Test Loss: 0.4828613
Validation loss decreased (0.194413 --> 0.190317).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2481486 Vali Loss: 0.1897997 Test Loss: 0.4723521
Validation loss decreased (0.190317 --> 0.189800).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2473821 Vali Loss: 0.1893027 Test Loss: 0.4751683
Validation loss decreased (0.189800 --> 0.189303).  Saving model ...
Updating learning rate to 6.25e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2615272914144126, 'val/loss': 0.20247944196065268, 'test/loss': 0.5251705646514893, '_timestamp': 1762882987.5781708}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2531822636497743, 'val/loss': 0.19441271324952444, 'test/loss': 0.4734808678428332, '_timestamp': 1762882989.8866668}).
Epoch: 6, Steps: 132 | Train Loss: 0.2483620 Vali Loss: 0.1892638 Test Loss: 0.4737699
Validation loss decreased (0.189303 --> 0.189264).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2519633 Vali Loss: 0.1893669 Test Loss: 0.4728495
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2465921 Vali Loss: 0.1891282 Test Loss: 0.4728412
Validation loss decreased (0.189264 --> 0.189128).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2480333 Vali Loss: 0.1892285 Test Loss: 0.4727271
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2498490 Vali Loss: 0.1892161 Test Loss: 0.4725776
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2477025 Vali Loss: 0.1890247 Test Loss: 0.4725772
Validation loss decreased (0.189128 --> 0.189025).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2469181 Vali Loss: 0.1887498 Test Loss: 0.4725660
Validation loss decreased (0.189025 --> 0.188750).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2469066 Vali Loss: 0.1893335 Test Loss: 0.4725622
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2502102 Vali Loss: 0.1892462 Test Loss: 0.4725576
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2477145 Vali Loss: 0.1889672 Test Loss: 0.4725572
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2471254 Vali Loss: 0.1890421 Test Loss: 0.4725565
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2477046 Vali Loss: 0.1896120 Test Loss: 0.4725566
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2468011 Vali Loss: 0.1887613 Test Loss: 0.4725566
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2498284 Vali Loss: 0.1892908 Test Loss: 0.4725565
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2471188 Vali Loss: 0.1890148 Test Loss: 0.4725565
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2468978 Vali Loss: 0.1890280 Test Loss: 0.4725565
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2469804 Vali Loss: 0.1893132 Test Loss: 0.4725565
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.001198106212541461, mae:0.026226796209812164, rmse:0.03461367264389992, r2:-0.006818652153015137, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0262, RMSE: 0.0346, RÂ²: -0.0068, MAPE: 386249.31%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.629 MB of 0.632 MB uploadedwandb: \ 0.629 MB of 0.632 MB uploadedwandb: | 0.629 MB of 0.632 MB uploadedwandb: / 0.632 MB of 0.632 MB uploadedwandb: - 0.632 MB of 0.632 MB uploadedwandb: \ 0.632 MB of 0.694 MB uploadedwandb: | 0.632 MB of 0.694 MB uploadedwandb: / 0.632 MB of 0.694 MB uploadedwandb: - 0.694 MB of 0.694 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–„â–ƒâ–‚â–ƒâ–ˆâ–â–ƒâ–…â–‚â–â–â–†â–‚â–‚â–‚â–â–…â–‚â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–‚â–â–„â–ƒâ–‚â–‚â–…â–â–ƒâ–‚â–‚â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 244012
wandb:     model/trainable_params 244012
wandb:                  test/loss 0.47256
wandb:                 train/loss 0.24698
wandb:   val/directional_accuracy 50.02148
wandb:                   val/loss 0.18931
wandb:                    val/mae 0.02623
wandb:                   val/mape 38624931.25
wandb:                    val/mse 0.0012
wandb:                     val/r2 -0.00682
wandb:                   val/rmse 0.03461
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/9gtz2zt9
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_194242-9gtz2zt9/logs
Completed: NVIDIA H=50

Training: Mamba on NVIDIA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_194730-lfwnvf4g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/lfwnvf4g
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H100   Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/lfwnvf4g
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.2800773 Vali Loss: 0.2406180 Test Loss: 0.7870048
Validation loss decreased (inf --> 0.240618).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2731235 Vali Loss: 0.2255243 Test Loss: 0.7158913
Validation loss decreased (0.240618 --> 0.225524).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.2694525 Vali Loss: 0.2172331 Test Loss: 0.7039820
Validation loss decreased (0.225524 --> 0.217233).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2677889 Vali Loss: 0.2184643 Test Loss: 0.7115207
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2673927 Vali Loss: 0.2146233 Test Loss: 0.7096625
Validation loss decreased (0.217233 --> 0.214623).  Saving model ...
Updating learning rate to 6.25e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.28007729282745947, 'val/loss': 0.24061804115772248, 'test/loss': 0.787004828453064, '_timestamp': 1762883278.6090732}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.27312346559304457, 'val/loss': 0.22552425861358644, 'test/loss': 0.715891307592392, '_timestamp': 1762883281.1690788}).
Epoch: 6, Steps: 130 | Train Loss: 0.2671477 Vali Loss: 0.2109172 Test Loss: 0.7100490
Validation loss decreased (0.214623 --> 0.210917).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2669462 Vali Loss: 0.2148377 Test Loss: 0.7089818
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2668263 Vali Loss: 0.2146146 Test Loss: 0.7083841
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2669373 Vali Loss: 0.2127169 Test Loss: 0.7082945
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2670536 Vali Loss: 0.2113600 Test Loss: 0.7082859
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2670277 Vali Loss: 0.2126807 Test Loss: 0.7082559
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2667092 Vali Loss: 0.2098785 Test Loss: 0.7082451
Validation loss decreased (0.210917 --> 0.209878).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2666866 Vali Loss: 0.2086979 Test Loss: 0.7082482
Validation loss decreased (0.209878 --> 0.208698).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.2669747 Vali Loss: 0.2029371 Test Loss: 0.7082455
Validation loss decreased (0.208698 --> 0.202937).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.2669690 Vali Loss: 0.2157741 Test Loss: 0.7082458
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 130 | Train Loss: 0.2669630 Vali Loss: 0.2118004 Test Loss: 0.7082431
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 130 | Train Loss: 0.2666310 Vali Loss: 0.2084715 Test Loss: 0.7082431
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 130 | Train Loss: 0.2666540 Vali Loss: 0.2108409 Test Loss: 0.7082430
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 130 | Train Loss: 0.2669899 Vali Loss: 0.2136769 Test Loss: 0.7082430
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 130 | Train Loss: 0.2668388 Vali Loss: 0.2127492 Test Loss: 0.7082430
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 130 | Train Loss: 0.2668005 Vali Loss: 0.2108926 Test Loss: 0.7082430
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 130 | Train Loss: 0.2668682 Vali Loss: 0.2111821 Test Loss: 0.7082430
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 130 | Train Loss: 0.2668800 Vali Loss: 0.2121913 Test Loss: 0.7082430
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 130 | Train Loss: 0.2665689 Vali Loss: 0.2140238 Test Loss: 0.7082430
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0012992359697818756, mae:0.027557600289583206, rmse:0.03604491427540779, r2:-0.009181737899780273, dtw:Not calculated


VAL - MSE: 0.0013, MAE: 0.0276, RMSE: 0.0360, RÂ²: -0.0092, MAPE: 267720.03%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.698 MB of 0.703 MB uploadedwandb: \ 0.698 MB of 0.703 MB uploadedwandb: | 0.703 MB of 0.703 MB uploadedwandb: / 0.703 MB of 0.703 MB uploadedwandb: - 0.703 MB of 0.765 MB uploadedwandb: \ 0.703 MB of 0.765 MB uploadedwandb: | 0.765 MB of 0.765 MB uploadedwandb: / 0.765 MB of 0.765 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–†â–‡â–†â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–ˆâ–†â–…â–†â–†â–…â–…â–…â–„â–„â–â–‡â–…â–ƒâ–…â–†â–…â–…â–…â–…â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 23
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 282712
wandb:     model/trainable_params 282712
wandb:                  test/loss 0.70824
wandb:                 train/loss 0.26657
wandb:   val/directional_accuracy 50.60606
wandb:                   val/loss 0.21402
wandb:                    val/mae 0.02756
wandb:                   val/mape 26772003.125
wandb:                    val/mse 0.0013
wandb:                     val/r2 -0.00918
wandb:                   val/rmse 0.03604
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/lfwnvf4g
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_194730-lfwnvf4g/logs
Completed: NVIDIA H=100

Training: Mamba on APPLE for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_195100-xv0uto7w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/xv0uto7w
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H3      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/xv0uto7w
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.2263088 Vali Loss: 0.0852255 Test Loss: 0.1248311
Validation loss decreased (inf --> 0.085226).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2150902 Vali Loss: 0.0827324 Test Loss: 0.1225828
Validation loss decreased (0.085226 --> 0.082732).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2099725 Vali Loss: 0.0844509 Test Loss: 0.1220815
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2079711 Vali Loss: 0.0809304 Test Loss: 0.1226852
Validation loss decreased (0.082732 --> 0.080930).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2072315 Vali Loss: 0.0806407 Test Loss: 0.1224847
Validation loss decreased (0.080930 --> 0.080641).  Saving model ...
Updating learning rate to 6.25e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.22630881131591654, 'val/loss': 0.08522551320493221, 'test/loss': 0.12483113259077072, '_timestamp': 1762883483.360817}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.21509021027643876, 'val/loss': 0.08273239154368639, 'test/loss': 0.1225828411988914, '_timestamp': 1762883486.0641434}).
Epoch: 6, Steps: 133 | Train Loss: 0.2072388 Vali Loss: 0.0799426 Test Loss: 0.1225117
Validation loss decreased (0.080641 --> 0.079943).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2069461 Vali Loss: 0.0833722 Test Loss: 0.1224745
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2067239 Vali Loss: 0.0841852 Test Loss: 0.1224918
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2063391 Vali Loss: 0.0825316 Test Loss: 0.1225034
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2068021 Vali Loss: 0.0824373 Test Loss: 0.1225071
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2066346 Vali Loss: 0.0820776 Test Loss: 0.1225098
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2063951 Vali Loss: 0.0826206 Test Loss: 0.1225088
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2065175 Vali Loss: 0.0849104 Test Loss: 0.1225090
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2059944 Vali Loss: 0.0822926 Test Loss: 0.1225090
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2065649 Vali Loss: 0.0822628 Test Loss: 0.1225090
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2061675 Vali Loss: 0.0833025 Test Loss: 0.1225091
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.00020157205290161073, mae:0.010241341777145863, rmse:0.014197607524693012, r2:-0.008148789405822754, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0102, RMSE: 0.0142, RÂ²: -0.0081, MAPE: 231756.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.456 MB of 0.456 MB uploadedwandb: \ 0.456 MB of 0.456 MB uploadedwandb: | 0.456 MB of 0.456 MB uploadedwandb: / 0.456 MB of 0.456 MB uploadedwandb: - 0.456 MB of 0.456 MB uploadedwandb: \ 0.456 MB of 0.456 MB uploadedwandb: | 0.456 MB of 0.456 MB uploadedwandb: / 0.505 MB of 0.566 MB uploaded (0.002 MB deduped)wandb: - 0.566 MB of 0.566 MB uploaded (0.002 MB deduped)wandb: \ 0.566 MB of 0.566 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†
wandb:                 train/loss â–ˆâ–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–‚â–‚â–â–†â–‡â–…â–…â–„â–…â–ˆâ–„â–„â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 207634
wandb:     model/trainable_params 207634
wandb:                  test/loss 0.12251
wandb:                 train/loss 0.20617
wandb:   val/directional_accuracy 50.0
wandb:                   val/loss 0.0833
wandb:                    val/mae 0.01024
wandb:                   val/mape 23175600.0
wandb:                    val/mse 0.0002
wandb:                     val/r2 -0.00815
wandb:                   val/rmse 0.0142
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/xv0uto7w
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_195100-xv0uto7w/logs
Completed: APPLE H=3

Training: Mamba on APPLE for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_195401-rpac1s1p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/rpac1s1p
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H5      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/rpac1s1p
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.2281183 Vali Loss: 0.0864146 Test Loss: 0.1283469
Validation loss decreased (inf --> 0.086415).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2179406 Vali Loss: 0.0858925 Test Loss: 0.1262199
Validation loss decreased (0.086415 --> 0.085893).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2137410 Vali Loss: 0.0837554 Test Loss: 0.1260747
Validation loss decreased (0.085893 --> 0.083755).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2114043 Vali Loss: 0.0832244 Test Loss: 0.1256937
Validation loss decreased (0.083755 --> 0.083224).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2102951 Vali Loss: 0.0829416 Test Loss: 0.1259594
Validation loss decreased (0.083224 --> 0.082942).  Saving model ...
Updating learning rate to 6.25e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.22811826492162576, 'val/loss': 0.08641458582133055, 'test/loss': 0.12834689673036337, '_timestamp': 1762883666.3785362}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.21794060242355318, 'val/loss': 0.08589252829551697, 'test/loss': 0.12621991895139217, '_timestamp': 1762883669.1742408}).
Epoch: 6, Steps: 133 | Train Loss: 0.2102064 Vali Loss: 0.0839668 Test Loss: 0.1259611
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2097681 Vali Loss: 0.0826008 Test Loss: 0.1259209
Validation loss decreased (0.082942 --> 0.082601).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2098277 Vali Loss: 0.0834474 Test Loss: 0.1258942
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2112108 Vali Loss: 0.0849065 Test Loss: 0.1258829
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2096325 Vali Loss: 0.0879019 Test Loss: 0.1258839
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2105278 Vali Loss: 0.0816559 Test Loss: 0.1258823
Validation loss decreased (0.082601 --> 0.081656).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2097007 Vali Loss: 0.0853014 Test Loss: 0.1258828
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2108512 Vali Loss: 0.0819594 Test Loss: 0.1258823
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2100399 Vali Loss: 0.0825588 Test Loss: 0.1258825
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2096001 Vali Loss: 0.0821327 Test Loss: 0.1258824
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2098487 Vali Loss: 0.0831638 Test Loss: 0.1258824
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2096887 Vali Loss: 0.0840825 Test Loss: 0.1258824
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2097366 Vali Loss: 0.0817455 Test Loss: 0.1258824
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2107097 Vali Loss: 0.0852917 Test Loss: 0.1258824
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2099263 Vali Loss: 0.0839320 Test Loss: 0.1258824
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2102193 Vali Loss: 0.0860192 Test Loss: 0.1258824
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.00020231649978086352, mae:0.010231520049273968, rmse:0.014223800040781498, r2:-0.007253408432006836, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0102, RMSE: 0.0142, RÂ²: -0.0073, MAPE: 241475.89%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.493 MB of 0.493 MB uploadedwandb: \ 0.493 MB of 0.493 MB uploadedwandb: | 0.493 MB of 0.493 MB uploadedwandb: / 0.493 MB of 0.493 MB uploadedwandb: - 0.493 MB of 0.555 MB uploadedwandb: \ 0.493 MB of 0.555 MB uploadedwandb: | 0.555 MB of 0.555 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–†â–†â–…â–…â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„
wandb:                 train/loss â–ˆâ–„â–‚â–‚â–â–â–„â–â–ƒâ–â–ƒâ–‚â–â–â–â–â–ƒâ–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–ƒâ–‚â–„â–‚â–ƒâ–…â–ˆâ–â–…â–â–‚â–‚â–ƒâ–„â–â–…â–„â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 209182
wandb:     model/trainable_params 209182
wandb:                  test/loss 0.12588
wandb:                 train/loss 0.21022
wandb:   val/directional_accuracy 48.82979
wandb:                   val/loss 0.08602
wandb:                    val/mae 0.01023
wandb:                   val/mape 24147589.0625
wandb:                    val/mse 0.0002
wandb:                     val/r2 -0.00725
wandb:                   val/rmse 0.01422
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/rpac1s1p
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_195401-rpac1s1p/logs
Completed: APPLE H=5

Training: Mamba on APPLE for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_195820-ocmf7yg8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/ocmf7yg8
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H10     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/ocmf7yg8
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2318261 Vali Loss: 0.0869337 Test Loss: 0.1307588
Validation loss decreased (inf --> 0.086934).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2222244 Vali Loss: 0.0854866 Test Loss: 0.1273496
Validation loss decreased (0.086934 --> 0.085487).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2179807 Vali Loss: 0.0836806 Test Loss: 0.1269948
Validation loss decreased (0.085487 --> 0.083681).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2153264 Vali Loss: 0.0834154 Test Loss: 0.1267720
Validation loss decreased (0.083681 --> 0.083415).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2149560 Vali Loss: 0.0826734 Test Loss: 0.1266581
Validation loss decreased (0.083415 --> 0.082673).  Saving model ...
Updating learning rate to 6.25e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23182611727624908, 'val/loss': 0.08693374320864677, 'test/loss': 0.13075881358236074, '_timestamp': 1762883924.7340622}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.22222436314686797, 'val/loss': 0.08548659086227417, 'test/loss': 0.1273495787754655, '_timestamp': 1762883927.420229}).
Epoch: 6, Steps: 133 | Train Loss: 0.2151654 Vali Loss: 0.0858363 Test Loss: 0.1267291
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2148583 Vali Loss: 0.0857754 Test Loss: 0.1266900
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2144736 Vali Loss: 0.0882701 Test Loss: 0.1266977
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2152756 Vali Loss: 0.0828769 Test Loss: 0.1266961
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2141506 Vali Loss: 0.0843111 Test Loss: 0.1266935
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2138279 Vali Loss: 0.0864694 Test Loss: 0.1266961
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2143887 Vali Loss: 0.0831351 Test Loss: 0.1266948
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2144457 Vali Loss: 0.0840213 Test Loss: 0.1266951
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2139415 Vali Loss: 0.0844397 Test Loss: 0.1266955
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2142019 Vali Loss: 0.0873162 Test Loss: 0.1266955
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.00020531646441668272, mae:0.010294080711901188, rmse:0.014328868128359318, r2:-0.014075040817260742, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0103, RMSE: 0.0143, RÂ²: -0.0141, MAPE: 158801.78%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.545 MB of 0.546 MB uploadedwandb: \ 0.546 MB of 0.546 MB uploadedwandb: | 0.546 MB of 0.546 MB uploadedwandb: / 0.546 MB of 0.607 MB uploadedwandb: - 0.607 MB of 0.607 MB uploadedwandb: \ 0.607 MB of 0.607 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–„â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–â–‚â–‚â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–‚â–â–…â–…â–ˆâ–â–ƒâ–†â–‚â–ƒâ–ƒâ–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 213052
wandb:     model/trainable_params 213052
wandb:                  test/loss 0.1267
wandb:                 train/loss 0.2142
wandb:   val/directional_accuracy 48.98551
wandb:                   val/loss 0.08732
wandb:                    val/mae 0.01029
wandb:                   val/mape 15880178.125
wandb:                    val/mse 0.00021
wandb:                     val/r2 -0.01408
wandb:                   val/rmse 0.01433
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/ocmf7yg8
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_195820-ocmf7yg8/logs
Completed: APPLE H=10

Training: Mamba on APPLE for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_200126-zvcep1vi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/zvcep1vi
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H22     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/zvcep1vi
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2390960 Vali Loss: 0.0893033 Test Loss: 0.1341875
Validation loss decreased (inf --> 0.089303).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2304122 Vali Loss: 0.0874874 Test Loss: 0.1332665
Validation loss decreased (0.089303 --> 0.087487).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2262215 Vali Loss: 0.0865322 Test Loss: 0.1328167
Validation loss decreased (0.087487 --> 0.086532).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2244094 Vali Loss: 0.0862051 Test Loss: 0.1322383
Validation loss decreased (0.086532 --> 0.086205).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2235476 Vali Loss: 0.0866491 Test Loss: 0.1329878
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23909603160890666, 'val/loss': 0.08930328169039317, 'test/loss': 0.13418746420315333, '_timestamp': 1762884110.1958115}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23041221217224092, 'val/loss': 0.08748736019645419, 'test/loss': 0.1332665447677885, '_timestamp': 1762884112.971893}).
Epoch: 6, Steps: 132 | Train Loss: 0.2234533 Vali Loss: 0.0863777 Test Loss: 0.1326905
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2230097 Vali Loss: 0.0864697 Test Loss: 0.1326239
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2228856 Vali Loss: 0.0863878 Test Loss: 0.1326567
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2229637 Vali Loss: 0.0862725 Test Loss: 0.1326245
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2226998 Vali Loss: 0.0865118 Test Loss: 0.1326246
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2230592 Vali Loss: 0.0864194 Test Loss: 0.1326255
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2228341 Vali Loss: 0.0864108 Test Loss: 0.1326233
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2229304 Vali Loss: 0.0863616 Test Loss: 0.1326249
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2229559 Vali Loss: 0.0863529 Test Loss: 0.1326246
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.0002102692233165726, mae:0.010411613620817661, rmse:0.014500662684440613, r2:-0.013323783874511719, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0104, RMSE: 0.0145, RÂ²: -0.0133, MAPE: 199745.17%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.631 MB of 0.633 MB uploadedwandb: \ 0.631 MB of 0.633 MB uploadedwandb: | 0.633 MB of 0.633 MB uploadedwandb: / 0.633 MB of 0.633 MB uploadedwandb: - 0.633 MB of 0.633 MB uploadedwandb: \ 0.633 MB of 0.694 MB uploadedwandb: | 0.694 MB of 0.694 MB uploadedwandb: / 0.694 MB of 0.694 MB uploadedwandb: - 0.694 MB of 0.694 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–†â–â–ˆâ–…â–…â–…â–…â–…â–…â–…â–…â–…
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–‚â–â–‚â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–â–ˆâ–„â–…â–„â–‚â–†â–„â–„â–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 13
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 222340
wandb:     model/trainable_params 222340
wandb:                  test/loss 0.13262
wandb:                 train/loss 0.22296
wandb:   val/directional_accuracy 48.97335
wandb:                   val/loss 0.08635
wandb:                    val/mae 0.01041
wandb:                   val/mape 19974517.1875
wandb:                    val/mse 0.00021
wandb:                     val/r2 -0.01332
wandb:                   val/rmse 0.0145
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/zvcep1vi
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_200126-zvcep1vi/logs
Completed: APPLE H=22

Training: Mamba on APPLE for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_200422-448ln416
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/448ln416
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H50     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/448ln416
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.2543629 Vali Loss: 0.0918698 Test Loss: 0.1541945
Validation loss decreased (inf --> 0.091870).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2427506 Vali Loss: 0.0897580 Test Loss: 0.1521673
Validation loss decreased (0.091870 --> 0.089758).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2397160 Vali Loss: 0.0902629 Test Loss: 0.1549309
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2379596 Vali Loss: 0.0897556 Test Loss: 0.1543347
Validation loss decreased (0.089758 --> 0.089756).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2372085 Vali Loss: 0.0893861 Test Loss: 0.1536758
Validation loss decreased (0.089756 --> 0.089386).  Saving model ...
Updating learning rate to 6.25e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2543629326603629, 'val/loss': 0.09186975533763568, 'test/loss': 0.15419445062677065, '_timestamp': 1762884287.625273}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24275059614217642, 'val/loss': 0.0897579975426197, 'test/loss': 0.15216730162501335, '_timestamp': 1762884289.6671832}).
Epoch: 6, Steps: 132 | Train Loss: 0.2383631 Vali Loss: 0.0894318 Test Loss: 0.1539114
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2411822 Vali Loss: 0.0894381 Test Loss: 0.1540957
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2370425 Vali Loss: 0.0893834 Test Loss: 0.1540970
Validation loss decreased (0.089386 --> 0.089383).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2380080 Vali Loss: 0.0893881 Test Loss: 0.1541154
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2390832 Vali Loss: 0.0894138 Test Loss: 0.1541353
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2370839 Vali Loss: 0.0894062 Test Loss: 0.1541328
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2368298 Vali Loss: 0.0894100 Test Loss: 0.1541355
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2377724 Vali Loss: 0.0894552 Test Loss: 0.1541344
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2402977 Vali Loss: 0.0893954 Test Loss: 0.1541362
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2408205 Vali Loss: 0.0894116 Test Loss: 0.1541360
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2368168 Vali Loss: 0.0893772 Test Loss: 0.1541373
Validation loss decreased (0.089383 --> 0.089377).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2372348 Vali Loss: 0.0894102 Test Loss: 0.1541373
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2384984 Vali Loss: 0.0894139 Test Loss: 0.1541372
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2382105 Vali Loss: 0.0894388 Test Loss: 0.1541372
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2370797 Vali Loss: 0.0894436 Test Loss: 0.1541372
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2381495 Vali Loss: 0.0894249 Test Loss: 0.1541372
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2369457 Vali Loss: 0.0894137 Test Loss: 0.1541372
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.2365597 Vali Loss: 0.0893841 Test Loss: 0.1541372
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 132 | Train Loss: 0.2366938 Vali Loss: 0.0894374 Test Loss: 0.1541372
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 132 | Train Loss: 0.2370217 Vali Loss: 0.0894369 Test Loss: 0.1541372
EarlyStopping counter: 9 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 132 | Train Loss: 0.2369272 Vali Loss: 0.0894061 Test Loss: 0.1541372
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.00022450138931162655, mae:0.010769568383693695, rmse:0.01498336996883154, r2:-0.011837124824523926, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0108, RMSE: 0.0150, RÂ²: -0.0118, MAPE: 204127.36%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.682 MB of 0.685 MB uploadedwandb: \ 0.682 MB of 0.685 MB uploadedwandb: | 0.685 MB of 0.685 MB uploadedwandb: / 0.685 MB of 0.685 MB uploadedwandb: - 0.685 MB of 0.685 MB uploadedwandb: \ 0.685 MB of 0.748 MB uploadedwandb: | 0.748 MB of 0.748 MB uploadedwandb: / 0.748 MB of 0.748 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„
wandb:                 train/loss â–†â–ƒâ–‚â–„â–ˆâ–‚â–ƒâ–…â–‚â–â–ƒâ–‡â–‡â–â–‚â–„â–ƒâ–‚â–ƒâ–‚â–â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 25
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 244012
wandb:     model/trainable_params 244012
wandb:                  test/loss 0.15414
wandb:                 train/loss 0.23693
wandb:   val/directional_accuracy 49.97852
wandb:                   val/loss 0.08941
wandb:                    val/mae 0.01077
wandb:                   val/mape 20412735.9375
wandb:                    val/mse 0.00022
wandb:                     val/r2 -0.01184
wandb:                   val/rmse 0.01498
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/448ln416
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_200422-448ln416/logs
Completed: APPLE H=50

Training: Mamba on APPLE for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_200834-fes21zs8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/fes21zs8
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H100    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/fes21zs8
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.2699870 Vali Loss: 0.0999606 Test Loss: 0.1728604
Validation loss decreased (inf --> 0.099961).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2601673 Vali Loss: 0.1000652 Test Loss: 0.1761984
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.2565636 Vali Loss: 0.1000885 Test Loss: 0.1772219
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2558869 Vali Loss: 0.1004552 Test Loss: 0.1776952
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2556527 Vali Loss: 0.0998096 Test Loss: 0.1777161
Validation loss decreased (0.099961 --> 0.099810).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2550388 Vali Loss: 0.0996211 Test Loss: 0.1781205
Validation loss decreased (0.099810 --> 0.099621).  Saving model ...
Updating learning rate to 3.125e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.26998699284516847, 'val/loss': 0.09996062964200973, 'test/loss': 0.17286038100719453, '_timestamp': 1762884543.6363912}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26016726195812223, 'val/loss': 0.10006517469882965, 'test/loss': 0.17619836330413818, '_timestamp': 1762884546.0229516}).
Epoch: 7, Steps: 130 | Train Loss: 0.2549485 Vali Loss: 0.0997410 Test Loss: 0.1777756
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2548890 Vali Loss: 0.0994243 Test Loss: 0.1777235
Validation loss decreased (0.099621 --> 0.099424).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2547969 Vali Loss: 0.0991864 Test Loss: 0.1777311
Validation loss decreased (0.099424 --> 0.099186).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2553480 Vali Loss: 0.0992682 Test Loss: 0.1777244
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2551657 Vali Loss: 0.0987410 Test Loss: 0.1777352
Validation loss decreased (0.099186 --> 0.098741).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2546871 Vali Loss: 0.0992457 Test Loss: 0.1777443
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2547244 Vali Loss: 0.0991092 Test Loss: 0.1777475
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.2549301 Vali Loss: 0.0983491 Test Loss: 0.1777481
Validation loss decreased (0.098741 --> 0.098349).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.2546895 Vali Loss: 0.0997555 Test Loss: 0.1777475
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 130 | Train Loss: 0.2552417 Vali Loss: 0.0992488 Test Loss: 0.1777476
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 130 | Train Loss: 0.2546184 Vali Loss: 0.0987230 Test Loss: 0.1777478
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 130 | Train Loss: 0.2546316 Vali Loss: 0.0988439 Test Loss: 0.1777478
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 130 | Train Loss: 0.2550231 Vali Loss: 0.1000353 Test Loss: 0.1777478
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 130 | Train Loss: 0.2546989 Vali Loss: 0.1000198 Test Loss: 0.1777478
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 130 | Train Loss: 0.2547342 Vali Loss: 0.0986814 Test Loss: 0.1777478
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 130 | Train Loss: 0.2547631 Vali Loss: 0.0986151 Test Loss: 0.1777478
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 130 | Train Loss: 0.2547311 Vali Loss: 0.0997568 Test Loss: 0.1777478
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 130 | Train Loss: 0.2546785 Vali Loss: 0.0997311 Test Loss: 0.1777478
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0002393358590779826, mae:0.0110451215878129, rmse:0.015470483340322971, r2:-0.013654828071594238, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0110, RMSE: 0.0155, RÂ²: -0.0137, MAPE: 379642.56%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.683 MB of 0.688 MB uploadedwandb: \ 0.683 MB of 0.688 MB uploadedwandb: | 0.688 MB of 0.688 MB uploadedwandb: / 0.688 MB of 0.688 MB uploadedwandb: - 0.688 MB of 0.751 MB uploadedwandb: \ 0.751 MB of 0.751 MB uploadedwandb: | 0.751 MB of 0.751 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–…â–…â–ˆâ–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…
wandb:                 train/loss â–ˆâ–†â–…â–ƒâ–‚â–‚â–‚â–„â–ƒâ–â–â–‚â–â–ƒâ–â–â–‚â–â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–ˆâ–†â–…â–†â–…â–„â–„â–‚â–„â–„â–â–†â–„â–‚â–ƒâ–‡â–‡â–‚â–‚â–†â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 23
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 282712
wandb:     model/trainable_params 282712
wandb:                  test/loss 0.17775
wandb:                 train/loss 0.25468
wandb:   val/directional_accuracy 49.79076
wandb:                   val/loss 0.09973
wandb:                    val/mae 0.01105
wandb:                   val/mape 37964256.25
wandb:                    val/mse 0.00024
wandb:                     val/r2 -0.01365
wandb:                   val/rmse 0.01547
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/fes21zs8
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_200834-fes21zs8/logs
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread ChkStopThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 268, in check_network_status
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 286, in check_stop_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 840, in deliver_stop_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 848, in deliver_network_status
        return self._deliver_network_status(status)return self._deliver_stop_status(status)

  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 510, in _deliver_network_status
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 494, in _deliver_stop_status
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: APPLE H=100

Training: Mamba on SP500 for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_201203-so5al189
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/so5al189
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H3      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/so5al189
>>>>>>>start training : long_term_forecast_Mamba_SP500_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 3
============================================================
train 4241
val 237
test 238
Epoch: 1, Steps: 133 | Train Loss: 0.1773027 Vali Loss: 0.0684052 Test Loss: 0.0758884
Validation loss decreased (inf --> 0.068405).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.1754296 Vali Loss: 0.0675304 Test Loss: 0.0734130
Validation loss decreased (0.068405 --> 0.067530).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.1711150 Vali Loss: 0.0652448 Test Loss: 0.0726236
Validation loss decreased (0.067530 --> 0.065245).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1695666 Vali Loss: 0.0653013 Test Loss: 0.0724955
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1681097 Vali Loss: 0.0660038 Test Loss: 0.0724550
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.17730273188729034, 'val/loss': 0.06840516487136483, 'test/loss': 0.07588837901130319, '_timestamp': 1762884748.9477956}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.17542955671486102, 'val/loss': 0.06753041502088308, 'test/loss': 0.07341295713558793, '_timestamp': 1762884751.9411495}).
Epoch: 6, Steps: 133 | Train Loss: 0.1688875 Vali Loss: 0.0644510 Test Loss: 0.0724460
Validation loss decreased (0.065245 --> 0.064451).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1679263 Vali Loss: 0.0652363 Test Loss: 0.0724367
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1678960 Vali Loss: 0.0673949 Test Loss: 0.0724363
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1675810 Vali Loss: 0.0663661 Test Loss: 0.0724373
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1683491 Vali Loss: 0.0664627 Test Loss: 0.0724365
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1675442 Vali Loss: 0.0673441 Test Loss: 0.0724365
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1675618 Vali Loss: 0.0647001 Test Loss: 0.0724366
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1679652 Vali Loss: 0.0657045 Test Loss: 0.0724365
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1673455 Vali Loss: 0.0634565 Test Loss: 0.0724365
Validation loss decreased (0.064451 --> 0.063456).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1688048 Vali Loss: 0.0647025 Test Loss: 0.0724364
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1664595 Vali Loss: 0.0651491 Test Loss: 0.0724364
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1675929 Vali Loss: 0.0656218 Test Loss: 0.0724364
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1683940 Vali Loss: 0.0660268 Test Loss: 0.0724364
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1677147 Vali Loss: 0.0657697 Test Loss: 0.0724364
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1671767 Vali Loss: 0.0646924 Test Loss: 0.0724364
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1675605 Vali Loss: 0.0656253 Test Loss: 0.0724364
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1683318 Vali Loss: 0.0660246 Test Loss: 0.0724364
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1674989 Vali Loss: 0.0662792 Test Loss: 0.0724364
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.1684243 Vali Loss: 0.0654876 Test Loss: 0.0724364
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SP500_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 238
test shape: (238, 3, 1) (238, 3, 1)
test shape: (238, 3, 1) (238, 3, 1)


	mse:6.605921953450888e-05, mae:0.005993920378386974, rmse:0.008127681910991669, r2:-0.016119718551635742, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0060, RMSE: 0.0081, RÂ²: -0.0161, MAPE: 1.57%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.484 MB of 0.484 MB uploadedwandb: \ 0.484 MB of 0.484 MB uploadedwandb: | 0.484 MB of 0.484 MB uploadedwandb: / 0.484 MB of 0.484 MB uploadedwandb: - 0.484 MB of 0.484 MB uploadedwandb: \ 0.484 MB of 0.484 MB uploadedwandb: | 0.484 MB of 0.484 MB uploadedwandb: / 0.484 MB of 0.484 MB uploadedwandb: - 0.533 MB of 0.596 MB uploaded (0.002 MB deduped)wandb: \ 0.596 MB of 0.596 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–†â–ƒâ–…â–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–‚â–…â–â–ƒâ–„â–ƒâ–‚â–ƒâ–„â–ƒâ–„
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–„â–†â–ƒâ–„â–ˆâ–†â–†â–ˆâ–ƒâ–…â–â–ƒâ–„â–…â–†â–…â–ƒâ–…â–†â–†â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 23
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 207634
wandb:     model/trainable_params 207634
wandb:                  test/loss 0.07244
wandb:                 train/loss 0.16842
wandb:   val/directional_accuracy 50.63025
wandb:                   val/loss 0.06549
wandb:                    val/mae 0.00599
wandb:                   val/mape 156.80927
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.01612
wandb:                   val/rmse 0.00813
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/so5al189
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_201203-so5al189/logs
Completed: SP500 H=3

Training: Mamba on SP500 for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_201531-c9eiofnz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/c9eiofnz
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H5      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/c9eiofnz
>>>>>>>start training : long_term_forecast_Mamba_SP500_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 5
============================================================
train 4239
val 235
test 236
Epoch: 1, Steps: 133 | Train Loss: 0.1784860 Vali Loss: 0.0703081 Test Loss: 0.0761892
Validation loss decreased (inf --> 0.070308).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.1768923 Vali Loss: 0.0671707 Test Loss: 0.0746205
Validation loss decreased (0.070308 --> 0.067171).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.1723971 Vali Loss: 0.0679355 Test Loss: 0.0734867
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1718942 Vali Loss: 0.0644216 Test Loss: 0.0732507
Validation loss decreased (0.067171 --> 0.064422).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1701109 Vali Loss: 0.0658343 Test Loss: 0.0732784
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.1784859937720729, 'val/loss': 0.07030806131660938, 'test/loss': 0.07618916593492031, '_timestamp': 1762884955.2282143}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.17689228926162073, 'val/loss': 0.06717073498293757, 'test/loss': 0.07462049787864089, '_timestamp': 1762884958.1144545}).
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.17689228926162073, 'val/loss': 0.06717073498293757, 'test/loss': 0.07462049787864089, '_timestamp': 1762884958.1144545}).
Epoch: 6, Steps: 133 | Train Loss: 0.1706484 Vali Loss: 0.0647019 Test Loss: 0.0732767
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1700280 Vali Loss: 0.0673211 Test Loss: 0.0732856
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1692422 Vali Loss: 0.0661082 Test Loss: 0.0732776
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1692771 Vali Loss: 0.0650144 Test Loss: 0.0732773
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1697742 Vali Loss: 0.0665167 Test Loss: 0.0732768
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1686372 Vali Loss: 0.0646711 Test Loss: 0.0732772
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1689975 Vali Loss: 0.0641585 Test Loss: 0.0732769
Validation loss decreased (0.064422 --> 0.064158).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1697749 Vali Loss: 0.0663089 Test Loss: 0.0732768
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1686918 Vali Loss: 0.0649914 Test Loss: 0.0732768
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1702556 Vali Loss: 0.0643578 Test Loss: 0.0732768
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1705686 Vali Loss: 0.0666716 Test Loss: 0.0732768
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1690644 Vali Loss: 0.0659758 Test Loss: 0.0732768
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1691300 Vali Loss: 0.0666516 Test Loss: 0.0732768
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1698527 Vali Loss: 0.0689138 Test Loss: 0.0732768
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1708254 Vali Loss: 0.0661101 Test Loss: 0.0732768
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1686974 Vali Loss: 0.0668888 Test Loss: 0.0732768
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1709237 Vali Loss: 0.0642227 Test Loss: 0.0732768
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SP500_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
test shape: (236, 5, 1) (236, 5, 1)
test shape: (236, 5, 1) (236, 5, 1)


	mse:6.569727702299133e-05, mae:0.005990021862089634, rmse:0.008105386048555374, r2:-0.0113295316696167, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0060, RMSE: 0.0081, RÂ²: -0.0113, MAPE: 1.54%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.519 MB of 0.519 MB uploadedwandb: \ 0.519 MB of 0.519 MB uploadedwandb: | 0.519 MB of 0.519 MB uploadedwandb: / 0.519 MB of 0.519 MB uploadedwandb: - 0.519 MB of 0.519 MB uploadedwandb: \ 0.519 MB of 0.581 MB uploadedwandb: | 0.581 MB of 0.581 MB uploadedwandb: / 0.581 MB of 0.581 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–‡â–„â–…â–„â–‚â–‚â–ƒâ–â–‚â–ƒâ–â–„â–…â–‚â–‚â–ƒâ–…â–â–…
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–â–ƒâ–‚â–†â–„â–‚â–„â–‚â–â–„â–‚â–â–…â–„â–…â–ˆâ–„â–…â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 209182
wandb:     model/trainable_params 209182
wandb:                  test/loss 0.07328
wandb:                 train/loss 0.17092
wandb:   val/directional_accuracy 51.90678
wandb:                   val/loss 0.06422
wandb:                    val/mae 0.00599
wandb:                   val/mape 153.60738
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.01133
wandb:                   val/rmse 0.00811
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/c9eiofnz
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_201531-c9eiofnz/logs
Completed: SP500 H=5

Training: Mamba on SP500 for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_201830-ues0l899
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/ues0l899
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H10     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/ues0l899
>>>>>>>start training : long_term_forecast_Mamba_SP500_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 10
============================================================
train 4234
val 230
test 231
Epoch: 1, Steps: 133 | Train Loss: 0.1798584 Vali Loss: 0.0683808 Test Loss: 0.0792246
Validation loss decreased (inf --> 0.068381).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.1793107 Vali Loss: 0.0685020 Test Loss: 0.0767109
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.1761535 Vali Loss: 0.0671749 Test Loss: 0.0761835
Validation loss decreased (0.068381 --> 0.067175).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1722317 Vali Loss: 0.0686871 Test Loss: 0.0760153
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1716995 Vali Loss: 0.0662143 Test Loss: 0.0759716
Validation loss decreased (0.067175 --> 0.066214).  Saving model ...
Updating learning rate to 6.25e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.1798584270746188, 'val/loss': 0.06838078843429685, 'test/loss': 0.07922458136454225, '_timestamp': 1762885135.1413765}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.17931066727951953, 'val/loss': 0.06850198842585087, 'test/loss': 0.07671089749783278, '_timestamp': 1762885138.1028214}).
Epoch: 6, Steps: 133 | Train Loss: 0.1709995 Vali Loss: 0.0661105 Test Loss: 0.0759702
Validation loss decreased (0.066214 --> 0.066111).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1708807 Vali Loss: 0.0667676 Test Loss: 0.0759692
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1707405 Vali Loss: 0.0652681 Test Loss: 0.0759666
Validation loss decreased (0.066111 --> 0.065268).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1708656 Vali Loss: 0.0663926 Test Loss: 0.0759660
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1707571 Vali Loss: 0.0651193 Test Loss: 0.0759659
Validation loss decreased (0.065268 --> 0.065119).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1720914 Vali Loss: 0.0643625 Test Loss: 0.0759660
Validation loss decreased (0.065119 --> 0.064362).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1712457 Vali Loss: 0.0661306 Test Loss: 0.0759660
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1705938 Vali Loss: 0.0674307 Test Loss: 0.0759659
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1708628 Vali Loss: 0.0632688 Test Loss: 0.0759659
Validation loss decreased (0.064362 --> 0.063269).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1711140 Vali Loss: 0.0649561 Test Loss: 0.0759659
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1708502 Vali Loss: 0.0656712 Test Loss: 0.0759659
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1726330 Vali Loss: 0.0658621 Test Loss: 0.0759659
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1704016 Vali Loss: 0.0652675 Test Loss: 0.0759659
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1710113 Vali Loss: 0.0654903 Test Loss: 0.0759659
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1712058 Vali Loss: 0.0695633 Test Loss: 0.0759659
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1711771 Vali Loss: 0.0655302 Test Loss: 0.0759659
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1712530 Vali Loss: 0.0683645 Test Loss: 0.0759659
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1714410 Vali Loss: 0.0653705 Test Loss: 0.0759659
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.1710821 Vali Loss: 0.0676864 Test Loss: 0.0759659
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SP500_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 231
test shape: (231, 10, 1) (231, 10, 1)
test shape: (231, 10, 1) (231, 10, 1)


	mse:6.572425627382472e-05, mae:0.005990153178572655, rmse:0.008107049390673637, r2:-0.011098861694335938, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0060, RMSE: 0.0081, RÂ²: -0.0111, MAPE: 1.59%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.334 MB of 0.542 MB uploadedwandb: \ 0.542 MB of 0.542 MB uploadedwandb: | 0.542 MB of 0.542 MB uploadedwandb: / 0.542 MB of 0.542 MB uploadedwandb: - 0.542 MB of 0.605 MB uploadedwandb: \ 0.605 MB of 0.605 MB uploadedwandb: | 0.605 MB of 0.605 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–ƒâ–ƒâ–‚â–‚â–â–‚â–â–ƒâ–‚â–â–‚â–‚â–‚â–„â–â–‚â–‚â–‚â–‚â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–‡â–„â–„â–…â–ƒâ–„â–ƒâ–‚â–„â–†â–â–ƒâ–„â–„â–ƒâ–ƒâ–ˆâ–„â–‡â–ƒâ–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 23
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 213052
wandb:     model/trainable_params 213052
wandb:                  test/loss 0.07597
wandb:                 train/loss 0.17108
wandb:   val/directional_accuracy 51.41895
wandb:                   val/loss 0.06769
wandb:                    val/mae 0.00599
wandb:                   val/mape 158.68951
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.0111
wandb:                   val/rmse 0.00811
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/ues0l899
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_201830-ues0l899/logs
Completed: SP500 H=10

Training: Mamba on SP500 for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_202234-sjvius63
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/sjvius63
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H22     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/sjvius63
>>>>>>>start training : long_term_forecast_Mamba_SP500_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 22
============================================================
train 4222
val 218
test 219
Epoch: 1, Steps: 132 | Train Loss: 0.1822157 Vali Loss: 0.0714352 Test Loss: 0.0733402
Validation loss decreased (inf --> 0.071435).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.1792108 Vali Loss: 0.0694813 Test Loss: 0.0706889
Validation loss decreased (0.071435 --> 0.069481).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.1762227 Vali Loss: 0.0686280 Test Loss: 0.0692094
Validation loss decreased (0.069481 --> 0.068628).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.1746635 Vali Loss: 0.0683314 Test Loss: 0.0690122
Validation loss decreased (0.068628 --> 0.068331).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.1742541 Vali Loss: 0.0683672 Test Loss: 0.0689253
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.18221565832694372, 'val/loss': 0.07143522373267583, 'test/loss': 0.07334024565560478, '_timestamp': 1762885378.587858}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.17921077273786068, 'val/loss': 0.06948127704007286, 'test/loss': 0.0706889485674245, '_timestamp': 1762885381.0304813}).
Epoch: 6, Steps: 132 | Train Loss: 0.1740667 Vali Loss: 0.0684091 Test Loss: 0.0688924
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.1738535 Vali Loss: 0.0682676 Test Loss: 0.0688804
Validation loss decreased (0.068331 --> 0.068268).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.1740037 Vali Loss: 0.0681551 Test Loss: 0.0688735
Validation loss decreased (0.068268 --> 0.068155).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.1738289 Vali Loss: 0.0683657 Test Loss: 0.0688705
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.1737059 Vali Loss: 0.0682757 Test Loss: 0.0688689
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.1736792 Vali Loss: 0.0683236 Test Loss: 0.0688681
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.1735296 Vali Loss: 0.0682680 Test Loss: 0.0688677
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.1738857 Vali Loss: 0.0683296 Test Loss: 0.0688675
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.1738604 Vali Loss: 0.0684358 Test Loss: 0.0688674
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.1736372 Vali Loss: 0.0682600 Test Loss: 0.0688674
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.1739655 Vali Loss: 0.0683511 Test Loss: 0.0688674
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.1738543 Vali Loss: 0.0682318 Test Loss: 0.0688674
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.1740172 Vali Loss: 0.0682238 Test Loss: 0.0688674
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SP500_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 219
test shape: (219, 22, 1) (219, 22, 1)
test shape: (219, 22, 1) (219, 22, 1)


	mse:6.45228210487403e-05, mae:0.00595540925860405, rmse:0.008032609708607197, r2:-0.010609745979309082, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0060, RMSE: 0.0080, RÂ²: -0.0106, MAPE: 1.89%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.604 MB of 0.605 MB uploadedwandb: \ 0.605 MB of 0.605 MB uploadedwandb: | 0.605 MB of 0.605 MB uploadedwandb: / 0.605 MB of 0.667 MB uploadedwandb: - 0.667 MB of 0.667 MB uploadedwandb: \ 0.667 MB of 0.667 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–‚â–‚â–â–‚â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–„â–…â–ƒâ–â–„â–ƒâ–ƒâ–ƒâ–„â–…â–ƒâ–„â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 17
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 222340
wandb:     model/trainable_params 222340
wandb:                  test/loss 0.06887
wandb:                 train/loss 0.17402
wandb:   val/directional_accuracy 50.6197
wandb:                   val/loss 0.06822
wandb:                    val/mae 0.00596
wandb:                   val/mape 189.34412
wandb:                    val/mse 6e-05
wandb:                     val/r2 -0.01061
wandb:                   val/rmse 0.00803
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/sjvius63
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_202234-sjvius63/logs
Completed: SP500 H=22

Training: Mamba on SP500 for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_202648-25u6ecwz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/25u6ecwz
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H50     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/25u6ecwz
>>>>>>>start training : long_term_forecast_Mamba_SP500_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 50
============================================================
train 4194
val 190
test 191
Epoch: 1, Steps: 132 | Train Loss: 0.1880069 Vali Loss: 0.0720400 Test Loss: 0.0786846
Validation loss decreased (inf --> 0.072040).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.1840172 Vali Loss: 0.0698119 Test Loss: 0.0736615
Validation loss decreased (0.072040 --> 0.069812).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.1815694 Vali Loss: 0.0693398 Test Loss: 0.0722875
Validation loss decreased (0.069812 --> 0.069340).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.1802222 Vali Loss: 0.0693421 Test Loss: 0.0719988
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.1898127 Vali Loss: 0.0693793 Test Loss: 0.0719610
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.1880069071934982, 'val/loss': 0.07204003011186917, 'test/loss': 0.07868459448218346, '_timestamp': 1762885633.2889717}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.1840172167867422, 'val/loss': 0.06981188555558522, 'test/loss': 0.07366146830221017, '_timestamp': 1762885636.2699635}).
Epoch: 6, Steps: 132 | Train Loss: 0.1790148 Vali Loss: 0.0693411 Test Loss: 0.0718969
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.1798097 Vali Loss: 0.0693695 Test Loss: 0.0718822
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.1798803 Vali Loss: 0.0693480 Test Loss: 0.0718776
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.1795896 Vali Loss: 0.0693796 Test Loss: 0.0718771
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.1786987 Vali Loss: 0.0693649 Test Loss: 0.0718763
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.1786651 Vali Loss: 0.0693986 Test Loss: 0.0718759
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.1793061 Vali Loss: 0.0693966 Test Loss: 0.0718750
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.1787641 Vali Loss: 0.0693353 Test Loss: 0.0718750
Validation loss decreased (0.069340 --> 0.069335).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.1786578 Vali Loss: 0.0693939 Test Loss: 0.0718749
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.1797000 Vali Loss: 0.0693587 Test Loss: 0.0718748
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.1795655 Vali Loss: 0.0693664 Test Loss: 0.0718748
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.1786215 Vali Loss: 0.0693840 Test Loss: 0.0718748
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.1787351 Vali Loss: 0.0693680 Test Loss: 0.0718748
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.1787115 Vali Loss: 0.0693610 Test Loss: 0.0718748
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.1783573 Vali Loss: 0.0693484 Test Loss: 0.0718748
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.1853781 Vali Loss: 0.0693610 Test Loss: 0.0718748
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.1793161 Vali Loss: 0.0693778 Test Loss: 0.0718748
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.1786591 Vali Loss: 0.0693559 Test Loss: 0.0718748
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SP500_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 191
test shape: (191, 50, 1) (191, 50, 1)
test shape: (191, 50, 1) (191, 50, 1)


	mse:6.555103755090386e-05, mae:0.006016037426888943, rmse:0.008096359670162201, r2:-0.007972955703735352, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0060, RMSE: 0.0081, RÂ²: -0.0080, MAPE: 2.04%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.689 MB of 0.692 MB uploadedwandb: \ 0.689 MB of 0.692 MB uploadedwandb: | 0.692 MB of 0.692 MB uploadedwandb: / 0.692 MB of 0.692 MB uploadedwandb: - 0.692 MB of 0.754 MB uploadedwandb: \ 0.754 MB of 0.754 MB uploadedwandb: | 0.754 MB of 0.754 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ƒâ–‚â–ˆâ–â–‚â–‚â–‚â–â–â–‚â–â–â–‚â–‚â–â–â–â–â–…â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–‚â–†â–‚â–…â–‚â–†â–„â–ˆâ–ˆâ–â–‡â–„â–„â–†â–…â–„â–‚â–„â–†â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 244012
wandb:     model/trainable_params 244012
wandb:                  test/loss 0.07187
wandb:                 train/loss 0.17866
wandb:   val/directional_accuracy 49.49247
wandb:                   val/loss 0.06936
wandb:                    val/mae 0.00602
wandb:                   val/mape 204.42145
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.00797
wandb:                   val/rmse 0.0081
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/25u6ecwz
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_202648-25u6ecwz/logs
Exception in thread ChkStopThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread IntMsgThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 286, in check_stop_status
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 300, in check_internal_messages
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 840, in deliver_stop_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 856, in deliver_internal_messages
    return self._deliver_stop_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 494, in _deliver_stop_status
    return self._deliver_internal_messages(internal_message)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 516, in _deliver_internal_messages
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: SP500 H=50

Training: Mamba on SP500 for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_203032-jcfdatjb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/jcfdatjb
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H100    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/jcfdatjb
>>>>>>>start training : long_term_forecast_Mamba_SP500_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 100
============================================================
train 4144
val 140
test 141
Epoch: 1, Steps: 130 | Train Loss: 0.1983821 Vali Loss: 0.0744703 Test Loss: 0.0934841
Validation loss decreased (inf --> 0.074470).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.1946866 Vali Loss: 0.0709042 Test Loss: 0.0820600
Validation loss decreased (0.074470 --> 0.070904).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.1907842 Vali Loss: 0.0705615 Test Loss: 0.0794584
Validation loss decreased (0.070904 --> 0.070562).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.1884934 Vali Loss: 0.0702440 Test Loss: 0.0792232
Validation loss decreased (0.070562 --> 0.070244).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.1880764 Vali Loss: 0.0699474 Test Loss: 0.0789646
Validation loss decreased (0.070244 --> 0.069947).  Saving model ...
Updating learning rate to 6.25e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.19838209616450164, 'val/loss': 0.07447027266025544, 'test/loss': 0.0934841051697731, '_timestamp': 1762885854.5738697}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19468663638600936, 'val/loss': 0.07090424597263337, 'test/loss': 0.08206001818180084, '_timestamp': 1762885857.2797534}).
Epoch: 6, Steps: 130 | Train Loss: 0.1882002 Vali Loss: 0.0697362 Test Loss: 0.0788490
Validation loss decreased (0.069947 --> 0.069736).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.1882622 Vali Loss: 0.0698627 Test Loss: 0.0788208
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.1880510 Vali Loss: 0.0699365 Test Loss: 0.0788000
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.1877471 Vali Loss: 0.0696643 Test Loss: 0.0787890
Validation loss decreased (0.069736 --> 0.069664).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.1880425 Vali Loss: 0.0696259 Test Loss: 0.0787882
Validation loss decreased (0.069664 --> 0.069626).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.1881493 Vali Loss: 0.0694427 Test Loss: 0.0787879
Validation loss decreased (0.069626 --> 0.069443).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.1884493 Vali Loss: 0.0697748 Test Loss: 0.0787881
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.1878073 Vali Loss: 0.0693637 Test Loss: 0.0787884
Validation loss decreased (0.069443 --> 0.069364).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.1880759 Vali Loss: 0.0695048 Test Loss: 0.0787885
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.1881509 Vali Loss: 0.0700810 Test Loss: 0.0787884
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 130 | Train Loss: 0.1880792 Vali Loss: 0.0697294 Test Loss: 0.0787884
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 130 | Train Loss: 0.1878068 Vali Loss: 0.0696775 Test Loss: 0.0787884
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 130 | Train Loss: 0.1880566 Vali Loss: 0.0698097 Test Loss: 0.0787884
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 130 | Train Loss: 0.1885775 Vali Loss: 0.0697205 Test Loss: 0.0787884
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 130 | Train Loss: 0.1880868 Vali Loss: 0.0697649 Test Loss: 0.0787884
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 130 | Train Loss: 0.1877860 Vali Loss: 0.0694603 Test Loss: 0.0787884
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 130 | Train Loss: 0.1884571 Vali Loss: 0.0694308 Test Loss: 0.0787884
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 130 | Train Loss: 0.1880153 Vali Loss: 0.0699450 Test Loss: 0.0787884
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SP500_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 141
test shape: (141, 100, 1) (141, 100, 1)
test shape: (141, 100, 1) (141, 100, 1)


	mse:6.88949876348488e-05, mae:0.006117071956396103, rmse:0.008300300687551498, r2:-0.0056438446044921875, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0083, RÂ²: -0.0056, MAPE: 2.35%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.791 MB of 0.796 MB uploadedwandb: \ 0.796 MB of 0.796 MB uploadedwandb: | 0.796 MB of 0.796 MB uploadedwandb: / 0.796 MB of 0.796 MB uploadedwandb: - 0.796 MB of 0.859 MB uploadedwandb: \ 0.859 MB of 0.859 MB uploadedwandb: | 0.859 MB of 0.859 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–†â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚â–ƒâ–â–‚â–‚â–‚â–â–‚â–ƒâ–‚â–â–ƒâ–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–„â–ƒâ–„â–„â–ƒâ–ƒâ–â–ƒâ–â–‚â–…â–ƒâ–ƒâ–„â–ƒâ–ƒâ–‚â–â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 282712
wandb:     model/trainable_params 282712
wandb:                  test/loss 0.07879
wandb:                 train/loss 0.18802
wandb:   val/directional_accuracy 50.24715
wandb:                   val/loss 0.06994
wandb:                    val/mae 0.00612
wandb:                   val/mape 235.12549
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.00564
wandb:                   val/rmse 0.0083
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/jcfdatjb
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_203032-jcfdatjb/logs
Completed: SP500 H=100

Training: Mamba on NASDAQ for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_203433-nfgouvkv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/nfgouvkv
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H3     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/nfgouvkv
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.2401576 Vali Loss: 0.1311073 Test Loss: 0.1217663
Validation loss decreased (inf --> 0.131107).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2285778 Vali Loss: 0.1217545 Test Loss: 0.1151711
Validation loss decreased (0.131107 --> 0.121754).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2225856 Vali Loss: 0.1243877 Test Loss: 0.1138726
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2203712 Vali Loss: 0.1230354 Test Loss: 0.1134707
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2190045 Vali Loss: 0.1235861 Test Loss: 0.1132726
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.24015755598482333, 'val/loss': 0.1311072986572981, 'test/loss': 0.12176630226895213, '_timestamp': 1762886096.4198203}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.22857782093429924, 'val/loss': 0.12175447214394808, 'test/loss': 0.11517107486724854, '_timestamp': 1762886099.0796516}).
Epoch: 6, Steps: 133 | Train Loss: 0.2178025 Vali Loss: 0.1223088 Test Loss: 0.1132168
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2175979 Vali Loss: 0.1199578 Test Loss: 0.1131872
Validation loss decreased (0.121754 --> 0.119958).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2176516 Vali Loss: 0.1233340 Test Loss: 0.1131675
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2175592 Vali Loss: 0.1245356 Test Loss: 0.1131626
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2171719 Vali Loss: 0.1246410 Test Loss: 0.1131528
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2173990 Vali Loss: 0.1233986 Test Loss: 0.1131499
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2178861 Vali Loss: 0.1340084 Test Loss: 0.1131477
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2172112 Vali Loss: 0.1190042 Test Loss: 0.1131472
Validation loss decreased (0.119958 --> 0.119004).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2170795 Vali Loss: 0.1319267 Test Loss: 0.1131468
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2174265 Vali Loss: 0.1202435 Test Loss: 0.1131468
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2168166 Vali Loss: 0.1229207 Test Loss: 0.1131467
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2173729 Vali Loss: 0.1224954 Test Loss: 0.1131467
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2189975 Vali Loss: 0.1222086 Test Loss: 0.1131467
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2174333 Vali Loss: 0.1225587 Test Loss: 0.1131467
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2170658 Vali Loss: 0.1233524 Test Loss: 0.1131467
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2172522 Vali Loss: 0.1209319 Test Loss: 0.1131467
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2162162 Vali Loss: 0.1199847 Test Loss: 0.1131467
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2181238 Vali Loss: 0.1213451 Test Loss: 0.1131467
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.00013964592653792351, mae:0.008535673841834068, rmse:0.011817188002169132, r2:-0.02598738670349121, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0085, RMSE: 0.0118, RÂ²: -0.0260, MAPE: 1239784.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.482 MB of 0.482 MB uploadedwandb: \ 0.482 MB of 0.482 MB uploadedwandb: | 0.482 MB of 0.482 MB uploadedwandb: / 0.482 MB of 0.482 MB uploadedwandb: - 0.482 MB of 0.482 MB uploadedwandb: \ 0.482 MB of 0.482 MB uploadedwandb: | 0.482 MB of 0.482 MB uploadedwandb: / 0.531 MB of 0.593 MB uploaded (0.002 MB deduped)wandb: - 0.531 MB of 0.593 MB uploaded (0.002 MB deduped)wandb: \ 0.593 MB of 0.593 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–†â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–„â–‚â–‚â–‚â–â–ƒ
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–ƒâ–ƒâ–ƒâ–â–ƒâ–„â–„â–ƒâ–ˆâ–â–‡â–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 207634
wandb:     model/trainable_params 207634
wandb:                  test/loss 0.11315
wandb:                 train/loss 0.21812
wandb:   val/directional_accuracy 49.36709
wandb:                   val/loss 0.12135
wandb:                    val/mae 0.00854
wandb:                   val/mape 123978400.0
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.02599
wandb:                   val/rmse 0.01182
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/nfgouvkv
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_203433-nfgouvkv/logs
Completed: NASDAQ H=3

Training: Mamba on NASDAQ for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_203812-3demzf3z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/3demzf3z
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H5     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/3demzf3z
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.2440980 Vali Loss: 0.1350244 Test Loss: 0.1273718
Validation loss decreased (inf --> 0.135024).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2342177 Vali Loss: 0.1293525 Test Loss: 0.1224705
Validation loss decreased (0.135024 --> 0.129353).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2277150 Vali Loss: 0.1266497 Test Loss: 0.1209216
Validation loss decreased (0.129353 --> 0.126650).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2256328 Vali Loss: 0.1239893 Test Loss: 0.1205080
Validation loss decreased (0.126650 --> 0.123989).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2243224 Vali Loss: 0.1328784 Test Loss: 0.1203143
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2240830 Vali Loss: 0.1229036 Test Loss: 0.1202477
Validation loss decreased (0.123989 --> 0.122904).  Saving model ...
Updating learning rate to 3.125e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.24409803636091992, 'val/loss': 0.13502443674951792, 'test/loss': 0.12737184576690197, '_timestamp': 1762886316.695308}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23421773740223475, 'val/loss': 0.12935252394527197, 'test/loss': 0.12247045710682869, '_timestamp': 1762886318.9958434}).
Epoch: 7, Steps: 133 | Train Loss: 0.2239151 Vali Loss: 0.1247823 Test Loss: 0.1202154
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2247117 Vali Loss: 0.1287677 Test Loss: 0.1202035
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2247646 Vali Loss: 0.1239032 Test Loss: 0.1201974
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2233330 Vali Loss: 0.1246726 Test Loss: 0.1201929
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2239899 Vali Loss: 0.1245785 Test Loss: 0.1201910
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2233180 Vali Loss: 0.1263093 Test Loss: 0.1201896
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2252488 Vali Loss: 0.1240339 Test Loss: 0.1201892
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2236110 Vali Loss: 0.1355198 Test Loss: 0.1201890
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2234340 Vali Loss: 0.1260886 Test Loss: 0.1201889
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2235617 Vali Loss: 0.1276371 Test Loss: 0.1201888
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.00013888617104385048, mae:0.008509375154972076, rmse:0.011784997768700123, r2:-0.01522815227508545, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0085, RMSE: 0.0118, RÂ²: -0.0152, MAPE: 1492873.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.520 MB of 0.520 MB uploadedwandb: \ 0.520 MB of 0.520 MB uploadedwandb: | 0.520 MB of 0.520 MB uploadedwandb: / 0.520 MB of 0.520 MB uploadedwandb: - 0.520 MB of 0.581 MB uploadedwandb: \ 0.520 MB of 0.581 MB uploadedwandb: | 0.581 MB of 0.581 MB uploadedwandb: / 0.581 MB of 0.581 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–ƒâ–ƒâ–â–‚â–â–„â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–‚â–‡â–â–‚â–„â–‚â–‚â–‚â–ƒâ–‚â–ˆâ–ƒâ–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 209182
wandb:     model/trainable_params 209182
wandb:                  test/loss 0.12019
wandb:                 train/loss 0.22356
wandb:   val/directional_accuracy 49.3617
wandb:                   val/loss 0.12764
wandb:                    val/mae 0.00851
wandb:                   val/mape 149287300.0
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.01523
wandb:                   val/rmse 0.01178
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/3demzf3z
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_203812-3demzf3z/logs
Completed: NASDAQ H=5

Training: Mamba on NASDAQ for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_204117-l5finxn2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/l5finxn2
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H10    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/l5finxn2
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2474159 Vali Loss: 0.1393624 Test Loss: 0.1309213
Validation loss decreased (inf --> 0.139362).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2400867 Vali Loss: 0.1311777 Test Loss: 0.1259680
Validation loss decreased (0.139362 --> 0.131178).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2358328 Vali Loss: 0.1312761 Test Loss: 0.1251832
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2324994 Vali Loss: 0.1447975 Test Loss: 0.1243475
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2320016 Vali Loss: 0.1443293 Test Loss: 0.1241553
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2321204 Vali Loss: 0.1330929 Test Loss: 0.1240722
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.24741587871895696, 'val/loss': 0.13936237338930368, 'test/loss': 0.1309212977066636, '_timestamp': 1762886500.632793}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24008670964635404, 'val/loss': 0.13117772806435823, 'test/loss': 0.12596804928034544, '_timestamp': 1762886503.3209696}).
Epoch: 7, Steps: 133 | Train Loss: 0.2334470 Vali Loss: 0.1311897 Test Loss: 0.1240275
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2325197 Vali Loss: 0.1317360 Test Loss: 0.1240044
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2321847 Vali Loss: 0.1327039 Test Loss: 0.1239924
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2317461 Vali Loss: 0.1290256 Test Loss: 0.1239889
Validation loss decreased (0.131178 --> 0.129026).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2312685 Vali Loss: 0.1287915 Test Loss: 0.1239870
Validation loss decreased (0.129026 --> 0.128791).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2312740 Vali Loss: 0.1469014 Test Loss: 0.1239861
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2309748 Vali Loss: 0.1338369 Test Loss: 0.1239854
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2314523 Vali Loss: 0.1322141 Test Loss: 0.1239850
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2313144 Vali Loss: 0.1297262 Test Loss: 0.1239849
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2315435 Vali Loss: 0.1299644 Test Loss: 0.1239849
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2309475 Vali Loss: 0.1402448 Test Loss: 0.1239848
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2314382 Vali Loss: 0.1315721 Test Loss: 0.1239848
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2309007 Vali Loss: 0.1325993 Test Loss: 0.1239848
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2305892 Vali Loss: 0.1342244 Test Loss: 0.1239848
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2316593 Vali Loss: 0.1296994 Test Loss: 0.1239848
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.00014191537047736347, mae:0.00863389577716589, rmse:0.011912823654711246, r2:-0.025206804275512695, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0086, RMSE: 0.0119, RÂ²: -0.0252, MAPE: 1925452.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.582 MB of 0.583 MB uploadedwandb: \ 0.582 MB of 0.583 MB uploadedwandb: | 0.582 MB of 0.583 MB uploadedwandb: / 0.583 MB of 0.583 MB uploadedwandb: - 0.583 MB of 0.645 MB uploadedwandb: \ 0.583 MB of 0.645 MB uploadedwandb: | 0.645 MB of 0.645 MB uploadedwandb: / 0.645 MB of 0.645 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–ƒâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–‡â–‡â–ƒâ–‚â–‚â–ƒâ–â–â–ˆâ–ƒâ–‚â–â–â–…â–‚â–‚â–ƒâ–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 213052
wandb:     model/trainable_params 213052
wandb:                  test/loss 0.12398
wandb:                 train/loss 0.23166
wandb:   val/directional_accuracy 48.11594
wandb:                   val/loss 0.1297
wandb:                    val/mae 0.00863
wandb:                   val/mape 192545275.0
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.02521
wandb:                   val/rmse 0.01191
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/l5finxn2
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_204117-l5finxn2/logs
Completed: NASDAQ H=10

Training: Mamba on NASDAQ for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_204442-sl0x37b9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/sl0x37b9
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H22    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/sl0x37b9
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2539983 Vali Loss: 0.1531386 Test Loss: 0.1335920
Validation loss decreased (inf --> 0.153139).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2487025 Vali Loss: 0.1493717 Test Loss: 0.1308390
Validation loss decreased (0.153139 --> 0.149372).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2441896 Vali Loss: 0.1481672 Test Loss: 0.1292520
Validation loss decreased (0.149372 --> 0.148167).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2419085 Vali Loss: 0.1481153 Test Loss: 0.1288636
Validation loss decreased (0.148167 --> 0.148115).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2412681 Vali Loss: 0.1475062 Test Loss: 0.1287178
Validation loss decreased (0.148115 --> 0.147506).  Saving model ...
Updating learning rate to 6.25e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.25399830880941765, 'val/loss': 0.15313864180019923, 'test/loss': 0.13359202657427108, '_timestamp': 1762886708.9291399}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24870247348691477, 'val/loss': 0.14937167827572143, 'test/loss': 0.13083901415978158, '_timestamp': 1762886712.0585248}).
Epoch: 6, Steps: 132 | Train Loss: 0.2408914 Vali Loss: 0.1476115 Test Loss: 0.1286188
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2406269 Vali Loss: 0.1469887 Test Loss: 0.1285821
Validation loss decreased (0.147506 --> 0.146989).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2407127 Vali Loss: 0.1472648 Test Loss: 0.1285613
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2406949 Vali Loss: 0.1483767 Test Loss: 0.1285525
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2407233 Vali Loss: 0.1476601 Test Loss: 0.1285485
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2408188 Vali Loss: 0.1477372 Test Loss: 0.1285462
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2408510 Vali Loss: 0.1467865 Test Loss: 0.1285452
Validation loss decreased (0.146989 --> 0.146787).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2406311 Vali Loss: 0.1479724 Test Loss: 0.1285446
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2408006 Vali Loss: 0.1467804 Test Loss: 0.1285444
Validation loss decreased (0.146787 --> 0.146780).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2406178 Vali Loss: 0.1474404 Test Loss: 0.1285443
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2406737 Vali Loss: 0.1479801 Test Loss: 0.1285442
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2409056 Vali Loss: 0.1478311 Test Loss: 0.1285442
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2406442 Vali Loss: 0.1478310 Test Loss: 0.1285442
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2409525 Vali Loss: 0.1475091 Test Loss: 0.1285442
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2405707 Vali Loss: 0.1474188 Test Loss: 0.1285442
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2407744 Vali Loss: 0.1470070 Test Loss: 0.1285442
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2406321 Vali Loss: 0.1474339 Test Loss: 0.1285442
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.2408229 Vali Loss: 0.1486758 Test Loss: 0.1285442
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 132 | Train Loss: 0.2407242 Vali Loss: 0.1470745 Test Loss: 0.1285442
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.00014366040704771876, mae:0.008698399178683758, rmse:0.011985842138528824, r2:-0.02669227123260498, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0087, RMSE: 0.0120, RÂ²: -0.0267, MAPE: 2748640.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.623 MB of 0.624 MB uploadedwandb: \ 0.623 MB of 0.624 MB uploadedwandb: | 0.624 MB of 0.624 MB uploadedwandb: / 0.624 MB of 0.624 MB uploadedwandb: - 0.624 MB of 0.686 MB uploadedwandb: \ 0.624 MB of 0.686 MB uploadedwandb: | 0.686 MB of 0.686 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–‚â–â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–†â–„â–„â–‚â–ƒâ–‡â–„â–…â–â–…â–â–ƒâ–…â–…â–…â–„â–ƒâ–‚â–ƒâ–ˆâ–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 23
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 222340
wandb:     model/trainable_params 222340
wandb:                  test/loss 0.12854
wandb:                 train/loss 0.24072
wandb:   val/directional_accuracy 50.19659
wandb:                   val/loss 0.14707
wandb:                    val/mae 0.0087
wandb:                   val/mape 274864050.0
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.02669
wandb:                   val/rmse 0.01199
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/sl0x37b9
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_204442-sl0x37b9/logs
Completed: NASDAQ H=22

Training: Mamba on NASDAQ for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_204848-u70gu7l2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/u70gu7l2
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H50    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/u70gu7l2
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.2643453 Vali Loss: 0.1631149 Test Loss: 0.1440009
Validation loss decreased (inf --> 0.163115).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2578597 Vali Loss: 0.1609415 Test Loss: 0.1398098
Validation loss decreased (0.163115 --> 0.160942).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2535456 Vali Loss: 0.1601037 Test Loss: 0.1380302
Validation loss decreased (0.160942 --> 0.160104).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2520481 Vali Loss: 0.1598715 Test Loss: 0.1376229
Validation loss decreased (0.160104 --> 0.159872).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2517443 Vali Loss: 0.1599309 Test Loss: 0.1374683
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2643452501206687, 'val/loss': 0.16311485320329666, 'test/loss': 0.14400094747543335, '_timestamp': 1762886953.5204608}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2578597030404842, 'val/loss': 0.16094152877728143, 'test/loss': 0.13980978603164354, '_timestamp': 1762886956.6298308}).
Epoch: 6, Steps: 132 | Train Loss: 0.2535387 Vali Loss: 0.1599405 Test Loss: 0.1374277
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2620007 Vali Loss: 0.1598682 Test Loss: 0.1374163
Validation loss decreased (0.159872 --> 0.159868).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2519605 Vali Loss: 0.1598234 Test Loss: 0.1374101
Validation loss decreased (0.159868 --> 0.159823).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2524667 Vali Loss: 0.1598845 Test Loss: 0.1374121
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2511485 Vali Loss: 0.1599385 Test Loss: 0.1374105
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2518948 Vali Loss: 0.1598022 Test Loss: 0.1374094
Validation loss decreased (0.159823 --> 0.159802).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2515380 Vali Loss: 0.1595875 Test Loss: 0.1374086
Validation loss decreased (0.159802 --> 0.159587).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2508909 Vali Loss: 0.1597115 Test Loss: 0.1374083
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2611806 Vali Loss: 0.1600878 Test Loss: 0.1374081
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2510879 Vali Loss: 0.1597490 Test Loss: 0.1374081
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2520234 Vali Loss: 0.1598219 Test Loss: 0.1374081
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2512734 Vali Loss: 0.1599549 Test Loss: 0.1374081
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2506736 Vali Loss: 0.1596248 Test Loss: 0.1374080
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2580591 Vali Loss: 0.1600237 Test Loss: 0.1374080
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2513488 Vali Loss: 0.1597810 Test Loss: 0.1374080
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2510079 Vali Loss: 0.1599144 Test Loss: 0.1374080
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2513083 Vali Loss: 0.1599655 Test Loss: 0.1374080
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.00014774961164221168, mae:0.00870135985314846, rmse:0.012155230157077312, r2:-0.02004873752593994, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0087, RMSE: 0.0122, RÂ²: -0.0200, MAPE: 2655021.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.654 MB of 0.657 MB uploadedwandb: \ 0.654 MB of 0.657 MB uploadedwandb: | 0.657 MB of 0.657 MB uploadedwandb: / 0.657 MB of 0.657 MB uploadedwandb: - 0.657 MB of 0.657 MB uploadedwandb: \ 0.657 MB of 0.719 MB uploadedwandb: | 0.719 MB of 0.719 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ƒâ–‚â–‚â–ƒâ–ˆâ–‚â–‚â–â–‚â–‚â–â–‡â–â–‚â–â–â–†â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–†â–†â–…â–„â–…â–†â–„â–â–ƒâ–ˆâ–ƒâ–„â–†â–‚â–‡â–„â–…â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 244012
wandb:     model/trainable_params 244012
wandb:                  test/loss 0.13741
wandb:                 train/loss 0.25131
wandb:   val/directional_accuracy 50.30075
wandb:                   val/loss 0.15997
wandb:                    val/mae 0.0087
wandb:                   val/mape 265502125.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.02005
wandb:                   val/rmse 0.01216
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/u70gu7l2
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_204848-u70gu7l2/logs
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread IntMsgThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 300, in check_internal_messages
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 268, in check_network_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    self._loop_check_status(
      File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 856, in deliver_internal_messages
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 848, in deliver_network_status
    return self._deliver_network_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 510, in _deliver_network_status
    return self._deliver_internal_messages(internal_message)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 516, in _deliver_internal_messages
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: NASDAQ H=50

Training: Mamba on NASDAQ for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_205208-m1ph6k51
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/m1ph6k51
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H100   Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/m1ph6k51
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.2732153 Vali Loss: 0.1772256 Test Loss: 0.1575561
Validation loss decreased (inf --> 0.177226).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2705991 Vali Loss: 0.1772186 Test Loss: 0.1547641
Validation loss decreased (0.177226 --> 0.177219).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.2669198 Vali Loss: 0.1773283 Test Loss: 0.1517672
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2653779 Vali Loss: 0.1788179 Test Loss: 0.1507866
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2648072 Vali Loss: 0.1761385 Test Loss: 0.1504671
Validation loss decreased (0.177219 --> 0.176139).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2645502 Vali Loss: 0.1761662 Test Loss: 0.1502839
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.27321528437045906, 'val/loss': 0.17722564935684204, 'test/loss': 0.15755608081817626, '_timestamp': 1762887151.0608125}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.27059909242850083, 'val/loss': 0.17721861898899077, 'test/loss': 0.15476409792900087, '_timestamp': 1762887153.5516055}).
Epoch: 7, Steps: 130 | Train Loss: 0.2645983 Vali Loss: 0.1758961 Test Loss: 0.1502415
Validation loss decreased (0.176139 --> 0.175896).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2644828 Vali Loss: 0.1770494 Test Loss: 0.1502207
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2644931 Vali Loss: 0.1763091 Test Loss: 0.1502077
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2650240 Vali Loss: 0.1763326 Test Loss: 0.1501970
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2647516 Vali Loss: 0.1756214 Test Loss: 0.1501949
Validation loss decreased (0.175896 --> 0.175621).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2639790 Vali Loss: 0.1733320 Test Loss: 0.1501927
Validation loss decreased (0.175621 --> 0.173332).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2639681 Vali Loss: 0.1735540 Test Loss: 0.1501914
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.2646099 Vali Loss: 0.1705796 Test Loss: 0.1501910
Validation loss decreased (0.173332 --> 0.170580).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.2643046 Vali Loss: 0.1772580 Test Loss: 0.1501907
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 130 | Train Loss: 0.2643097 Vali Loss: 0.1756556 Test Loss: 0.1501906
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 130 | Train Loss: 0.2640808 Vali Loss: 0.1741098 Test Loss: 0.1501906
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 130 | Train Loss: 0.2638618 Vali Loss: 0.1750189 Test Loss: 0.1501905
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 130 | Train Loss: 0.2640702 Vali Loss: 0.1777514 Test Loss: 0.1501905
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 130 | Train Loss: 0.2644580 Vali Loss: 0.1767678 Test Loss: 0.1501905
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 130 | Train Loss: 0.2642072 Vali Loss: 0.1749942 Test Loss: 0.1501905
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 130 | Train Loss: 0.2643670 Vali Loss: 0.1749984 Test Loss: 0.1501905
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 130 | Train Loss: 0.2642012 Vali Loss: 0.1757878 Test Loss: 0.1501905
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 130 | Train Loss: 0.2641559 Vali Loss: 0.1764823 Test Loss: 0.1501905
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.00015570252435281873, mae:0.008701075799763203, rmse:0.01247808150947094, r2:-0.026560664176940918, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0087, RMSE: 0.0125, RÂ²: -0.0266, MAPE: 1721601.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.734 MB of 0.739 MB uploadedwandb: \ 0.734 MB of 0.739 MB uploadedwandb: | 0.739 MB of 0.739 MB uploadedwandb: / 0.739 MB of 0.739 MB uploadedwandb: - 0.739 MB of 0.739 MB uploadedwandb: \ 0.739 MB of 0.802 MB uploadedwandb: | 0.802 MB of 0.802 MB uploadedwandb: / 0.802 MB of 0.802 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–„â–ƒâ–â–â–ƒâ–‚â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–ˆâ–†â–†â–†â–†â–†â–†â–…â–ƒâ–„â–â–‡â–…â–„â–…â–‡â–†â–…â–…â–…â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 23
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 282712
wandb:     model/trainable_params 282712
wandb:                  test/loss 0.15019
wandb:                 train/loss 0.26416
wandb:   val/directional_accuracy 49.98557
wandb:                   val/loss 0.17648
wandb:                    val/mae 0.0087
wandb:                   val/mape 172160150.0
wandb:                    val/mse 0.00016
wandb:                     val/r2 -0.02656
wandb:                   val/rmse 0.01248
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/m1ph6k51
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_205208-m1ph6k51/logs
Completed: NASDAQ H=100

Training: Mamba on ABSA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_205452-vf6fma39
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/vf6fma39
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H3       Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/vf6fma39
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 238
Epoch: 1, Steps: 133 | Train Loss: 0.2975721 Vali Loss: 0.1677257 Test Loss: 0.1554616
Validation loss decreased (inf --> 0.167726).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2813018 Vali Loss: 0.1594345 Test Loss: 0.1529753
Validation loss decreased (0.167726 --> 0.159435).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2686430 Vali Loss: 0.1606917 Test Loss: 0.1530500
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2627085 Vali Loss: 0.1588036 Test Loss: 0.1522817
Validation loss decreased (0.159435 --> 0.158804).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2616968 Vali Loss: 0.1577294 Test Loss: 0.1525645
Validation loss decreased (0.158804 --> 0.157729).  Saving model ...
Updating learning rate to 6.25e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29757205839443923, 'val/loss': 0.16772574745118618, 'test/loss': 0.15546162612736225, '_timestamp': 1762887314.1895192}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2813018239978561, 'val/loss': 0.15943452063947916, 'test/loss': 0.15297529846429825, '_timestamp': 1762887316.8371353}).
Epoch: 6, Steps: 133 | Train Loss: 0.2616368 Vali Loss: 0.1585012 Test Loss: 0.1525392
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2594415 Vali Loss: 0.1572582 Test Loss: 0.1525329
Validation loss decreased (0.157729 --> 0.157258).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2595775 Vali Loss: 0.1623150 Test Loss: 0.1524981
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2593854 Vali Loss: 0.1575481 Test Loss: 0.1524880
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2594871 Vali Loss: 0.1600065 Test Loss: 0.1524848
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2600991 Vali Loss: 0.1641366 Test Loss: 0.1524830
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2596060 Vali Loss: 0.1565096 Test Loss: 0.1524821
Validation loss decreased (0.157258 --> 0.156510).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2600353 Vali Loss: 0.1618778 Test Loss: 0.1524810
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2586246 Vali Loss: 0.1579445 Test Loss: 0.1524805
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2593101 Vali Loss: 0.1610229 Test Loss: 0.1524804
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2589419 Vali Loss: 0.1605933 Test Loss: 0.1524804
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2599299 Vali Loss: 0.1669127 Test Loss: 0.1524804
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2591848 Vali Loss: 0.1618419 Test Loss: 0.1524804
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2595904 Vali Loss: 0.1610217 Test Loss: 0.1524804
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2603924 Vali Loss: 0.1599310 Test Loss: 0.1524804
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2599367 Vali Loss: 0.1568512 Test Loss: 0.1524804
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2592985 Vali Loss: 0.1618805 Test Loss: 0.1524804
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 238
test shape: (238, 3, 1) (238, 3, 1)
test shape: (238, 3, 1) (238, 3, 1)


	mse:0.00045547744957730174, mae:0.016247011721134186, rmse:0.021341918036341667, r2:-0.0002593994140625, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0162, RMSE: 0.0213, RÂ²: -0.0003, MAPE: 1.21%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.500 MB of 0.500 MB uploadedwandb: \ 0.500 MB of 0.500 MB uploadedwandb: | 0.500 MB of 0.500 MB uploadedwandb: / 0.500 MB of 0.500 MB uploadedwandb: - 0.500 MB of 0.500 MB uploadedwandb: \ 0.500 MB of 0.500 MB uploadedwandb: | 0.500 MB of 0.500 MB uploadedwandb: / 0.500 MB of 0.500 MB uploadedwandb: - 0.500 MB of 0.500 MB uploadedwandb: \ 0.500 MB of 0.500 MB uploadedwandb: | 0.549 MB of 0.611 MB uploaded (0.002 MB deduped)wandb: / 0.611 MB of 0.611 MB uploaded (0.002 MB deduped)wandb: - 0.611 MB of 0.611 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–‚â–‚â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–ƒâ–‚â–‚â–‚â–…â–‚â–ƒâ–†â–â–…â–‚â–„â–„â–ˆâ–…â–„â–ƒâ–â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 207634
wandb:     model/trainable_params 207634
wandb:                  test/loss 0.15248
wandb:                 train/loss 0.2593
wandb:   val/directional_accuracy 52.94118
wandb:                   val/loss 0.16188
wandb:                    val/mae 0.01625
wandb:                   val/mape 121.49211
wandb:                    val/mse 0.00046
wandb:                     val/r2 -0.00026
wandb:                   val/rmse 0.02134
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/vf6fma39
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_205452-vf6fma39/logs
Completed: ABSA H=3

Training: Mamba on ABSA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_205817-axtt7jwx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/axtt7jwx
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H5       Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/axtt7jwx
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 236
Epoch: 1, Steps: 133 | Train Loss: 0.3021501 Vali Loss: 0.1685150 Test Loss: 0.1589616
Validation loss decreased (inf --> 0.168515).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2853429 Vali Loss: 0.1709655 Test Loss: 0.1591018
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2759546 Vali Loss: 0.1700970 Test Loss: 0.1571864
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2711422 Vali Loss: 0.1659477 Test Loss: 0.1580498
Validation loss decreased (0.168515 --> 0.165948).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2691373 Vali Loss: 0.1677441 Test Loss: 0.1583999
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3021501414757922, 'val/loss': 0.16851498745381832, 'test/loss': 0.15896155685186386, '_timestamp': 1762887521.5800781}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.28534294944956784, 'val/loss': 0.17096553556621075, 'test/loss': 0.15910182055085897, '_timestamp': 1762887524.4882429}).
Epoch: 6, Steps: 133 | Train Loss: 0.2674897 Vali Loss: 0.1618105 Test Loss: 0.1581784
Validation loss decreased (0.165948 --> 0.161811).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2679180 Vali Loss: 0.1695670 Test Loss: 0.1581065
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2674193 Vali Loss: 0.1666205 Test Loss: 0.1581153
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2673849 Vali Loss: 0.1636193 Test Loss: 0.1581044
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2669762 Vali Loss: 0.1643563 Test Loss: 0.1581117
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2671314 Vali Loss: 0.1691731 Test Loss: 0.1581134
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2676510 Vali Loss: 0.1637759 Test Loss: 0.1581134
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2687997 Vali Loss: 0.1743757 Test Loss: 0.1581134
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2672116 Vali Loss: 0.1664777 Test Loss: 0.1581134
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2671660 Vali Loss: 0.1664207 Test Loss: 0.1581134
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2669545 Vali Loss: 0.1704938 Test Loss: 0.1581133
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
test shape: (236, 5, 1) (236, 5, 1)
test shape: (236, 5, 1) (236, 5, 1)


	mse:0.0004578202497214079, mae:0.016308419406414032, rmse:0.021396733820438385, r2:0.0004413723945617676, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0163, RMSE: 0.0214, RÂ²: 0.0004, MAPE: 1.16%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.523 MB of 0.523 MB uploadedwandb: \ 0.523 MB of 0.523 MB uploadedwandb: | 0.523 MB of 0.523 MB uploadedwandb: / 0.523 MB of 0.523 MB uploadedwandb: - 0.523 MB of 0.523 MB uploadedwandb: \ 0.523 MB of 0.583 MB uploadedwandb: | 0.529 MB of 0.583 MB uploadedwandb: / 0.529 MB of 0.583 MB uploadedwandb: - 0.583 MB of 0.583 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–†â–ˆâ–‡â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†
wandb:                 train/loss â–ˆâ–„â–ƒâ–â–‚â–â–â–â–â–‚â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–ƒâ–„â–â–…â–„â–‚â–‚â–…â–‚â–ˆâ–„â–„â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 209182
wandb:     model/trainable_params 209182
wandb:                  test/loss 0.15811
wandb:                 train/loss 0.26695
wandb:   val/directional_accuracy 52.01271
wandb:                   val/loss 0.17049
wandb:                    val/mae 0.01631
wandb:                   val/mape 116.48171
wandb:                    val/mse 0.00046
wandb:                     val/r2 0.00044
wandb:                   val/rmse 0.0214
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/axtt7jwx
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_205817-axtt7jwx/logs
Completed: ABSA H=5

Training: Mamba on ABSA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_210117-z91mwsp7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/z91mwsp7
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H10      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/z91mwsp7
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 231
Epoch: 1, Steps: 133 | Train Loss: 0.3065333 Vali Loss: 0.1754690 Test Loss: 0.1604011
Validation loss decreased (inf --> 0.175469).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2928330 Vali Loss: 0.1732225 Test Loss: 0.1592104
Validation loss decreased (0.175469 --> 0.173222).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2841045 Vali Loss: 0.1711159 Test Loss: 0.1593679
Validation loss decreased (0.173222 --> 0.171116).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2799223 Vali Loss: 0.1749809 Test Loss: 0.1592212
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2782704 Vali Loss: 0.1795203 Test Loss: 0.1594165
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3065332621335983, 'val/loss': 0.17546902783215046, 'test/loss': 0.1604010509327054, '_timestamp': 1762887703.9833674}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.29283298395181956, 'val/loss': 0.1732224691659212, 'test/loss': 0.15921044535934925, '_timestamp': 1762887706.8455906}).
Epoch: 6, Steps: 133 | Train Loss: 0.2787560 Vali Loss: 0.1724071 Test Loss: 0.1593534
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2808515 Vali Loss: 0.1727662 Test Loss: 0.1593713
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2792492 Vali Loss: 0.1732277 Test Loss: 0.1593472
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2785824 Vali Loss: 0.1708915 Test Loss: 0.1593452
Validation loss decreased (0.171116 --> 0.170891).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2774907 Vali Loss: 0.1701290 Test Loss: 0.1593524
Validation loss decreased (0.170891 --> 0.170129).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2771883 Vali Loss: 0.1705684 Test Loss: 0.1593513
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2777032 Vali Loss: 0.1747642 Test Loss: 0.1593508
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2770701 Vali Loss: 0.1770164 Test Loss: 0.1593501
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2780269 Vali Loss: 0.1681919 Test Loss: 0.1593501
Validation loss decreased (0.170129 --> 0.168192).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2777589 Vali Loss: 0.1687731 Test Loss: 0.1593499
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2785255 Vali Loss: 0.1725549 Test Loss: 0.1593498
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2773997 Vali Loss: 0.1750154 Test Loss: 0.1593498
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2790702 Vali Loss: 0.1687302 Test Loss: 0.1593498
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2769000 Vali Loss: 0.1734110 Test Loss: 0.1593498
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2772938 Vali Loss: 0.1750945 Test Loss: 0.1593498
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2782078 Vali Loss: 0.1695162 Test Loss: 0.1593498
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2771745 Vali Loss: 0.1749213 Test Loss: 0.1593498
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2780829 Vali Loss: 0.1737100 Test Loss: 0.1593498
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2769322 Vali Loss: 0.1754158 Test Loss: 0.1593498
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 231
test shape: (231, 10, 1) (231, 10, 1)
test shape: (231, 10, 1) (231, 10, 1)


	mse:0.0004636493104044348, mae:0.016388481482863426, rmse:0.021532516926527023, r2:-0.0042743682861328125, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0164, RMSE: 0.0215, RÂ²: -0.0043, MAPE: 1.26%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.561 MB of 0.562 MB uploadedwandb: \ 0.561 MB of 0.562 MB uploadedwandb: | 0.561 MB of 0.562 MB uploadedwandb: / 0.562 MB of 0.562 MB uploadedwandb: - 0.562 MB of 0.562 MB uploadedwandb: \ 0.562 MB of 0.624 MB uploadedwandb: | 0.562 MB of 0.624 MB uploadedwandb: / 0.624 MB of 0.624 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–†â–â–ˆâ–†â–†â–†â–…â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†
wandb:                 train/loss â–ˆâ–„â–‚â–ƒâ–…â–ƒâ–ƒâ–‚â–â–‚â–â–‚â–‚â–ƒâ–â–ƒâ–â–â–‚â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–…â–ˆâ–„â–„â–„â–ƒâ–‚â–‚â–…â–†â–â–â–„â–…â–â–„â–…â–‚â–…â–„â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 23
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 213052
wandb:     model/trainable_params 213052
wandb:                  test/loss 0.15935
wandb:                 train/loss 0.27693
wandb:   val/directional_accuracy 50.02405
wandb:                   val/loss 0.17542
wandb:                    val/mae 0.01639
wandb:                   val/mape 126.46968
wandb:                    val/mse 0.00046
wandb:                     val/r2 -0.00427
wandb:                   val/rmse 0.02153
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/z91mwsp7
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_210117-z91mwsp7/logs
Completed: ABSA H=10

Training: Mamba on ABSA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_210457-zfs0rixj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/zfs0rixj
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H22      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/zfs0rixj
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 219
Epoch: 1, Steps: 132 | Train Loss: 0.3205839 Vali Loss: 0.1796955 Test Loss: 0.1557553
Validation loss decreased (inf --> 0.179696).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.3094405 Vali Loss: 0.1777862 Test Loss: 0.1558368
Validation loss decreased (0.179696 --> 0.177786).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.3004411 Vali Loss: 0.1791422 Test Loss: 0.1566419
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2968417 Vali Loss: 0.1780761 Test Loss: 0.1564002
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2956341 Vali Loss: 0.1785404 Test Loss: 0.1566140
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2952464 Vali Loss: 0.1787156 Test Loss: 0.1566802
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3205839261186845, 'val/loss': 0.17969553811209543, 'test/loss': 0.15575531550816127, '_timestamp': 1762887920.9822938}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.30944053625518625, 'val/loss': 0.17778616505009787, 'test/loss': 0.15583682911736624, '_timestamp': 1762887923.6105528}).
Epoch: 7, Steps: 132 | Train Loss: 0.2943381 Vali Loss: 0.1783156 Test Loss: 0.1567085
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2947551 Vali Loss: 0.1778581 Test Loss: 0.1567155
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2943174 Vali Loss: 0.1784259 Test Loss: 0.1567279
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2945852 Vali Loss: 0.1783868 Test Loss: 0.1567294
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2944617 Vali Loss: 0.1787380 Test Loss: 0.1567306
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2946932 Vali Loss: 0.1786574 Test Loss: 0.1567316
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 219
test shape: (219, 22, 1) (219, 22, 1)
test shape: (219, 22, 1) (219, 22, 1)


	mse:0.0004711713409051299, mae:0.016528068110346794, rmse:0.021706482395529747, r2:-0.005655884742736816, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0165, RMSE: 0.0217, RÂ²: -0.0057, MAPE: 1.44%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.627 MB of 0.628 MB uploadedwandb: \ 0.627 MB of 0.628 MB uploadedwandb: | 0.627 MB of 0.628 MB uploadedwandb: / 0.628 MB of 0.628 MB uploadedwandb: - 0.628 MB of 0.628 MB uploadedwandb: \ 0.628 MB of 0.689 MB uploadedwandb: | 0.689 MB of 0.689 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–†â–â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‚â–…â–†â–ƒâ–â–„â–„â–†â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 222340
wandb:     model/trainable_params 222340
wandb:                  test/loss 0.15673
wandb:                 train/loss 0.29469
wandb:   val/directional_accuracy 50.05436
wandb:                   val/loss 0.17866
wandb:                    val/mae 0.01653
wandb:                   val/mape 143.91295
wandb:                    val/mse 0.00047
wandb:                     val/r2 -0.00566
wandb:                   val/rmse 0.02171
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/zfs0rixj
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_210457-zfs0rixj/logs
Completed: ABSA H=22

Training: Mamba on ABSA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_210734-jb9mldhu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/jb9mldhu
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H50      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/jb9mldhu
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 191
Epoch: 1, Steps: 132 | Train Loss: 0.3430412 Vali Loss: 0.1800976 Test Loss: 0.1552841
Validation loss decreased (inf --> 0.180098).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.3327851 Vali Loss: 0.1820468 Test Loss: 0.1554592
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.3235045 Vali Loss: 0.1814355 Test Loss: 0.1556187
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.3214575 Vali Loss: 0.1821373 Test Loss: 0.1556719
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.3205922 Vali Loss: 0.1820608 Test Loss: 0.1556838
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.3199912 Vali Loss: 0.1820641 Test Loss: 0.1557085
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.34304120369029767, 'val/loss': 0.18009759734074274, 'test/loss': 0.15528409679730734, '_timestamp': 1762888079.5623715}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.33278510222832364, 'val/loss': 0.18204675614833832, 'test/loss': 0.15545920530954996, '_timestamp': 1762888082.6524408}).
Epoch: 7, Steps: 132 | Train Loss: 0.3316634 Vali Loss: 0.1820757 Test Loss: 0.1557134
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.3197131 Vali Loss: 0.1821149 Test Loss: 0.1557115
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.3197338 Vali Loss: 0.1821029 Test Loss: 0.1557154
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.3195076 Vali Loss: 0.1821077 Test Loss: 0.1557160
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.3203091 Vali Loss: 0.1821131 Test Loss: 0.1557167
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 191
test shape: (191, 50, 1) (191, 50, 1)
test shape: (191, 50, 1) (191, 50, 1)


	mse:0.0004907325492240489, mae:0.016955915838479996, rmse:0.022152483463287354, r2:-0.008720874786376953, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0170, RMSE: 0.0222, RÂ²: -0.0087, MAPE: 1.18%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.723 MB of 0.726 MB uploadedwandb: \ 0.723 MB of 0.726 MB uploadedwandb: | 0.723 MB of 0.726 MB uploadedwandb: / 0.726 MB of 0.726 MB uploadedwandb: - 0.726 MB of 0.726 MB uploadedwandb: \ 0.726 MB of 0.786 MB uploadedwandb: | 0.786 MB of 0.786 MB uploadedwandb: / 0.786 MB of 0.786 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–…â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–ƒâ–‚â–‚â–â–ˆâ–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 244012
wandb:     model/trainable_params 244012
wandb:                  test/loss 0.15572
wandb:                 train/loss 0.32031
wandb:   val/directional_accuracy 49.78096
wandb:                   val/loss 0.18211
wandb:                    val/mae 0.01696
wandb:                   val/mape 117.75619
wandb:                    val/mse 0.00049
wandb:                     val/r2 -0.00872
wandb:                   val/rmse 0.02215
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/jb9mldhu
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_210734-jb9mldhu/logs
Completed: ABSA H=50

Training: Mamba on ABSA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_211044-wx5kpfnr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/wx5kpfnr
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H100     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/wx5kpfnr
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 141
Epoch: 1, Steps: 130 | Train Loss: 0.3805037 Vali Loss: 0.1959199 Test Loss: 0.1642504
Validation loss decreased (inf --> 0.195920).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.3714949 Vali Loss: 0.1998988 Test Loss: 0.1645807
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.3600358 Vali Loss: 0.2000021 Test Loss: 0.1643900
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.3569176 Vali Loss: 0.1977444 Test Loss: 0.1643838
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.3558850 Vali Loss: 0.2000815 Test Loss: 0.1644525
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.3550756 Vali Loss: 0.2004802 Test Loss: 0.1644842
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.38050366869339575, 'val/loss': 0.19591990113258362, 'test/loss': 0.16425043791532518, '_timestamp': 1762888271.990186}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3714948571645297, 'val/loss': 0.1998988151550293, 'test/loss': 0.16458070278167725, '_timestamp': 1762888274.4598806}).
Epoch: 7, Steps: 130 | Train Loss: 0.3556631 Vali Loss: 0.1997997 Test Loss: 0.1644345
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.3549646 Vali Loss: 0.1998649 Test Loss: 0.1644430
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.3550508 Vali Loss: 0.2009414 Test Loss: 0.1644377
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.3559604 Vali Loss: 0.2010746 Test Loss: 0.1644354
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.3554393 Vali Loss: 0.2008345 Test Loss: 0.1644352
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 141
test shape: (141, 100, 1) (141, 100, 1)
test shape: (141, 100, 1) (141, 100, 1)


	mse:0.0005203444161452353, mae:0.017424466088414192, rmse:0.022811058908700943, r2:-0.008353352546691895, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0174, RMSE: 0.0228, RÂ²: -0.0084, MAPE: 1.18%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.803 MB of 0.807 MB uploadedwandb: \ 0.807 MB of 0.807 MB uploadedwandb: | 0.807 MB of 0.807 MB uploadedwandb: / 0.807 MB of 0.868 MB uploadedwandb: - 0.868 MB of 0.868 MB uploadedwandb: \ 0.868 MB of 0.868 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–â–†â–ˆâ–…â–…â–…â–…â–…
wandb:                 train/loss â–ˆâ–„â–‚â–â–‚â–â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–â–†â–‡â–…â–…â–ˆâ–ˆâ–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 282712
wandb:     model/trainable_params 282712
wandb:                  test/loss 0.16444
wandb:                 train/loss 0.35544
wandb:   val/directional_accuracy 49.38033
wandb:                   val/loss 0.20083
wandb:                    val/mae 0.01742
wandb:                   val/mape 117.57524
wandb:                    val/mse 0.00052
wandb:                     val/r2 -0.00835
wandb:                   val/rmse 0.02281
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/wx5kpfnr
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_211044-wx5kpfnr/logs
Completed: ABSA H=100

Training: Mamba on SASOL for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_211408-ki6pdnok
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/ki6pdnok
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H3      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/ki6pdnok
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 3
============================================================
train 3772
val 211
test 212
Epoch: 1, Steps: 118 | Train Loss: 0.2233043 Vali Loss: 0.1055269 Test Loss: 0.1480328
Validation loss decreased (inf --> 0.105527).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2007298 Vali Loss: 0.1004678 Test Loss: 0.1417563
Validation loss decreased (0.105527 --> 0.100468).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 118 | Train Loss: 0.1916919 Vali Loss: 0.1001845 Test Loss: 0.1400148
Validation loss decreased (0.100468 --> 0.100185).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.1899973 Vali Loss: 0.0965422 Test Loss: 0.1396095
Validation loss decreased (0.100185 --> 0.096542).  Saving model ...
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.22330427573899092, 'val/loss': 0.10552694967814855, 'test/loss': 0.14803275678839004, '_timestamp': 1762888473.9364634}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2007298124164848, 'val/loss': 0.10046776277678353, 'test/loss': 0.14175628977162497, '_timestamp': 1762888477.0374858}).
Epoch: 5, Steps: 118 | Train Loss: 0.1888548 Vali Loss: 0.0974609 Test Loss: 0.1394454
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.1881346 Vali Loss: 0.0989775 Test Loss: 0.1392988
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.1874574 Vali Loss: 0.1006960 Test Loss: 0.1392677
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.1885669 Vali Loss: 0.0980676 Test Loss: 0.1392490
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.1883736 Vali Loss: 0.0950631 Test Loss: 0.1392390
Validation loss decreased (0.096542 --> 0.095063).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.1875673 Vali Loss: 0.0995450 Test Loss: 0.1392285
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.1882579 Vali Loss: 0.0958037 Test Loss: 0.1392259
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.1877063 Vali Loss: 0.0960408 Test Loss: 0.1392246
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.1876950 Vali Loss: 0.0965637 Test Loss: 0.1392237
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.1876719 Vali Loss: 0.0974889 Test Loss: 0.1392232
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.1880037 Vali Loss: 0.0955357 Test Loss: 0.1392231
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.1882818 Vali Loss: 0.0968162 Test Loss: 0.1392230
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.1874911 Vali Loss: 0.0974245 Test Loss: 0.1392230
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.1871570 Vali Loss: 0.0958268 Test Loss: 0.1392230
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.1877978 Vali Loss: 0.0983504 Test Loss: 0.1392229
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 212
test shape: (212, 3, 1) (212, 3, 1)
test shape: (212, 3, 1) (212, 3, 1)


	mse:0.0022178359795361757, mae:0.03497450798749924, rmse:0.04709390550851822, r2:-0.0067327022552490234, dtw:Not calculated


VAL - MSE: 0.0022, MAE: 0.0350, RMSE: 0.0471, RÂ²: -0.0067, MAPE: 6205100.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.456 MB of 0.456 MB uploadedwandb: \ 0.456 MB of 0.456 MB uploadedwandb: | 0.456 MB of 0.456 MB uploadedwandb: / 0.456 MB of 0.456 MB uploadedwandb: - 0.456 MB of 0.456 MB uploadedwandb: \ 0.456 MB of 0.456 MB uploadedwandb: | 0.456 MB of 0.456 MB uploadedwandb: / 0.456 MB of 0.456 MB uploadedwandb: - 0.456 MB of 0.456 MB uploadedwandb: \ 0.505 MB of 0.567 MB uploaded (0.002 MB deduped)wandb: | 0.505 MB of 0.567 MB uploaded (0.002 MB deduped)wandb: / 0.567 MB of 0.567 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–„â–ƒâ–â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–‚â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–ƒâ–„â–†â–ˆâ–…â–â–‡â–‚â–‚â–ƒâ–„â–‚â–ƒâ–„â–‚â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 18
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 207634
wandb:     model/trainable_params 207634
wandb:                  test/loss 0.13922
wandb:                 train/loss 0.1878
wandb:   val/directional_accuracy 50.0
wandb:                   val/loss 0.09835
wandb:                    val/mae 0.03497
wandb:                   val/mape 620510050.0
wandb:                    val/mse 0.00222
wandb:                     val/r2 -0.00673
wandb:                   val/rmse 0.04709
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/ki6pdnok
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_211408-ki6pdnok/logs
Completed: SASOL H=3

Training: Mamba on SASOL for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_211739-nh3j8qjb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/nh3j8qjb
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H5      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/nh3j8qjb
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 5
============================================================
train 3770
val 209
test 210
Epoch: 1, Steps: 118 | Train Loss: 0.2265562 Vali Loss: 0.1068203 Test Loss: 0.1501072
Validation loss decreased (inf --> 0.106820).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2034770 Vali Loss: 0.1024600 Test Loss: 0.1433875
Validation loss decreased (0.106820 --> 0.102460).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 118 | Train Loss: 0.1955011 Vali Loss: 0.0992773 Test Loss: 0.1424121
Validation loss decreased (0.102460 --> 0.099277).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.1929571 Vali Loss: 0.0972698 Test Loss: 0.1418698
Validation loss decreased (0.099277 --> 0.097270).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.1924023 Vali Loss: 0.1024438 Test Loss: 0.1416807
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.1916780 Vali Loss: 0.0985499 Test Loss: 0.1416586
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.22655617609872655, 'val/loss': 0.1068203289593969, 'test/loss': 0.1501071516956602, '_timestamp': 1762888684.1366723}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.20347699946013548, 'val/loss': 0.10245997778006963, 'test/loss': 0.1433875315955707, '_timestamp': 1762888686.9011729}).
Epoch: 7, Steps: 118 | Train Loss: 0.1920740 Vali Loss: 0.1008303 Test Loss: 0.1416147
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.1916882 Vali Loss: 0.0980117 Test Loss: 0.1415944
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.1911533 Vali Loss: 0.1023456 Test Loss: 0.1415842
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.1916509 Vali Loss: 0.0987075 Test Loss: 0.1415791
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.1914031 Vali Loss: 0.0981564 Test Loss: 0.1415748
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.1917839 Vali Loss: 0.1015310 Test Loss: 0.1415743
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.1918698 Vali Loss: 0.0984436 Test Loss: 0.1415738
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.1908843 Vali Loss: 0.0970522 Test Loss: 0.1415735
Validation loss decreased (0.097270 --> 0.097052).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.1916752 Vali Loss: 0.0967009 Test Loss: 0.1415734
Validation loss decreased (0.097052 --> 0.096701).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.1915589 Vali Loss: 0.1003739 Test Loss: 0.1415733
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.1918930 Vali Loss: 0.0980935 Test Loss: 0.1415733
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.1914816 Vali Loss: 0.1007929 Test Loss: 0.1415733
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.1913164 Vali Loss: 0.0975779 Test Loss: 0.1415733
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.1914370 Vali Loss: 0.0976317 Test Loss: 0.1415733
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 118 | Train Loss: 0.1913402 Vali Loss: 0.1014660 Test Loss: 0.1415733
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 118 | Train Loss: 0.1908950 Vali Loss: 0.0992455 Test Loss: 0.1415733
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 118 | Train Loss: 0.1919152 Vali Loss: 0.1005081 Test Loss: 0.1415733
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 118 | Train Loss: 0.1914170 Vali Loss: 0.0977999 Test Loss: 0.1415733
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 118 | Train Loss: 0.1915505 Vali Loss: 0.0998761 Test Loss: 0.1415733
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 210
test shape: (210, 5, 1) (210, 5, 1)
test shape: (210, 5, 1) (210, 5, 1)


	mse:0.0022302193101495504, mae:0.03509265556931496, rmse:0.04722519963979721, r2:-0.0048749446868896484, dtw:Not calculated


VAL - MSE: 0.0022, MAE: 0.0351, RMSE: 0.0472, RÂ²: -0.0049, MAPE: 6346508.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.496 MB of 0.496 MB uploadedwandb: \ 0.496 MB of 0.496 MB uploadedwandb: | 0.496 MB of 0.496 MB uploadedwandb: / 0.496 MB of 0.559 MB uploadedwandb: - 0.559 MB of 0.559 MB uploadedwandb: \ 0.559 MB of 0.559 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–ƒâ–‚â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–ƒâ–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–‚â–ˆâ–ƒâ–†â–ƒâ–ˆâ–ƒâ–ƒâ–‡â–ƒâ–â–â–…â–ƒâ–†â–‚â–‚â–‡â–„â–†â–‚â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 24
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 209182
wandb:     model/trainable_params 209182
wandb:                  test/loss 0.14157
wandb:                 train/loss 0.19155
wandb:   val/directional_accuracy 49.7619
wandb:                   val/loss 0.09988
wandb:                    val/mae 0.03509
wandb:                   val/mape 634650800.0
wandb:                    val/mse 0.00223
wandb:                     val/r2 -0.00487
wandb:                   val/rmse 0.04723
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/nh3j8qjb
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_211739-nh3j8qjb/logs
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread ChkStopThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 268, in check_network_status
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 286, in check_stop_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 840, in deliver_stop_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 848, in deliver_network_status
    return self._deliver_network_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 510, in _deliver_network_status
    return self._deliver_stop_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 494, in _deliver_stop_status
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: SASOL H=5

Training: Mamba on SASOL for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_212054-2v2y7iy4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/2v2y7iy4
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H10     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/2v2y7iy4
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 10
============================================================
train 3765
val 204
test 205
Epoch: 1, Steps: 118 | Train Loss: 0.2342974 Vali Loss: 0.1081566 Test Loss: 0.1521993
Validation loss decreased (inf --> 0.108157).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2110610 Vali Loss: 0.1078675 Test Loss: 0.1457403
Validation loss decreased (0.108157 --> 0.107867).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 118 | Train Loss: 0.2027307 Vali Loss: 0.1029677 Test Loss: 0.1442989
Validation loss decreased (0.107867 --> 0.102968).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2007116 Vali Loss: 0.1023227 Test Loss: 0.1437450
Validation loss decreased (0.102968 --> 0.102323).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.1997616 Vali Loss: 0.1029817 Test Loss: 0.1435584
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23429736128803028, 'val/loss': 0.10815659058945519, 'test/loss': 0.1521993258169719, '_timestamp': 1762888878.8887737}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.21106099356281555, 'val/loss': 0.10786745165075574, 'test/loss': 0.14574032596179418, '_timestamp': 1762888881.4120388}).
Epoch: 6, Steps: 118 | Train Loss: 0.1989671 Vali Loss: 0.1040435 Test Loss: 0.1434783
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.1990013 Vali Loss: 0.1038902 Test Loss: 0.1434355
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.1995552 Vali Loss: 0.1076459 Test Loss: 0.1434155
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.1986315 Vali Loss: 0.1063865 Test Loss: 0.1434043
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.1992587 Vali Loss: 0.1040384 Test Loss: 0.1433991
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.1990035 Vali Loss: 0.1022892 Test Loss: 0.1433966
Validation loss decreased (0.102323 --> 0.102289).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.1988851 Vali Loss: 0.1035104 Test Loss: 0.1433952
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.1995275 Vali Loss: 0.1015376 Test Loss: 0.1433946
Validation loss decreased (0.102289 --> 0.101538).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.1990501 Vali Loss: 0.1081341 Test Loss: 0.1433943
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.1992944 Vali Loss: 0.1027351 Test Loss: 0.1433942
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.1991162 Vali Loss: 0.1078135 Test Loss: 0.1433941
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.1990958 Vali Loss: 0.1022597 Test Loss: 0.1433941
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.1989981 Vali Loss: 0.1025083 Test Loss: 0.1433941
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.1989320 Vali Loss: 0.1054638 Test Loss: 0.1433941
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.1988103 Vali Loss: 0.1059250 Test Loss: 0.1433941
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 118 | Train Loss: 0.1987566 Vali Loss: 0.1069458 Test Loss: 0.1433941
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 118 | Train Loss: 0.1989278 Vali Loss: 0.1055548 Test Loss: 0.1433941
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 118 | Train Loss: 0.1989256 Vali Loss: 0.1023340 Test Loss: 0.1433941
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 205
test shape: (205, 10, 1) (205, 10, 1)
test shape: (205, 10, 1) (205, 10, 1)


	mse:0.0022327254991978407, mae:0.035004355013370514, rmse:0.04725172370672226, r2:-0.005867719650268555, dtw:Not calculated


VAL - MSE: 0.0022, MAE: 0.0350, RMSE: 0.0473, RÂ²: -0.0059, MAPE: 5254366.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.536 MB of 0.536 MB uploadedwandb: \ 0.536 MB of 0.536 MB uploadedwandb: | 0.536 MB of 0.536 MB uploadedwandb: / 0.536 MB of 0.536 MB uploadedwandb: - 0.536 MB of 0.536 MB uploadedwandb: \ 0.536 MB of 0.598 MB uploadedwandb: | 0.598 MB of 0.598 MB uploadedwandb: / 0.598 MB of 0.598 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–ƒâ–â–‚â–‚â–â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–‚â–ƒâ–„â–ƒâ–‡â–†â–„â–‚â–ƒâ–â–ˆâ–‚â–ˆâ–‚â–‚â–…â–†â–‡â–…â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 213052
wandb:     model/trainable_params 213052
wandb:                  test/loss 0.14339
wandb:                 train/loss 0.19893
wandb:   val/directional_accuracy 50.51491
wandb:                   val/loss 0.10233
wandb:                    val/mae 0.035
wandb:                   val/mape 525436600.0
wandb:                    val/mse 0.00223
wandb:                     val/r2 -0.00587
wandb:                   val/rmse 0.04725
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/2v2y7iy4
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_212054-2v2y7iy4/logs
Completed: SASOL H=10

Training: Mamba on SASOL for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_212434-elhelvxk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/elhelvxk
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H22     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/elhelvxk
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 22
============================================================
train 3753
val 192
test 193
Epoch: 1, Steps: 118 | Train Loss: 0.2494917 Vali Loss: 0.1126783 Test Loss: 0.1551232
Validation loss decreased (inf --> 0.112678).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2233602 Vali Loss: 0.1070769 Test Loss: 0.1494636
Validation loss decreased (0.112678 --> 0.107077).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 118 | Train Loss: 0.2151649 Vali Loss: 0.1064497 Test Loss: 0.1487508
Validation loss decreased (0.107077 --> 0.106450).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2132554 Vali Loss: 0.1063912 Test Loss: 0.1486573
Validation loss decreased (0.106450 --> 0.106391).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2126205 Vali Loss: 0.1062874 Test Loss: 0.1485972
Validation loss decreased (0.106391 --> 0.106287).  Saving model ...
Updating learning rate to 6.25e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2494916679748034, 'val/loss': 0.11267828072110812, 'test/loss': 0.15512323592390334, '_timestamp': 1762889098.492415}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.22336024222737652, 'val/loss': 0.10707688083251317, 'test/loss': 0.14946356735059194, '_timestamp': 1762889101.1930873}).
Epoch: 6, Steps: 118 | Train Loss: 0.2132534 Vali Loss: 0.1062571 Test Loss: 0.1485661
Validation loss decreased (0.106287 --> 0.106257).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2120493 Vali Loss: 0.1062187 Test Loss: 0.1485785
Validation loss decreased (0.106257 --> 0.106219).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2128343 Vali Loss: 0.1062064 Test Loss: 0.1485709
Validation loss decreased (0.106219 --> 0.106206).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2132072 Vali Loss: 0.1062119 Test Loss: 0.1485748
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2121859 Vali Loss: 0.1062078 Test Loss: 0.1485724
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2125682 Vali Loss: 0.1062067 Test Loss: 0.1485738
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2117970 Vali Loss: 0.1062061 Test Loss: 0.1485737
Validation loss decreased (0.106206 --> 0.106206).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2121868 Vali Loss: 0.1062058 Test Loss: 0.1485737
Validation loss decreased (0.106206 --> 0.106206).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2117922 Vali Loss: 0.1062057 Test Loss: 0.1485737
Validation loss decreased (0.106206 --> 0.106206).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.2118803 Vali Loss: 0.1062057 Test Loss: 0.1485737
Validation loss decreased (0.106206 --> 0.106206).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.2136289 Vali Loss: 0.1062056 Test Loss: 0.1485737
Validation loss decreased (0.106206 --> 0.106206).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.2117610 Vali Loss: 0.1062056 Test Loss: 0.1485737
Validation loss decreased (0.106206 --> 0.106206).  Saving model ...
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.2125182 Vali Loss: 0.1062056 Test Loss: 0.1485737
Validation loss decreased (0.106206 --> 0.106206).  Saving model ...
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.2118016 Vali Loss: 0.1062056 Test Loss: 0.1485737
Validation loss decreased (0.106206 --> 0.106206).  Saving model ...
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.2123007 Vali Loss: 0.1062056 Test Loss: 0.1485737
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 118 | Train Loss: 0.2117666 Vali Loss: 0.1062056 Test Loss: 0.1485737
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 118 | Train Loss: 0.2132529 Vali Loss: 0.1062056 Test Loss: 0.1485737
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 118 | Train Loss: 0.2126881 Vali Loss: 0.1062056 Test Loss: 0.1485737
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 118 | Train Loss: 0.2116657 Vali Loss: 0.1062056 Test Loss: 0.1485737
Validation loss decreased (0.106206 --> 0.106206).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 118 | Train Loss: 0.2130068 Vali Loss: 0.1062056 Test Loss: 0.1485737
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 118 | Train Loss: 0.2118872 Vali Loss: 0.1062056 Test Loss: 0.1485737
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 118 | Train Loss: 0.2121797 Vali Loss: 0.1062056 Test Loss: 0.1485737
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 118 | Train Loss: 0.2118471 Vali Loss: 0.1062056 Test Loss: 0.1485737
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 118 | Train Loss: 0.2122017 Vali Loss: 0.1062056 Test Loss: 0.1485737
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 118 | Train Loss: 0.2125367 Vali Loss: 0.1062056 Test Loss: 0.1485737
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 118 | Train Loss: 0.2133334 Vali Loss: 0.1062056 Test Loss: 0.1485737
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 118 | Train Loss: 0.2119424 Vali Loss: 0.1062056 Test Loss: 0.1485737
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 118 | Train Loss: 0.2128050 Vali Loss: 0.1062056 Test Loss: 0.1485737
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 118 | Train Loss: 0.2117165 Vali Loss: 0.1062056 Test Loss: 0.1485737
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 193
test shape: (193, 22, 1) (193, 22, 1)
test shape: (193, 22, 1) (193, 22, 1)


	mse:0.002265137154608965, mae:0.03507164865732193, rmse:0.047593455761671066, r2:-0.009361863136291504, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0351, RMSE: 0.0476, RÂ²: -0.0094, MAPE: 5589058.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.590 MB of 0.591 MB uploadedwandb: \ 0.590 MB of 0.591 MB uploadedwandb: | 0.591 MB of 0.591 MB uploadedwandb: / 0.591 MB of 0.591 MB uploadedwandb: - 0.591 MB of 0.655 MB uploadedwandb: \ 0.655 MB of 0.655 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–„â–‚â–ƒâ–„â–‚â–ƒâ–â–‚â–â–â–…â–â–ƒâ–â–‚â–â–„â–ƒâ–â–„â–â–‚â–â–‚â–ƒâ–„â–‚â–ƒâ–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 33
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 222340
wandb:     model/trainable_params 222340
wandb:                  test/loss 0.14857
wandb:                 train/loss 0.21172
wandb:   val/directional_accuracy 49.44486
wandb:                   val/loss 0.10621
wandb:                    val/mae 0.03507
wandb:                   val/mape 558905850.0
wandb:                    val/mse 0.00227
wandb:                     val/r2 -0.00936
wandb:                   val/rmse 0.04759
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/elhelvxk
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_212434-elhelvxk/logs
Completed: SASOL H=22

Training: Mamba on SASOL for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_212740-lkvzl87p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/lkvzl87p
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H50     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/lkvzl87p
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 50
============================================================
train 3725
val 164
test 165
Epoch: 1, Steps: 117 | Train Loss: 0.2808104 Vali Loss: 0.1166942 Test Loss: 0.1615227
Validation loss decreased (inf --> 0.116694).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 117 | Train Loss: 0.2517145 Vali Loss: 0.1083287 Test Loss: 0.1673732
Validation loss decreased (0.116694 --> 0.108329).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 117 | Train Loss: 0.2447244 Vali Loss: 0.1094584 Test Loss: 0.1644417
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 117 | Train Loss: 0.2424775 Vali Loss: 0.1072397 Test Loss: 0.1639124
Validation loss decreased (0.108329 --> 0.107240).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 117 | Train Loss: 0.2416798 Vali Loss: 0.1090287 Test Loss: 0.1638061
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 117 | Train Loss: 0.2410463 Vali Loss: 0.1049569 Test Loss: 0.1639465
Validation loss decreased (0.107240 --> 0.104957).  Saving model ...
Updating learning rate to 3.125e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.28081043382995147, 'val/loss': 0.11669423555334409, 'test/loss': 0.16152270014087358, '_timestamp': 1762889284.613138}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25171447195049024, 'val/loss': 0.1083286851644516, 'test/loss': 0.16737323378523192, '_timestamp': 1762889287.4049685}).
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
Epoch: 7, Steps: 117 | Train Loss: 0.2413927 Vali Loss: 0.1056435 Test Loss: 0.1639401
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 117 | Train Loss: 0.2409682 Vali Loss: 0.1082757 Test Loss: 0.1639398
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 117 | Train Loss: 0.2410339 Vali Loss: 0.1071200 Test Loss: 0.1639505
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 117 | Train Loss: 0.2412926 Vali Loss: 0.1057397 Test Loss: 0.1639602
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 117 | Train Loss: 0.2408355 Vali Loss: 0.1073397 Test Loss: 0.1639662
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 117 | Train Loss: 0.2411306 Vali Loss: 0.1045979 Test Loss: 0.1639669
Validation loss decreased (0.104957 --> 0.104598).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 117 | Train Loss: 0.2410030 Vali Loss: 0.1068035 Test Loss: 0.1639676
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 117 | Train Loss: 0.2416981 Vali Loss: 0.1071159 Test Loss: 0.1639688
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 117 | Train Loss: 0.2411029 Vali Loss: 0.1084173 Test Loss: 0.1639688
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 117 | Train Loss: 0.2409623 Vali Loss: 0.1029313 Test Loss: 0.1639689
Validation loss decreased (0.104598 --> 0.102931).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 117 | Train Loss: 0.2413266 Vali Loss: 0.1087413 Test Loss: 0.1639689
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 117 | Train Loss: 0.2410793 Vali Loss: 0.1082406 Test Loss: 0.1639689
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 117 | Train Loss: 0.2414545 Vali Loss: 0.1063722 Test Loss: 0.1639689
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 117 | Train Loss: 0.2411612 Vali Loss: 0.1020190 Test Loss: 0.1639689
Validation loss decreased (0.102931 --> 0.102019).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 117 | Train Loss: 0.2411606 Vali Loss: 0.1106696 Test Loss: 0.1639689
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 117 | Train Loss: 0.2416187 Vali Loss: 0.1049674 Test Loss: 0.1639689
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 117 | Train Loss: 0.2411390 Vali Loss: 0.1065348 Test Loss: 0.1639689
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 117 | Train Loss: 0.2412668 Vali Loss: 0.1106334 Test Loss: 0.1639689
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 117 | Train Loss: 0.2413292 Vali Loss: 0.1075928 Test Loss: 0.1639689
EarlyStopping counter: 5 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 117 | Train Loss: 0.2414339 Vali Loss: 0.1032471 Test Loss: 0.1639689
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 117 | Train Loss: 0.2412954 Vali Loss: 0.1019647 Test Loss: 0.1639689
Validation loss decreased (0.102019 --> 0.101965).  Saving model ...
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 117 | Train Loss: 0.2411166 Vali Loss: 0.1128022 Test Loss: 0.1639689
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 117 | Train Loss: 0.2412643 Vali Loss: 0.1084375 Test Loss: 0.1639689
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 117 | Train Loss: 0.2420532 Vali Loss: 0.1058469 Test Loss: 0.1639689
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 117 | Train Loss: 0.2411735 Vali Loss: 0.1101788 Test Loss: 0.1639689
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 117 | Train Loss: 0.2415998 Vali Loss: 0.1051070 Test Loss: 0.1639689
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 117 | Train Loss: 0.2415860 Vali Loss: 0.1084293 Test Loss: 0.1639689
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 117 | Train Loss: 0.2418698 Vali Loss: 0.1039168 Test Loss: 0.1639689
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.1641532182693482e-14
Epoch: 35, Steps: 117 | Train Loss: 0.2417431 Vali Loss: 0.1042823 Test Loss: 0.1639689
EarlyStopping counter: 8 out of 10
Updating learning rate to 5.820766091346741e-15
Epoch: 36, Steps: 117 | Train Loss: 0.2413759 Vali Loss: 0.1071061 Test Loss: 0.1639689
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.9103830456733705e-15
Epoch: 37, Steps: 117 | Train Loss: 0.2409765 Vali Loss: 0.1056609 Test Loss: 0.1639689
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 165
test shape: (165, 50, 1) (165, 50, 1)
test shape: (165, 50, 1) (165, 50, 1)


	mse:0.0020666762720793486, mae:0.033704739063978195, rmse:0.04546071216464043, r2:-0.008743762969970703, dtw:Not calculated


VAL - MSE: 0.0021, MAE: 0.0337, RMSE: 0.0455, RÂ²: -0.0087, MAPE: 5641024.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.615 MB of 0.617 MB uploadedwandb: \ 0.615 MB of 0.617 MB uploadedwandb: | 0.617 MB of 0.617 MB uploadedwandb: / 0.617 MB of 0.617 MB uploadedwandb: - 0.617 MB of 0.682 MB uploadedwandb: \ 0.682 MB of 0.682 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–â–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–„â–ƒâ–â–‚â–â–â–‚â–â–‚â–â–ƒâ–â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–„â–†â–ƒâ–ƒâ–…â–„â–ƒâ–„â–ƒâ–„â–„â–…â–‚â–…â–…â–„â–â–‡â–ƒâ–„â–‡â–…â–‚â–â–ˆâ–…â–„â–†â–ƒâ–…â–‚â–‚â–„â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 36
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 244012
wandb:     model/trainable_params 244012
wandb:                  test/loss 0.16397
wandb:                 train/loss 0.24098
wandb:   val/directional_accuracy 49.11565
wandb:                   val/loss 0.10566
wandb:                    val/mae 0.0337
wandb:                   val/mape 564102400.0
wandb:                    val/mse 0.00207
wandb:                     val/r2 -0.00874
wandb:                   val/rmse 0.04546
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/lkvzl87p
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_212740-lkvzl87p/logs
Completed: SASOL H=50

Training: Mamba on SASOL for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_213051-i66yvpqz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/i66yvpqz
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H100    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/i66yvpqz
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 100
============================================================
train 3675
val 114
test 115
Epoch: 1, Steps: 115 | Train Loss: 0.3458830 Vali Loss: 0.1341678 Test Loss: 0.1718463
Validation loss decreased (inf --> 0.134168).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 115 | Train Loss: 0.3139357 Vali Loss: 0.1239274 Test Loss: 0.1752697
Validation loss decreased (0.134168 --> 0.123927).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 115 | Train Loss: 0.3024375 Vali Loss: 0.1221429 Test Loss: 0.1755214
Validation loss decreased (0.123927 --> 0.122143).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 115 | Train Loss: 0.3006806 Vali Loss: 0.1216888 Test Loss: 0.1751594
Validation loss decreased (0.122143 --> 0.121689).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 115 | Train Loss: 0.3001403 Vali Loss: 0.1211270 Test Loss: 0.1750276
Validation loss decreased (0.121689 --> 0.121127).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 115 | Train Loss: 0.2994057 Vali Loss: 0.1214103 Test Loss: 0.1751103
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.34588299069715583, 'val/loss': 0.13416775315999985, 'test/loss': 0.17184625938534737, '_timestamp': 1762889476.239347}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.31393565742865853, 'val/loss': 0.1239274125546217, 'test/loss': 0.1752696894109249, '_timestamp': 1762889478.8638785}).
Epoch: 7, Steps: 115 | Train Loss: 0.2988558 Vali Loss: 0.1216659 Test Loss: 0.1751154
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 115 | Train Loss: 0.2989834 Vali Loss: 0.1225142 Test Loss: 0.1751307
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 115 | Train Loss: 0.2989961 Vali Loss: 0.1217485 Test Loss: 0.1751291
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 115 | Train Loss: 0.2986423 Vali Loss: 0.1225046 Test Loss: 0.1751340
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 115 | Train Loss: 0.2984279 Vali Loss: 0.1229827 Test Loss: 0.1751320
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 115 | Train Loss: 0.2988560 Vali Loss: 0.1221371 Test Loss: 0.1751330
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 115 | Train Loss: 0.2988841 Vali Loss: 0.1218950 Test Loss: 0.1751341
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 115 | Train Loss: 0.2988669 Vali Loss: 0.1226091 Test Loss: 0.1751342
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 115 | Train Loss: 0.2991775 Vali Loss: 0.1219056 Test Loss: 0.1751342
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 115
test shape: (115, 100, 1) (115, 100, 1)
test shape: (115, 100, 1) (115, 100, 1)


	mse:0.001995205180719495, mae:0.033143848180770874, rmse:0.044667720794677734, r2:-0.0054351091384887695, dtw:Not calculated


VAL - MSE: 0.0020, MAE: 0.0331, RMSE: 0.0447, RÂ²: -0.0054, MAPE: 5986487.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.650 MB of 0.655 MB uploadedwandb: \ 0.655 MB of 0.655 MB uploadedwandb: | 0.655 MB of 0.655 MB uploadedwandb: / 0.655 MB of 0.655 MB uploadedwandb: - 0.655 MB of 0.716 MB uploadedwandb: \ 0.716 MB of 0.716 MB uploadedwandb: | 0.716 MB of 0.716 MB uploadedwandb: / 0.716 MB of 0.716 MB uploadedwandb: - 0.716 MB of 0.716 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–â–â–‚â–‚â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–ƒâ–â–‚â–ƒâ–†â–ƒâ–†â–ˆâ–…â–„â–‡â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 282712
wandb:     model/trainable_params 282712
wandb:                  test/loss 0.17513
wandb:                 train/loss 0.29918
wandb:   val/directional_accuracy 49.35441
wandb:                   val/loss 0.12191
wandb:                    val/mae 0.03314
wandb:                   val/mape 598648700.0
wandb:                    val/mse 0.002
wandb:                     val/r2 -0.00544
wandb:                   val/rmse 0.04467
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/i66yvpqz
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_213051-i66yvpqz/logs
Completed: SASOL H=100

Training: Mamba on DRD_GOLD for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_213329-j3yh5nj2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/j3yh5nj2
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_DRD_GOLD_H3   Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/j3yh5nj2
>>>>>>>start training : long_term_forecast_Mamba_DRD_GOLD_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.2577664 Vali Loss: 0.1319398 Test Loss: 0.1319078
Validation loss decreased (inf --> 0.131940).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2197959 Vali Loss: 0.1238826 Test Loss: 0.1262225
Validation loss decreased (0.131940 --> 0.123883).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2131132 Vali Loss: 0.1229882 Test Loss: 0.1251204
Validation loss decreased (0.123883 --> 0.122988).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2108782 Vali Loss: 0.1210086 Test Loss: 0.1246615
Validation loss decreased (0.122988 --> 0.121009).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2106498 Vali Loss: 0.1289662 Test Loss: 0.1246062
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2577663970396931, 'val/loss': 0.1319397622719407, 'test/loss': 0.1319077955558896, '_timestamp': 1762889633.8009357}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.21979593446380213, 'val/loss': 0.12388259451836348, 'test/loss': 0.12622251082211733, '_timestamp': 1762889636.5562472}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.21979593446380213, 'val/loss': 0.12388259451836348, 'test/loss': 0.12622251082211733, '_timestamp': 1762889636.5562472}).
Epoch: 6, Steps: 133 | Train Loss: 0.2097328 Vali Loss: 0.1228305 Test Loss: 0.1244994
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2095281 Vali Loss: 0.1230358 Test Loss: 0.1244641
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2088057 Vali Loss: 0.1256935 Test Loss: 0.1244390
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2087228 Vali Loss: 0.1234123 Test Loss: 0.1244324
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2093911 Vali Loss: 0.1208372 Test Loss: 0.1244273
Validation loss decreased (0.121009 --> 0.120837).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2090415 Vali Loss: 0.1259041 Test Loss: 0.1244263
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2097482 Vali Loss: 0.1216859 Test Loss: 0.1244245
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2087832 Vali Loss: 0.1255766 Test Loss: 0.1244242
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2090830 Vali Loss: 0.1235620 Test Loss: 0.1244242
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2092184 Vali Loss: 0.1219125 Test Loss: 0.1244241
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2091143 Vali Loss: 0.1218370 Test Loss: 0.1244240
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2087015 Vali Loss: 0.1235647 Test Loss: 0.1244240
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2087038 Vali Loss: 0.1206054 Test Loss: 0.1244240
Validation loss decreased (0.120837 --> 0.120605).  Saving model ...
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2090760 Vali Loss: 0.1262232 Test Loss: 0.1244240
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2091529 Vali Loss: 0.1209560 Test Loss: 0.1244240
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2091493 Vali Loss: 0.1202260 Test Loss: 0.1244240
Validation loss decreased (0.120605 --> 0.120226).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2091258 Vali Loss: 0.1204893 Test Loss: 0.1244240
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2091003 Vali Loss: 0.1255079 Test Loss: 0.1244240
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2092399 Vali Loss: 0.1313752 Test Loss: 0.1244240
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2091435 Vali Loss: 0.1261276 Test Loss: 0.1244240
EarlyStopping counter: 4 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2082960 Vali Loss: 0.1263054 Test Loss: 0.1244240
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.2091900 Vali Loss: 0.1240329 Test Loss: 0.1244240
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.2091753 Vali Loss: 0.1250792 Test Loss: 0.1244240
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.2095571 Vali Loss: 0.1240615 Test Loss: 0.1244240
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 133 | Train Loss: 0.2094137 Vali Loss: 0.1197717 Test Loss: 0.1244240
Validation loss decreased (0.120226 --> 0.119772).  Saving model ...
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 133 | Train Loss: 0.2084720 Vali Loss: 0.1234611 Test Loss: 0.1244240
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 133 | Train Loss: 0.2091691 Vali Loss: 0.1285691 Test Loss: 0.1244240
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 133 | Train Loss: 0.2091821 Vali Loss: 0.1283394 Test Loss: 0.1244240
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 133 | Train Loss: 0.2094858 Vali Loss: 0.1223461 Test Loss: 0.1244240
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.1641532182693482e-14
Epoch: 35, Steps: 133 | Train Loss: 0.2087680 Vali Loss: 0.1244790 Test Loss: 0.1244240
EarlyStopping counter: 5 out of 10
Updating learning rate to 5.820766091346741e-15
Epoch: 36, Steps: 133 | Train Loss: 0.2087815 Vali Loss: 0.1255480 Test Loss: 0.1244240
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.9103830456733705e-15
Epoch: 37, Steps: 133 | Train Loss: 0.2092501 Vali Loss: 0.1235681 Test Loss: 0.1244240
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.4551915228366853e-15
Epoch: 38, Steps: 133 | Train Loss: 0.2088513 Vali Loss: 0.1192877 Test Loss: 0.1244240
Validation loss decreased (0.119772 --> 0.119288).  Saving model ...
Updating learning rate to 7.275957614183426e-16
Epoch: 39, Steps: 133 | Train Loss: 0.2090473 Vali Loss: 0.1227316 Test Loss: 0.1244240
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.637978807091713e-16
Epoch: 40, Steps: 133 | Train Loss: 0.2095585 Vali Loss: 0.1240518 Test Loss: 0.1244240
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.8189894035458566e-16
Epoch: 41, Steps: 133 | Train Loss: 0.2084239 Vali Loss: 0.1232750 Test Loss: 0.1244240
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.094947017729283e-17
Epoch: 42, Steps: 133 | Train Loss: 0.2094863 Vali Loss: 0.1226149 Test Loss: 0.1244240
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.5474735088646414e-17
Epoch: 43, Steps: 133 | Train Loss: 0.2090347 Vali Loss: 0.1222442 Test Loss: 0.1244240
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.2737367544323207e-17
Epoch: 44, Steps: 133 | Train Loss: 0.2094369 Vali Loss: 0.1289481 Test Loss: 0.1244240
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.1368683772161604e-17
Epoch: 45, Steps: 133 | Train Loss: 0.2093881 Vali Loss: 0.1264758 Test Loss: 0.1244240
EarlyStopping counter: 7 out of 10
Updating learning rate to 5.684341886080802e-18
Epoch: 46, Steps: 133 | Train Loss: 0.2097199 Vali Loss: 0.1284783 Test Loss: 0.1244240
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.842170943040401e-18
Epoch: 47, Steps: 133 | Train Loss: 0.2088471 Vali Loss: 0.1228125 Test Loss: 0.1244240
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.4210854715202004e-18
Epoch: 48, Steps: 133 | Train Loss: 0.2091880 Vali Loss: 0.1209591 Test Loss: 0.1244240
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_DRD_GOLD_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.000937787932343781, mae:0.021682050079107285, rmse:0.03062332421541214, r2:-0.011229753494262695, dtw:Not calculated


VAL - MSE: 0.0009, MAE: 0.0217, RMSE: 0.0306, RÂ²: -0.0112, MAPE: 1137064.62%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.473 MB of 0.473 MB uploadedwandb: \ 0.473 MB of 0.473 MB uploadedwandb: | 0.473 MB of 0.473 MB uploadedwandb: / 0.473 MB of 0.473 MB uploadedwandb: - 0.473 MB of 0.473 MB uploadedwandb: \ 0.473 MB of 0.473 MB uploadedwandb: | 0.473 MB of 0.473 MB uploadedwandb: / 0.473 MB of 0.473 MB uploadedwandb: - 0.473 MB of 0.473 MB uploadedwandb: \ 0.473 MB of 0.473 MB uploadedwandb: | 0.522 MB of 0.588 MB uploaded (0.002 MB deduped)wandb: / 0.588 MB of 0.588 MB uploaded (0.002 MB deduped)wandb: - 0.588 MB of 0.588 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–ƒâ–ƒâ–â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–‚â–‡â–ƒâ–ƒâ–…â–ƒâ–…â–‚â–…â–ƒâ–ƒâ–‚â–ƒâ–…â–‚â–‚â–‚â–…â–ˆâ–…â–„â–„â–„â–â–ƒâ–†â–ƒâ–„â–…â–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–‡â–…â–†â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 47
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 207634
wandb:     model/trainable_params 207634
wandb:                  test/loss 0.12442
wandb:                 train/loss 0.20919
wandb:   val/directional_accuracy 44.93671
wandb:                   val/loss 0.12096
wandb:                    val/mae 0.02168
wandb:                   val/mape 113706462.5
wandb:                    val/mse 0.00094
wandb:                     val/r2 -0.01123
wandb:                   val/rmse 0.03062
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/j3yh5nj2
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_213329-j3yh5nj2/logs
Completed: DRD_GOLD H=3

Training: Mamba on DRD_GOLD for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_213831-qggtv4iv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/qggtv4iv
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_DRD_GOLD_H5   Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/qggtv4iv
>>>>>>>start training : long_term_forecast_Mamba_DRD_GOLD_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.2635272 Vali Loss: 0.1380018 Test Loss: 0.1343839
Validation loss decreased (inf --> 0.138002).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2255933 Vali Loss: 0.1297001 Test Loss: 0.1296298
Validation loss decreased (0.138002 --> 0.129700).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2184093 Vali Loss: 0.1269098 Test Loss: 0.1283277
Validation loss decreased (0.129700 --> 0.126910).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2169157 Vali Loss: 0.1267552 Test Loss: 0.1278477
Validation loss decreased (0.126910 --> 0.126755).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2161343 Vali Loss: 0.1294808 Test Loss: 0.1277099
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2635272334616883, 'val/loss': 0.1380017539486289, 'test/loss': 0.13438388518989086, '_timestamp': 1762889936.5093567}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.22559329307168946, 'val/loss': 0.1297001140192151, 'test/loss': 0.12962975446134806, '_timestamp': 1762889939.6215222}).
Epoch: 6, Steps: 133 | Train Loss: 0.2159233 Vali Loss: 0.1251220 Test Loss: 0.1276295
Validation loss decreased (0.126755 --> 0.125122).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2158967 Vali Loss: 0.1332898 Test Loss: 0.1276272
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2157319 Vali Loss: 0.1275444 Test Loss: 0.1276204
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2163220 Vali Loss: 0.1259805 Test Loss: 0.1276135
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2153629 Vali Loss: 0.1332178 Test Loss: 0.1276121
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2151298 Vali Loss: 0.1303959 Test Loss: 0.1276111
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2157213 Vali Loss: 0.1280248 Test Loss: 0.1276112
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2153787 Vali Loss: 0.1318762 Test Loss: 0.1276107
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2152245 Vali Loss: 0.1259324 Test Loss: 0.1276109
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2152376 Vali Loss: 0.1299262 Test Loss: 0.1276108
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2153493 Vali Loss: 0.1310358 Test Loss: 0.1276108
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_DRD_GOLD_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.000940491387154907, mae:0.02174087055027485, rmse:0.03066743165254593, r2:-0.007172226905822754, dtw:Not calculated


VAL - MSE: 0.0009, MAE: 0.0217, RMSE: 0.0307, RÂ²: -0.0072, MAPE: 983534.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.504 MB of 0.505 MB uploadedwandb: \ 0.504 MB of 0.505 MB uploadedwandb: | 0.505 MB of 0.505 MB uploadedwandb: / 0.505 MB of 0.505 MB uploadedwandb: - 0.505 MB of 0.566 MB uploadedwandb: \ 0.566 MB of 0.566 MB uploadedwandb: | 0.566 MB of 0.566 MB uploadedwandb: / 0.566 MB of 0.566 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–ƒâ–‚â–„â–â–â–‚â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–‚â–…â–â–ˆâ–ƒâ–‚â–ˆâ–†â–ƒâ–‡â–‚â–…â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 209182
wandb:     model/trainable_params 209182
wandb:                  test/loss 0.12761
wandb:                 train/loss 0.21535
wandb:   val/directional_accuracy 51.80851
wandb:                   val/loss 0.13104
wandb:                    val/mae 0.02174
wandb:                   val/mape 98353425.0
wandb:                    val/mse 0.00094
wandb:                     val/r2 -0.00717
wandb:                   val/rmse 0.03067
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/qggtv4iv
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_213831-qggtv4iv/logs
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread IntMsgThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 268, in check_network_status
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 300, in check_internal_messages
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 856, in deliver_internal_messages
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 848, in deliver_network_status
    return self._deliver_internal_messages(internal_message)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 516, in _deliver_internal_messages
    return self._deliver_network_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 510, in _deliver_network_status
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: DRD_GOLD H=5

Training: Mamba on DRD_GOLD for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_214104-dyxpda1o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/dyxpda1o
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_DRD_GOLD_H10  Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/dyxpda1o
>>>>>>>start training : long_term_forecast_Mamba_DRD_GOLD_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2739902 Vali Loss: 0.1466862 Test Loss: 0.1416196
Validation loss decreased (inf --> 0.146686).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2387635 Vali Loss: 0.1357932 Test Loss: 0.1390091
Validation loss decreased (0.146686 --> 0.135793).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2306257 Vali Loss: 0.1387450 Test Loss: 0.1365703
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2288375 Vali Loss: 0.1356223 Test Loss: 0.1361006
Validation loss decreased (0.135793 --> 0.135622).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2279111 Vali Loss: 0.1506876 Test Loss: 0.1359715
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2273621 Vali Loss: 0.1320123 Test Loss: 0.1358388
Validation loss decreased (0.135622 --> 0.132012).  Saving model ...
Updating learning rate to 3.125e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2739901615488798, 'val/loss': 0.14668617118149996, 'test/loss': 0.141619554720819, '_timestamp': 1762890087.8774402}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2387634625560359, 'val/loss': 0.1357931811362505, 'test/loss': 0.13900912925601006, '_timestamp': 1762890089.9450157}).
Epoch: 7, Steps: 133 | Train Loss: 0.2282594 Vali Loss: 0.1348986 Test Loss: 0.1358328
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2279128 Vali Loss: 0.1378496 Test Loss: 0.1357853
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2270534 Vali Loss: 0.1384147 Test Loss: 0.1357809
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2271788 Vali Loss: 0.1363353 Test Loss: 0.1357761
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2270295 Vali Loss: 0.1351427 Test Loss: 0.1357767
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2271459 Vali Loss: 0.1424554 Test Loss: 0.1357757
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2273197 Vali Loss: 0.1455281 Test Loss: 0.1357745
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2270817 Vali Loss: 0.1353528 Test Loss: 0.1357745
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2269580 Vali Loss: 0.1388136 Test Loss: 0.1357747
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2273736 Vali Loss: 0.1340502 Test Loss: 0.1357746
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_DRD_GOLD_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.0009550619288347661, mae:0.02203974314033985, rmse:0.030904076993465424, r2:-0.007170200347900391, dtw:Not calculated


VAL - MSE: 0.0010, MAE: 0.0220, RMSE: 0.0309, RÂ²: -0.0072, MAPE: 1003163.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.529 MB of 0.530 MB uploadedwandb: \ 0.529 MB of 0.530 MB uploadedwandb: | 0.529 MB of 0.530 MB uploadedwandb: / 0.530 MB of 0.530 MB uploadedwandb: - 0.530 MB of 0.530 MB uploadedwandb: \ 0.530 MB of 0.591 MB uploadedwandb: | 0.591 MB of 0.591 MB uploadedwandb: / 0.591 MB of 0.591 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–ƒâ–ƒâ–â–â–â–â–‚â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–‚â–ˆâ–â–‚â–ƒâ–ƒâ–ƒâ–‚â–…â–†â–‚â–„â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 213052
wandb:     model/trainable_params 213052
wandb:                  test/loss 0.13577
wandb:                 train/loss 0.22737
wandb:   val/directional_accuracy 50.82126
wandb:                   val/loss 0.13405
wandb:                    val/mae 0.02204
wandb:                   val/mape 100316350.0
wandb:                    val/mse 0.00096
wandb:                     val/r2 -0.00717
wandb:                   val/rmse 0.0309
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/dyxpda1o
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_214104-dyxpda1o/logs
Completed: DRD_GOLD H=10

Training: Mamba on DRD_GOLD for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_214343-fc58sr3c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/fc58sr3c
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_DRD_GOLD_H22  Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/fc58sr3c
>>>>>>>start training : long_term_forecast_Mamba_DRD_GOLD_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2972357 Vali Loss: 0.1714598 Test Loss: 0.1491979
Validation loss decreased (inf --> 0.171460).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2609931 Vali Loss: 0.1605318 Test Loss: 0.1509822
Validation loss decreased (0.171460 --> 0.160532).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2544147 Vali Loss: 0.1595119 Test Loss: 0.1495313
Validation loss decreased (0.160532 --> 0.159512).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2523341 Vali Loss: 0.1592856 Test Loss: 0.1489757
Validation loss decreased (0.159512 --> 0.159286).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2519444 Vali Loss: 0.1596263 Test Loss: 0.1490022
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2972356794458447, 'val/loss': 0.17145978552954538, 'test/loss': 0.14919785729476384, '_timestamp': 1762890247.5943542}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2609931341174877, 'val/loss': 0.1605317954506193, 'test/loss': 0.1509821851338659, '_timestamp': 1762890250.4132373}).
Epoch: 6, Steps: 132 | Train Loss: 0.2514002 Vali Loss: 0.1591667 Test Loss: 0.1488688
Validation loss decreased (0.159286 --> 0.159167).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2514438 Vali Loss: 0.1593601 Test Loss: 0.1488591
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2510934 Vali Loss: 0.1588030 Test Loss: 0.1487921
Validation loss decreased (0.159167 --> 0.158803).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2510896 Vali Loss: 0.1591358 Test Loss: 0.1487947
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2511991 Vali Loss: 0.1594928 Test Loss: 0.1487945
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2512307 Vali Loss: 0.1594841 Test Loss: 0.1487984
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2512826 Vali Loss: 0.1599701 Test Loss: 0.1487996
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2510096 Vali Loss: 0.1592278 Test Loss: 0.1487996
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2511031 Vali Loss: 0.1592568 Test Loss: 0.1488005
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2510467 Vali Loss: 0.1594658 Test Loss: 0.1488004
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2510572 Vali Loss: 0.1591078 Test Loss: 0.1488005
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2513893 Vali Loss: 0.1588934 Test Loss: 0.1488005
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2510812 Vali Loss: 0.1597287 Test Loss: 0.1488005
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_DRD_GOLD_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.000959757948294282, mae:0.022039227187633514, rmse:0.030979961156845093, r2:-0.007852673530578613, dtw:Not calculated


VAL - MSE: 0.0010, MAE: 0.0220, RMSE: 0.0310, RÂ²: -0.0079, MAPE: 1184837.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.584 MB of 0.585 MB uploadedwandb: \ 0.584 MB of 0.585 MB uploadedwandb: | 0.585 MB of 0.585 MB uploadedwandb: / 0.585 MB of 0.585 MB uploadedwandb: - 0.585 MB of 0.647 MB uploadedwandb: \ 0.585 MB of 0.647 MB uploadedwandb: | 0.647 MB of 0.647 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–‚â–â–â–â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–„â–†â–ƒâ–„â–â–ƒâ–…â–…â–ˆâ–„â–„â–…â–ƒâ–‚â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 17
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 222340
wandb:     model/trainable_params 222340
wandb:                  test/loss 0.1488
wandb:                 train/loss 0.25108
wandb:   val/directional_accuracy 49.75972
wandb:                   val/loss 0.15973
wandb:                    val/mae 0.02204
wandb:                   val/mape 118483775.0
wandb:                    val/mse 0.00096
wandb:                     val/r2 -0.00785
wandb:                   val/rmse 0.03098
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/fc58sr3c
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_214343-fc58sr3c/logs
Completed: DRD_GOLD H=22

Training: Mamba on DRD_GOLD for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_214702-7gg0ypi7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/7gg0ypi7
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_DRD_GOLD_H50  Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/7gg0ypi7
>>>>>>>start training : long_term_forecast_Mamba_DRD_GOLD_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.3440821 Vali Loss: 0.2058152 Test Loss: 0.1532309
Validation loss decreased (inf --> 0.205815).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.3061840 Vali Loss: 0.1969448 Test Loss: 0.1558080
Validation loss decreased (0.205815 --> 0.196945).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.3015274 Vali Loss: 0.1959352 Test Loss: 0.1561946
Validation loss decreased (0.196945 --> 0.195935).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.3002663 Vali Loss: 0.1960387 Test Loss: 0.1570501
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2988305 Vali Loss: 0.1960269 Test Loss: 0.1564666
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3440821193384402, 'val/loss': 0.2058151513338089, 'test/loss': 0.15323093781868616, '_timestamp': 1762890444.1846673}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3061839614176389, 'val/loss': 0.19694480548302332, 'test/loss': 0.15580797443787256, '_timestamp': 1762890447.1928937}).
Epoch: 6, Steps: 132 | Train Loss: 0.3002362 Vali Loss: 0.1960945 Test Loss: 0.1567829
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.3020526 Vali Loss: 0.1959018 Test Loss: 0.1567686
Validation loss decreased (0.195935 --> 0.195902).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2980610 Vali Loss: 0.1957272 Test Loss: 0.1567489
Validation loss decreased (0.195902 --> 0.195727).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2979281 Vali Loss: 0.1959955 Test Loss: 0.1567676
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2993820 Vali Loss: 0.1959941 Test Loss: 0.1567614
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2976239 Vali Loss: 0.1961338 Test Loss: 0.1567673
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2983715 Vali Loss: 0.1960948 Test Loss: 0.1567726
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2979241 Vali Loss: 0.1960363 Test Loss: 0.1567716
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.3039987 Vali Loss: 0.1963394 Test Loss: 0.1567720
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2980767 Vali Loss: 0.1957989 Test Loss: 0.1567706
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2980720 Vali Loss: 0.1958077 Test Loss: 0.1567706
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2969850 Vali Loss: 0.1960605 Test Loss: 0.1567707
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2973775 Vali Loss: 0.1960017 Test Loss: 0.1567707
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_DRD_GOLD_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.0009395423694513738, mae:0.021672595292329788, rmse:0.03065195493400097, r2:-0.008977293968200684, dtw:Not calculated


VAL - MSE: 0.0009, MAE: 0.0217, RMSE: 0.0307, RÂ²: -0.0090, MAPE: 1107406.12%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.683 MB of 0.685 MB uploadedwandb: \ 0.685 MB of 0.685 MB uploadedwandb: | 0.685 MB of 0.685 MB uploadedwandb: / 0.685 MB of 0.758 MB uploadedwandb: - 0.758 MB of 0.758 MB uploadedwandb: \ 0.758 MB of 0.758 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–ƒâ–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†
wandb:                 train/loss â–†â–„â–ƒâ–„â–†â–‚â–‚â–ƒâ–‚â–‚â–‚â–ˆâ–‚â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–…â–„â–…â–ƒâ–â–„â–„â–†â–…â–…â–ˆâ–‚â–‚â–…â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 17
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 244012
wandb:     model/trainable_params 244012
wandb:                  test/loss 0.15677
wandb:                 train/loss 0.29738
wandb:   val/directional_accuracy 51.24597
wandb:                   val/loss 0.196
wandb:                    val/mae 0.02167
wandb:                   val/mape 110740612.5
wandb:                    val/mse 0.00094
wandb:                     val/r2 -0.00898
wandb:                   val/rmse 0.03065
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/7gg0ypi7
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_214702-7gg0ypi7/logs
Completed: DRD_GOLD H=50

Training: Mamba on DRD_GOLD for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_215102-cxiybbgj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/cxiybbgj
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_DRD_GOLD_H100 Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/cxiybbgj
>>>>>>>start training : long_term_forecast_Mamba_DRD_GOLD_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.4287239 Vali Loss: 0.2655134 Test Loss: 0.1603657
Validation loss decreased (inf --> 0.265513).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.3875821 Vali Loss: 0.2582370 Test Loss: 0.1590045
Validation loss decreased (0.265513 --> 0.258237).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.3807841 Vali Loss: 0.2705622 Test Loss: 0.1590510
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.3781321 Vali Loss: 0.2639610 Test Loss: 0.1578508
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.3780835 Vali Loss: 0.2646005 Test Loss: 0.1582771
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.3762639 Vali Loss: 0.2707385 Test Loss: 0.1582785
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.4287238788146239, 'val/loss': 0.2655133992433548, 'test/loss': 0.16036573350429534, '_timestamp': 1762890685.51822}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3875820828171877, 'val/loss': 0.258237025141716, 'test/loss': 0.15900454819202423, '_timestamp': 1762890688.4306111}).
Epoch: 7, Steps: 130 | Train Loss: 0.3767075 Vali Loss: 0.2653563 Test Loss: 0.1582545
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.3762078 Vali Loss: 0.2662885 Test Loss: 0.1582428
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.3769165 Vali Loss: 0.2673979 Test Loss: 0.1582145
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.3767110 Vali Loss: 0.2701672 Test Loss: 0.1582064
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.3768828 Vali Loss: 0.2642862 Test Loss: 0.1582018
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.3764406 Vali Loss: 0.2613291 Test Loss: 0.1582017
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_DRD_GOLD_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0009105276549234986, mae:0.021176446229219437, rmse:0.030174950137734413, r2:-0.007726907730102539, dtw:Not calculated


VAL - MSE: 0.0009, MAE: 0.0212, RMSE: 0.0302, RÂ²: -0.0077, MAPE: 932081.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.772 MB of 0.776 MB uploadedwandb: \ 0.772 MB of 0.776 MB uploadedwandb: | 0.776 MB of 0.776 MB uploadedwandb: / 0.776 MB of 0.776 MB uploadedwandb: - 0.776 MB of 0.837 MB uploadedwandb: \ 0.837 MB of 0.837 MB uploadedwandb: | 0.837 MB of 0.837 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–„â–„â–â–‚â–â–‚â–‚â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–ƒâ–ˆâ–„â–…â–†â–ˆâ–ƒâ–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 282712
wandb:     model/trainable_params 282712
wandb:                  test/loss 0.1582
wandb:                 train/loss 0.37644
wandb:   val/directional_accuracy 50.33911
wandb:                   val/loss 0.26133
wandb:                    val/mae 0.02118
wandb:                   val/mape 93208150.0
wandb:                    val/mse 0.00091
wandb:                     val/r2 -0.00773
wandb:                   val/rmse 0.03017
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/cxiybbgj
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_215102-cxiybbgj/logs
Completed: DRD_GOLD H=100

Training: Mamba on ANGLO_AMERICAN for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_215355-eqcjjas3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/eqcjjas3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ANGLO_AMERICAN_H3Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/eqcjjas3
>>>>>>>start training : long_term_forecast_Mamba_ANGLO_AMERICAN_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 3
============================================================
train 778
val 44
test 46
Epoch: 1, Steps: 25 | Train Loss: 0.3385584 Vali Loss: 0.6330730 Test Loss: 0.8721127
Validation loss decreased (inf --> 0.633073).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 25 | Train Loss: 0.3135240 Vali Loss: 0.5564463 Test Loss: 0.8223571
Validation loss decreased (0.633073 --> 0.556446).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 25 | Train Loss: 0.2960955 Vali Loss: 0.5313601 Test Loss: 0.7772618
Validation loss decreased (0.556446 --> 0.531360).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 25 | Train Loss: 0.2843366 Vali Loss: 0.4978733 Test Loss: 0.7534041
Validation loss decreased (0.531360 --> 0.497873).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 25 | Train Loss: 0.2799022 Vali Loss: 0.4682990 Test Loss: 0.7423721
Validation loss decreased (0.497873 --> 0.468299).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 25 | Train Loss: 0.2782836 Vali Loss: 0.4601221 Test Loss: 0.7377653
Validation loss decreased (0.468299 --> 0.460122).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 25 | Train Loss: 0.2775651 Vali Loss: 0.4633386 Test Loss: 0.7356433
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 25 | Train Loss: 0.2814548 Vali Loss: 0.4677520 Test Loss: 0.7344873
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 25 | Train Loss: 0.2741144 Vali Loss: 0.4856682 Test Loss: 0.7339173
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 25 | Train Loss: 0.2745502 Vali Loss: 0.4499770 Test Loss: 0.7336534
Validation loss decreased (0.460122 --> 0.449977).  Saving model ...
Updating learning rate to 1.953125e-07
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3385583585500717, 'val/loss': 0.6330729722976685, 'test/loss': 0.8721127212047577, '_timestamp': 1762890859.1005256}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3135239988565445, 'val/loss': 0.5564463138580322, 'test/loss': 0.822357103228569, '_timestamp': 1762890860.770246}).
Epoch: 11, Steps: 25 | Train Loss: 0.2752515 Vali Loss: 0.4445076 Test Loss: 0.7335258
Validation loss decreased (0.449977 --> 0.444508).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 25 | Train Loss: 0.2733003 Vali Loss: 0.4769822 Test Loss: 0.7334658
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 25 | Train Loss: 0.2766589 Vali Loss: 0.4706589 Test Loss: 0.7334324
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 25 | Train Loss: 0.2785086 Vali Loss: 0.4659491 Test Loss: 0.7334166
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 25 | Train Loss: 0.2761218 Vali Loss: 0.4637443 Test Loss: 0.7334091
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 25 | Train Loss: 0.2757313 Vali Loss: 0.4543036 Test Loss: 0.7334057
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 25 | Train Loss: 0.2749951 Vali Loss: 0.4884012 Test Loss: 0.7334044
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 25 | Train Loss: 0.2741375 Vali Loss: 0.4756642 Test Loss: 0.7334040
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 25 | Train Loss: 0.2758634 Vali Loss: 0.4679003 Test Loss: 0.7334040
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 25 | Train Loss: 0.2767391 Vali Loss: 0.4936949 Test Loss: 0.7334040
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 25 | Train Loss: 0.2801867 Vali Loss: 0.4584802 Test Loss: 0.7334040
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_ANGLO_AMERICAN_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 46
test shape: (46, 3, 1) (46, 3, 1)
test shape: (46, 3, 1) (46, 3, 1)


	mse:0.006947326473891735, mae:0.05768810585141182, rmse:0.08335062116384506, r2:-0.025452494621276855, dtw:Not calculated


VAL - MSE: 0.0069, MAE: 0.0577, RMSE: 0.0834, RÂ²: -0.0255, MAPE: 42932888.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.453 MB of 0.453 MB uploadedwandb: \ 0.453 MB of 0.453 MB uploadedwandb: | 0.453 MB of 0.453 MB uploadedwandb: / 0.453 MB of 0.453 MB uploadedwandb: - 0.453 MB of 0.453 MB uploadedwandb: \ 0.453 MB of 0.453 MB uploadedwandb: | 0.453 MB of 0.453 MB uploadedwandb: / 0.453 MB of 0.453 MB uploadedwandb: - 0.453 MB of 0.453 MB uploadedwandb: \ 0.502 MB of 0.563 MB uploaded (0.002 MB deduped)wandb: | 0.563 MB of 0.563 MB uploaded (0.002 MB deduped)wandb: / 0.563 MB of 0.563 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–ƒâ–‚â–„â–â–â–‚â–â–‚â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–ƒ
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–ƒâ–‚â–ƒâ–ƒâ–„â–â–â–„â–ƒâ–ƒâ–ƒâ–‚â–…â–„â–ƒâ–…â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 207634
wandb:     model/trainable_params 207634
wandb:                  test/loss 0.7334
wandb:                 train/loss 0.28019
wandb:   val/directional_accuracy 44.56522
wandb:                   val/loss 0.45848
wandb:                    val/mae 0.05769
wandb:                   val/mape 4293288800.0
wandb:                    val/mse 0.00695
wandb:                     val/r2 -0.02545
wandb:                   val/rmse 0.08335
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/eqcjjas3
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_215355-eqcjjas3/logs
Completed: ANGLO_AMERICAN H=3

Training: Mamba on ANGLO_AMERICAN for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_215658-q86nryer
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/q86nryer
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ANGLO_AMERICAN_H5Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/q86nryer
>>>>>>>start training : long_term_forecast_Mamba_ANGLO_AMERICAN_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 5
============================================================
train 776
val 42
test 44
Epoch: 1, Steps: 25 | Train Loss: 0.3413926 Vali Loss: 0.6746962 Test Loss: 0.8948146
Validation loss decreased (inf --> 0.674696).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 25 | Train Loss: 0.3221381 Vali Loss: 0.5569424 Test Loss: 0.8320456
Validation loss decreased (0.674696 --> 0.556942).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 25 | Train Loss: 0.3074403 Vali Loss: 0.5110201 Test Loss: 0.7898743
Validation loss decreased (0.556942 --> 0.511020).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 25 | Train Loss: 0.2923143 Vali Loss: 0.4844282 Test Loss: 0.7687943
Validation loss decreased (0.511020 --> 0.484428).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 25 | Train Loss: 0.2854083 Vali Loss: 0.4640790 Test Loss: 0.7609968
Validation loss decreased (0.484428 --> 0.464079).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 25 | Train Loss: 0.2882258 Vali Loss: 0.4702542 Test Loss: 0.7573783
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 25 | Train Loss: 0.2830881 Vali Loss: 0.4732758 Test Loss: 0.7554399
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 25 | Train Loss: 0.2855978 Vali Loss: 0.4714707 Test Loss: 0.7546010
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 25 | Train Loss: 0.2841644 Vali Loss: 0.4841722 Test Loss: 0.7542161
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 25 | Train Loss: 0.2789293 Vali Loss: 0.4846273 Test Loss: 0.7540138
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 25 | Train Loss: 0.2806175 Vali Loss: 0.4817451 Test Loss: 0.7539166
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 25 | Train Loss: 0.2829826 Vali Loss: 0.4818029 Test Loss: 0.7538681
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.34139259576797487, 'val/loss': 0.6746962070465088, 'test/loss': 0.8948146104812622, '_timestamp': 1762891044.288277}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.32213806092739106, 'val/loss': 0.5569424331188202, 'test/loss': 0.8320455551147461, '_timestamp': 1762891045.670847}).
Epoch: 13, Steps: 25 | Train Loss: 0.2796623 Vali Loss: 0.4480227 Test Loss: 0.7538431
Validation loss decreased (0.464079 --> 0.448023).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 25 | Train Loss: 0.2840669 Vali Loss: 0.4768965 Test Loss: 0.7538306
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 25 | Train Loss: 0.2834803 Vali Loss: 0.4413597 Test Loss: 0.7538252
Validation loss decreased (0.448023 --> 0.441360).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 25 | Train Loss: 0.2816368 Vali Loss: 0.4887669 Test Loss: 0.7538226
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 25 | Train Loss: 0.2825608 Vali Loss: 0.4528689 Test Loss: 0.7538217
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 25 | Train Loss: 0.2833264 Vali Loss: 0.4973508 Test Loss: 0.7538214
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 25 | Train Loss: 0.2832557 Vali Loss: 0.4589362 Test Loss: 0.7538214
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 25 | Train Loss: 0.2833323 Vali Loss: 0.4723969 Test Loss: 0.7538213
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 25 | Train Loss: 0.2832350 Vali Loss: 0.4736618 Test Loss: 0.7538213
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 25 | Train Loss: 0.2835245 Vali Loss: 0.4730288 Test Loss: 0.7538213
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 25 | Train Loss: 0.2942795 Vali Loss: 0.4634695 Test Loss: 0.7538213
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 25 | Train Loss: 0.2864045 Vali Loss: 0.4537459 Test Loss: 0.7538213
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 25 | Train Loss: 0.2813345 Vali Loss: 0.4915187 Test Loss: 0.7538213
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_ANGLO_AMERICAN_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 44
test shape: (44, 5, 1) (44, 5, 1)
test shape: (44, 5, 1) (44, 5, 1)


	mse:0.007178858388215303, mae:0.05889872461557388, rmse:0.08472814410924911, r2:-0.01800084114074707, dtw:Not calculated


VAL - MSE: 0.0072, MAE: 0.0589, RMSE: 0.0847, RÂ²: -0.0180, MAPE: 20775690.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.475 MB of 0.475 MB uploadedwandb: \ 0.475 MB of 0.475 MB uploadedwandb: | 0.475 MB of 0.475 MB uploadedwandb: / 0.475 MB of 0.475 MB uploadedwandb: - 0.475 MB of 0.475 MB uploadedwandb: \ 0.475 MB of 0.475 MB uploadedwandb: | 0.475 MB of 0.537 MB uploadedwandb: / 0.537 MB of 0.537 MB uploadedwandb: - 0.537 MB of 0.537 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–…â–ƒâ–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–ƒâ–„â–„â–„â–…â–…â–…â–…â–‚â–…â–â–†â–‚â–‡â–ƒâ–„â–„â–„â–ƒâ–‚â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 24
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 209182
wandb:     model/trainable_params 209182
wandb:                  test/loss 0.75382
wandb:                 train/loss 0.28133
wandb:   val/directional_accuracy 43.75
wandb:                   val/loss 0.49152
wandb:                    val/mae 0.0589
wandb:                   val/mape 2077569000.0
wandb:                    val/mse 0.00718
wandb:                     val/r2 -0.018
wandb:                   val/rmse 0.08473
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/q86nryer
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_215658-q86nryer/logs
Completed: ANGLO_AMERICAN H=5

Training: Mamba on ANGLO_AMERICAN for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_220101-825esmk6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/825esmk6
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ANGLO_AMERICAN_H10Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/825esmk6
>>>>>>>start training : long_term_forecast_Mamba_ANGLO_AMERICAN_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 10
============================================================
train 771
val 37
test 39
Epoch: 1, Steps: 25 | Train Loss: 0.3784522 Vali Loss: 0.6922914 Test Loss: 0.9330660
Validation loss decreased (inf --> 0.692291).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 25 | Train Loss: 0.3325632 Vali Loss: 0.6175533 Test Loss: 0.8834497
Validation loss decreased (0.692291 --> 0.617553).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 25 | Train Loss: 0.3391330 Vali Loss: 0.5974133 Test Loss: 0.8475625
Validation loss decreased (0.617553 --> 0.597413).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 25 | Train Loss: 0.3139964 Vali Loss: 0.5734878 Test Loss: 0.8300554
Validation loss decreased (0.597413 --> 0.573488).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 25 | Train Loss: 0.3134597 Vali Loss: 0.5649068 Test Loss: 0.8222448
Validation loss decreased (0.573488 --> 0.564907).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 25 | Train Loss: 0.3010530 Vali Loss: 0.5540695 Test Loss: 0.8188262
Validation loss decreased (0.564907 --> 0.554070).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 25 | Train Loss: 0.3048556 Vali Loss: 0.5431016 Test Loss: 0.8172657
Validation loss decreased (0.554070 --> 0.543102).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 25 | Train Loss: 0.3134244 Vali Loss: 0.5520887 Test Loss: 0.8164507
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 25 | Train Loss: 0.3039068 Vali Loss: 0.5353226 Test Loss: 0.8160659
Validation loss decreased (0.543102 --> 0.535323).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 25 | Train Loss: 0.3072083 Vali Loss: 0.5775439 Test Loss: 0.8158821
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 25 | Train Loss: 0.3017299 Vali Loss: 0.5439742 Test Loss: 0.8157873
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3784522044658661, 'val/loss': 0.6922914087772369, 'test/loss': 0.9330659657716751, '_timestamp': 1762891286.6497622}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3325632327795029, 'val/loss': 0.6175532639026642, 'test/loss': 0.8834496736526489, '_timestamp': 1762891287.5817266}).
Epoch: 12, Steps: 25 | Train Loss: 0.3024987 Vali Loss: 0.5451309 Test Loss: 0.8157387
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 25 | Train Loss: 0.3014145 Vali Loss: 0.5298766 Test Loss: 0.8157158
Validation loss decreased (0.535323 --> 0.529877).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 25 | Train Loss: 0.3007470 Vali Loss: 0.5246348 Test Loss: 0.8157042
Validation loss decreased (0.529877 --> 0.524635).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 25 | Train Loss: 0.2988392 Vali Loss: 0.5404261 Test Loss: 0.8156985
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 25 | Train Loss: 0.3056560 Vali Loss: 0.5679037 Test Loss: 0.8156957
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 25 | Train Loss: 0.3024354 Vali Loss: 0.5547768 Test Loss: 0.8156946
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 25 | Train Loss: 0.2993019 Vali Loss: 0.5606707 Test Loss: 0.8156944
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 25 | Train Loss: 0.3044040 Vali Loss: 0.5654101 Test Loss: 0.8156944
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 25 | Train Loss: 0.3040357 Vali Loss: 0.5690033 Test Loss: 0.8156944
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 25 | Train Loss: 0.3033501 Vali Loss: 0.5686160 Test Loss: 0.8156944
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 25 | Train Loss: 0.3103010 Vali Loss: 0.5335023 Test Loss: 0.8156944
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 25 | Train Loss: 0.3028080 Vali Loss: 0.5516854 Test Loss: 0.8156944
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 25 | Train Loss: 0.3040160 Vali Loss: 0.5178125 Test Loss: 0.8156944
Validation loss decreased (0.524635 --> 0.517812).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 25 | Train Loss: 0.3026147 Vali Loss: 0.5534363 Test Loss: 0.8156944
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 25 | Train Loss: 0.2986332 Vali Loss: 0.5370312 Test Loss: 0.8156944
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 25 | Train Loss: 0.3202354 Vali Loss: 0.5523416 Test Loss: 0.8156944
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 25 | Train Loss: 0.3093593 Vali Loss: 0.5711780 Test Loss: 0.8156944
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 25 | Train Loss: 0.3099123 Vali Loss: 0.5690533 Test Loss: 0.8156944
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 25 | Train Loss: 0.3023245 Vali Loss: 0.5627860 Test Loss: 0.8156944
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 25 | Train Loss: 0.3095053 Vali Loss: 0.5584703 Test Loss: 0.8156944
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 25 | Train Loss: 0.3063867 Vali Loss: 0.5762127 Test Loss: 0.8156944
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 25 | Train Loss: 0.3184131 Vali Loss: 0.5434639 Test Loss: 0.8156944
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 25 | Train Loss: 0.3075173 Vali Loss: 0.5266820 Test Loss: 0.8156944
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_ANGLO_AMERICAN_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 39
test shape: (39, 10, 1) (39, 10, 1)
test shape: (39, 10, 1) (39, 10, 1)


	mse:0.007929620333015919, mae:0.062255777418613434, rmse:0.08904841542243958, r2:-0.023374319076538086, dtw:Not calculated


VAL - MSE: 0.0079, MAE: 0.0623, RMSE: 0.0890, RÂ²: -0.0234, MAPE: 17282492.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.519 MB of 0.519 MB uploadedwandb: \ 0.519 MB of 0.519 MB uploadedwandb: | 0.519 MB of 0.519 MB uploadedwandb: / 0.519 MB of 0.519 MB uploadedwandb: - 0.519 MB of 0.519 MB uploadedwandb: \ 0.519 MB of 0.519 MB uploadedwandb: | 0.519 MB of 0.583 MB uploadedwandb: / 0.583 MB of 0.583 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–„â–â–‚â–„â–‚â–‚â–‚â–‚â–â–â–â–‚â–‚â–â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–â–…â–ƒâ–ƒâ–‚â–ƒâ–‚â–„â–ƒ
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–…â–„â–ƒâ–„â–ƒâ–†â–ƒâ–ƒâ–‚â–‚â–ƒâ–…â–„â–…â–…â–†â–…â–‚â–„â–â–„â–ƒâ–„â–†â–†â–…â–…â–†â–ƒâ–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 33
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 213052
wandb:     model/trainable_params 213052
wandb:                  test/loss 0.81569
wandb:                 train/loss 0.30752
wandb:   val/directional_accuracy 47.00855
wandb:                   val/loss 0.52668
wandb:                    val/mae 0.06226
wandb:                   val/mape 1728249200.0
wandb:                    val/mse 0.00793
wandb:                     val/r2 -0.02337
wandb:                   val/rmse 0.08905
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/825esmk6
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_220101-825esmk6/logs
Completed: ANGLO_AMERICAN H=10

Training: Mamba on ANGLO_AMERICAN for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_220427-2zkqmk1r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/2zkqmk1r
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ANGLO_AMERICAN_H22Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/2zkqmk1r
>>>>>>>start training : long_term_forecast_Mamba_ANGLO_AMERICAN_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 22
============================================================
train 759
val 25
test 27
Epoch: 1, Steps: 24 | Train Loss: 0.3902935 Vali Loss: 0.8570480 Test Loss: 1.4280871
Validation loss decreased (inf --> 0.857048).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 24 | Train Loss: 0.3784230 Vali Loss: 0.7993087 Test Loss: 1.3561307
Validation loss decreased (0.857048 --> 0.799309).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 24 | Train Loss: 0.3630522 Vali Loss: 0.7328650 Test Loss: 1.2864170
Validation loss decreased (0.799309 --> 0.732865).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 24 | Train Loss: 0.3527278 Vali Loss: 0.6907116 Test Loss: 1.2485952
Validation loss decreased (0.732865 --> 0.690712).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 24 | Train Loss: 0.3470980 Vali Loss: 0.6719106 Test Loss: 1.2303739
Validation loss decreased (0.690712 --> 0.671911).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 24 | Train Loss: 0.3455147 Vali Loss: 0.6639783 Test Loss: 1.2212738
Validation loss decreased (0.671911 --> 0.663978).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 24 | Train Loss: 0.3436862 Vali Loss: 0.6598310 Test Loss: 1.2176027
Validation loss decreased (0.663978 --> 0.659831).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 24 | Train Loss: 0.3432809 Vali Loss: 0.6576480 Test Loss: 1.2157156
Validation loss decreased (0.659831 --> 0.657648).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 24 | Train Loss: 0.3426616 Vali Loss: 0.6566665 Test Loss: 1.2148290
Validation loss decreased (0.657648 --> 0.656667).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 24 | Train Loss: 0.3424356 Vali Loss: 0.6561641 Test Loss: 1.2144109
Validation loss decreased (0.656667 --> 0.656164).  Saving model ...
Updating learning rate to 1.953125e-07
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.39029354726274806, 'val/loss': 0.857047975063324, 'test/loss': 1.4280871152877808, '_timestamp': 1762891488.397426}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3784229854742686, 'val/loss': 0.7993086576461792, 'test/loss': 1.3561307191848755, '_timestamp': 1762891489.3590395}).
Epoch: 11, Steps: 24 | Train Loss: 0.3436305 Vali Loss: 0.6559224 Test Loss: 1.2141879
Validation loss decreased (0.656164 --> 0.655922).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 24 | Train Loss: 0.3439595 Vali Loss: 0.6558031 Test Loss: 1.2140779
Validation loss decreased (0.655922 --> 0.655803).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 24 | Train Loss: 0.3436103 Vali Loss: 0.6557472 Test Loss: 1.2140226
Validation loss decreased (0.655803 --> 0.655747).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 24 | Train Loss: 0.3424959 Vali Loss: 0.6557178 Test Loss: 1.2139962
Validation loss decreased (0.655747 --> 0.655718).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 24 | Train Loss: 0.3427046 Vali Loss: 0.6557035 Test Loss: 1.2139828
Validation loss decreased (0.655718 --> 0.655704).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 24 | Train Loss: 0.3435398 Vali Loss: 0.6556962 Test Loss: 1.2139767
Validation loss decreased (0.655704 --> 0.655696).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 24 | Train Loss: 0.3433452 Vali Loss: 0.6556934 Test Loss: 1.2139741
Validation loss decreased (0.655696 --> 0.655693).  Saving model ...
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 24 | Train Loss: 0.3440092 Vali Loss: 0.6556929 Test Loss: 1.2139735
Validation loss decreased (0.655693 --> 0.655693).  Saving model ...
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 24 | Train Loss: 0.3436974 Vali Loss: 0.6556926 Test Loss: 1.2139734
Validation loss decreased (0.655693 --> 0.655693).  Saving model ...
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 24 | Train Loss: 0.3444198 Vali Loss: 0.6556926 Test Loss: 1.2139733
Validation loss decreased (0.655693 --> 0.655693).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 24 | Train Loss: 0.3422936 Vali Loss: 0.6556926 Test Loss: 1.2139734
Validation loss decreased (0.655693 --> 0.655693).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 24 | Train Loss: 0.3431229 Vali Loss: 0.6556926 Test Loss: 1.2139734
Validation loss decreased (0.655693 --> 0.655693).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 24 | Train Loss: 0.3427632 Vali Loss: 0.6556926 Test Loss: 1.2139734
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 24 | Train Loss: 0.3434242 Vali Loss: 0.6556926 Test Loss: 1.2139733
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 24 | Train Loss: 0.3443746 Vali Loss: 0.6556926 Test Loss: 1.2139733
EarlyStopping counter: 3 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 24 | Train Loss: 0.3432097 Vali Loss: 0.6556926 Test Loss: 1.2139733
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 24 | Train Loss: 0.3423774 Vali Loss: 0.6556926 Test Loss: 1.2139733
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 24 | Train Loss: 0.3426027 Vali Loss: 0.6556926 Test Loss: 1.2139733
Validation loss decreased (0.655693 --> 0.655693).  Saving model ...
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 24 | Train Loss: 0.3432514 Vali Loss: 0.6556926 Test Loss: 1.2139733
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 24 | Train Loss: 0.3438916 Vali Loss: 0.6556926 Test Loss: 1.2139733
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 24 | Train Loss: 0.3428861 Vali Loss: 0.6556926 Test Loss: 1.2139733
Validation loss decreased (0.655693 --> 0.655693).  Saving model ...
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 24 | Train Loss: 0.3422439 Vali Loss: 0.6556926 Test Loss: 1.2139733
Validation loss decreased (0.655693 --> 0.655693).  Saving model ...
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 24 | Train Loss: 0.3437036 Vali Loss: 0.6556926 Test Loss: 1.2139733
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 24 | Train Loss: 0.3433527 Vali Loss: 0.6556926 Test Loss: 1.2139733
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.1641532182693482e-14
Epoch: 35, Steps: 24 | Train Loss: 0.3433448 Vali Loss: 0.6556926 Test Loss: 1.2139733
Validation loss decreased (0.655693 --> 0.655693).  Saving model ...
Updating learning rate to 5.820766091346741e-15
Epoch: 36, Steps: 24 | Train Loss: 0.3426026 Vali Loss: 0.6556926 Test Loss: 1.2139733
Validation loss decreased (0.655693 --> 0.655693).  Saving model ...
Updating learning rate to 2.9103830456733705e-15
Epoch: 37, Steps: 24 | Train Loss: 0.3430267 Vali Loss: 0.6556926 Test Loss: 1.2139733
Validation loss decreased (0.655693 --> 0.655693).  Saving model ...
Updating learning rate to 1.4551915228366853e-15
Epoch: 38, Steps: 24 | Train Loss: 0.3430235 Vali Loss: 0.6556926 Test Loss: 1.2139733
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.275957614183426e-16
Epoch: 39, Steps: 24 | Train Loss: 0.3433419 Vali Loss: 0.6556926 Test Loss: 1.2139733
Validation loss decreased (0.655693 --> 0.655693).  Saving model ...
Updating learning rate to 3.637978807091713e-16
Epoch: 40, Steps: 24 | Train Loss: 0.3440465 Vali Loss: 0.6556926 Test Loss: 1.2139733
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.8189894035458566e-16
Epoch: 41, Steps: 24 | Train Loss: 0.3428085 Vali Loss: 0.6556926 Test Loss: 1.2139733
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.094947017729283e-17
Epoch: 42, Steps: 24 | Train Loss: 0.3438157 Vali Loss: 0.6556926 Test Loss: 1.2139733
Validation loss decreased (0.655693 --> 0.655693).  Saving model ...
Updating learning rate to 4.5474735088646414e-17
Epoch: 43, Steps: 24 | Train Loss: 0.3424331 Vali Loss: 0.6556926 Test Loss: 1.2139733
Validation loss decreased (0.655693 --> 0.655693).  Saving model ...
Updating learning rate to 2.2737367544323207e-17
Epoch: 44, Steps: 24 | Train Loss: 0.3428115 Vali Loss: 0.6556926 Test Loss: 1.2139733
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.1368683772161604e-17
Epoch: 45, Steps: 24 | Train Loss: 0.3438658 Vali Loss: 0.6556926 Test Loss: 1.2139733
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.684341886080802e-18
Epoch: 46, Steps: 24 | Train Loss: 0.3432008 Vali Loss: 0.6556926 Test Loss: 1.2139733
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.842170943040401e-18
Epoch: 47, Steps: 24 | Train Loss: 0.3428397 Vali Loss: 0.6556926 Test Loss: 1.2139733
Validation loss decreased (0.655693 --> 0.655693).  Saving model ...
Updating learning rate to 1.4210854715202004e-18
Epoch: 48, Steps: 24 | Train Loss: 0.3431109 Vali Loss: 0.6556926 Test Loss: 1.2139733
Validation loss decreased (0.655693 --> 0.655693).  Saving model ...
Updating learning rate to 7.105427357601002e-19
Epoch: 49, Steps: 24 | Train Loss: 0.3427547 Vali Loss: 0.6556926 Test Loss: 1.2139733
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.552713678800501e-19
Epoch: 50, Steps: 24 | Train Loss: 0.3435564 Vali Loss: 0.6556926 Test Loss: 1.2139733
Validation loss decreased (0.655693 --> 0.655693).  Saving model ...
Updating learning rate to 1.7763568394002505e-19
Epoch: 51, Steps: 24 | Train Loss: 0.3429603 Vali Loss: 0.6556926 Test Loss: 1.2139733
Validation loss decreased (0.655693 --> 0.655693).  Saving model ...
Updating learning rate to 8.881784197001253e-20
Epoch: 52, Steps: 24 | Train Loss: 0.3428839 Vali Loss: 0.6556926 Test Loss: 1.2139733
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.4408920985006264e-20
Epoch: 53, Steps: 24 | Train Loss: 0.3432162 Vali Loss: 0.6556926 Test Loss: 1.2139733
Validation loss decreased (0.655693 --> 0.655693).  Saving model ...
Updating learning rate to 2.2204460492503132e-20
Epoch: 54, Steps: 24 | Train Loss: 0.3418858 Vali Loss: 0.6556926 Test Loss: 1.2139733
Validation loss decreased (0.655693 --> 0.655693).  Saving model ...
Updating learning rate to 1.1102230246251566e-20
Epoch: 55, Steps: 24 | Train Loss: 0.3428579 Vali Loss: 0.6556926 Test Loss: 1.2139733
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.551115123125783e-21
Epoch: 56, Steps: 24 | Train Loss: 0.3438638 Vali Loss: 0.6556926 Test Loss: 1.2139733
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.7755575615628915e-21
Epoch: 57, Steps: 24 | Train Loss: 0.3422716 Vali Loss: 0.6556926 Test Loss: 1.2139733
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.3877787807814457e-21
Epoch: 58, Steps: 24 | Train Loss: 0.3424879 Vali Loss: 0.6556926 Test Loss: 1.2139733
Validation loss decreased (0.655693 --> 0.655693).  Saving model ...
Updating learning rate to 6.938893903907229e-22
Epoch: 59, Steps: 24 | Train Loss: 0.3425843 Vali Loss: 0.6556926 Test Loss: 1.2139733
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.4694469519536144e-22
Epoch: 60, Steps: 24 | Train Loss: 0.3440663 Vali Loss: 0.6556926 Test Loss: 1.2139733
Validation loss decreased (0.655693 --> 0.655693).  Saving model ...
Updating learning rate to 1.7347234759768072e-22
Epoch: 61, Steps: 24 | Train Loss: 0.3429534 Vali Loss: 0.6556926 Test Loss: 1.2139733
EarlyStopping counter: 1 out of 10
Updating learning rate to 8.673617379884036e-23
Epoch: 62, Steps: 24 | Train Loss: 0.3441020 Vali Loss: 0.6556926 Test Loss: 1.2139733
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.336808689942018e-23
Epoch: 63, Steps: 24 | Train Loss: 0.3430984 Vali Loss: 0.6556926 Test Loss: 1.2139733
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.168404344971009e-23
Epoch: 64, Steps: 24 | Train Loss: 0.3426843 Vali Loss: 0.6556926 Test Loss: 1.2139733
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.0842021724855045e-23
Epoch: 65, Steps: 24 | Train Loss: 0.3440547 Vali Loss: 0.6556926 Test Loss: 1.2139733
EarlyStopping counter: 5 out of 10
Updating learning rate to 5.4210108624275224e-24
Epoch: 66, Steps: 24 | Train Loss: 0.3438431 Vali Loss: 0.6556926 Test Loss: 1.2139733
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.7105054312137612e-24
Epoch: 67, Steps: 24 | Train Loss: 0.3422563 Vali Loss: 0.6556926 Test Loss: 1.2139733
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.3552527156068806e-24
Epoch: 68, Steps: 24 | Train Loss: 0.3428090 Vali Loss: 0.6556926 Test Loss: 1.2139733
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.776263578034403e-25
Epoch: 69, Steps: 24 | Train Loss: 0.3424770 Vali Loss: 0.6556926 Test Loss: 1.2139733
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.3881317890172015e-25
Epoch: 70, Steps: 24 | Train Loss: 0.3431443 Vali Loss: 0.6556926 Test Loss: 1.2139733
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_ANGLO_AMERICAN_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 27
test shape: (27, 22, 1) (27, 22, 1)
test shape: (27, 22, 1) (27, 22, 1)


	mse:0.008160154335200787, mae:0.06493005156517029, rmse:0.09033357352018356, r2:-0.03935837745666504, dtw:Not calculated


VAL - MSE: 0.0082, MAE: 0.0649, RMSE: 0.0903, RÂ²: -0.0394, MAPE: 12019149.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.496 MB of 0.497 MB uploadedwandb: \ 0.496 MB of 0.497 MB uploadedwandb: | 0.497 MB of 0.497 MB uploadedwandb: / 0.497 MB of 0.497 MB uploadedwandb: - 0.497 MB of 0.568 MB uploadedwandb: \ 0.568 MB of 0.568 MB uploadedwandb: | 0.568 MB of 0.568 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–‚â–â–â–‚â–‚â–â–‚â–‚â–‚â–â–â–‚â–â–â–‚â–â–‚â–â–â–â–‚â–‚â–â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 69
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 222340
wandb:     model/trainable_params 222340
wandb:                  test/loss 1.21397
wandb:                 train/loss 0.34314
wandb:   val/directional_accuracy 50.26455
wandb:                   val/loss 0.65569
wandb:                    val/mae 0.06493
wandb:                   val/mape 1201914900.0
wandb:                    val/mse 0.00816
wandb:                     val/r2 -0.03936
wandb:                   val/rmse 0.09033
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/2zkqmk1r
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_220427-2zkqmk1r/logs
Completed: ANGLO_AMERICAN H=22

Training: Mamba on ANGLO_AMERICAN for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_220758-etb8lymj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/etb8lymj
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ANGLO_AMERICAN_H50Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/etb8lymj
>>>>>>>start training : long_term_forecast_Mamba_ANGLO_AMERICAN_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 50
============================================================
train 731
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.005 MB of 0.005 MB uploadedwandb: / 0.005 MB of 0.005 MB uploadedwandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.015 MB of 0.019 MB uploaded (0.002 MB deduped)wandb: / 0.015 MB of 0.022 MB uploaded (0.002 MB deduped)wandb: - 0.022 MB of 0.022 MB uploaded (0.002 MB deduped)wandb: ðŸš€ View run Mamba_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/etb8lymj
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_220758-etb8lymj/logs
Traceback (most recent call last):
  File "run.py", line 207, in <module>
    exp.train(setting)
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 93, in train
    vali_data, vali_loader = self._get_data(flag='val')
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 43, in _get_data
    data_set, data_loader = data_provider(self.args, flag)
  File "/home-mscluster/bkodze/research_project/forecast-research/data_provider/data_factory.py", line 79, in data_provider
    print(flag, len(data_set))
ValueError: __len__() should return >= 0
Completed: ANGLO_AMERICAN H=50

Training: Mamba on ANGLO_AMERICAN for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_221013-1n5xkr7p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/1n5xkr7p
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ANGLO_AMERICAN_H100Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/1n5xkr7p
>>>>>>>start training : long_term_forecast_Mamba_ANGLO_AMERICAN_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm128_nh8_el2_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 100
============================================================
train 681
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.005 MB of 0.007 MB uploadedwandb: / 0.005 MB of 0.012 MB uploadedwandb: - 0.012 MB of 0.012 MB uploadedwandb: \ 0.012 MB of 0.012 MB uploadedwandb: ðŸš€ View run Mamba_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/1n5xkr7p
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_221013-1n5xkr7p/logs
Traceback (most recent call last):
  File "run.py", line 207, in <module>
    exp.train(setting)
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 93, in train
    vali_data, vali_loader = self._get_data(flag='val')
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 43, in _get_data
    data_set, data_loader = data_provider(self.args, flag)
  File "/home-mscluster/bkodze/research_project/forecast-research/data_provider/data_factory.py", line 79, in data_provider
    print(flag, len(data_set))
ValueError: __len__() should return >= 0
Completed: ANGLO_AMERICAN H=100

Mamba training completed for all datasets!
