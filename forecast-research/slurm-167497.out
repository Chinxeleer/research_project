##############################################################################
# Training Informer Model on All Datasets
##############################################################################
Training: Informer on NVIDIA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_203453-tupplknk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/tupplknk
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NVIDIA_H3  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/tupplknk
>>>>>>>start training : long_term_forecast_Informer_NVIDIA_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
Overriding target from 'OT' to 'close' for stock data
val 237
Overriding target from 'OT' to 'close' for stock data
test 237
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.30907795326154036, 'val/loss': 0.23594262078404427, 'test/loss': 1.0199121609330177, '_timestamp': 1762886159.5090504}).
Epoch: 1, Steps: 133 | Train Loss: 0.3090780 Vali Loss: 0.2359426 Test Loss: 1.0199122
Validation loss decreased (inf --> 0.235943).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2477920 Vali Loss: 0.2655017 Test Loss: 1.3140456
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2332184 Vali Loss: 0.1899116 Test Loss: 1.0507687
Validation loss decreased (0.235943 --> 0.189912).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2477919787616658, 'val/loss': 0.26550169847905636, 'test/loss': 1.3140456303954124, '_timestamp': 1762886171.417734}).
Epoch: 4, Steps: 133 | Train Loss: 0.2290964 Vali Loss: 0.1883605 Test Loss: 0.9963233
Validation loss decreased (0.189912 --> 0.188360).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2263850 Vali Loss: 0.1898160 Test Loss: 0.9838662
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2243420 Vali Loss: 0.1844373 Test Loss: 0.9776885
Validation loss decreased (0.188360 --> 0.184437).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2243088 Vali Loss: 0.1844619 Test Loss: 0.9954844
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2231505 Vali Loss: 0.1894055 Test Loss: 0.9989923
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2235674 Vali Loss: 0.1850821 Test Loss: 0.9825927
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2231149 Vali Loss: 0.1850791 Test Loss: 0.9961470
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2236121 Vali Loss: 0.1877169 Test Loss: 0.9886430
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2233111 Vali Loss: 0.1872440 Test Loss: 1.0028035
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2231965 Vali Loss: 0.1883993 Test Loss: 0.9996792
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2225095 Vali Loss: 0.1833391 Test Loss: 0.9805655
Validation loss decreased (0.184437 --> 0.183339).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2233693 Vali Loss: 0.2035725 Test Loss: 0.9838506
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2229040 Vali Loss: 0.1814696 Test Loss: 0.9944524
Validation loss decreased (0.183339 --> 0.181470).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2230016 Vali Loss: 0.1959407 Test Loss: 0.9919140
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2227763 Vali Loss: 0.1876741 Test Loss: 0.9994053
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2228209 Vali Loss: 0.1861452 Test Loss: 0.9774379
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2226706 Vali Loss: 0.1859628 Test Loss: 0.9939143
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2223336 Vali Loss: 0.1861433 Test Loss: 0.9767952
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2224889 Vali Loss: 0.1817781 Test Loss: 0.9883707
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2229131 Vali Loss: 0.2049149 Test Loss: 0.9933498
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2232508 Vali Loss: 0.1853924 Test Loss: 0.9934534
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2234425 Vali Loss: 0.2061842 Test Loss: 0.9895246
EarlyStopping counter: 9 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2229017 Vali Loss: 0.1864138 Test Loss: 1.0034095
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NVIDIA_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.0011413254542276263, mae:0.025362703949213028, rmse:0.03378351032733917, r2:-0.01741766929626465, dtw:Not calculated


VAL - MSE: 0.0011, MAE: 0.0254, RMSE: 0.0338, RÂ²: -0.0174, MAPE: 67592.30%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.469 MB of 0.470 MB uploadedwandb: \ 0.469 MB of 0.470 MB uploadedwandb: | 0.469 MB of 0.470 MB uploadedwandb: / 0.470 MB of 0.470 MB uploadedwandb: - 0.470 MB of 0.470 MB uploadedwandb: \ 0.470 MB of 0.470 MB uploadedwandb: | 0.470 MB of 0.470 MB uploadedwandb: / 0.470 MB of 0.470 MB uploadedwandb: - 0.470 MB of 0.470 MB uploadedwandb: \ 0.651 MB of 0.940 MB uploaded (0.002 MB deduped)wandb: | 0.940 MB of 0.940 MB uploaded (0.002 MB deduped)wandb: / 0.940 MB of 0.940 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–â–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–ƒâ–â–‚â–ƒâ–‚â–ƒâ–â–ƒâ–â–‚â–ƒâ–ƒâ–‚â–„
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–‚â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–‡â–â–…â–ƒâ–‚â–‚â–‚â–â–ˆâ–‚â–ˆâ–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 25
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 1.00341
wandb:                 train/loss 0.2229
wandb:   val/directional_accuracy 49.57806
wandb:                   val/loss 0.18641
wandb:                    val/mae 0.02536
wandb:                   val/mape 6759229.6875
wandb:                    val/mse 0.00114
wandb:                     val/r2 -0.01742
wandb:                   val/rmse 0.03378
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/tupplknk
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_203453-tupplknk/logs
Completed: NVIDIA H=3

Training: Informer on NVIDIA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_204116-qw9ukn20
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/qw9ukn20
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NVIDIA_H5  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/qw9ukn20
>>>>>>>start training : long_term_forecast_Informer_NVIDIA_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
Overriding target from 'OT' to 'close' for stock data
val 235
Overriding target from 'OT' to 'close' for stock data
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.3089434 Vali Loss: 0.2174681 Test Loss: 1.1781488
Validation loss decreased (inf --> 0.217468).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.30894341325401364, 'val/loss': 0.2174681480973959, 'test/loss': 1.1781487949192524, '_timestamp': 1762886504.2862575}).
Epoch: 2, Steps: 133 | Train Loss: 0.2496105 Vali Loss: 0.2266200 Test Loss: 1.2165991
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2376220 Vali Loss: 0.2195540 Test Loss: 1.0702388
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24961053294346744, 'val/loss': 0.22661997005343437, 'test/loss': 1.2165991142392159, '_timestamp': 1762886516.390623}).
Epoch: 4, Steps: 133 | Train Loss: 0.2324082 Vali Loss: 0.2308223 Test Loss: 1.1426253
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2294186 Vali Loss: 0.2032618 Test Loss: 1.0436853
Validation loss decreased (0.217468 --> 0.203262).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2278991 Vali Loss: 0.1985656 Test Loss: 1.0454984
Validation loss decreased (0.203262 --> 0.198566).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2276066 Vali Loss: 0.2026830 Test Loss: 0.9980988
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2269953 Vali Loss: 0.1950433 Test Loss: 1.0154117
Validation loss decreased (0.198566 --> 0.195043).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2265990 Vali Loss: 0.1936720 Test Loss: 1.0178979
Validation loss decreased (0.195043 --> 0.193672).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2278260 Vali Loss: 0.1956629 Test Loss: 1.0133000
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2263486 Vali Loss: 0.1906665 Test Loss: 1.0220758
Validation loss decreased (0.193672 --> 0.190666).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2266722 Vali Loss: 0.2110316 Test Loss: 1.0106430
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2262710 Vali Loss: 0.2251162 Test Loss: 1.0180035
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2271387 Vali Loss: 0.2123345 Test Loss: 1.0173937
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2266183 Vali Loss: 0.1912300 Test Loss: 1.0220823
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2264280 Vali Loss: 0.1926816 Test Loss: 1.0073764
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2265778 Vali Loss: 0.1913699 Test Loss: 1.0059874
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2276083 Vali Loss: 0.2018702 Test Loss: 1.0162912
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2261994 Vali Loss: 0.1890017 Test Loss: 1.0087638
Validation loss decreased (0.190666 --> 0.189002).  Saving model ...
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2269169 Vali Loss: 0.1934864 Test Loss: 1.0134874
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2267181 Vali Loss: 0.1963465 Test Loss: 1.0154312
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2262312 Vali Loss: 0.1903733 Test Loss: 1.0109260
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2263914 Vali Loss: 0.2104363 Test Loss: 0.9983279
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2259606 Vali Loss: 0.1905388 Test Loss: 1.0078316
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2267145 Vali Loss: 0.1909401 Test Loss: 1.0091659
EarlyStopping counter: 6 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2264428 Vali Loss: 0.2002069 Test Loss: 1.0176871
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.2260989 Vali Loss: 0.1920194 Test Loss: 1.0149975
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.2260360 Vali Loss: 0.1909487 Test Loss: 1.0043907
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.2267164 Vali Loss: 0.1995279 Test Loss: 1.0101412
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NVIDIA_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.0011450611054897308, mae:0.025354759767651558, rmse:0.033838752657175064, r2:-0.01415395736694336, dtw:Not calculated


VAL - MSE: 0.0011, MAE: 0.0254, RMSE: 0.0338, RÂ²: -0.0142, MAPE: 79558.66%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.519 MB of 0.520 MB uploadedwandb: \ 0.519 MB of 0.520 MB uploadedwandb: | 0.520 MB of 0.520 MB uploadedwandb: / 0.520 MB of 0.520 MB uploadedwandb: - 0.520 MB of 0.809 MB uploadedwandb: \ 0.809 MB of 0.809 MB uploadedwandb: | 0.809 MB of 0.809 MB uploadedwandb: / 0.809 MB of 0.809 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–„â–ˆâ–ƒâ–ƒâ–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–‚â–â–â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–ˆâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–…â–‡â–…â–â–‚â–â–ƒâ–â–‚â–‚â–â–…â–â–â–ƒâ–‚â–â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 28
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 1.01014
wandb:                 train/loss 0.22672
wandb:   val/directional_accuracy 54.14894
wandb:                   val/loss 0.19953
wandb:                    val/mae 0.02535
wandb:                   val/mape 7955866.40625
wandb:                    val/mse 0.00115
wandb:                     val/r2 -0.01415
wandb:                   val/rmse 0.03384
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/qw9ukn20
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_204116-qw9ukn20/logs
Completed: NVIDIA H=5

Training: Informer on NVIDIA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_204847-3brlb132
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/3brlb132
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NVIDIA_H10 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/3brlb132
>>>>>>>start training : long_term_forecast_Informer_NVIDIA_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
Overriding target from 'OT' to 'close' for stock data
val 230
Overriding target from 'OT' to 'close' for stock data
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.3095976 Vali Loss: 0.2178848 Test Loss: 1.3084031
Validation loss decreased (inf --> 0.217885).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3095975712053758, 'val/loss': 0.21788483671844006, 'test/loss': 1.3084030598402023, '_timestamp': 1762886960.4064312}).
Epoch: 2, Steps: 133 | Train Loss: 0.2517955 Vali Loss: 0.2111927 Test Loss: 1.2107652
Validation loss decreased (0.217885 --> 0.211193).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2517955404027064, 'val/loss': 0.2111926656216383, 'test/loss': 1.2107652314007282, '_timestamp': 1762886971.8688757}).
Epoch: 3, Steps: 133 | Train Loss: 0.2414224 Vali Loss: 0.2146138 Test Loss: 1.1696899
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2347572 Vali Loss: 0.2201705 Test Loss: 1.0766171
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2328137 Vali Loss: 0.2111050 Test Loss: 1.0233993
Validation loss decreased (0.211193 --> 0.211105).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2322835 Vali Loss: 0.2109193 Test Loss: 1.1394008
Validation loss decreased (0.211105 --> 0.210919).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2299169 Vali Loss: 0.2033600 Test Loss: 1.1118825
Validation loss decreased (0.210919 --> 0.203360).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2300267 Vali Loss: 0.2269200 Test Loss: 1.1113746
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2295794 Vali Loss: 0.2091118 Test Loss: 1.0992289
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2288261 Vali Loss: 0.2209729 Test Loss: 1.1167679
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2293492 Vali Loss: 0.2161507 Test Loss: 1.1008116
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2288500 Vali Loss: 0.2079727 Test Loss: 1.0970891
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2290067 Vali Loss: 0.2026569 Test Loss: 1.1153990
Validation loss decreased (0.203360 --> 0.202657).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2283091 Vali Loss: 0.2085063 Test Loss: 1.1120914
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2295509 Vali Loss: 0.2058028 Test Loss: 1.1117591
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2290529 Vali Loss: 0.2210482 Test Loss: 1.1153397
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2285486 Vali Loss: 0.2040605 Test Loss: 1.1078305
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2287275 Vali Loss: 0.2080546 Test Loss: 1.0975477
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2291807 Vali Loss: 0.2066564 Test Loss: 1.1091302
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2291713 Vali Loss: 0.2081793 Test Loss: 1.1167215
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2282871 Vali Loss: 0.2037098 Test Loss: 1.1102500
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2281885 Vali Loss: 0.2037575 Test Loss: 1.1129382
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2289488 Vali Loss: 0.2061370 Test Loss: 1.1101114
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NVIDIA_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.001165394322015345, mae:0.025631139054894447, rmse:0.034137871116399765, r2:-0.015926122665405273, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0256, RMSE: 0.0341, RÂ²: -0.0159, MAPE: 73819.72%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.543 MB of 0.544 MB uploadedwandb: \ 0.543 MB of 0.544 MB uploadedwandb: | 0.544 MB of 0.544 MB uploadedwandb: / 0.544 MB of 0.544 MB uploadedwandb: - 0.544 MB of 0.833 MB uploadedwandb: \ 0.758 MB of 0.833 MB uploadedwandb: | 0.833 MB of 0.833 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–â–‡â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…
wandb:                 train/loss â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–‚â–â–â–â–‚â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–†â–ƒâ–ƒâ–â–ˆâ–ƒâ–†â–…â–ƒâ–â–ƒâ–‚â–†â–â–ƒâ–‚â–ƒâ–â–â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 1.11011
wandb:                 train/loss 0.22895
wandb:   val/directional_accuracy 51.69082
wandb:                   val/loss 0.20614
wandb:                    val/mae 0.02563
wandb:                   val/mape 7381971.875
wandb:                    val/mse 0.00117
wandb:                     val/r2 -0.01593
wandb:                   val/rmse 0.03414
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/3brlb132
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_204847-3brlb132/logs
Completed: NVIDIA H=10

Training: Informer on NVIDIA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_205329-aq75m9bs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/aq75m9bs
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NVIDIA_H22 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/aq75m9bs
>>>>>>>start training : long_term_forecast_Informer_NVIDIA_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
Overriding target from 'OT' to 'close' for stock data
val 218
Overriding target from 'OT' to 'close' for stock data
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.3124394 Vali Loss: 0.2076419 Test Loss: 1.3480887
Validation loss decreased (inf --> 0.207642).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.312439383543802, 'val/loss': 0.20764194216047013, 'test/loss': 1.3480886561529977, '_timestamp': 1762887238.1535928}).
Epoch: 2, Steps: 132 | Train Loss: 0.2559846 Vali Loss: 0.2641911 Test Loss: 1.4033269
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2445719 Vali Loss: 0.2556026 Test Loss: 1.4225749
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2559845906992753, 'val/loss': 0.26419106125831604, 'test/loss': 1.4033268690109253, '_timestamp': 1762887248.4393508}).
Epoch: 4, Steps: 132 | Train Loss: 0.2396913 Vali Loss: 0.2247535 Test Loss: 1.2401764
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2365429 Vali Loss: 0.2275083 Test Loss: 1.2385017
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2353472 Vali Loss: 0.2273296 Test Loss: 1.2731779
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2341644 Vali Loss: 0.2258919 Test Loss: 1.2638005
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2339760 Vali Loss: 0.2272685 Test Loss: 1.2501929
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2337418 Vali Loss: 0.2261413 Test Loss: 1.2519857
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2336947 Vali Loss: 0.2258582 Test Loss: 1.2438090
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2335049 Vali Loss: 0.2233327 Test Loss: 1.2466287
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NVIDIA_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.0012052309466525912, mae:0.026006432250142097, rmse:0.03471643477678299, r2:-0.0215071439743042, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0260, RMSE: 0.0347, RÂ²: -0.0215, MAPE: 187731.80%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.586 MB of 0.587 MB uploadedwandb: \ 0.587 MB of 0.587 MB uploadedwandb: | 0.587 MB of 0.587 MB uploadedwandb: / 0.587 MB of 0.587 MB uploadedwandb: - 0.587 MB of 0.587 MB uploadedwandb: \ 0.587 MB of 0.874 MB uploadedwandb: | 0.874 MB of 0.874 MB uploadedwandb: / 0.874 MB of 0.874 MB uploadedwandb: - 0.874 MB of 0.874 MB uploadedwandb: \ 0.874 MB of 0.874 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–â–‚â–‚â–â–‚â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–‚â–‚â–‚â–‚â–‚â–‚â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 1.24663
wandb:                 train/loss 0.2335
wandb:   val/directional_accuracy 50.52425
wandb:                   val/loss 0.22333
wandb:                    val/mae 0.02601
wandb:                   val/mape 18773179.6875
wandb:                    val/mse 0.00121
wandb:                     val/r2 -0.02151
wandb:                   val/rmse 0.03472
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/aq75m9bs
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_205329-aq75m9bs/logs
Completed: NVIDIA H=22

Training: Informer on NVIDIA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_205702-67d5q481
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/67d5q481
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NVIDIA_H50 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/67d5q481
>>>>>>>start training : long_term_forecast_Informer_NVIDIA_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
Overriding target from 'OT' to 'close' for stock data
val 190
Overriding target from 'OT' to 'close' for stock data
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.3227510 Vali Loss: 0.2325967 Test Loss: 1.4062444
Validation loss decreased (inf --> 0.232597).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.32275099104101007, 'val/loss': 0.23259673019250235, 'test/loss': 1.4062443772951763, '_timestamp': 1762887452.681022}).
Epoch: 2, Steps: 132 | Train Loss: 0.2645316 Vali Loss: 0.2693596 Test Loss: 1.4280916
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2528625 Vali Loss: 0.2744678 Test Loss: 1.5509893
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2645315890285102, 'val/loss': 0.26935959855715436, 'test/loss': 1.4280916154384613, '_timestamp': 1762887462.1612554}).
Epoch: 4, Steps: 132 | Train Loss: 0.2470241 Vali Loss: 0.2808856 Test Loss: 1.4656407
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2426073 Vali Loss: 0.2758056 Test Loss: 1.4553665
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2431358 Vali Loss: 0.2744447 Test Loss: 1.4472105
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2442462 Vali Loss: 0.2633338 Test Loss: 1.4453296
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2411306 Vali Loss: 0.2894912 Test Loss: 1.4504512
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2458788 Vali Loss: 0.2692629 Test Loss: 1.4508270
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2394702 Vali Loss: 0.2825329 Test Loss: 1.4353715
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2440100 Vali Loss: 0.2840427 Test Loss: 1.4420362
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NVIDIA_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.001221477403305471, mae:0.026568986475467682, rmse:0.034949641674757004, r2:-0.0264585018157959, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0266, RMSE: 0.0349, RÂ²: -0.0265, MAPE: 235582.44%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.628 MB of 0.630 MB uploadedwandb: \ 0.628 MB of 0.630 MB uploadedwandb: | 0.630 MB of 0.630 MB uploadedwandb: / 0.630 MB of 0.630 MB uploadedwandb: - 0.630 MB of 0.917 MB uploadedwandb: \ 0.630 MB of 0.917 MB uploadedwandb: | 0.917 MB of 0.917 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–ƒâ–‚â–„â–â–ƒ
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–†â–„â–„â–â–ˆâ–ƒâ–†â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 1.44204
wandb:                 train/loss 0.24401
wandb:   val/directional_accuracy 50.08593
wandb:                   val/loss 0.28404
wandb:                    val/mae 0.02657
wandb:                   val/mape 23558243.75
wandb:                    val/mse 0.00122
wandb:                     val/r2 -0.02646
wandb:                   val/rmse 0.03495
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/67d5q481
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_205702-67d5q481/logs
Completed: NVIDIA H=50

Training: Informer on NVIDIA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_210005-t9bqk8ov
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/t9bqk8ov
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NVIDIA_H100Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/t9bqk8ov
>>>>>>>start training : long_term_forecast_Informer_NVIDIA_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
Overriding target from 'OT' to 'close' for stock data
val 140
Overriding target from 'OT' to 'close' for stock data
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.3291895 Vali Loss: 0.3004247 Test Loss: 1.4704060
Validation loss decreased (inf --> 0.300425).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3291895171770683, 'val/loss': 0.30042468905448916, 'test/loss': 1.4704059600830077, '_timestamp': 1762887636.420096}).
Epoch: 2, Steps: 130 | Train Loss: 0.2681608 Vali Loss: 0.3246790 Test Loss: 1.7699635
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.2529387 Vali Loss: 0.2485608 Test Loss: 1.6118742
Validation loss decreased (0.300425 --> 0.248561).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26816084339068486, 'val/loss': 0.32467899918556214, 'test/loss': 1.7699634790420533, '_timestamp': 1762887647.9014645}).
Epoch: 4, Steps: 130 | Train Loss: 0.2477517 Vali Loss: 0.2746989 Test Loss: 1.6028073
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2443825 Vali Loss: 0.2678479 Test Loss: 1.5740202
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2430283 Vali Loss: 0.2632840 Test Loss: 1.6055302
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2422590 Vali Loss: 0.2655793 Test Loss: 1.5655578
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2418673 Vali Loss: 0.2659615 Test Loss: 1.5768132
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2413041 Vali Loss: 0.2675679 Test Loss: 1.5668411
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2415902 Vali Loss: 0.2611090 Test Loss: 1.5638728
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2418539 Vali Loss: 0.2660324 Test Loss: 1.5718395
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2410790 Vali Loss: 0.2646608 Test Loss: 1.5713485
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2413349 Vali Loss: 0.2598883 Test Loss: 1.5687943
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NVIDIA_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0012928708456456661, mae:0.027470799162983894, rmse:0.03595651313662529, r2:-0.004237532615661621, dtw:Not calculated


VAL - MSE: 0.0013, MAE: 0.0275, RMSE: 0.0360, RÂ²: -0.0042, MAPE: 77429.05%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.650 MB of 0.655 MB uploadedwandb: \ 0.650 MB of 0.655 MB uploadedwandb: | 0.650 MB of 0.655 MB uploadedwandb: / 0.655 MB of 0.655 MB uploadedwandb: - 0.655 MB of 0.942 MB uploadedwandb: \ 0.655 MB of 0.942 MB uploadedwandb: | 0.942 MB of 0.942 MB uploadedwandb: / 0.942 MB of 0.942 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‡â–‚â–‡â–â–ƒâ–â–â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ˆâ–†â–…â–†â–†â–†â–„â–†â–…â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 1.56879
wandb:                 train/loss 0.24133
wandb:   val/directional_accuracy 50.07937
wandb:                   val/loss 0.25989
wandb:                    val/mae 0.02747
wandb:                   val/mape 7742905.46875
wandb:                    val/mse 0.00129
wandb:                     val/r2 -0.00424
wandb:                   val/rmse 0.03596
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/t9bqk8ov
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_210005-t9bqk8ov/logs
Completed: NVIDIA H=100

Training: Informer on APPLE for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_210456-qqk4tuc9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/qqk4tuc9
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_APPLE_H3   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/qqk4tuc9
>>>>>>>start training : long_term_forecast_Informer_APPLE_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
Overriding target from 'OT' to 'close' for stock data
val 237
Overriding target from 'OT' to 'close' for stock data
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.2922488 Vali Loss: 0.1151581 Test Loss: 0.1719303
Validation loss decreased (inf --> 0.115158).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2922488107044894, 'val/loss': 0.11515811551362276, 'test/loss': 0.17193029262125492, '_timestamp': 1762887925.2288089}).
Epoch: 2, Steps: 133 | Train Loss: 0.2375860 Vali Loss: 0.1121000 Test Loss: 0.1677570
Validation loss decreased (0.115158 --> 0.112100).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23758598590703836, 'val/loss': 0.11210004333406687, 'test/loss': 0.16775698214769363, '_timestamp': 1762887937.0202355}).
Epoch: 3, Steps: 133 | Train Loss: 0.2226276 Vali Loss: 0.0906053 Test Loss: 0.1392131
Validation loss decreased (0.112100 --> 0.090605).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2169917 Vali Loss: 0.0877250 Test Loss: 0.1431059
Validation loss decreased (0.090605 --> 0.087725).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2160040 Vali Loss: 0.0886639 Test Loss: 0.1433326
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2136646 Vali Loss: 0.0861995 Test Loss: 0.1413377
Validation loss decreased (0.087725 --> 0.086199).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2123750 Vali Loss: 0.0915179 Test Loss: 0.1461465
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2123177 Vali Loss: 0.0940767 Test Loss: 0.1455847
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2120210 Vali Loss: 0.0892041 Test Loss: 0.1439587
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2114406 Vali Loss: 0.0879991 Test Loss: 0.1435883
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2117210 Vali Loss: 0.0893611 Test Loss: 0.1439970
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2116385 Vali Loss: 0.0872855 Test Loss: 0.1430572
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2114275 Vali Loss: 0.0881183 Test Loss: 0.1445014
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2109384 Vali Loss: 0.0910953 Test Loss: 0.1471849
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2116855 Vali Loss: 0.0878642 Test Loss: 0.1437203
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2117183 Vali Loss: 0.0891133 Test Loss: 0.1446602
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_APPLE_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.0002036441583186388, mae:0.010401125997304916, rmse:0.01427039410918951, r2:-0.018512248992919922, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0104, RMSE: 0.0143, RÂ²: -0.0185, MAPE: 316569.97%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.472 MB of 0.472 MB uploadedwandb: \ 0.472 MB of 0.472 MB uploadedwandb: | 0.472 MB of 0.472 MB uploadedwandb: / 0.472 MB of 0.472 MB uploadedwandb: - 0.472 MB of 0.472 MB uploadedwandb: \ 0.472 MB of 0.472 MB uploadedwandb: | 0.472 MB of 0.472 MB uploadedwandb: / 0.472 MB of 0.472 MB uploadedwandb: - 0.472 MB of 0.472 MB uploadedwandb: \ 0.472 MB of 0.472 MB uploadedwandb: | 0.654 MB of 0.941 MB uploaded (0.002 MB deduped)wandb: / 0.941 MB of 0.941 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–„â–…â–ƒâ–‡â–‡â–…â–…â–…â–„â–†â–ˆâ–…â–†
wandb:                 train/loss â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–‚â–ƒâ–â–†â–ˆâ–„â–ƒâ–„â–‚â–ƒâ–…â–‚â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.14466
wandb:                 train/loss 0.21172
wandb:   val/directional_accuracy 48.73418
wandb:                   val/loss 0.08911
wandb:                    val/mae 0.0104
wandb:                   val/mape 31656996.875
wandb:                    val/mse 0.0002
wandb:                     val/r2 -0.01851
wandb:                   val/rmse 0.01427
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/qqk4tuc9
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_210456-qqk4tuc9/logs
Completed: APPLE H=3

Training: Informer on APPLE for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_210852-yxry9d2u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/yxry9d2u
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_APPLE_H5   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/yxry9d2u
>>>>>>>start training : long_term_forecast_Informer_APPLE_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
Overriding target from 'OT' to 'close' for stock data
val 235
Overriding target from 'OT' to 'close' for stock data
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.2904528 Vali Loss: 0.1090467 Test Loss: 0.1693119
Validation loss decreased (inf --> 0.109047).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2904527988424875, 'val/loss': 0.10904671717435122, 'test/loss': 0.16931186895817518, '_timestamp': 1762888160.9835541}).
Epoch: 2, Steps: 133 | Train Loss: 0.2384599 Vali Loss: 0.0909818 Test Loss: 0.1540171
Validation loss decreased (0.109047 --> 0.090982).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2384598882574784, 'val/loss': 0.09098182152956724, 'test/loss': 0.15401713363826275, '_timestamp': 1762888173.050392}).
Epoch: 3, Steps: 133 | Train Loss: 0.2249077 Vali Loss: 0.0920107 Test Loss: 0.1549160
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2190987 Vali Loss: 0.0934338 Test Loss: 0.1650170
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2164117 Vali Loss: 0.1000273 Test Loss: 0.1670099
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2147816 Vali Loss: 0.0878869 Test Loss: 0.1513183
Validation loss decreased (0.090982 --> 0.087887).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2144810 Vali Loss: 0.0957871 Test Loss: 0.1601460
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2137484 Vali Loss: 0.0941874 Test Loss: 0.1582643
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2131382 Vali Loss: 0.0961417 Test Loss: 0.1597139
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2140886 Vali Loss: 0.0937598 Test Loss: 0.1586776
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2130937 Vali Loss: 0.0948795 Test Loss: 0.1609788
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2126639 Vali Loss: 0.0925813 Test Loss: 0.1584602
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2136036 Vali Loss: 0.0918282 Test Loss: 0.1576347
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2134504 Vali Loss: 0.0911556 Test Loss: 0.1562513
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2135133 Vali Loss: 0.0956379 Test Loss: 0.1593466
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2137644 Vali Loss: 0.0957657 Test Loss: 0.1584580
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_APPLE_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.00020234021940268576, mae:0.010322656482458115, rmse:0.014224634505808353, r2:-0.00737154483795166, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0103, RMSE: 0.0142, RÂ²: -0.0074, MAPE: 476253.53%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.493 MB of 0.494 MB uploadedwandb: \ 0.493 MB of 0.494 MB uploadedwandb: | 0.493 MB of 0.494 MB uploadedwandb: / 0.494 MB of 0.494 MB uploadedwandb: - 0.494 MB of 0.494 MB uploadedwandb: \ 0.494 MB of 0.781 MB uploadedwandb: | 0.781 MB of 0.781 MB uploadedwandb: / 0.781 MB of 0.781 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ƒâ–‡â–ˆâ–â–…â–„â–…â–„â–…â–„â–„â–ƒâ–…â–„
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–‚â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–„â–ˆâ–â–†â–…â–†â–„â–…â–„â–ƒâ–ƒâ–…â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.15846
wandb:                 train/loss 0.21376
wandb:   val/directional_accuracy 50.21277
wandb:                   val/loss 0.09577
wandb:                    val/mae 0.01032
wandb:                   val/mape 47625353.125
wandb:                    val/mse 0.0002
wandb:                     val/r2 -0.00737
wandb:                   val/rmse 0.01422
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/yxry9d2u
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_210852-yxry9d2u/logs
Completed: APPLE H=5

Training: Informer on APPLE for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_211349-vih4cz4r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/vih4cz4r
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_APPLE_H10  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/vih4cz4r
>>>>>>>start training : long_term_forecast_Informer_APPLE_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
Overriding target from 'OT' to 'close' for stock data
val 230
Overriding target from 'OT' to 'close' for stock data
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2938980 Vali Loss: 0.1553015 Test Loss: 0.2001270
Validation loss decreased (inf --> 0.155301).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2938980117328185, 'val/loss': 0.15530145168304443, 'test/loss': 0.20012704469263554, '_timestamp': 1762888459.6397645}).
Epoch: 2, Steps: 133 | Train Loss: 0.2390847 Vali Loss: 0.0968427 Test Loss: 0.1618234
Validation loss decreased (0.155301 --> 0.096843).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2272880 Vali Loss: 0.1011493 Test Loss: 0.1710697
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23908467590808868, 'val/loss': 0.09684272948652506, 'test/loss': 0.1618234496563673, '_timestamp': 1762888470.9238431}).
Epoch: 4, Steps: 133 | Train Loss: 0.2208088 Vali Loss: 0.1243081 Test Loss: 0.1959959
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2179735 Vali Loss: 0.1078378 Test Loss: 0.1744145
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2188845 Vali Loss: 0.1153404 Test Loss: 0.1868649
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2160167 Vali Loss: 0.1127622 Test Loss: 0.1814321
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2155147 Vali Loss: 0.1111362 Test Loss: 0.1815697
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2153718 Vali Loss: 0.1093271 Test Loss: 0.1806711
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2155716 Vali Loss: 0.1073140 Test Loss: 0.1786765
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2151456 Vali Loss: 0.1117232 Test Loss: 0.1795274
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2151021 Vali Loss: 0.1103228 Test Loss: 0.1775684
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_APPLE_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.00021370025933720171, mae:0.01077884342521429, rmse:0.014618489891290665, r2:-0.055483341217041016, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0108, RMSE: 0.0146, RÂ²: -0.0555, MAPE: 500231.69%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.574 MB of 0.574 MB uploadedwandb: \ 0.574 MB of 0.574 MB uploadedwandb: | 0.574 MB of 0.574 MB uploadedwandb: / 0.574 MB of 0.574 MB uploadedwandb: - 0.574 MB of 0.574 MB uploadedwandb: \ 0.574 MB of 0.574 MB uploadedwandb: | 0.574 MB of 0.861 MB uploadedwandb: / 0.861 MB of 0.861 MB uploadedwandb: - 0.861 MB of 0.861 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–‚â–…â–„â–„â–„â–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–„â–ƒâ–ƒâ–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ˆâ–ƒâ–…â–…â–„â–ƒâ–ƒâ–„â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.17757
wandb:                 train/loss 0.2151
wandb:   val/directional_accuracy 48.98551
wandb:                   val/loss 0.11032
wandb:                    val/mae 0.01078
wandb:                   val/mape 50023168.75
wandb:                    val/mse 0.00021
wandb:                     val/r2 -0.05548
wandb:                   val/rmse 0.01462
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/vih4cz4r
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_211349-vih4cz4r/logs
Completed: APPLE H=10

Training: Informer on APPLE for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_211731-ljwo8j7n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/ljwo8j7n
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_APPLE_H22  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/ljwo8j7n
>>>>>>>start training : long_term_forecast_Informer_APPLE_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
Overriding target from 'OT' to 'close' for stock data
val 218
Overriding target from 'OT' to 'close' for stock data
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2957994 Vali Loss: 0.1375158 Test Loss: 0.2064857
Validation loss decreased (inf --> 0.137516).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2957994068662326, 'val/loss': 0.13751583014215743, 'test/loss': 0.20648570890937532, '_timestamp': 1762888678.7341435}).
Epoch: 2, Steps: 132 | Train Loss: 0.2416339 Vali Loss: 0.1419409 Test Loss: 0.2229548
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2300521 Vali Loss: 0.1099492 Test Loss: 0.1950955
Validation loss decreased (0.137516 --> 0.109949).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24163385424198527, 'val/loss': 0.1419408619403839, 'test/loss': 0.22295481179441726, '_timestamp': 1762888689.6226296}).
Epoch: 4, Steps: 132 | Train Loss: 0.2240921 Vali Loss: 0.1058533 Test Loss: 0.1892488
Validation loss decreased (0.109949 --> 0.105853).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2207441 Vali Loss: 0.1123762 Test Loss: 0.1948856
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2196277 Vali Loss: 0.1158902 Test Loss: 0.2015996
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2183727 Vali Loss: 0.1131752 Test Loss: 0.1986035
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2180997 Vali Loss: 0.1174783 Test Loss: 0.2029095
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2180186 Vali Loss: 0.1168906 Test Loss: 0.2025300
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2176767 Vali Loss: 0.1154887 Test Loss: 0.2008064
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2180380 Vali Loss: 0.1154684 Test Loss: 0.2010822
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2176411 Vali Loss: 0.1172473 Test Loss: 0.2022465
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2178616 Vali Loss: 0.1156950 Test Loss: 0.2018884
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2174159 Vali Loss: 0.1163395 Test Loss: 0.2011171
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_APPLE_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.00020988624601159245, mae:0.010388155467808247, rmse:0.014487450942397118, r2:-0.011478066444396973, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0104, RMSE: 0.0145, RÂ²: -0.0115, MAPE: 679365.06%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.622 MB of 0.623 MB uploadedwandb: \ 0.623 MB of 0.623 MB uploadedwandb: | 0.623 MB of 0.623 MB uploadedwandb: / 0.623 MB of 0.623 MB uploadedwandb: - 0.623 MB of 0.623 MB uploadedwandb: \ 0.623 MB of 0.911 MB uploadedwandb: | 0.688 MB of 0.911 MB uploadedwandb: / 0.911 MB of 0.911 MB uploadedwandb: - 0.911 MB of 0.911 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–„â–â–„â–‡â–†â–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–‡
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–â–…â–‡â–…â–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 13
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.20112
wandb:                 train/loss 0.21742
wandb:   val/directional_accuracy 48.55832
wandb:                   val/loss 0.11634
wandb:                    val/mae 0.01039
wandb:                   val/mape 67936506.25
wandb:                    val/mse 0.00021
wandb:                     val/r2 -0.01148
wandb:                   val/rmse 0.01449
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/ljwo8j7n
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_211731-ljwo8j7n/logs
Completed: APPLE H=22

Training: Informer on APPLE for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_212128-7njuss4l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/7njuss4l
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_APPLE_H50  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/7njuss4l
>>>>>>>start training : long_term_forecast_Informer_APPLE_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
Overriding target from 'OT' to 'close' for stock data
val 190
Overriding target from 'OT' to 'close' for stock data
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.3044676 Vali Loss: 0.1642454 Test Loss: 0.2488539
Validation loss decreased (inf --> 0.164245).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.30446757206862624, 'val/loss': 0.16424543410539627, 'test/loss': 0.2488538697361946, '_timestamp': 1762888918.1371884}).
Epoch: 2, Steps: 132 | Train Loss: 0.2525473 Vali Loss: 0.1590381 Test Loss: 0.2543400
Validation loss decreased (0.164245 --> 0.159038).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2390103 Vali Loss: 0.2105540 Test Loss: 0.3278087
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25254725766452873, 'val/loss': 0.1590381016333898, 'test/loss': 0.2543400103847186, '_timestamp': 1762888927.7936835}).
Epoch: 4, Steps: 132 | Train Loss: 0.2316402 Vali Loss: 0.1484501 Test Loss: 0.2614371
Validation loss decreased (0.159038 --> 0.148450).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2266977 Vali Loss: 0.1587857 Test Loss: 0.2716779
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2254639 Vali Loss: 0.1631082 Test Loss: 0.2807356
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2246070 Vali Loss: 0.1594867 Test Loss: 0.2728259
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2255285 Vali Loss: 0.1565357 Test Loss: 0.2740873
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2248610 Vali Loss: 0.1560568 Test Loss: 0.2736245
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2255186 Vali Loss: 0.1680724 Test Loss: 0.2845949
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2233151 Vali Loss: 0.1561222 Test Loss: 0.2713193
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2251927 Vali Loss: 0.1430492 Test Loss: 0.2552100
Validation loss decreased (0.148450 --> 0.143049).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2245296 Vali Loss: 0.1520056 Test Loss: 0.2676882
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2259864 Vali Loss: 0.1478682 Test Loss: 0.2609280
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2240039 Vali Loss: 0.1533524 Test Loss: 0.2663769
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2233427 Vali Loss: 0.1575613 Test Loss: 0.2715961
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2265337 Vali Loss: 0.1494463 Test Loss: 0.2634767
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2242186 Vali Loss: 0.1505709 Test Loss: 0.2665388
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2241328 Vali Loss: 0.1621580 Test Loss: 0.2777703
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2257202 Vali Loss: 0.1517913 Test Loss: 0.2688720
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2241254 Vali Loss: 0.1559544 Test Loss: 0.2701879
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2234955 Vali Loss: 0.1617138 Test Loss: 0.2774019
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_APPLE_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.00022347053163684905, mae:0.01073873694986105, rmse:0.014948930591344833, r2:-0.007191061973571777, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0107, RMSE: 0.0149, RÂ²: -0.0072, MAPE: 203874.39%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.672 MB of 0.675 MB uploadedwandb: \ 0.672 MB of 0.675 MB uploadedwandb: | 0.672 MB of 0.675 MB uploadedwandb: / 0.675 MB of 0.675 MB uploadedwandb: - 0.675 MB of 0.675 MB uploadedwandb: \ 0.675 MB of 0.964 MB uploadedwandb: | 0.964 MB of 0.964 MB uploadedwandb: / 0.964 MB of 0.964 MB uploadedwandb: - 0.964 MB of 0.964 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–â–‚â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–ƒ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–‚â–â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–„â–‚â–â–‚â–â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.2774
wandb:                 train/loss 0.2235
wandb:   val/directional_accuracy 49.48443
wandb:                   val/loss 0.16171
wandb:                    val/mae 0.01074
wandb:                   val/mape 20387439.0625
wandb:                    val/mse 0.00022
wandb:                     val/r2 -0.00719
wandb:                   val/rmse 0.01495
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/7njuss4l
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_212128-7njuss4l/logs
Completed: APPLE H=50

Training: Informer on APPLE for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_212605-re64e306
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/re64e306
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_APPLE_H100 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/re64e306
>>>>>>>start training : long_term_forecast_Informer_APPLE_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
Overriding target from 'OT' to 'close' for stock data
val 140
Overriding target from 'OT' to 'close' for stock data
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.3123678 Vali Loss: 0.1475979 Test Loss: 0.2410445
Validation loss decreased (inf --> 0.147598).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.312367784174589, 'val/loss': 0.14759786128997804, 'test/loss': 0.24104453325271608, '_timestamp': 1762889194.0976405}).
Epoch: 2, Steps: 130 | Train Loss: 0.2572894 Vali Loss: 0.1605479 Test Loss: 0.2832776
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.2419821 Vali Loss: 0.1677709 Test Loss: 0.3249416
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2572894233923692, 'val/loss': 0.16054790318012238, 'test/loss': 0.2832775950431824, '_timestamp': 1762889205.8735647}).
Epoch: 4, Steps: 130 | Train Loss: 0.2341838 Vali Loss: 0.1843405 Test Loss: 0.3369863
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2303408 Vali Loss: 0.1720330 Test Loss: 0.3196011
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2285483 Vali Loss: 0.1591586 Test Loss: 0.3086533
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2281902 Vali Loss: 0.1670466 Test Loss: 0.3182054
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2271204 Vali Loss: 0.1646797 Test Loss: 0.3128772
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2268391 Vali Loss: 0.1674129 Test Loss: 0.3161693
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2273108 Vali Loss: 0.1650424 Test Loss: 0.3132679
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2269535 Vali Loss: 0.1654595 Test Loss: 0.3192510
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_APPLE_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0002403646649327129, mae:0.01111705880612135, rmse:0.015503698028624058, r2:-0.018012166023254395, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0111, RMSE: 0.0155, RÂ²: -0.0180, MAPE: 191398.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.699 MB of 0.704 MB uploadedwandb: \ 0.699 MB of 0.704 MB uploadedwandb: | 0.699 MB of 0.704 MB uploadedwandb: / 0.704 MB of 0.704 MB uploadedwandb: - 0.704 MB of 0.704 MB uploadedwandb: \ 0.704 MB of 0.991 MB uploadedwandb: | 0.704 MB of 0.991 MB uploadedwandb: / 0.991 MB of 0.991 MB uploadedwandb: - 0.991 MB of 0.991 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–ˆâ–„â–â–ƒâ–‚â–ƒâ–‚â–„
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–ˆâ–…â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.31925
wandb:                 train/loss 0.22695
wandb:   val/directional_accuracy 50.37518
wandb:                   val/loss 0.16546
wandb:                    val/mae 0.01112
wandb:                   val/mape 19139825.0
wandb:                    val/mse 0.00024
wandb:                     val/r2 -0.01801
wandb:                   val/rmse 0.0155
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/re64e306
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_212605-re64e306/logs
Completed: APPLE H=100

Training: Informer on SP500 for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_212945-3dbg66gf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/3dbg66gf
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SP500_H3   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/3dbg66gf
>>>>>>>start training : long_term_forecast_Informer_SP500_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 3
============================================================
train 4241
Overriding target from 'OT' to 'close' for stock data
val 237
Overriding target from 'OT' to 'close' for stock data
test 238
Epoch: 1, Steps: 133 | Train Loss: 0.2328127 Vali Loss: 0.0762365 Test Loss: 0.1073270
Validation loss decreased (inf --> 0.076237).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23281265660784298, 'val/loss': 0.07623651344329119, 'test/loss': 0.10732700116932392, '_timestamp': 1762889416.752042}).
Epoch: 2, Steps: 133 | Train Loss: 0.1928877 Vali Loss: 0.0885682 Test Loss: 0.0852199
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.1807816 Vali Loss: 0.0793452 Test Loss: 0.0817555
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.1928877468059834, 'val/loss': 0.08856821153312922, 'test/loss': 0.085219856351614, '_timestamp': 1762889426.9131982}).
Epoch: 4, Steps: 133 | Train Loss: 0.1780436 Vali Loss: 0.0682422 Test Loss: 0.0917572
Validation loss decreased (0.076237 --> 0.068242).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1755338 Vali Loss: 0.0680530 Test Loss: 0.0932499
Validation loss decreased (0.068242 --> 0.068053).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1726842 Vali Loss: 0.0684111 Test Loss: 0.1048576
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1723610 Vali Loss: 0.0688560 Test Loss: 0.0997468
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1725989 Vali Loss: 0.0687042 Test Loss: 0.0930485
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1723575 Vali Loss: 0.0667233 Test Loss: 0.0988523
Validation loss decreased (0.068053 --> 0.066723).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1713788 Vali Loss: 0.0661133 Test Loss: 0.0981757
Validation loss decreased (0.066723 --> 0.066113).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1722250 Vali Loss: 0.0672611 Test Loss: 0.0978686
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1713713 Vali Loss: 0.0668060 Test Loss: 0.0950917
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1716166 Vali Loss: 0.0681056 Test Loss: 0.0937167
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1718138 Vali Loss: 0.0661008 Test Loss: 0.0975313
Validation loss decreased (0.066113 --> 0.066101).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1718433 Vali Loss: 0.0710588 Test Loss: 0.0911394
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1711293 Vali Loss: 0.0653969 Test Loss: 0.0941615
Validation loss decreased (0.066101 --> 0.065397).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1713828 Vali Loss: 0.0689738 Test Loss: 0.0946551
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1719182 Vali Loss: 0.0675519 Test Loss: 0.0928575
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1718752 Vali Loss: 0.0671604 Test Loss: 0.0985706
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1715373 Vali Loss: 0.0673620 Test Loss: 0.0969675
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1713394 Vali Loss: 0.0670615 Test Loss: 0.0952637
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1739843 Vali Loss: 0.0678098 Test Loss: 0.0961810
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1714992 Vali Loss: 0.0678682 Test Loss: 0.0952541
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.1707473 Vali Loss: 0.0664417 Test Loss: 0.0934853
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.1716253 Vali Loss: 0.0663117 Test Loss: 0.0922062
EarlyStopping counter: 9 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.1712918 Vali Loss: 0.0659640 Test Loss: 0.0938027
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SP500_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 238
test shape: (238, 3, 1) (238, 3, 1)
test shape: (238, 3, 1) (238, 3, 1)


	mse:6.769052561139688e-05, mae:0.006205558776855469, rmse:0.008227425627410412, r2:-0.04121243953704834, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0062, RMSE: 0.0082, RÂ²: -0.0412, MAPE: 2.45%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.481 MB of 0.482 MB uploadedwandb: \ 0.481 MB of 0.482 MB uploadedwandb: | 0.481 MB of 0.482 MB uploadedwandb: / 0.482 MB of 0.482 MB uploadedwandb: - 0.482 MB of 0.482 MB uploadedwandb: \ 0.482 MB of 0.482 MB uploadedwandb: | 0.482 MB of 0.482 MB uploadedwandb: / 0.482 MB of 0.482 MB uploadedwandb: - 0.482 MB of 0.482 MB uploadedwandb: \ 0.482 MB of 0.482 MB uploadedwandb: | 0.663 MB of 0.952 MB uploaded (0.002 MB deduped)wandb: / 0.952 MB of 0.952 MB uploaded (0.002 MB deduped)wandb: - 0.952 MB of 0.952 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–„â–„â–ˆâ–†â–„â–†â–†â–†â–…â–…â–†â–„â–…â–…â–„â–†â–†â–…â–…â–…â–…â–„â–…
wandb:                 train/loss â–ˆâ–†â–„â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–â–ƒâ–‚â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–â–‚â–‚â–‚â–â–„â–â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 25
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.0938
wandb:                 train/loss 0.17129
wandb:   val/directional_accuracy 49.36975
wandb:                   val/loss 0.06596
wandb:                    val/mae 0.00621
wandb:                   val/mape 245.08488
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.04121
wandb:                   val/rmse 0.00823
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/3dbg66gf
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_212945-3dbg66gf/logs
Completed: SP500 H=3

Training: Informer on SP500 for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_213505-qp4g7rmq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/qp4g7rmq
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SP500_H5   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/qp4g7rmq
>>>>>>>start training : long_term_forecast_Informer_SP500_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 5
============================================================
train 4239
Overriding target from 'OT' to 'close' for stock data
val 235
Overriding target from 'OT' to 'close' for stock data
test 236
Epoch: 1, Steps: 133 | Train Loss: 0.2364835 Vali Loss: 0.0960377 Test Loss: 0.1286995
Validation loss decreased (inf --> 0.096038).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2364835378930981, 'val/loss': 0.096037732437253, 'test/loss': 0.12869950756430626, '_timestamp': 1762889731.5789948}).
Epoch: 2, Steps: 133 | Train Loss: 0.1901264 Vali Loss: 0.1082275 Test Loss: 0.0826716
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.1832012 Vali Loss: 0.0685479 Test Loss: 0.1006935
Validation loss decreased (0.096038 --> 0.068548).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19012637094671564, 'val/loss': 0.10822745971381664, 'test/loss': 0.08267162414267659, '_timestamp': 1762889743.5296688}).
Epoch: 4, Steps: 133 | Train Loss: 0.1812085 Vali Loss: 0.0714355 Test Loss: 0.1121804
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1781031 Vali Loss: 0.0688927 Test Loss: 0.0854292
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1770689 Vali Loss: 0.0684760 Test Loss: 0.0856057
Validation loss decreased (0.068548 --> 0.068476).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1764273 Vali Loss: 0.0707393 Test Loss: 0.0886672
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1762146 Vali Loss: 0.0684219 Test Loss: 0.0878795
Validation loss decreased (0.068476 --> 0.068422).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1766049 Vali Loss: 0.0692587 Test Loss: 0.0902258
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1770752 Vali Loss: 0.0668420 Test Loss: 0.0889051
Validation loss decreased (0.068422 --> 0.066842).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1765189 Vali Loss: 0.0698286 Test Loss: 0.0903533
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1759597 Vali Loss: 0.0672315 Test Loss: 0.0874409
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1762133 Vali Loss: 0.0684970 Test Loss: 0.0894829
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1771410 Vali Loss: 0.0677548 Test Loss: 0.0873052
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1766700 Vali Loss: 0.0684063 Test Loss: 0.0889979
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1765681 Vali Loss: 0.0679964 Test Loss: 0.0889124
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1767324 Vali Loss: 0.0668131 Test Loss: 0.0872268
Validation loss decreased (0.066842 --> 0.066813).  Saving model ...
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1789870 Vali Loss: 0.0692795 Test Loss: 0.0889766
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1760287 Vali Loss: 0.0686981 Test Loss: 0.0874955
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1764505 Vali Loss: 0.0673684 Test Loss: 0.0884513
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1765470 Vali Loss: 0.0675926 Test Loss: 0.0885789
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1766346 Vali Loss: 0.0694463 Test Loss: 0.0878315
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1762837 Vali Loss: 0.0679144 Test Loss: 0.0869535
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.1770221 Vali Loss: 0.0667869 Test Loss: 0.0918241
Validation loss decreased (0.066813 --> 0.066787).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.1764225 Vali Loss: 0.0686919 Test Loss: 0.0903097
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.1765118 Vali Loss: 0.0674230 Test Loss: 0.0896594
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.1766174 Vali Loss: 0.0671392 Test Loss: 0.0900326
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.1762749 Vali Loss: 0.0691518 Test Loss: 0.0862505
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.1761747 Vali Loss: 0.0670760 Test Loss: 0.0904136
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 133 | Train Loss: 0.1765916 Vali Loss: 0.0688887 Test Loss: 0.0869965
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 133 | Train Loss: 0.1761758 Vali Loss: 0.0675250 Test Loss: 0.0891175
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 133 | Train Loss: 0.1763877 Vali Loss: 0.0685297 Test Loss: 0.0881991
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 133 | Train Loss: 0.1759437 Vali Loss: 0.0685206 Test Loss: 0.0883508
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 133 | Train Loss: 0.1779751 Vali Loss: 0.0678972 Test Loss: 0.0908771
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SP500_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 236
test shape: (236, 5, 1) (236, 5, 1)
test shape: (236, 5, 1) (236, 5, 1)


	mse:6.689918518532068e-05, mae:0.006110414396971464, rmse:0.008179192431271076, r2:-0.029831528663635254, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0082, RÂ²: -0.0298, MAPE: 1.73%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.522 MB of 0.523 MB uploadedwandb: \ 0.522 MB of 0.523 MB uploadedwandb: | 0.522 MB of 0.523 MB uploadedwandb: / 0.523 MB of 0.523 MB uploadedwandb: - 0.523 MB of 0.523 MB uploadedwandb: \ 0.523 MB of 0.523 MB uploadedwandb: | 0.523 MB of 0.813 MB uploadedwandb: / 0.813 MB of 0.813 MB uploadedwandb: - 0.813 MB of 0.813 MB uploadedwandb: \ 0.813 MB of 0.813 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–ˆâ–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–ƒâ–‚â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–†â–ƒâ–‚â–â–â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–„â–â–â–‚â–‚â–â–‚â–â–‚â–‚â–â–â–‚â–â–â–â–ƒ
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–ˆâ–„â–„â–‡â–ƒâ–…â–â–†â–‚â–„â–‚â–ƒâ–ƒâ–â–…â–„â–‚â–‚â–…â–ƒâ–â–„â–‚â–‚â–…â–â–„â–‚â–„â–„â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 33
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.09088
wandb:                 train/loss 0.17798
wandb:   val/directional_accuracy 48.94068
wandb:                   val/loss 0.0679
wandb:                    val/mae 0.00611
wandb:                   val/mape 173.36864
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.02983
wandb:                   val/rmse 0.00818
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/qp4g7rmq
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_213505-qp4g7rmq/logs
Completed: SP500 H=5

Training: Informer on SP500 for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_214104-g5e4aus6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/g5e4aus6
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SP500_H10  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/g5e4aus6
>>>>>>>start training : long_term_forecast_Informer_SP500_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 10
============================================================
train 4234
Overriding target from 'OT' to 'close' for stock data
val 230
Overriding target from 'OT' to 'close' for stock data
test 231
Epoch: 1, Steps: 133 | Train Loss: 0.2346061 Vali Loss: 0.0873606 Test Loss: 0.0996205
Validation loss decreased (inf --> 0.087361).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23460608961662852, 'val/loss': 0.08736055810004473, 'test/loss': 0.09962054248899221, '_timestamp': 1762890093.401587}).
Epoch: 2, Steps: 133 | Train Loss: 0.1892838 Vali Loss: 0.0727462 Test Loss: 0.0973327
Validation loss decreased (0.087361 --> 0.072746).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.1822914 Vali Loss: 0.0692767 Test Loss: 0.1062914
Validation loss decreased (0.072746 --> 0.069277).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.18928377543176925, 'val/loss': 0.07274624006822705, 'test/loss': 0.09733272483572364, '_timestamp': 1762890106.4387617}).
Epoch: 4, Steps: 133 | Train Loss: 0.1789620 Vali Loss: 0.0677130 Test Loss: 0.1106885
Validation loss decreased (0.069277 --> 0.067713).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1773833 Vali Loss: 0.0671215 Test Loss: 0.1013515
Validation loss decreased (0.067713 --> 0.067121).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1768355 Vali Loss: 0.0672547 Test Loss: 0.1045746
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1766470 Vali Loss: 0.0649422 Test Loss: 0.1063725
Validation loss decreased (0.067121 --> 0.064942).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1761328 Vali Loss: 0.0683651 Test Loss: 0.1000211
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1765040 Vali Loss: 0.0684801 Test Loss: 0.0996938
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1762134 Vali Loss: 0.0675830 Test Loss: 0.0993807
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1761845 Vali Loss: 0.0665516 Test Loss: 0.1022322
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1760236 Vali Loss: 0.0665733 Test Loss: 0.0982877
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1764536 Vali Loss: 0.0681448 Test Loss: 0.1000238
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1752574 Vali Loss: 0.0682607 Test Loss: 0.1022073
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1778538 Vali Loss: 0.0667222 Test Loss: 0.0988466
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1763444 Vali Loss: 0.0690760 Test Loss: 0.0988407
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1765252 Vali Loss: 0.0688962 Test Loss: 0.1025494
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SP500_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 231
test shape: (231, 10, 1) (231, 10, 1)
test shape: (231, 10, 1) (231, 10, 1)


	mse:6.66872292640619e-05, mae:0.006081628147512674, rmse:0.00816622469574213, r2:-0.025913238525390625, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0082, RÂ²: -0.0259, MAPE: 1.65%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.539 MB of 0.540 MB uploadedwandb: \ 0.539 MB of 0.540 MB uploadedwandb: | 0.539 MB of 0.540 MB uploadedwandb: / 0.540 MB of 0.540 MB uploadedwandb: - 0.540 MB of 0.540 MB uploadedwandb: \ 0.540 MB of 0.828 MB uploadedwandb: | 0.823 MB of 0.828 MB uploadedwandb: / 0.828 MB of 0.828 MB uploadedwandb: - 0.828 MB of 0.828 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–†â–ˆâ–ƒâ–…â–†â–‚â–‚â–‚â–ƒâ–â–‚â–ƒâ–â–â–ƒ
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–„â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–…â–…â–â–‡â–‡â–…â–„â–„â–†â–†â–„â–ˆâ–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.10255
wandb:                 train/loss 0.17653
wandb:   val/directional_accuracy 47.47475
wandb:                   val/loss 0.0689
wandb:                    val/mae 0.00608
wandb:                   val/mape 165.41799
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.02591
wandb:                   val/rmse 0.00817
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/g5e4aus6
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_214104-g5e4aus6/logs
Completed: SP500 H=10

Training: Informer on SP500 for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_214549-afaawgy3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/afaawgy3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SP500_H22  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/afaawgy3
>>>>>>>start training : long_term_forecast_Informer_SP500_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 22
============================================================
train 4222
Overriding target from 'OT' to 'close' for stock data
val 218
Overriding target from 'OT' to 'close' for stock data
test 219
Epoch: 1, Steps: 132 | Train Loss: 0.2367385 Vali Loss: 0.0752724 Test Loss: 0.1387338
Validation loss decreased (inf --> 0.075272).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23673854604589217, 'val/loss': 0.07527239514248711, 'test/loss': 0.13873382125582015, '_timestamp': 1762890380.7597923}).
Epoch: 2, Steps: 132 | Train Loss: 0.1902849 Vali Loss: 0.0769757 Test Loss: 0.0828543
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.1837911 Vali Loss: 0.0729020 Test Loss: 0.0878778
Validation loss decreased (0.075272 --> 0.072902).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19028490194768616, 'val/loss': 0.07697570323944092, 'test/loss': 0.08285428264311381, '_timestamp': 1762890390.9937646}).
Epoch: 4, Steps: 132 | Train Loss: 0.1812561 Vali Loss: 0.0727551 Test Loss: 0.0896340
Validation loss decreased (0.072902 --> 0.072755).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.1798072 Vali Loss: 0.0717414 Test Loss: 0.0882033
Validation loss decreased (0.072755 --> 0.071741).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.1790748 Vali Loss: 0.0696117 Test Loss: 0.0915275
Validation loss decreased (0.071741 --> 0.069612).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.1788830 Vali Loss: 0.0709659 Test Loss: 0.0891175
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.1787661 Vali Loss: 0.0693189 Test Loss: 0.0957317
Validation loss decreased (0.069612 --> 0.069319).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.1786172 Vali Loss: 0.0689190 Test Loss: 0.0969937
Validation loss decreased (0.069319 --> 0.068919).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.1785348 Vali Loss: 0.0692327 Test Loss: 0.0950570
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.1782732 Vali Loss: 0.0690546 Test Loss: 0.0970898
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.1785116 Vali Loss: 0.0687957 Test Loss: 0.0964791
Validation loss decreased (0.068919 --> 0.068796).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.1784772 Vali Loss: 0.0690593 Test Loss: 0.0969248
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.1785650 Vali Loss: 0.0688715 Test Loss: 0.0971916
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.1784333 Vali Loss: 0.0688502 Test Loss: 0.0982299
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.1781852 Vali Loss: 0.0690668 Test Loss: 0.0967614
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.1784897 Vali Loss: 0.0695203 Test Loss: 0.0942066
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.1785268 Vali Loss: 0.0691721 Test Loss: 0.0960759
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.1785109 Vali Loss: 0.0690830 Test Loss: 0.0967575
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.1785522 Vali Loss: 0.0690602 Test Loss: 0.0963488
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.1785280 Vali Loss: 0.0690727 Test Loss: 0.0971219
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.1786457 Vali Loss: 0.0688471 Test Loss: 0.0986626
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SP500_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 219
test shape: (219, 22, 1) (219, 22, 1)
test shape: (219, 22, 1) (219, 22, 1)


	mse:6.563820352312177e-05, mae:0.0060449023731052876, rmse:0.00810174085199833, r2:-0.028079748153686523, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0060, RMSE: 0.0081, RÂ²: -0.0281, MAPE: 1.54%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.613 MB of 0.614 MB uploadedwandb: \ 0.613 MB of 0.614 MB uploadedwandb: | 0.614 MB of 0.614 MB uploadedwandb: / 0.614 MB of 0.614 MB uploadedwandb: - 0.614 MB of 0.903 MB uploadedwandb: \ 0.614 MB of 0.903 MB uploadedwandb: | 0.903 MB of 0.903 MB uploadedwandb: / 0.903 MB of 0.903 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–‚â–â–ƒâ–‚â–†â–‡â–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–…â–†â–‡â–†â–‡â–ˆ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ˆâ–†â–‚â–…â–‚â–â–‚â–â–â–â–â–â–â–‚â–‚â–â–â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.09866
wandb:                 train/loss 0.17865
wandb:   val/directional_accuracy 50.55447
wandb:                   val/loss 0.06885
wandb:                    val/mae 0.00604
wandb:                   val/mape 153.82365
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.02808
wandb:                   val/rmse 0.0081
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/afaawgy3
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_214549-afaawgy3/logs
Completed: SP500 H=22

Training: Informer on SP500 for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_215053-147mdynj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/147mdynj
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SP500_H50  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/147mdynj
>>>>>>>start training : long_term_forecast_Informer_SP500_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 50
============================================================
train 4194
Overriding target from 'OT' to 'close' for stock data
val 190
Overriding target from 'OT' to 'close' for stock data
test 191
Epoch: 1, Steps: 132 | Train Loss: 0.2440292 Vali Loss: 0.0837555 Test Loss: 0.1073344
Validation loss decreased (inf --> 0.083756).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.24402919462458653, 'val/loss': 0.08375554531812668, 'test/loss': 0.10733444119493167, '_timestamp': 1762890687.6901395}).
Epoch: 2, Steps: 132 | Train Loss: 0.1944141 Vali Loss: 0.0829122 Test Loss: 0.1827227
Validation loss decreased (0.083756 --> 0.082912).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.1890086 Vali Loss: 0.0722603 Test Loss: 0.1079990
Validation loss decreased (0.082912 --> 0.072260).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.1944140965175448, 'val/loss': 0.08291221658388774, 'test/loss': 0.18272273366649947, '_timestamp': 1762890698.4563186}).
Epoch: 4, Steps: 132 | Train Loss: 0.1842796 Vali Loss: 0.0700668 Test Loss: 0.1130043
Validation loss decreased (0.072260 --> 0.070067).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.1820215 Vali Loss: 0.0707343 Test Loss: 0.1136488
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.1827792 Vali Loss: 0.0694160 Test Loss: 0.1125855
Validation loss decreased (0.070067 --> 0.069416).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.1838177 Vali Loss: 0.0696087 Test Loss: 0.1135153
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.1816286 Vali Loss: 0.0693487 Test Loss: 0.1224255
Validation loss decreased (0.069416 --> 0.069349).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.1824567 Vali Loss: 0.0697008 Test Loss: 0.1183995
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.1816710 Vali Loss: 0.0696241 Test Loss: 0.1149991
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.1844610 Vali Loss: 0.0691551 Test Loss: 0.1173316
Validation loss decreased (0.069349 --> 0.069155).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.1808662 Vali Loss: 0.0696346 Test Loss: 0.1143550
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.1816640 Vali Loss: 0.0696573 Test Loss: 0.1147795
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.1806236 Vali Loss: 0.0695967 Test Loss: 0.1144653
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.1811002 Vali Loss: 0.0693767 Test Loss: 0.1173332
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.1811484 Vali Loss: 0.0695174 Test Loss: 0.1146883
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.1809528 Vali Loss: 0.0699356 Test Loss: 0.1137620
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.1817467 Vali Loss: 0.0697030 Test Loss: 0.1170761
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.1877781 Vali Loss: 0.0694232 Test Loss: 0.1145469
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.1895210 Vali Loss: 0.0692125 Test Loss: 0.1164998
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.1815003 Vali Loss: 0.0694490 Test Loss: 0.1153689
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SP500_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 191
test shape: (191, 50, 1) (191, 50, 1)
test shape: (191, 50, 1) (191, 50, 1)


	mse:6.616122118430212e-05, mae:0.006062426138669252, rmse:0.008133954368531704, r2:-0.017355680465698242, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0081, RÂ²: -0.0174, MAPE: 1.35%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.671 MB of 0.673 MB uploadedwandb: \ 0.671 MB of 0.673 MB uploadedwandb: | 0.673 MB of 0.673 MB uploadedwandb: / 0.673 MB of 0.962 MB uploadedwandb: - 0.962 MB of 0.962 MB uploadedwandb: \ 0.962 MB of 0.962 MB uploadedwandb: | 0.962 MB of 0.962 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ƒâ–„â–ƒâ–„â–ˆâ–†â–„â–†â–„â–„â–„â–†â–„â–„â–…â–„â–…â–…
wandb:                 train/loss â–ˆâ–„â–‚â–ƒâ–„â–‚â–‚â–‚â–„â–â–‚â–â–â–â–â–‚â–‡â–ˆâ–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–…â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–ƒâ–‚â–‚â–â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.11537
wandb:                 train/loss 0.1815
wandb:   val/directional_accuracy 49.36425
wandb:                   val/loss 0.06945
wandb:                    val/mae 0.00606
wandb:                   val/mape 134.83139
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.01736
wandb:                   val/rmse 0.00813
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/147mdynj
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_215053-147mdynj/logs
Completed: SP500 H=50

Training: Informer on SP500 for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_215630-tsi6s5ud
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/tsi6s5ud
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SP500_H100 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/tsi6s5ud
>>>>>>>start training : long_term_forecast_Informer_SP500_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 100
============================================================
train 4144
Overriding target from 'OT' to 'close' for stock data
val 140
Overriding target from 'OT' to 'close' for stock data
test 141
Epoch: 1, Steps: 130 | Train Loss: 0.2515311 Vali Loss: 0.0760529 Test Loss: 0.2090924
Validation loss decreased (inf --> 0.076053).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2515311273244711, 'val/loss': 0.07605292946100235, 'test/loss': 0.20909244418144227, '_timestamp': 1762891022.8909695}).
Epoch: 2, Steps: 130 | Train Loss: 0.2017186 Vali Loss: 0.0699931 Test Loss: 0.1753203
Validation loss decreased (0.076053 --> 0.069993).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2017185682287583, 'val/loss': 0.0699931263923645, 'test/loss': 0.17532027512788773, '_timestamp': 1762891033.9138381}).
Epoch: 3, Steps: 130 | Train Loss: 0.1899278 Vali Loss: 0.0667302 Test Loss: 0.1680098
Validation loss decreased (0.069993 --> 0.066730).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.1859022 Vali Loss: 0.0710464 Test Loss: 0.1932896
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.1840988 Vali Loss: 0.0651722 Test Loss: 0.1745353
Validation loss decreased (0.066730 --> 0.065172).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.1832519 Vali Loss: 0.0662699 Test Loss: 0.1667445
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.1831200 Vali Loss: 0.0655418 Test Loss: 0.1637450
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.1833503 Vali Loss: 0.0652136 Test Loss: 0.1654859
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.1824637 Vali Loss: 0.0655733 Test Loss: 0.1702740
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.1825624 Vali Loss: 0.0659658 Test Loss: 0.1654091
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.1828573 Vali Loss: 0.0658664 Test Loss: 0.1632343
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.1824258 Vali Loss: 0.0654033 Test Loss: 0.1653164
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.1825684 Vali Loss: 0.0659635 Test Loss: 0.1660392
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.1822657 Vali Loss: 0.0658589 Test Loss: 0.1674638
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.1829379 Vali Loss: 0.0652964 Test Loss: 0.1638020
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SP500_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 141
test shape: (141, 100, 1) (141, 100, 1)
test shape: (141, 100, 1) (141, 100, 1)


	mse:6.994514842517674e-05, mae:0.006221783347427845, rmse:0.008363321423530579, r2:-0.020972728729248047, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0062, RMSE: 0.0084, RÂ²: -0.0210, MAPE: 1.51%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.769 MB of 0.769 MB uploadedwandb: \ 0.769 MB of 0.769 MB uploadedwandb: | 0.769 MB of 0.769 MB uploadedwandb: / 0.769 MB of 1.057 MB uploadedwandb: - 0.769 MB of 1.057 MB uploadedwandb: \ 1.057 MB of 1.057 MB uploadedwandb: | 1.057 MB of 1.057 MB uploadedwandb: / 1.057 MB of 1.057 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‚â–ˆâ–„â–‚â–â–‚â–ƒâ–‚â–â–â–‚â–‚â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–‚â–â–â–‚â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–ˆâ–â–‚â–â–â–â–‚â–‚â–â–‚â–‚â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.1638
wandb:                 train/loss 0.18294
wandb:   val/directional_accuracy 49.25854
wandb:                   val/loss 0.0653
wandb:                    val/mae 0.00622
wandb:                   val/mape 150.63251
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.02097
wandb:                   val/rmse 0.00836
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/tsi6s5ud
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_215630-tsi6s5ud/logs
Completed: SP500 H=100

Training: Informer on NASDAQ for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_220059-iwpqfkv6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/iwpqfkv6
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NASDAQ_H3  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/iwpqfkv6
>>>>>>>start training : long_term_forecast_Informer_NASDAQ_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
Overriding target from 'OT' to 'close' for stock data
val 237
Overriding target from 'OT' to 'close' for stock data
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.3078509 Vali Loss: 0.2001298 Test Loss: 0.1657778
Validation loss decreased (inf --> 0.200130).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.30785088622032253, 'val/loss': 0.20012975670397282, 'test/loss': 0.16577780339866877, '_timestamp': 1762891292.1911535}).
Epoch: 2, Steps: 133 | Train Loss: 0.2527859 Vali Loss: 0.1545137 Test Loss: 0.1499259
Validation loss decreased (0.200130 --> 0.154514).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2527859162791331, 'val/loss': 0.15451369807124138, 'test/loss': 0.1499259313568473, '_timestamp': 1762891304.1162534}).
Epoch: 3, Steps: 133 | Train Loss: 0.2372118 Vali Loss: 0.1404916 Test Loss: 0.1291191
Validation loss decreased (0.154514 --> 0.140492).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2321052 Vali Loss: 0.1391210 Test Loss: 0.1251912
Validation loss decreased (0.140492 --> 0.139121).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2294239 Vali Loss: 0.1415206 Test Loss: 0.1375023
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2284399 Vali Loss: 0.1453415 Test Loss: 0.1240312
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2269616 Vali Loss: 0.1384880 Test Loss: 0.1243834
Validation loss decreased (0.139121 --> 0.138488).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2254080 Vali Loss: 0.1379649 Test Loss: 0.1277889
Validation loss decreased (0.138488 --> 0.137965).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2259237 Vali Loss: 0.1367617 Test Loss: 0.1256379
Validation loss decreased (0.137965 --> 0.136762).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2258526 Vali Loss: 0.1377461 Test Loss: 0.1247416
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2263786 Vali Loss: 0.1357337 Test Loss: 0.1234333
Validation loss decreased (0.136762 --> 0.135734).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2252865 Vali Loss: 0.1328430 Test Loss: 0.1245960
Validation loss decreased (0.135734 --> 0.132843).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2263507 Vali Loss: 0.1376468 Test Loss: 0.1246917
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2255827 Vali Loss: 0.1383907 Test Loss: 0.1253922
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2254853 Vali Loss: 0.1333162 Test Loss: 0.1231302
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2257751 Vali Loss: 0.1374634 Test Loss: 0.1238427
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2260413 Vali Loss: 0.1377854 Test Loss: 0.1272850
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2255510 Vali Loss: 0.1340884 Test Loss: 0.1224221
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2248947 Vali Loss: 0.1370697 Test Loss: 0.1247474
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2258104 Vali Loss: 0.1330632 Test Loss: 0.1236592
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2253149 Vali Loss: 0.1385087 Test Loss: 0.1267552
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2252140 Vali Loss: 0.1320316 Test Loss: 0.1221948
Validation loss decreased (0.132843 --> 0.132032).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2254363 Vali Loss: 0.1345826 Test Loss: 0.1241104
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2253557 Vali Loss: 0.1341932 Test Loss: 0.1250631
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2255606 Vali Loss: 0.1378654 Test Loss: 0.1250456
EarlyStopping counter: 3 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2251903 Vali Loss: 0.1325003 Test Loss: 0.1232116
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.2248274 Vali Loss: 0.1372070 Test Loss: 0.1237239
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.2249632 Vali Loss: 0.1396060 Test Loss: 0.1257765
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.2275620 Vali Loss: 0.1344410 Test Loss: 0.1220178
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 133 | Train Loss: 0.2255305 Vali Loss: 0.1321575 Test Loss: 0.1214384
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 133 | Train Loss: 0.2254529 Vali Loss: 0.1362502 Test Loss: 0.1251462
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 133 | Train Loss: 0.2254106 Vali Loss: 0.1344068 Test Loss: 0.1236254
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NASDAQ_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.00013950820721220225, mae:0.008621093817055225, rmse:0.011811358854174614, r2:-0.02497541904449463, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0086, RMSE: 0.0118, RÂ²: -0.0250, MAPE: 2785437.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.496 MB of 0.496 MB uploadedwandb: \ 0.496 MB of 0.496 MB uploadedwandb: | 0.496 MB of 0.496 MB uploadedwandb: / 0.496 MB of 0.496 MB uploadedwandb: - 0.496 MB of 0.496 MB uploadedwandb: \ 0.496 MB of 0.496 MB uploadedwandb: | 0.496 MB of 0.496 MB uploadedwandb: / 0.496 MB of 0.496 MB uploadedwandb: - 0.678 MB of 0.968 MB uploaded (0.002 MB deduped)wandb: \ 0.968 MB of 0.968 MB uploaded (0.002 MB deduped)wandb: | 0.968 MB of 0.968 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–„â–ƒâ–ˆâ–‚â–‚â–„â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–„â–â–‚â–‚â–ƒâ–â–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–â–â–ƒâ–‚
wandb:                 train/loss â–ˆâ–…â–„â–ƒâ–‚â–â–‚â–‚â–‚â–â–‚â–â–â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–ƒâ–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–…â–†â–ˆâ–„â–„â–ƒâ–„â–ƒâ–â–„â–„â–‚â–„â–„â–‚â–„â–‚â–„â–â–‚â–‚â–„â–â–„â–…â–‚â–â–ƒâ–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 31
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.12363
wandb:                 train/loss 0.22541
wandb:   val/directional_accuracy 49.78903
wandb:                   val/loss 0.13441
wandb:                    val/mae 0.00862
wandb:                   val/mape 278543725.0
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.02498
wandb:                   val/rmse 0.01181
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/iwpqfkv6
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_220059-iwpqfkv6/logs
Completed: NASDAQ H=3

Training: Informer on NASDAQ for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_220700-xvdouuxc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/xvdouuxc
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NASDAQ_H5  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/xvdouuxc
>>>>>>>start training : long_term_forecast_Informer_NASDAQ_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
Overriding target from 'OT' to 'close' for stock data
val 235
Overriding target from 'OT' to 'close' for stock data
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.3108010 Vali Loss: 0.1675459 Test Loss: 0.1469621
Validation loss decreased (inf --> 0.167546).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.310801009150376, 'val/loss': 0.16754593700170517, 'test/loss': 0.14696206618100405, '_timestamp': 1762891648.5359302}).
Epoch: 2, Steps: 133 | Train Loss: 0.2552466 Vali Loss: 0.1615192 Test Loss: 0.1585983
Validation loss decreased (0.167546 --> 0.161519).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2429625 Vali Loss: 0.1637280 Test Loss: 0.1397658
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2552465656422135, 'val/loss': 0.16151915304362774, 'test/loss': 0.15859832521528006, '_timestamp': 1762891660.7969549}).
Epoch: 4, Steps: 133 | Train Loss: 0.2364773 Vali Loss: 0.1517436 Test Loss: 0.1630792
Validation loss decreased (0.161519 --> 0.151744).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2336755 Vali Loss: 0.1390382 Test Loss: 0.1389146
Validation loss decreased (0.151744 --> 0.139038).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2318628 Vali Loss: 0.1421192 Test Loss: 0.1317008
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2311711 Vali Loss: 0.1407373 Test Loss: 0.1333030
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2306335 Vali Loss: 0.1436358 Test Loss: 0.1370284
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2303132 Vali Loss: 0.1464904 Test Loss: 0.1370648
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2316996 Vali Loss: 0.1493517 Test Loss: 0.1330484
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2314671 Vali Loss: 0.1399864 Test Loss: 0.1327184
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2303843 Vali Loss: 0.1445927 Test Loss: 0.1382367
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2308954 Vali Loss: 0.1432140 Test Loss: 0.1311101
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2305752 Vali Loss: 0.1540578 Test Loss: 0.1416154
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2306113 Vali Loss: 0.1403457 Test Loss: 0.1317641
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NASDAQ_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.00014387928240466863, mae:0.008790248073637486, rmse:0.011994969099760056, r2:-0.0517268180847168, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0088, RMSE: 0.0120, RÂ²: -0.0517, MAPE: 2993833.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.520 MB of 0.520 MB uploadedwandb: \ 0.520 MB of 0.520 MB uploadedwandb: | 0.520 MB of 0.520 MB uploadedwandb: / 0.520 MB of 0.520 MB uploadedwandb: - 0.520 MB of 0.807 MB uploadedwandb: \ 0.807 MB of 0.807 MB uploadedwandb: | 0.807 MB of 0.807 MB uploadedwandb: / 0.807 MB of 0.807 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ƒâ–ˆâ–ƒâ–â–â–‚â–‚â–â–â–ƒâ–â–ƒâ–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–â–â–â–‚â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–â–‚â–â–‚â–ƒâ–„â–â–ƒâ–‚â–…â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.13176
wandb:                 train/loss 0.23061
wandb:   val/directional_accuracy 51.17021
wandb:                   val/loss 0.14035
wandb:                    val/mae 0.00879
wandb:                   val/mape 299383375.0
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.05173
wandb:                   val/rmse 0.01199
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/xvdouuxc
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_220700-xvdouuxc/logs
Completed: NASDAQ H=5

Training: Informer on NASDAQ for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_221140-45rxe3gc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/45rxe3gc
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NASDAQ_H10 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/45rxe3gc
>>>>>>>start training : long_term_forecast_Informer_NASDAQ_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
Overriding target from 'OT' to 'close' for stock data
val 230
Overriding target from 'OT' to 'close' for stock data
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.3080753 Vali Loss: 0.1757493 Test Loss: 0.1623670
Validation loss decreased (inf --> 0.175749).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3080752572618929, 'val/loss': 0.17574934475123882, 'test/loss': 0.1623670319095254, '_timestamp': 1762891931.1512182}).
Epoch: 2, Steps: 133 | Train Loss: 0.2574428 Vali Loss: 0.1537605 Test Loss: 0.1449611
Validation loss decreased (0.175749 --> 0.153761).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2464515 Vali Loss: 0.1627855 Test Loss: 0.1618981
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2574427840404941, 'val/loss': 0.15376052260398865, 'test/loss': 0.1449610684067011, '_timestamp': 1762891942.1735864}).
Epoch: 4, Steps: 133 | Train Loss: 0.2387094 Vali Loss: 0.1696743 Test Loss: 0.1776080
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2380318 Vali Loss: 0.1566809 Test Loss: 0.1581627
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2374937 Vali Loss: 0.1496934 Test Loss: 0.1502517
Validation loss decreased (0.153761 --> 0.149693).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2348796 Vali Loss: 0.1502135 Test Loss: 0.1519314
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2345608 Vali Loss: 0.1600928 Test Loss: 0.1658733
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2349562 Vali Loss: 0.1579066 Test Loss: 0.1605252
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2342142 Vali Loss: 0.1537008 Test Loss: 0.1508713
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2341929 Vali Loss: 0.1522947 Test Loss: 0.1540880
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2345559 Vali Loss: 0.1593248 Test Loss: 0.1663216
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2340275 Vali Loss: 0.1504899 Test Loss: 0.1468273
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2338784 Vali Loss: 0.1552855 Test Loss: 0.1584948
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2344169 Vali Loss: 0.1513366 Test Loss: 0.1496061
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2346334 Vali Loss: 0.1502494 Test Loss: 0.1485021
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NASDAQ_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.00014476667274720967, mae:0.008776024915277958, rmse:0.012031902559101582, r2:-0.04580485820770264, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0088, RMSE: 0.0120, RÂ²: -0.0458, MAPE: 1987089.38%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.591 MB of 0.592 MB uploadedwandb: \ 0.591 MB of 0.592 MB uploadedwandb: | 0.592 MB of 0.592 MB uploadedwandb: / 0.592 MB of 0.592 MB uploadedwandb: - 0.592 MB of 0.879 MB uploadedwandb: \ 0.879 MB of 0.879 MB uploadedwandb: | 0.879 MB of 0.879 MB uploadedwandb: / 0.879 MB of 0.879 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–„â–ˆâ–„â–‚â–‚â–…â–„â–‚â–ƒâ–…â–â–„â–‚â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–ƒâ–‚â–â–‚â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–ˆâ–ƒâ–â–â–…â–„â–‚â–‚â–„â–â–ƒâ–‚â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.1485
wandb:                 train/loss 0.23463
wandb:   val/directional_accuracy 50.28986
wandb:                   val/loss 0.15025
wandb:                    val/mae 0.00878
wandb:                   val/mape 198708937.5
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.0458
wandb:                   val/rmse 0.01203
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/45rxe3gc
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_221140-45rxe3gc/logs
Completed: NASDAQ H=10

Training: Informer on NASDAQ for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_221602-97patmlc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/97patmlc
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NASDAQ_H22 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/97patmlc
>>>>>>>start training : long_term_forecast_Informer_NASDAQ_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
Overriding target from 'OT' to 'close' for stock data
val 218
Overriding target from 'OT' to 'close' for stock data
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.3129992 Vali Loss: 0.2213912 Test Loss: 0.1794226
Validation loss decreased (inf --> 0.221391).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3129992275075479, 'val/loss': 0.2213912159204483, 'test/loss': 0.17942258822066443, '_timestamp': 1762892192.5076632}).
Epoch: 2, Steps: 132 | Train Loss: 0.2618070 Vali Loss: 0.1762828 Test Loss: 0.1581532
Validation loss decreased (0.221391 --> 0.176283).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26180700867465045, 'val/loss': 0.1762828060558864, 'test/loss': 0.15815318482262747, '_timestamp': 1762892204.641294}).
Epoch: 3, Steps: 132 | Train Loss: 0.2489336 Vali Loss: 0.1608659 Test Loss: 0.1495761
Validation loss decreased (0.176283 --> 0.160866).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2447930 Vali Loss: 0.1611300 Test Loss: 0.1385236
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2415089 Vali Loss: 0.1615687 Test Loss: 0.1500198
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2397753 Vali Loss: 0.1595087 Test Loss: 0.1453496
Validation loss decreased (0.160866 --> 0.159509).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2388158 Vali Loss: 0.1589798 Test Loss: 0.1415134
Validation loss decreased (0.159509 --> 0.158980).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2385549 Vali Loss: 0.1600063 Test Loss: 0.1445506
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2380958 Vali Loss: 0.1595629 Test Loss: 0.1460730
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2381599 Vali Loss: 0.1622718 Test Loss: 0.1443216
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2383451 Vali Loss: 0.1585845 Test Loss: 0.1428986
Validation loss decreased (0.158980 --> 0.158584).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2379937 Vali Loss: 0.1606481 Test Loss: 0.1441040
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2379852 Vali Loss: 0.1593678 Test Loss: 0.1441231
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2381774 Vali Loss: 0.1604343 Test Loss: 0.1432236
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2385386 Vali Loss: 0.1592085 Test Loss: 0.1458127
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2379270 Vali Loss: 0.1603424 Test Loss: 0.1413900
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2379238 Vali Loss: 0.1610212 Test Loss: 0.1403871
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2376762 Vali Loss: 0.1595821 Test Loss: 0.1444021
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2379646 Vali Loss: 0.1596777 Test Loss: 0.1453303
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2381216 Vali Loss: 0.1601874 Test Loss: 0.1448835
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2383045 Vali Loss: 0.1593298 Test Loss: 0.1461294
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NASDAQ_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.00015272072050720453, mae:0.009097772650420666, rmse:0.012358022853732109, r2:-0.09144318103790283, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0091, RMSE: 0.0124, RÂ²: -0.0914, MAPE: 2685120.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.637 MB of 0.638 MB uploadedwandb: \ 0.637 MB of 0.638 MB uploadedwandb: | 0.637 MB of 0.638 MB uploadedwandb: / 0.638 MB of 0.638 MB uploadedwandb: - 0.638 MB of 0.638 MB uploadedwandb: \ 0.638 MB of 0.927 MB uploadedwandb: | 0.927 MB of 0.927 MB uploadedwandb: / 0.927 MB of 0.927 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–ˆâ–…â–ƒâ–…â–†â–…â–„â–„â–„â–„â–…â–ƒâ–‚â–…â–…â–…â–†
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–†â–‡â–ƒâ–‚â–„â–ƒâ–ˆâ–â–…â–‚â–…â–‚â–„â–†â–ƒâ–ƒâ–„â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.14613
wandb:                 train/loss 0.2383
wandb:   val/directional_accuracy 51.3543
wandb:                   val/loss 0.15933
wandb:                    val/mae 0.0091
wandb:                   val/mape 268512050.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.09144
wandb:                   val/rmse 0.01236
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/97patmlc
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_221602-97patmlc/logs
Completed: NASDAQ H=22

Training: Informer on NASDAQ for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_222113-14l6a7zy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/14l6a7zy
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NASDAQ_H50 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/14l6a7zy
>>>>>>>start training : long_term_forecast_Informer_NASDAQ_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
Overriding target from 'OT' to 'close' for stock data
val 190
Overriding target from 'OT' to 'close' for stock data
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.3185100 Vali Loss: 0.2281539 Test Loss: 0.2150934
Validation loss decreased (inf --> 0.228154).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3185099949213592, 'val/loss': 0.22815388441085815, 'test/loss': 0.21509340157111487, '_timestamp': 1762892503.4350636}).
Epoch: 2, Steps: 132 | Train Loss: 0.2662575 Vali Loss: 0.1847209 Test Loss: 0.1690681
Validation loss decreased (0.228154 --> 0.184721).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2547648 Vali Loss: 0.2101880 Test Loss: 0.1525653
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2662574991470937, 'val/loss': 0.1847208688656489, 'test/loss': 0.16906810676058134, '_timestamp': 1762892513.784459}).
Epoch: 4, Steps: 132 | Train Loss: 0.2495041 Vali Loss: 0.2013926 Test Loss: 0.1596282
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2441768 Vali Loss: 0.1859103 Test Loss: 0.1597997
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2440260 Vali Loss: 0.1844756 Test Loss: 0.1615782
Validation loss decreased (0.184721 --> 0.184476).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2429785 Vali Loss: 0.1905314 Test Loss: 0.1585504
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2420644 Vali Loss: 0.1778531 Test Loss: 0.1709664
Validation loss decreased (0.184476 --> 0.177853).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2438005 Vali Loss: 0.1794590 Test Loss: 0.1682223
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2408008 Vali Loss: 0.1804473 Test Loss: 0.1667014
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2413543 Vali Loss: 0.1775290 Test Loss: 0.1679222
Validation loss decreased (0.177853 --> 0.177529).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2430310 Vali Loss: 0.1990568 Test Loss: 0.1594012
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2415970 Vali Loss: 0.1865535 Test Loss: 0.1620700
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2431498 Vali Loss: 0.1939628 Test Loss: 0.1600098
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2435975 Vali Loss: 0.1923012 Test Loss: 0.1589020
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2408345 Vali Loss: 0.1801023 Test Loss: 0.1650682
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2430551 Vali Loss: 0.1894125 Test Loss: 0.1619254
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2452220 Vali Loss: 0.1877686 Test Loss: 0.1637778
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2413148 Vali Loss: 0.1798885 Test Loss: 0.1660489
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2416544 Vali Loss: 0.1786307 Test Loss: 0.1696529
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2409970 Vali Loss: 0.1793566 Test Loss: 0.1656642
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NASDAQ_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.00016087321273516864, mae:0.009287681430578232, rmse:0.012683580629527569, r2:-0.11065280437469482, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0093, RMSE: 0.0127, RÂ²: -0.1107, MAPE: 3332407.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.701 MB of 0.703 MB uploadedwandb: \ 0.703 MB of 0.703 MB uploadedwandb: | 0.703 MB of 0.703 MB uploadedwandb: / 0.703 MB of 0.992 MB uploadedwandb: - 0.992 MB of 0.992 MB uploadedwandb: \ 0.992 MB of 0.992 MB uploadedwandb: | 0.992 MB of 0.992 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–„â–„â–„â–ƒâ–ˆâ–‡â–†â–‡â–„â–…â–„â–ƒâ–†â–…â–…â–†â–ˆâ–†
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–ƒâ–â–â–‚â–â–‚â–‚â–â–‚â–ƒâ–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–ƒâ–‚â–„â–â–â–‚â–â–†â–ƒâ–…â–„â–‚â–„â–ƒâ–‚â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.16566
wandb:                 train/loss 0.241
wandb:   val/directional_accuracy 50.63373
wandb:                   val/loss 0.17936
wandb:                    val/mae 0.00929
wandb:                   val/mape 333240700.0
wandb:                    val/mse 0.00016
wandb:                     val/r2 -0.11065
wandb:                   val/rmse 0.01268
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/14l6a7zy
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_222113-14l6a7zy/logs
Completed: NASDAQ H=50

Training: Informer on NASDAQ for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_222622-j5np5blx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/j5np5blx
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NASDAQ_H100Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/j5np5blx
>>>>>>>start training : long_term_forecast_Informer_NASDAQ_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
Overriding target from 'OT' to 'close' for stock data
val 140
Overriding target from 'OT' to 'close' for stock data
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.3241422 Vali Loss: 0.3725828 Test Loss: 0.2886428
Validation loss decreased (inf --> 0.372583).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.324142246062939, 'val/loss': 0.37258278727531435, 'test/loss': 0.2886428415775299, '_timestamp': 1762892809.7301905}).
Epoch: 2, Steps: 130 | Train Loss: 0.2673918 Vali Loss: 0.3429027 Test Loss: 0.3179494
Validation loss decreased (0.372583 --> 0.342903).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2673918375602135, 'val/loss': 0.3429027199745178, 'test/loss': 0.31794939041137693, '_timestamp': 1762892822.436924}).
Epoch: 3, Steps: 130 | Train Loss: 0.2540364 Vali Loss: 0.3615100 Test Loss: 0.3461128
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2485585 Vali Loss: 0.3017842 Test Loss: 0.3413272
Validation loss decreased (0.342903 --> 0.301784).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2453818 Vali Loss: 0.3069612 Test Loss: 0.4199531
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2441764 Vali Loss: 0.2966392 Test Loss: 0.3742879
Validation loss decreased (0.301784 --> 0.296639).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2438939 Vali Loss: 0.2992055 Test Loss: 0.3880313
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2432836 Vali Loss: 0.2989686 Test Loss: 0.3908074
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2430794 Vali Loss: 0.2987978 Test Loss: 0.3919598
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2434334 Vali Loss: 0.2963323 Test Loss: 0.3866476
Validation loss decreased (0.296639 --> 0.296332).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2432619 Vali Loss: 0.3026733 Test Loss: 0.3970619
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2424878 Vali Loss: 0.2989491 Test Loss: 0.4050438
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2428316 Vali Loss: 0.2963631 Test Loss: 0.3871646
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.2432038 Vali Loss: 0.2992741 Test Loss: 0.3875040
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.2426095 Vali Loss: 0.2982504 Test Loss: 0.3883775
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 130 | Train Loss: 0.2427500 Vali Loss: 0.3024502 Test Loss: 0.3880799
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 130 | Train Loss: 0.2428884 Vali Loss: 0.3039979 Test Loss: 0.3927994
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 130 | Train Loss: 0.2429898 Vali Loss: 0.3006746 Test Loss: 0.3919361
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 130 | Train Loss: 0.2427105 Vali Loss: 0.3043824 Test Loss: 0.4120401
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 130 | Train Loss: 0.2433599 Vali Loss: 0.2989320 Test Loss: 0.3954442
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NASDAQ_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.00015993912529665977, mae:0.008906099013984203, rmse:0.012646703980863094, r2:-0.054492950439453125, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0089, RMSE: 0.0126, RÂ²: -0.0545, MAPE: 1849303.62%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.750 MB of 0.755 MB uploadedwandb: \ 0.750 MB of 0.755 MB uploadedwandb: | 0.755 MB of 0.755 MB uploadedwandb: / 0.755 MB of 0.755 MB uploadedwandb: - 0.755 MB of 1.044 MB uploadedwandb: \ 1.044 MB of 1.044 MB uploadedwandb: | 1.044 MB of 1.044 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–â–ˆâ–„â–…â–…â–†â–…â–†â–‡â–…â–…â–…â–…â–†â–†â–‡â–†
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–‚â–‚â–â–‚â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.39544
wandb:                 train/loss 0.24336
wandb:   val/directional_accuracy 49.72583
wandb:                   val/loss 0.29893
wandb:                    val/mae 0.00891
wandb:                   val/mape 184930362.5
wandb:                    val/mse 0.00016
wandb:                     val/r2 -0.05449
wandb:                   val/rmse 0.01265
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/j5np5blx
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_222622-j5np5blx/logs
Completed: NASDAQ H=100

Training: Informer on ABSA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_223137-kaf6kzza
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/kaf6kzza
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ABSA_H3    Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/kaf6kzza
>>>>>>>start training : long_term_forecast_Informer_ABSA_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 3
============================================================
train 4240
Overriding target from 'OT' to 'close' for stock data
val 237
Overriding target from 'OT' to 'close' for stock data
test 238
Epoch: 1, Steps: 133 | Train Loss: 0.3539383 Vali Loss: 0.1936444 Test Loss: 0.1837414
Validation loss decreased (inf --> 0.193644).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3539382762702784, 'val/loss': 0.1936444230377674, 'test/loss': 0.18374137207865715, '_timestamp': 1762893126.378558}).
Epoch: 2, Steps: 133 | Train Loss: 0.2909369 Vali Loss: 0.2065696 Test Loss: 0.1989945
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2771263 Vali Loss: 0.1739622 Test Loss: 0.1631626
Validation loss decreased (0.193644 --> 0.173962).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.29093685947862785, 'val/loss': 0.2065695971250534, 'test/loss': 0.19899445958435535, '_timestamp': 1762893136.9670138}).
Epoch: 4, Steps: 133 | Train Loss: 0.2702388 Vali Loss: 0.1633305 Test Loss: 0.1542807
Validation loss decreased (0.173962 --> 0.163331).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2655982 Vali Loss: 0.1638710 Test Loss: 0.1560257
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2649166 Vali Loss: 0.1614568 Test Loss: 0.1536585
Validation loss decreased (0.163331 --> 0.161457).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2625029 Vali Loss: 0.1652499 Test Loss: 0.1567703
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2610041 Vali Loss: 0.1590892 Test Loss: 0.1554601
Validation loss decreased (0.161457 --> 0.159089).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2621742 Vali Loss: 0.1650916 Test Loss: 0.1549118
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2632973 Vali Loss: 0.1614562 Test Loss: 0.1552742
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2610906 Vali Loss: 0.1612305 Test Loss: 0.1542121
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2602014 Vali Loss: 0.1602033 Test Loss: 0.1545925
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2629169 Vali Loss: 0.1624794 Test Loss: 0.1545978
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2598828 Vali Loss: 0.1637809 Test Loss: 0.1544237
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2630988 Vali Loss: 0.1678565 Test Loss: 0.1555094
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2611495 Vali Loss: 0.1657393 Test Loss: 0.1548591
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2616668 Vali Loss: 0.1612053 Test Loss: 0.1558276
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2617200 Vali Loss: 0.1602326 Test Loss: 0.1551681
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ABSA_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 238
test shape: (238, 3, 1) (238, 3, 1)
test shape: (238, 3, 1) (238, 3, 1)


	mse:0.000464273092802614, mae:0.016408996656537056, rmse:0.021546997129917145, r2:-0.01957523822784424, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0164, RMSE: 0.0215, RÂ²: -0.0196, MAPE: 1.16%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.502 MB of 0.502 MB uploadedwandb: \ 0.502 MB of 0.502 MB uploadedwandb: | 0.502 MB of 0.502 MB uploadedwandb: / 0.502 MB of 0.502 MB uploadedwandb: - 0.502 MB of 0.502 MB uploadedwandb: \ 0.502 MB of 0.502 MB uploadedwandb: | 0.502 MB of 0.502 MB uploadedwandb: / 0.502 MB of 0.502 MB uploadedwandb: - 0.502 MB of 0.502 MB uploadedwandb: \ 0.684 MB of 0.971 MB uploaded (0.002 MB deduped)wandb: | 0.971 MB of 0.971 MB uploaded (0.002 MB deduped)wandb: / 0.971 MB of 0.971 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–ƒâ–â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–â–‚â–‚â–â–â–‚â–â–‚â–‚â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–ƒâ–‚â–„â–â–„â–‚â–‚â–‚â–ƒâ–ƒâ–…â–„â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 17
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.15517
wandb:                 train/loss 0.26172
wandb:   val/directional_accuracy 43.90756
wandb:                   val/loss 0.16023
wandb:                    val/mae 0.01641
wandb:                   val/mape 116.38179
wandb:                    val/mse 0.00046
wandb:                     val/r2 -0.01958
wandb:                   val/rmse 0.02155
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/kaf6kzza
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_223137-kaf6kzza/logs
Completed: ABSA H=3

Training: Informer on ABSA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_223658-30azwx6a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/30azwx6a
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ABSA_H5    Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/30azwx6a
>>>>>>>start training : long_term_forecast_Informer_ABSA_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 5
============================================================
train 4238
Overriding target from 'OT' to 'close' for stock data
val 235
Overriding target from 'OT' to 'close' for stock data
test 236
Epoch: 1, Steps: 133 | Train Loss: 0.3629193 Vali Loss: 0.1779332 Test Loss: 0.1662829
Validation loss decreased (inf --> 0.177933).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3629192640458731, 'val/loss': 0.17793321795761585, 'test/loss': 0.16628288757055998, '_timestamp': 1762893449.3873246}).
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
Epoch: 2, Steps: 133 | Train Loss: 0.2964567 Vali Loss: 0.1784384 Test Loss: 0.1661164
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2833556 Vali Loss: 0.1703058 Test Loss: 0.1609304
Validation loss decreased (0.177933 --> 0.170306).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2964566597588977, 'val/loss': 0.17843837663531303, 'test/loss': 0.1661163531243801, '_timestamp': 1762893457.0548322}).
Epoch: 4, Steps: 133 | Train Loss: 0.2767126 Vali Loss: 0.1711163 Test Loss: 0.1687300
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2721121 Vali Loss: 0.1674136 Test Loss: 0.1613185
Validation loss decreased (0.170306 --> 0.167414).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2702326 Vali Loss: 0.1647971 Test Loss: 0.1606620
Validation loss decreased (0.167414 --> 0.164797).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2689917 Vali Loss: 0.1678053 Test Loss: 0.1599375
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2691309 Vali Loss: 0.1732335 Test Loss: 0.1601852
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2679752 Vali Loss: 0.1710485 Test Loss: 0.1604015
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2683798 Vali Loss: 0.1660362 Test Loss: 0.1596463
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2679023 Vali Loss: 0.1645554 Test Loss: 0.1601682
Validation loss decreased (0.164797 --> 0.164555).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2674347 Vali Loss: 0.1668984 Test Loss: 0.1613585
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2703283 Vali Loss: 0.1594334 Test Loss: 0.1595289
Validation loss decreased (0.164555 --> 0.159433).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2685282 Vali Loss: 0.1672795 Test Loss: 0.1609208
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2671238 Vali Loss: 0.1663666 Test Loss: 0.1595393
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2670391 Vali Loss: 0.1642653 Test Loss: 0.1603544
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2676489 Vali Loss: 0.1639430 Test Loss: 0.1598055
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2677260 Vali Loss: 0.1654104 Test Loss: 0.1601434
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2682392 Vali Loss: 0.1646079 Test Loss: 0.1604660
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2690249 Vali Loss: 0.1643433 Test Loss: 0.1599803
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2696546 Vali Loss: 0.1636282 Test Loss: 0.1603802
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2685268 Vali Loss: 0.1648696 Test Loss: 0.1602789
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2678988 Vali Loss: 0.1630506 Test Loss: 0.1615911
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ABSA_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 236
test shape: (236, 5, 1) (236, 5, 1)
test shape: (236, 5, 1) (236, 5, 1)


	mse:0.0004637132224161178, mae:0.016373084858059883, rmse:0.02153400145471096, r2:-0.012424826622009277, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0164, RMSE: 0.0215, RÂ²: -0.0124, MAPE: 1.32%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.530 MB of 0.530 MB uploadedwandb: \ 0.530 MB of 0.530 MB uploadedwandb: | 0.530 MB of 0.530 MB uploadedwandb: / 0.530 MB of 0.530 MB uploadedwandb: - 0.530 MB of 0.530 MB uploadedwandb: \ 0.530 MB of 0.819 MB uploadedwandb: | 0.819 MB of 0.819 MB uploadedwandb: / 0.819 MB of 0.819 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‚â–ˆâ–‚â–‚â–â–â–‚â–â–â–‚â–â–‚â–â–‚â–â–â–‚â–â–‚â–‚â–ƒ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–‚â–‚â–â–â–â–â–‚â–‚â–‚â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–‡â–…â–„â–…â–ˆâ–‡â–„â–„â–…â–â–…â–…â–ƒâ–ƒâ–„â–„â–ƒâ–ƒâ–„â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.16159
wandb:                 train/loss 0.2679
wandb:   val/directional_accuracy 50.52966
wandb:                   val/loss 0.16305
wandb:                    val/mae 0.01637
wandb:                   val/mape 131.64139
wandb:                    val/mse 0.00046
wandb:                     val/r2 -0.01242
wandb:                   val/rmse 0.02153
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/30azwx6a
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_223658-30azwx6a/logs
Completed: ABSA H=5

Training: Informer on ABSA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_224131-1ijw4dfr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/1ijw4dfr
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ABSA_H10   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/1ijw4dfr
>>>>>>>start training : long_term_forecast_Informer_ABSA_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 10
============================================================
train 4233
Overriding target from 'OT' to 'close' for stock data
val 230
Overriding target from 'OT' to 'close' for stock data
test 231
Epoch: 1, Steps: 133 | Train Loss: 0.3641435 Vali Loss: 0.1827920 Test Loss: 0.1712601
Validation loss decreased (inf --> 0.182792).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.3006190 Vali Loss: 0.1849914 Test Loss: 0.1840802
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.36414348070782826, 'val/loss': 0.18279197439551353, 'test/loss': 0.17126006539911032, '_timestamp': 1762893716.0607743}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3006189635821751, 'val/loss': 0.18499142117798328, 'test/loss': 0.18408018723130226, '_timestamp': 1762893723.3725936}).
Epoch: 3, Steps: 133 | Train Loss: 0.2876375 Vali Loss: 0.1694513 Test Loss: 0.1648121
Validation loss decreased (0.182792 --> 0.169451).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2793056 Vali Loss: 0.1772806 Test Loss: 0.1692831
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2759416 Vali Loss: 0.1707348 Test Loss: 0.1700309
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2743197 Vali Loss: 0.1699904 Test Loss: 0.1694060
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2713688 Vali Loss: 0.1689891 Test Loss: 0.1692933
Validation loss decreased (0.169451 --> 0.168989).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2705150 Vali Loss: 0.1744096 Test Loss: 0.1686959
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2707834 Vali Loss: 0.1758565 Test Loss: 0.1703556
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2705388 Vali Loss: 0.1733181 Test Loss: 0.1687872
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2693950 Vali Loss: 0.1670161 Test Loss: 0.1677975
Validation loss decreased (0.168989 --> 0.167016).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2708564 Vali Loss: 0.1690476 Test Loss: 0.1699482
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2710988 Vali Loss: 0.1681453 Test Loss: 0.1685421
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2702833 Vali Loss: 0.1749414 Test Loss: 0.1686026
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2702600 Vali Loss: 0.1656822 Test Loss: 0.1677544
Validation loss decreased (0.167016 --> 0.165682).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2700297 Vali Loss: 0.1691431 Test Loss: 0.1679469
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2697547 Vali Loss: 0.1726583 Test Loss: 0.1680460
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2693870 Vali Loss: 0.1722761 Test Loss: 0.1685221
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2695364 Vali Loss: 0.1737763 Test Loss: 0.1695418
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2706414 Vali Loss: 0.1670905 Test Loss: 0.1685175
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2693841 Vali Loss: 0.1706513 Test Loss: 0.1689696
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2694953 Vali Loss: 0.1737344 Test Loss: 0.1692602
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2692508 Vali Loss: 0.1696076 Test Loss: 0.1689711
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2707676 Vali Loss: 0.1688933 Test Loss: 0.1682090
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2690974 Vali Loss: 0.1714764 Test Loss: 0.1686757
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ABSA_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 231
test shape: (231, 10, 1) (231, 10, 1)
test shape: (231, 10, 1) (231, 10, 1)


	mse:0.00046720996033400297, mae:0.016429709270596504, rmse:0.02161503955721855, r2:-0.011986732482910156, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0164, RMSE: 0.0216, RÂ²: -0.0120, MAPE: 1.20%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.556 MB of 0.557 MB uploadedwandb: \ 0.556 MB of 0.557 MB uploadedwandb: | 0.556 MB of 0.557 MB uploadedwandb: / 0.556 MB of 0.557 MB uploadedwandb: - 0.557 MB of 0.557 MB uploadedwandb: \ 0.557 MB of 0.846 MB uploadedwandb: | 0.846 MB of 0.846 MB uploadedwandb: / 0.846 MB of 0.846 MB uploadedwandb: - 0.846 MB of 0.846 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–‡â–ˆâ–‡â–‡â–†â–ˆâ–†â–…â–‡â–†â–†â–…â–…â–…â–†â–‡â–†â–†â–‡â–†â–…â–†
wandb:                 train/loss â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–ˆâ–„â–„â–ƒâ–†â–‡â–†â–‚â–ƒâ–‚â–‡â–â–ƒâ–…â–…â–†â–‚â–„â–†â–ƒâ–ƒâ–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 24
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.16868
wandb:                 train/loss 0.2691
wandb:   val/directional_accuracy 50.79365
wandb:                   val/loss 0.17148
wandb:                    val/mae 0.01643
wandb:                   val/mape 120.11245
wandb:                    val/mse 0.00047
wandb:                     val/r2 -0.01199
wandb:                   val/rmse 0.02162
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/1ijw4dfr
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_224131-1ijw4dfr/logs
Completed: ABSA H=10

Training: Informer on ABSA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_224600-fayhccxf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/fayhccxf
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ABSA_H22   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/fayhccxf
>>>>>>>start training : long_term_forecast_Informer_ABSA_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 22
============================================================
train 4221
Overriding target from 'OT' to 'close' for stock data
val 218
Overriding target from 'OT' to 'close' for stock data
test 219
Epoch: 1, Steps: 132 | Train Loss: 0.3722007 Vali Loss: 0.2062867 Test Loss: 0.1802008
Validation loss decreased (inf --> 0.206287).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3722006937093807, 'val/loss': 0.20628670922347478, 'test/loss': 0.18020075772489821, '_timestamp': 1762893990.2005575}).
Epoch: 2, Steps: 132 | Train Loss: 0.3104312 Vali Loss: 0.2014303 Test Loss: 0.1781225
Validation loss decreased (0.206287 --> 0.201430).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2935772 Vali Loss: 0.1888596 Test Loss: 0.1696176
Validation loss decreased (0.201430 --> 0.188860).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.310431188367533, 'val/loss': 0.20143031222479685, 'test/loss': 0.1781224895800863, '_timestamp': 1762894000.631397}).
Epoch: 4, Steps: 132 | Train Loss: 0.2845776 Vali Loss: 0.1838175 Test Loss: 0.1657704
Validation loss decreased (0.188860 --> 0.183817).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2795378 Vali Loss: 0.1825001 Test Loss: 0.1638036
Validation loss decreased (0.183817 --> 0.182500).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2761457 Vali Loss: 0.1812624 Test Loss: 0.1621922
Validation loss decreased (0.182500 --> 0.181262).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2743131 Vali Loss: 0.1792931 Test Loss: 0.1629537
Validation loss decreased (0.181262 --> 0.179293).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2736700 Vali Loss: 0.1807906 Test Loss: 0.1619335
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2732236 Vali Loss: 0.1802347 Test Loss: 0.1628974
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2731500 Vali Loss: 0.1800503 Test Loss: 0.1625111
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2729556 Vali Loss: 0.1802104 Test Loss: 0.1629772
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2729172 Vali Loss: 0.1800384 Test Loss: 0.1628179
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2731190 Vali Loss: 0.1796474 Test Loss: 0.1637700
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2726035 Vali Loss: 0.1789108 Test Loss: 0.1624201
Validation loss decreased (0.179293 --> 0.178911).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2728048 Vali Loss: 0.1797941 Test Loss: 0.1633209
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2726172 Vali Loss: 0.1797490 Test Loss: 0.1621875
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2731508 Vali Loss: 0.1796381 Test Loss: 0.1625390
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2726449 Vali Loss: 0.1794677 Test Loss: 0.1633396
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2727857 Vali Loss: 0.1803454 Test Loss: 0.1628724
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2725241 Vali Loss: 0.1799566 Test Loss: 0.1630447
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2732478 Vali Loss: 0.1797037 Test Loss: 0.1634804
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2728912 Vali Loss: 0.1800371 Test Loss: 0.1623078
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.2730405 Vali Loss: 0.1796974 Test Loss: 0.1630591
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 132 | Train Loss: 0.2726673 Vali Loss: 0.1792041 Test Loss: 0.1624242
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ABSA_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 219
test shape: (219, 22, 1) (219, 22, 1)
test shape: (219, 22, 1) (219, 22, 1)


	mse:0.00047580956015735865, mae:0.01656239666044712, rmse:0.021813059225678444, r2:-0.015555620193481445, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0166, RMSE: 0.0218, RÂ²: -0.0156, MAPE: 1.33%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.622 MB of 0.623 MB uploadedwandb: \ 0.622 MB of 0.623 MB uploadedwandb: | 0.623 MB of 0.623 MB uploadedwandb: / 0.623 MB of 0.912 MB uploadedwandb: - 0.623 MB of 0.912 MB uploadedwandb: \ 0.912 MB of 0.912 MB uploadedwandb: | 0.912 MB of 0.912 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–ƒâ–â–‚â–â–‚â–‚â–‚â–‚â–ƒâ–â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–‚â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–„â–ƒâ–â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 23
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.16242
wandb:                 train/loss 0.27267
wandb:   val/directional_accuracy 51.22853
wandb:                   val/loss 0.1792
wandb:                    val/mae 0.01656
wandb:                   val/mape 132.74983
wandb:                    val/mse 0.00048
wandb:                     val/r2 -0.01556
wandb:                   val/rmse 0.02181
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/fayhccxf
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_224600-fayhccxf/logs
Completed: ABSA H=22

Training: Informer on ABSA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_225019-tzmkqvd8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/tzmkqvd8
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ABSA_H50   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/tzmkqvd8
>>>>>>>start training : long_term_forecast_Informer_ABSA_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 50
============================================================
train 4193
Overriding target from 'OT' to 'close' for stock data
val 190
Overriding target from 'OT' to 'close' for stock data
test 191
Epoch: 1, Steps: 132 | Train Loss: 0.3892307 Vali Loss: 0.2189373 Test Loss: 0.1820902
Validation loss decreased (inf --> 0.218937).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.38923073277780507, 'val/loss': 0.21893726040919623, 'test/loss': 0.18209024518728256, '_timestamp': 1762894244.6670656}).
Epoch: 2, Steps: 132 | Train Loss: 0.3193575 Vali Loss: 0.1845378 Test Loss: 0.1636275
Validation loss decreased (0.218937 --> 0.184538).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.3063432 Vali Loss: 0.2033508 Test Loss: 0.1644157
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.31935749101367866, 'val/loss': 0.18453783045212427, 'test/loss': 0.16362754379709563, '_timestamp': 1762894255.5654092}).
Epoch: 4, Steps: 132 | Train Loss: 0.2976300 Vali Loss: 0.1859319 Test Loss: 0.1582933
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2901955 Vali Loss: 0.1836521 Test Loss: 0.1593617
Validation loss decreased (0.184538 --> 0.183652).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2883222 Vali Loss: 0.1860588 Test Loss: 0.1599101
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2870965 Vali Loss: 0.1882801 Test Loss: 0.1603887
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2853961 Vali Loss: 0.1862451 Test Loss: 0.1624081
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2865318 Vali Loss: 0.1907604 Test Loss: 0.1660228
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2840260 Vali Loss: 0.1895164 Test Loss: 0.1613162
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2843872 Vali Loss: 0.1851402 Test Loss: 0.1591000
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2843723 Vali Loss: 0.1849438 Test Loss: 0.1601395
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2863984 Vali Loss: 0.1811365 Test Loss: 0.1605772
Validation loss decreased (0.183652 --> 0.181136).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2858895 Vali Loss: 0.1838351 Test Loss: 0.1608827
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2861926 Vali Loss: 0.1839174 Test Loss: 0.1604138
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2842070 Vali Loss: 0.1867807 Test Loss: 0.1606515
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2854812 Vali Loss: 0.1831752 Test Loss: 0.1601467
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2861123 Vali Loss: 0.1868420 Test Loss: 0.1625593
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2852973 Vali Loss: 0.1886473 Test Loss: 0.1622720
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2869396 Vali Loss: 0.1861786 Test Loss: 0.1645729
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2844298 Vali Loss: 0.1847690 Test Loss: 0.1590266
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2839623 Vali Loss: 0.1871012 Test Loss: 0.1613973
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.2880429 Vali Loss: 0.1878560 Test Loss: 0.1613114
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ABSA_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 191
test shape: (191, 50, 1) (191, 50, 1)
test shape: (191, 50, 1) (191, 50, 1)


	mse:0.0004895641468465328, mae:0.016930822283029556, rmse:0.022126097232103348, r2:-0.006319165229797363, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0169, RMSE: 0.0221, RÂ²: -0.0063, MAPE: 1.09%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.703 MB of 0.705 MB uploadedwandb: \ 0.703 MB of 0.705 MB uploadedwandb: | 0.705 MB of 0.705 MB uploadedwandb: / 0.705 MB of 0.705 MB uploadedwandb: - 0.705 MB of 0.994 MB uploadedwandb: \ 0.991 MB of 0.994 MB uploadedwandb: | 0.994 MB of 0.994 MB uploadedwandb: / 0.994 MB of 0.994 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‡â–â–‚â–‚â–ƒâ–…â–ˆâ–„â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–…â–…â–‡â–‚â–„â–„
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–‚â–â–â–â–‚â–‚â–‚â–â–â–‚â–â–‚â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–„â–„â–‚â–‚â–â–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.16131
wandb:                 train/loss 0.28804
wandb:   val/directional_accuracy 49.86644
wandb:                   val/loss 0.18786
wandb:                    val/mae 0.01693
wandb:                   val/mape 109.04415
wandb:                    val/mse 0.00049
wandb:                     val/r2 -0.00632
wandb:                   val/rmse 0.02213
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/tzmkqvd8
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_225019-tzmkqvd8/logs
Completed: ABSA H=50

Training: Informer on ABSA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_225422-6ejypyk2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/6ejypyk2
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ABSA_H100  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/6ejypyk2
>>>>>>>start training : long_term_forecast_Informer_ABSA_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 100
============================================================
train 4143
Overriding target from 'OT' to 'close' for stock data
val 140
Overriding target from 'OT' to 'close' for stock data
test 141
Epoch: 1, Steps: 130 | Train Loss: 0.4022575 Vali Loss: 0.2560111 Test Loss: 0.2264359
Validation loss decreased (inf --> 0.256011).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.4022575162924253, 'val/loss': 0.256011089682579, 'test/loss': 0.22643593251705169, '_timestamp': 1762894486.5905435}).
Epoch: 2, Steps: 130 | Train Loss: 0.3284642 Vali Loss: 0.2273065 Test Loss: 0.1824503
Validation loss decreased (0.256011 --> 0.227306).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.3106466 Vali Loss: 0.2537942 Test Loss: 0.2061190
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.32846420235358753, 'val/loss': 0.22730645537376404, 'test/loss': 0.18245026469230652, '_timestamp': 1762894496.8941784}).
Epoch: 4, Steps: 130 | Train Loss: 0.3038838 Vali Loss: 0.2238481 Test Loss: 0.1786213
Validation loss decreased (0.227306 --> 0.223848).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2997098 Vali Loss: 0.2249859 Test Loss: 0.1795276
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2974678 Vali Loss: 0.2220589 Test Loss: 0.1790400
Validation loss decreased (0.223848 --> 0.222059).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2965222 Vali Loss: 0.2459112 Test Loss: 0.1929798
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2963604 Vali Loss: 0.2431995 Test Loss: 0.1930853
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2955009 Vali Loss: 0.2372974 Test Loss: 0.1899166
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2959280 Vali Loss: 0.2362656 Test Loss: 0.1865323
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2954880 Vali Loss: 0.2362070 Test Loss: 0.1892353
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2947252 Vali Loss: 0.2473263 Test Loss: 0.1914613
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2948328 Vali Loss: 0.2403060 Test Loss: 0.1886445
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.2950770 Vali Loss: 0.2352709 Test Loss: 0.1878932
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.2945305 Vali Loss: 0.2394699 Test Loss: 0.1880282
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 130 | Train Loss: 0.2954370 Vali Loss: 0.2408900 Test Loss: 0.1905170
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ABSA_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 141
test shape: (141, 100, 1) (141, 100, 1)
test shape: (141, 100, 1) (141, 100, 1)


	mse:0.0005178855499252677, mae:0.01733848638832569, rmse:0.022757098078727722, r2:-0.003588438034057617, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0173, RMSE: 0.0228, RÂ²: -0.0036, MAPE: 1.03%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.791 MB of 0.796 MB uploadedwandb: \ 0.796 MB of 0.796 MB uploadedwandb: | 0.796 MB of 0.796 MB uploadedwandb: / 0.796 MB of 0.796 MB uploadedwandb: - 0.796 MB of 0.796 MB uploadedwandb: \ 0.796 MB of 1.084 MB uploadedwandb: | 1.084 MB of 1.084 MB uploadedwandb: / 1.084 MB of 1.084 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–â–â–…â–…â–„â–ƒâ–„â–„â–„â–ƒâ–ƒâ–„
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–‚â–â–†â–†â–„â–„â–„â–‡â–…â–„â–…â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.19052
wandb:                 train/loss 0.29544
wandb:   val/directional_accuracy 50.24715
wandb:                   val/loss 0.24089
wandb:                    val/mae 0.01734
wandb:                   val/mape 102.99274
wandb:                    val/mse 0.00052
wandb:                     val/r2 -0.00359
wandb:                   val/rmse 0.02276
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/6ejypyk2
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_225422-6ejypyk2/logs
Completed: ABSA H=100

Training: Informer on SASOL for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_225756-b7m3s1lx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/b7m3s1lx
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SASOL_H3   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/b7m3s1lx
>>>>>>>start training : long_term_forecast_Informer_SASOL_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 3
============================================================
train 3772
Overriding target from 'OT' to 'close' for stock data
val 211
Overriding target from 'OT' to 'close' for stock data
test 212
Epoch: 1, Steps: 118 | Train Loss: 0.2684689 Vali Loss: 0.1163470 Test Loss: 0.1550946
Validation loss decreased (inf --> 0.116347).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.268468919952037, 'val/loss': 0.11634696807180132, 'test/loss': 0.1550945799265589, '_timestamp': 1762894702.6985946}).
Epoch: 2, Steps: 118 | Train Loss: 0.2173302 Vali Loss: 0.1094194 Test Loss: 0.1505769
Validation loss decreased (0.116347 --> 0.109419).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 118 | Train Loss: 0.2049078 Vali Loss: 0.1168938 Test Loss: 0.1728297
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.1997743 Vali Loss: 0.1105035 Test Loss: 0.1533896
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2173302361141827, 'val/loss': 0.10941939055919647, 'test/loss': 0.15057691718850816, '_timestamp': 1762894710.944911}).
Epoch: 5, Steps: 118 | Train Loss: 0.1976161 Vali Loss: 0.1017303 Test Loss: 0.1477092
Validation loss decreased (0.109419 --> 0.101730).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.1958967 Vali Loss: 0.1030424 Test Loss: 0.1490563
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.1953763 Vali Loss: 0.1032800 Test Loss: 0.1488807
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.1943651 Vali Loss: 0.1057311 Test Loss: 0.1497441
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.1945738 Vali Loss: 0.1018676 Test Loss: 0.1474269
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.1948837 Vali Loss: 0.1033960 Test Loss: 0.1479821
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.1944008 Vali Loss: 0.1027910 Test Loss: 0.1473836
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.1943489 Vali Loss: 0.1018624 Test Loss: 0.1475391
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.1947908 Vali Loss: 0.1015773 Test Loss: 0.1462751
Validation loss decreased (0.101730 --> 0.101577).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.1936383 Vali Loss: 0.1050277 Test Loss: 0.1472179
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.1946062 Vali Loss: 0.1015105 Test Loss: 0.1480825
Validation loss decreased (0.101577 --> 0.101510).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.1948892 Vali Loss: 0.1067188 Test Loss: 0.1492130
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.1938296 Vali Loss: 0.1032779 Test Loss: 0.1488713
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.1948719 Vali Loss: 0.1024356 Test Loss: 0.1468933
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.1941937 Vali Loss: 0.1034636 Test Loss: 0.1485417
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.1943336 Vali Loss: 0.1029293 Test Loss: 0.1469680
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 118 | Train Loss: 0.1947544 Vali Loss: 0.1017922 Test Loss: 0.1469737
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 118 | Train Loss: 0.1940685 Vali Loss: 0.1045390 Test Loss: 0.1481140
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 118 | Train Loss: 0.1945382 Vali Loss: 0.1029131 Test Loss: 0.1483258
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 118 | Train Loss: 0.1954240 Vali Loss: 0.1049315 Test Loss: 0.1481465
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 118 | Train Loss: 0.1944808 Vali Loss: 0.1002873 Test Loss: 0.1481789
Validation loss decreased (0.101510 --> 0.100287).  Saving model ...
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 118 | Train Loss: 0.1944704 Vali Loss: 0.1040629 Test Loss: 0.1482228
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 118 | Train Loss: 0.1945703 Vali Loss: 0.1016908 Test Loss: 0.1462412
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 118 | Train Loss: 0.1940652 Vali Loss: 0.1023538 Test Loss: 0.1472119
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 118 | Train Loss: 0.1946585 Vali Loss: 0.1030806 Test Loss: 0.1464835
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 118 | Train Loss: 0.1944599 Vali Loss: 0.1027932 Test Loss: 0.1477028
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 118 | Train Loss: 0.1938822 Vali Loss: 0.1015427 Test Loss: 0.1471092
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 118 | Train Loss: 0.1949926 Vali Loss: 0.1052312 Test Loss: 0.1474099
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 118 | Train Loss: 0.1945985 Vali Loss: 0.1052664 Test Loss: 0.1480655
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 118 | Train Loss: 0.1946929 Vali Loss: 0.1020760 Test Loss: 0.1476941
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1641532182693482e-14
Epoch: 35, Steps: 118 | Train Loss: 0.1946852 Vali Loss: 0.1045120 Test Loss: 0.1470676
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SASOL_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 212
test shape: (212, 3, 1) (212, 3, 1)
test shape: (212, 3, 1) (212, 3, 1)


	mse:0.0023493163753300905, mae:0.03672056645154953, rmse:0.048469748347997665, r2:-0.06641507148742676, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0367, RMSE: 0.0485, RÂ²: -0.0664, MAPE: 22555990.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.466 MB of 0.467 MB uploadedwandb: \ 0.467 MB of 0.467 MB uploadedwandb: | 0.467 MB of 0.467 MB uploadedwandb: / 0.467 MB of 0.467 MB uploadedwandb: - 0.467 MB of 0.467 MB uploadedwandb: \ 0.467 MB of 0.467 MB uploadedwandb: | 0.467 MB of 0.467 MB uploadedwandb: / 0.467 MB of 0.467 MB uploadedwandb: - 0.648 MB of 0.939 MB uploaded (0.002 MB deduped)wandb: \ 0.939 MB of 0.939 MB uploaded (0.002 MB deduped)wandb: | 0.939 MB of 0.939 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–‚â–‚â–‚â–â–â–â–â–â–â–â–‚â–‚â–â–‚â–â–â–â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–‚â–‚â–â–â–‚â–â–‚â–‚â–â–‚â–â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–„â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–â–ƒâ–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 34
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.14707
wandb:                 train/loss 0.19469
wandb:   val/directional_accuracy 47.16981
wandb:                   val/loss 0.10451
wandb:                    val/mae 0.03672
wandb:                   val/mape 2255599000.0
wandb:                    val/mse 0.00235
wandb:                     val/r2 -0.06642
wandb:                   val/rmse 0.04847
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/b7m3s1lx
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_225756-b7m3s1lx/logs
Completed: SASOL H=3

Training: Informer on SASOL for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_230311-rv2sq7ow
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/rv2sq7ow
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SASOL_H5   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/rv2sq7ow
>>>>>>>start training : long_term_forecast_Informer_SASOL_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 5
============================================================
train 3770
Overriding target from 'OT' to 'close' for stock data
val 209
Overriding target from 'OT' to 'close' for stock data
test 210
Epoch: 1, Steps: 118 | Train Loss: 0.2842299 Vali Loss: 0.1135333 Test Loss: 0.1558501
Validation loss decreased (inf --> 0.113533).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.28422990340297505, 'val/loss': 0.11353332017149244, 'test/loss': 0.15585012521062577, '_timestamp': 1762895019.0628874}).
Epoch: 2, Steps: 118 | Train Loss: 0.2202126 Vali Loss: 0.1182009 Test Loss: 0.1818510
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 118 | Train Loss: 0.2083796 Vali Loss: 0.1061902 Test Loss: 0.1560747
Validation loss decreased (0.113533 --> 0.106190).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.22021256513514761, 'val/loss': 0.11820090455668313, 'test/loss': 0.18185103365353175, '_timestamp': 1762895027.4164777}).
Epoch: 4, Steps: 118 | Train Loss: 0.2033838 Vali Loss: 0.1093520 Test Loss: 0.1508386
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2012658 Vali Loss: 0.1063459 Test Loss: 0.1513131
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2002383 Vali Loss: 0.1101333 Test Loss: 0.1525613
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.1988899 Vali Loss: 0.1102087 Test Loss: 0.1526981
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.1987762 Vali Loss: 0.1065263 Test Loss: 0.1513364
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.1988904 Vali Loss: 0.1064657 Test Loss: 0.1500770
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.1989213 Vali Loss: 0.1053177 Test Loss: 0.1504739
Validation loss decreased (0.106190 --> 0.105318).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.1981035 Vali Loss: 0.1054908 Test Loss: 0.1503489
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.1987217 Vali Loss: 0.1060169 Test Loss: 0.1522337
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.1984481 Vali Loss: 0.1089907 Test Loss: 0.1506261
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.1981788 Vali Loss: 0.1076899 Test Loss: 0.1507448
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.1988834 Vali Loss: 0.1077472 Test Loss: 0.1507413
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.1987177 Vali Loss: 0.1088502 Test Loss: 0.1511210
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.1986076 Vali Loss: 0.1067126 Test Loss: 0.1530603
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.1984754 Vali Loss: 0.1061084 Test Loss: 0.1517694
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.1980722 Vali Loss: 0.1061976 Test Loss: 0.1527441
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.1981798 Vali Loss: 0.1055989 Test Loss: 0.1508526
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SASOL_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 210
test shape: (210, 5, 1) (210, 5, 1)
test shape: (210, 5, 1) (210, 5, 1)


	mse:0.0023003777023404837, mae:0.03620528057217598, rmse:0.0479622520506382, r2:-0.03648638725280762, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0362, RMSE: 0.0480, RÂ²: -0.0365, MAPE: 19582340.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.500 MB of 0.500 MB uploadedwandb: \ 0.500 MB of 0.500 MB uploadedwandb: | 0.500 MB of 0.500 MB uploadedwandb: / 0.500 MB of 0.500 MB uploadedwandb: - 0.500 MB of 0.788 MB uploadedwandb: \ 0.500 MB of 0.788 MB uploadedwandb: | 0.788 MB of 0.788 MB uploadedwandb: / 0.788 MB of 0.788 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–‚â–„â–„â–‚â–â–â–â–„â–‚â–‚â–‚â–‚â–„â–ƒâ–„â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–‡â–‚â–ˆâ–ˆâ–ƒâ–ƒâ–â–â–‚â–†â–„â–„â–†â–ƒâ–‚â–‚â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.15085
wandb:                 train/loss 0.19818
wandb:   val/directional_accuracy 50.47619
wandb:                   val/loss 0.1056
wandb:                    val/mae 0.03621
wandb:                   val/mape 1958234000.0
wandb:                    val/mse 0.0023
wandb:                     val/r2 -0.03649
wandb:                   val/rmse 0.04796
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/rv2sq7ow
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_230311-rv2sq7ow/logs
Completed: SASOL H=5

Training: Informer on SASOL for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_230703-r9t81g3t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/r9t81g3t
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SASOL_H10  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/r9t81g3t
>>>>>>>start training : long_term_forecast_Informer_SASOL_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 10
============================================================
train 3765
Overriding target from 'OT' to 'close' for stock data
val 204
Overriding target from 'OT' to 'close' for stock data
test 205
Epoch: 1, Steps: 118 | Train Loss: 0.2796351 Vali Loss: 0.1151208 Test Loss: 0.1555472
Validation loss decreased (inf --> 0.115121).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.27963508993892344, 'val/loss': 0.1151207664183208, 'test/loss': 0.15554716225181306, '_timestamp': 1762895252.7470267}).
Epoch: 2, Steps: 118 | Train Loss: 0.2234879 Vali Loss: 0.1107595 Test Loss: 0.1558430
Validation loss decreased (0.115121 --> 0.110760).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 118 | Train Loss: 0.2111518 Vali Loss: 0.1142283 Test Loss: 0.1627208
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2068526 Vali Loss: 0.1137726 Test Loss: 0.1536567
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.22348786574804178, 'val/loss': 0.11075952755553382, 'test/loss': 0.1558429683957781, '_timestamp': 1762895260.4113557}).
Epoch: 5, Steps: 118 | Train Loss: 0.2046074 Vali Loss: 0.1095988 Test Loss: 0.1528086
Validation loss decreased (0.110760 --> 0.109599).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2029450 Vali Loss: 0.1103655 Test Loss: 0.1534329
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2020794 Vali Loss: 0.1097662 Test Loss: 0.1515101
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2026480 Vali Loss: 0.1127730 Test Loss: 0.1519358
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2018196 Vali Loss: 0.1095049 Test Loss: 0.1524099
Validation loss decreased (0.109599 --> 0.109505).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2014972 Vali Loss: 0.1122568 Test Loss: 0.1521196
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2014555 Vali Loss: 0.1095484 Test Loss: 0.1526438
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2013296 Vali Loss: 0.1095564 Test Loss: 0.1523470
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2022227 Vali Loss: 0.1117726 Test Loss: 0.1528647
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2021205 Vali Loss: 0.1108877 Test Loss: 0.1525565
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.2021467 Vali Loss: 0.1084255 Test Loss: 0.1531572
Validation loss decreased (0.109505 --> 0.108426).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.2015333 Vali Loss: 0.1113357 Test Loss: 0.1520076
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.2016126 Vali Loss: 0.1121872 Test Loss: 0.1523112
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.2013105 Vali Loss: 0.1108468 Test Loss: 0.1529859
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.2015708 Vali Loss: 0.1096055 Test Loss: 0.1522808
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.2020360 Vali Loss: 0.1094106 Test Loss: 0.1531252
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 118 | Train Loss: 0.2016884 Vali Loss: 0.1101835 Test Loss: 0.1526355
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 118 | Train Loss: 0.2016409 Vali Loss: 0.1122707 Test Loss: 0.1514806
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 118 | Train Loss: 0.2012406 Vali Loss: 0.1114091 Test Loss: 0.1531142
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 118 | Train Loss: 0.2016816 Vali Loss: 0.1110399 Test Loss: 0.1524393
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 118 | Train Loss: 0.2013948 Vali Loss: 0.1090886 Test Loss: 0.1534858
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SASOL_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 205
test shape: (205, 10, 1) (205, 10, 1)
test shape: (205, 10, 1) (205, 10, 1)


	mse:0.0023134478833526373, mae:0.03630196675658226, rmse:0.04809831455349922, r2:-0.042234063148498535, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0363, RMSE: 0.0481, RÂ²: -0.0422, MAPE: 17837046.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.541 MB of 0.542 MB uploadedwandb: \ 0.541 MB of 0.542 MB uploadedwandb: | 0.542 MB of 0.542 MB uploadedwandb: / 0.542 MB of 0.542 MB uploadedwandb: - 0.542 MB of 0.831 MB uploadedwandb: \ 0.819 MB of 0.831 MB uploadedwandb: | 0.831 MB of 0.831 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–‚â–‚â–â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–‚â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‡â–‚â–ƒâ–ƒâ–†â–‚â–†â–‚â–‚â–…â–„â–â–…â–†â–„â–‚â–‚â–ƒâ–†â–…â–„â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 24
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.15349
wandb:                 train/loss 0.20139
wandb:   val/directional_accuracy 49.5935
wandb:                   val/loss 0.10909
wandb:                    val/mae 0.0363
wandb:                   val/mape 1783704600.0
wandb:                    val/mse 0.00231
wandb:                     val/r2 -0.04223
wandb:                   val/rmse 0.0481
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/r9t81g3t
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_230703-r9t81g3t/logs
Completed: SASOL H=10

Training: Informer on SASOL for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.23.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_231119-206164bw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/206164bw
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SASOL_H22  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/206164bw
>>>>>>>start training : long_term_forecast_Informer_SASOL_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 22
============================================================
train 3753
Overriding target from 'OT' to 'close' for stock data
val 192
Overriding target from 'OT' to 'close' for stock data
test 193
Epoch: 1, Steps: 118 | Train Loss: 0.2894202 Vali Loss: 0.1350086 Test Loss: 0.1772672
Validation loss decreased (inf --> 0.135009).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2894201872207351, 'val/loss': 0.1350086455543836, 'test/loss': 0.17726716186319078, '_timestamp': 1762895503.0491502}).
Epoch: 2, Steps: 118 | Train Loss: 0.2327240 Vali Loss: 0.1222397 Test Loss: 0.1696809
Validation loss decreased (0.135009 --> 0.122240).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 118 | Train Loss: 0.2191667 Vali Loss: 0.1265419 Test Loss: 0.1657728
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23272398657212823, 'val/loss': 0.12223971386750539, 'test/loss': 0.16968094131776265, '_timestamp': 1762895513.7498872}).
Epoch: 4, Steps: 118 | Train Loss: 0.2145950 Vali Loss: 0.1200133 Test Loss: 0.1742559
Validation loss decreased (0.122240 --> 0.120013).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2120840 Vali Loss: 0.1189398 Test Loss: 0.1662705
Validation loss decreased (0.120013 --> 0.118940).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2093828 Vali Loss: 0.1226724 Test Loss: 0.1666082
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2093744 Vali Loss: 0.1184320 Test Loss: 0.1660075
Validation loss decreased (0.118940 --> 0.118432).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2084554 Vali Loss: 0.1196578 Test Loss: 0.1681998
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2086071 Vali Loss: 0.1197646 Test Loss: 0.1679418
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2075234 Vali Loss: 0.1183179 Test Loss: 0.1656830
Validation loss decreased (0.118432 --> 0.118318).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2085268 Vali Loss: 0.1188080 Test Loss: 0.1674632
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2100251 Vali Loss: 0.1184417 Test Loss: 0.1643864
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2087010 Vali Loss: 0.1176878 Test Loss: 0.1666494
Validation loss decreased (0.118318 --> 0.117688).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2084469 Vali Loss: 0.1201094 Test Loss: 0.1691025
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.2099042 Vali Loss: 0.1195439 Test Loss: 0.1670616
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.2079304 Vali Loss: 0.1184198 Test Loss: 0.1668016
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.2081606 Vali Loss: 0.1192439 Test Loss: 0.1663794
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.2076642 Vali Loss: 0.1205632 Test Loss: 0.1678653
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.2081667 Vali Loss: 0.1192646 Test Loss: 0.1685788
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.2090128 Vali Loss: 0.1188021 Test Loss: 0.1675731
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 118 | Train Loss: 0.2082585 Vali Loss: 0.1192804 Test Loss: 0.1651609
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 118 | Train Loss: 0.2082904 Vali Loss: 0.1210798 Test Loss: 0.1683052
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 118 | Train Loss: 0.2081727 Vali Loss: 0.1189715 Test Loss: 0.1686405
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SASOL_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 193
test shape: (193, 22, 1) (193, 22, 1)
test shape: (193, 22, 1) (193, 22, 1)


	mse:0.002360345097258687, mae:0.03659258782863617, rmse:0.04858338460326195, r2:-0.05178713798522949, dtw:Not calculated


VAL - MSE: 0.0024, MAE: 0.0366, RMSE: 0.0486, RÂ²: -0.0518, MAPE: 19605366.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.595 MB of 0.596 MB uploadedwandb: \ 0.595 MB of 0.596 MB uploadedwandb: | 0.596 MB of 0.596 MB uploadedwandb: / 0.596 MB of 0.885 MB uploadedwandb: - 0.596 MB of 0.885 MB uploadedwandb: \ 0.885 MB of 0.885 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‚â–ˆâ–‚â–ƒâ–‚â–„â–„â–‚â–ƒâ–â–ƒâ–„â–ƒâ–ƒâ–‚â–ƒâ–„â–ƒâ–‚â–„â–„
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–â–‚â–ƒâ–‚â–‚â–‚â–â–â–â–â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–‚â–…â–‚â–ƒâ–ƒâ–â–‚â–‚â–â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–„â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.16864
wandb:                 train/loss 0.20817
wandb:   val/directional_accuracy 50.57982
wandb:                   val/loss 0.11897
wandb:                    val/mae 0.03659
wandb:                   val/mape 1960536600.0
wandb:                    val/mse 0.00236
wandb:                     val/r2 -0.05179
wandb:                   val/rmse 0.04858
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/206164bw
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_231119-206164bw/logs
Completed: SASOL H=22

Training: Informer on SASOL for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.23.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_231536-okizx6rr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/okizx6rr
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SASOL_H50  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/okizx6rr
>>>>>>>start training : long_term_forecast_Informer_SASOL_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 50
============================================================
train 3725
Overriding target from 'OT' to 'close' for stock data
val 164
Overriding target from 'OT' to 'close' for stock data
test 165
Epoch: 1, Steps: 117 | Train Loss: 0.3136440 Vali Loss: 0.1478872 Test Loss: 0.1887924
Validation loss decreased (inf --> 0.147887).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.31364400481056964, 'val/loss': 0.1478872299194336, 'test/loss': 0.18879244973262152, '_timestamp': 1762895763.2436256}).
Epoch: 2, Steps: 117 | Train Loss: 0.2449359 Vali Loss: 0.1506035 Test Loss: 0.3598133
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 117 | Train Loss: 0.2303878 Vali Loss: 0.1201164 Test Loss: 0.3808143
Validation loss decreased (0.147887 --> 0.120116).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2449358878736822, 'val/loss': 0.15060346076885858, 'test/loss': 0.35981330772240955, '_timestamp': 1762895774.0098865}).
Epoch: 4, Steps: 117 | Train Loss: 0.2223651 Vali Loss: 0.1452556 Test Loss: 0.3154894
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 117 | Train Loss: 0.2182148 Vali Loss: 0.1299642 Test Loss: 0.4230369
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 117 | Train Loss: 0.2162272 Vali Loss: 0.1296397 Test Loss: 0.3796368
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 117 | Train Loss: 0.2153248 Vali Loss: 0.1338621 Test Loss: 0.3949286
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 117 | Train Loss: 0.2153045 Vali Loss: 0.1324151 Test Loss: 0.4328370
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 117 | Train Loss: 0.2144154 Vali Loss: 0.1377058 Test Loss: 0.4246798
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 117 | Train Loss: 0.2140038 Vali Loss: 0.1288664 Test Loss: 0.4171987
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 117 | Train Loss: 0.2138355 Vali Loss: 0.1345696 Test Loss: 0.4200006
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 117 | Train Loss: 0.2149235 Vali Loss: 0.1353378 Test Loss: 0.4060671
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 117 | Train Loss: 0.2141238 Vali Loss: 0.1329196 Test Loss: 0.4139118
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SASOL_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 165
test shape: (165, 50, 1) (165, 50, 1)
test shape: (165, 50, 1) (165, 50, 1)


	mse:0.0023651423398405313, mae:0.03766592592000961, rmse:0.048632729798555374, r2:-0.15442490577697754, dtw:Not calculated


VAL - MSE: 0.0024, MAE: 0.0377, RMSE: 0.0486, RÂ²: -0.1544, MAPE: 31582914.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.643 MB of 0.645 MB uploadedwandb: \ 0.643 MB of 0.645 MB uploadedwandb: | 0.645 MB of 0.645 MB uploadedwandb: / 0.645 MB of 0.933 MB uploadedwandb: - 0.733 MB of 0.933 MB uploadedwandb: \ 0.933 MB of 0.933 MB uploadedwandb: | 0.933 MB of 0.933 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–â–‡â–…â–†â–ˆâ–ˆâ–‡â–‡â–†â–‡
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ˆâ–„â–„â–…â–„â–†â–ƒâ–…â–…â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.41391
wandb:                 train/loss 0.21412
wandb:   val/directional_accuracy 48.99196
wandb:                   val/loss 0.13292
wandb:                    val/mae 0.03767
wandb:                   val/mape 3158291400.0
wandb:                    val/mse 0.00237
wandb:                     val/r2 -0.15443
wandb:                   val/rmse 0.04863
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/okizx6rr
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_231536-okizx6rr/logs
Completed: SASOL H=50

Training: Informer on SASOL for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.23.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_231837-05n30jvt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/05n30jvt
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SASOL_H100 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/05n30jvt
>>>>>>>start training : long_term_forecast_Informer_SASOL_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 100
============================================================
train 3675
Overriding target from 'OT' to 'close' for stock data
val 114
Overriding target from 'OT' to 'close' for stock data
test 115
Epoch: 1, Steps: 115 | Train Loss: 0.3474920 Vali Loss: 0.1603899 Test Loss: 0.4693695
Validation loss decreased (inf --> 0.160390).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.34749202274757884, 'val/loss': 0.16038987040519714, 'test/loss': 0.4693695232272148, '_timestamp': 1762895946.8385887}).
Epoch: 2, Steps: 115 | Train Loss: 0.2769031 Vali Loss: 0.1760081 Test Loss: 0.4647633
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 115 | Train Loss: 0.2492650 Vali Loss: 0.1420128 Test Loss: 1.0185742
Validation loss decreased (0.160390 --> 0.142013).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 115 | Train Loss: 0.2345392 Vali Loss: 0.1467476 Test Loss: 1.2030623
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2769031319929206, 'val/loss': 0.17600810900330544, 'test/loss': 0.4647633358836174, '_timestamp': 1762895955.972321}).
Epoch: 5, Steps: 115 | Train Loss: 0.2281107 Vali Loss: 0.1534656 Test Loss: 1.2267877
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 115 | Train Loss: 0.2264588 Vali Loss: 0.1542757 Test Loss: 1.1694416
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 115 | Train Loss: 0.2254743 Vali Loss: 0.1532339 Test Loss: 1.2283805
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 115 | Train Loss: 0.2251448 Vali Loss: 0.1539008 Test Loss: 1.2863492
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 115 | Train Loss: 0.2248198 Vali Loss: 0.1547991 Test Loss: 1.1659144
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 115 | Train Loss: 0.2237494 Vali Loss: 0.1543883 Test Loss: 1.1878087
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 115 | Train Loss: 0.2245106 Vali Loss: 0.1575713 Test Loss: 1.1686727
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 115 | Train Loss: 0.2234837 Vali Loss: 0.1558130 Test Loss: 1.2108550
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 115 | Train Loss: 0.2237662 Vali Loss: 0.1586084 Test Loss: 1.2049829
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SASOL_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 115
test shape: (115, 100, 1) (115, 100, 1)
test shape: (115, 100, 1) (115, 100, 1)


	mse:0.002306374255567789, mae:0.03767179697751999, rmse:0.048024725168943405, r2:-0.16224133968353271, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0377, RMSE: 0.0480, RÂ²: -0.1622, MAPE: 34809484.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.690 MB of 0.695 MB uploadedwandb: \ 0.695 MB of 0.695 MB uploadedwandb: | 0.695 MB of 0.695 MB uploadedwandb: / 0.695 MB of 0.695 MB uploadedwandb: - 0.695 MB of 0.982 MB uploadedwandb: \ 0.982 MB of 0.982 MB uploadedwandb: | 0.982 MB of 0.982 MB uploadedwandb: / 0.982 MB of 0.982 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–†â–†â–…â–†â–ˆâ–…â–…â–…â–†â–†
wandb:                 train/loss â–ˆâ–„â–‚â–‚â–‚â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ƒâ–†â–†â–†â–†â–†â–†â–ˆâ–‡â–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 1.20498
wandb:                 train/loss 0.22377
wandb:   val/directional_accuracy 48.08081
wandb:                   val/loss 0.15861
wandb:                    val/mae 0.03767
wandb:                   val/mape 3480948400.0
wandb:                    val/mse 0.00231
wandb:                     val/r2 -0.16224
wandb:                   val/rmse 0.04802
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/05n30jvt
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_231837-05n30jvt/logs
Exception in thread IntMsgThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread ChkStopThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 286, in check_stop_status
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 300, in check_internal_messages
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 840, in deliver_stop_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 856, in deliver_internal_messages
    return self._deliver_stop_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 494, in _deliver_stop_status
    return self._deliver_internal_messages(internal_message)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 516, in _deliver_internal_messages
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
        return self._deliver_record(record)handle = mailbox._deliver_record(record, interface=self)

  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
        self.send_server_request(server_req)self._send_message(msg)

  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: SASOL H=100

Training: Informer on DRD_GOLD for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.23.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_232128-dckphx5a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/dckphx5a
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_DRD_GOLD_H3Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/dckphx5a
>>>>>>>start training : long_term_forecast_Informer_DRD_GOLD_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
Overriding target from 'OT' to 'close' for stock data
val 237
Overriding target from 'OT' to 'close' for stock data
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.3001946 Vali Loss: 0.1521420 Test Loss: 0.1520637
Validation loss decreased (inf --> 0.152142).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.30019458769855645, 'val/loss': 0.15214196406304836, 'test/loss': 0.15206374507397413, '_timestamp': 1762896110.8037274}).
Epoch: 2, Steps: 133 | Train Loss: 0.2420629 Vali Loss: 0.1690735 Test Loss: 0.1557065
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2267017 Vali Loss: 0.1336267 Test Loss: 0.1298567
Validation loss decreased (0.152142 --> 0.133627).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24206290366058064, 'val/loss': 0.16907345969229937, 'test/loss': 0.15570647083222866, '_timestamp': 1762896120.6430407}).
Epoch: 4, Steps: 133 | Train Loss: 0.2195115 Vali Loss: 0.1266999 Test Loss: 0.1266562
Validation loss decreased (0.133627 --> 0.126700).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2168288 Vali Loss: 0.1195901 Test Loss: 0.1252598
Validation loss decreased (0.126700 --> 0.119590).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2154042 Vali Loss: 0.1241492 Test Loss: 0.1243281
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2145318 Vali Loss: 0.1277120 Test Loss: 0.1245185
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2143285 Vali Loss: 0.1215831 Test Loss: 0.1237119
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2128320 Vali Loss: 0.1287936 Test Loss: 0.1244074
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2132229 Vali Loss: 0.1227228 Test Loss: 0.1246967
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2139855 Vali Loss: 0.1253787 Test Loss: 0.1245415
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2136702 Vali Loss: 0.1216421 Test Loss: 0.1246137
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2132459 Vali Loss: 0.1219582 Test Loss: 0.1243003
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2134830 Vali Loss: 0.1211430 Test Loss: 0.1234549
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2134159 Vali Loss: 0.1230099 Test Loss: 0.1243830
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_DRD_GOLD_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.0009388645994476974, mae:0.021694641560316086, rmse:0.030640898272395134, r2:-0.012390732765197754, dtw:Not calculated


VAL - MSE: 0.0009, MAE: 0.0217, RMSE: 0.0306, RÂ²: -0.0124, MAPE: 1004091.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.476 MB of 0.477 MB uploadedwandb: \ 0.476 MB of 0.477 MB uploadedwandb: | 0.476 MB of 0.477 MB uploadedwandb: / 0.477 MB of 0.477 MB uploadedwandb: - 0.477 MB of 0.477 MB uploadedwandb: \ 0.477 MB of 0.477 MB uploadedwandb: | 0.477 MB of 0.477 MB uploadedwandb: / 0.477 MB of 0.477 MB uploadedwandb: - 0.658 MB of 0.957 MB uploaded (0.002 MB deduped)wandb: \ 0.957 MB of 0.957 MB uploaded (0.002 MB deduped)wandb: | 0.957 MB of 0.957 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–‚
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–‚â–â–â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–â–ƒâ–…â–‚â–†â–ƒâ–„â–‚â–‚â–‚â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.12438
wandb:                 train/loss 0.21342
wandb:   val/directional_accuracy 50.42194
wandb:                   val/loss 0.12301
wandb:                    val/mae 0.02169
wandb:                   val/mape 100409175.0
wandb:                    val/mse 0.00094
wandb:                     val/r2 -0.01239
wandb:                   val/rmse 0.03064
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/dckphx5a
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_232128-dckphx5a/logs
Completed: DRD_GOLD H=3

Training: Informer on DRD_GOLD for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.23.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_232504-acdofht6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/acdofht6
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_DRD_GOLD_H5Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/acdofht6
>>>>>>>start training : long_term_forecast_Informer_DRD_GOLD_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
Overriding target from 'OT' to 'close' for stock data
val 235
Overriding target from 'OT' to 'close' for stock data
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.3040031 Vali Loss: 0.1339365 Test Loss: 0.1342589
Validation loss decreased (inf --> 0.133937).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3040031236141248, 'val/loss': 0.13393651135265827, 'test/loss': 0.1342589408159256, '_timestamp': 1762896329.1673944}).
Epoch: 2, Steps: 133 | Train Loss: 0.2473489 Vali Loss: 0.1580534 Test Loss: 0.1469911
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2311028 Vali Loss: 0.1358403 Test Loss: 0.1311441
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2473489148052115, 'val/loss': 0.15805343817919493, 'test/loss': 0.1469910927116871, '_timestamp': 1762896340.2057524}).
Epoch: 4, Steps: 133 | Train Loss: 0.2245596 Vali Loss: 0.1337291 Test Loss: 0.1292649
Validation loss decreased (0.133937 --> 0.133729).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2218272 Vali Loss: 0.1274534 Test Loss: 0.1285177
Validation loss decreased (0.133729 --> 0.127453).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2210541 Vali Loss: 0.1277943 Test Loss: 0.1294617
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2192658 Vali Loss: 0.1351904 Test Loss: 0.1284536
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2188122 Vali Loss: 0.1307909 Test Loss: 0.1298633
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2185623 Vali Loss: 0.1355541 Test Loss: 0.1292674
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2187676 Vali Loss: 0.1353027 Test Loss: 0.1282540
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2181589 Vali Loss: 0.1297275 Test Loss: 0.1294531
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2183070 Vali Loss: 0.1370522 Test Loss: 0.1293748
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2191959 Vali Loss: 0.1302028 Test Loss: 0.1297607
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2182318 Vali Loss: 0.1296812 Test Loss: 0.1289780
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2179852 Vali Loss: 0.1325810 Test Loss: 0.1297890
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_DRD_GOLD_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.0009417471010237932, mae:0.021812085062265396, rmse:0.03068789839744568, r2:-0.008516907691955566, dtw:Not calculated


VAL - MSE: 0.0009, MAE: 0.0218, RMSE: 0.0307, RÂ²: -0.0085, MAPE: 705768.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.504 MB of 0.505 MB uploadedwandb: \ 0.504 MB of 0.505 MB uploadedwandb: | 0.505 MB of 0.505 MB uploadedwandb: / 0.505 MB of 0.505 MB uploadedwandb: - 0.505 MB of 0.792 MB uploadedwandb: \ 0.505 MB of 0.792 MB uploadedwandb: | 0.792 MB of 0.792 MB uploadedwandb: / 0.792 MB of 0.792 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–„â–â–…â–ƒâ–â–„â–„â–…â–ƒâ–…
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–â–â–â–â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–†â–â–â–‡â–ƒâ–‡â–‡â–ƒâ–ˆâ–ƒâ–ƒâ–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.12979
wandb:                 train/loss 0.21799
wandb:   val/directional_accuracy 47.97872
wandb:                   val/loss 0.13258
wandb:                    val/mae 0.02181
wandb:                   val/mape 70576825.0
wandb:                    val/mse 0.00094
wandb:                     val/r2 -0.00852
wandb:                   val/rmse 0.03069
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/acdofht6
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_232504-acdofht6/logs
Completed: DRD_GOLD H=5

Training: Informer on DRD_GOLD for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.23.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_232821-6ib4lgsr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/6ib4lgsr
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_DRD_GOLD_H10Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/6ib4lgsr
>>>>>>>start training : long_term_forecast_Informer_DRD_GOLD_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
Overriding target from 'OT' to 'close' for stock data
val 230
Overriding target from 'OT' to 'close' for stock data
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.3156946 Vali Loss: 0.1680717 Test Loss: 0.1593209
Validation loss decreased (inf --> 0.168072).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.31569456897283854, 'val/loss': 0.16807172168046236, 'test/loss': 0.1593208648264408, '_timestamp': 1762896529.389967}).
Epoch: 2, Steps: 133 | Train Loss: 0.2560173 Vali Loss: 0.1486947 Test Loss: 0.1433439
Validation loss decreased (0.168072 --> 0.148695).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2428454 Vali Loss: 0.1579091 Test Loss: 0.1535898
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2560172720735234, 'val/loss': 0.14869470708072186, 'test/loss': 0.14334386214613914, '_timestamp': 1762896539.4258816}).
Epoch: 4, Steps: 133 | Train Loss: 0.2347404 Vali Loss: 0.1572182 Test Loss: 0.1408594
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2293898 Vali Loss: 0.1412435 Test Loss: 0.1373461
Validation loss decreased (0.148695 --> 0.141244).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2299180 Vali Loss: 0.1384781 Test Loss: 0.1409489
Validation loss decreased (0.141244 --> 0.138478).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2273123 Vali Loss: 0.1407457 Test Loss: 0.1392315
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2267040 Vali Loss: 0.1445926 Test Loss: 0.1374386
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2269573 Vali Loss: 0.1399054 Test Loss: 0.1388651
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2267260 Vali Loss: 0.1373270 Test Loss: 0.1378583
Validation loss decreased (0.138478 --> 0.137327).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2269772 Vali Loss: 0.1373704 Test Loss: 0.1383545
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2262751 Vali Loss: 0.1400752 Test Loss: 0.1370949
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2267789 Vali Loss: 0.1413199 Test Loss: 0.1379630
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2268797 Vali Loss: 0.1433214 Test Loss: 0.1383492
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2263672 Vali Loss: 0.1385041 Test Loss: 0.1384953
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2259264 Vali Loss: 0.1407275 Test Loss: 0.1375169
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2271866 Vali Loss: 0.1481340 Test Loss: 0.1375822
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2272088 Vali Loss: 0.1408249 Test Loss: 0.1374045
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2264387 Vali Loss: 0.1411677 Test Loss: 0.1374474
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2263874 Vali Loss: 0.1419561 Test Loss: 0.1377113
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_DRD_GOLD_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.0009565944201312959, mae:0.02198413759469986, rmse:0.03092886134982109, r2:-0.008786439895629883, dtw:Not calculated


VAL - MSE: 0.0010, MAE: 0.0220, RMSE: 0.0309, RÂ²: -0.0088, MAPE: 1297159.62%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.522 MB of 0.523 MB uploadedwandb: \ 0.523 MB of 0.523 MB uploadedwandb: | 0.523 MB of 0.523 MB uploadedwandb: / 0.523 MB of 0.811 MB uploadedwandb: - 0.737 MB of 0.811 MB uploadedwandb: \ 0.811 MB of 0.811 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–ƒâ–‚â–â–‚â–â–‚â–â–â–‚â–‚â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–‚â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–‚â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ˆâ–‚â–â–‚â–ƒâ–‚â–â–â–‚â–‚â–ƒâ–â–‚â–…â–‚â–‚â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.13771
wandb:                 train/loss 0.22639
wandb:   val/directional_accuracy 51.49758
wandb:                   val/loss 0.14196
wandb:                    val/mae 0.02198
wandb:                   val/mape 129715962.5
wandb:                    val/mse 0.00096
wandb:                     val/r2 -0.00879
wandb:                   val/rmse 0.03093
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/6ib4lgsr
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_232821-6ib4lgsr/logs
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread IntMsgThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 268, in check_network_status
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 300, in check_internal_messages
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 856, in deliver_internal_messages
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 848, in deliver_network_status
        return self._deliver_network_status(status)return self._deliver_internal_messages(internal_message)

  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 510, in _deliver_network_status
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 516, in _deliver_internal_messages
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: DRD_GOLD H=10

Training: Informer on DRD_GOLD for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.23.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_233232-qgimj0q2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/qgimj0q2
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_DRD_GOLD_H22Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/qgimj0q2
>>>>>>>start training : long_term_forecast_Informer_DRD_GOLD_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
Overriding target from 'OT' to 'close' for stock data
val 218
Overriding target from 'OT' to 'close' for stock data
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.3438296 Vali Loss: 0.1758273 Test Loss: 0.1654187
Validation loss decreased (inf --> 0.175827).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3438295594444781, 'val/loss': 0.1758273116179875, 'test/loss': 0.16541868022509984, '_timestamp': 1762896781.319154}).
Epoch: 2, Steps: 132 | Train Loss: 0.2724549 Vali Loss: 0.1836062 Test Loss: 0.1711542
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2568685 Vali Loss: 0.1750903 Test Loss: 0.1734553
Validation loss decreased (0.175827 --> 0.175090).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.27245486595413904, 'val/loss': 0.18360617331096105, 'test/loss': 0.17115423934800283, '_timestamp': 1762896791.9597547}).
Epoch: 4, Steps: 132 | Train Loss: 0.2457259 Vali Loss: 0.1773699 Test Loss: 0.1809907
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2414903 Vali Loss: 0.1687618 Test Loss: 0.1703585
Validation loss decreased (0.175090 --> 0.168762).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2388257 Vali Loss: 0.1820228 Test Loss: 0.1884012
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2377821 Vali Loss: 0.1768819 Test Loss: 0.1831351
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2377317 Vali Loss: 0.1785787 Test Loss: 0.1860031
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2370234 Vali Loss: 0.1759905 Test Loss: 0.1798252
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2365886 Vali Loss: 0.1747524 Test Loss: 0.1784913
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2362511 Vali Loss: 0.1771067 Test Loss: 0.1828370
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2367491 Vali Loss: 0.1732788 Test Loss: 0.1764219
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2369099 Vali Loss: 0.1744641 Test Loss: 0.1804886
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2363736 Vali Loss: 0.1737510 Test Loss: 0.1770140
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2370958 Vali Loss: 0.1744288 Test Loss: 0.1797930
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_DRD_GOLD_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.0009865578031167388, mae:0.022413190454244614, rmse:0.03140951693058014, r2:-0.0359954833984375, dtw:Not calculated


VAL - MSE: 0.0010, MAE: 0.0224, RMSE: 0.0314, RÂ²: -0.0360, MAPE: 2180096.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.592 MB of 0.593 MB uploadedwandb: \ 0.592 MB of 0.593 MB uploadedwandb: | 0.593 MB of 0.593 MB uploadedwandb: / 0.593 MB of 0.881 MB uploadedwandb: - 0.593 MB of 0.881 MB uploadedwandb: \ 0.881 MB of 0.881 MB uploadedwandb: | 0.881 MB of 0.881 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‚â–…â–â–ˆâ–†â–‡â–…â–„â–†â–ƒâ–…â–„â–…
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–†â–â–ˆâ–…â–†â–…â–„â–…â–ƒâ–„â–„â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.17979
wandb:                 train/loss 0.2371
wandb:   val/directional_accuracy 52.22805
wandb:                   val/loss 0.17443
wandb:                    val/mae 0.02241
wandb:                   val/mape 218009650.0
wandb:                    val/mse 0.00099
wandb:                     val/r2 -0.036
wandb:                   val/rmse 0.03141
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/qgimj0q2
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_233232-qgimj0q2/logs
Completed: DRD_GOLD H=22

Training: Informer on DRD_GOLD for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.23.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_233555-2upy1tda
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/2upy1tda
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_DRD_GOLD_H50Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/2upy1tda
>>>>>>>start training : long_term_forecast_Informer_DRD_GOLD_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
Overriding target from 'OT' to 'close' for stock data
val 190
Overriding target from 'OT' to 'close' for stock data
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.3785680 Vali Loss: 0.2172377 Test Loss: 0.1656474
Validation loss decreased (inf --> 0.217238).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.37856796022617456, 'val/loss': 0.2172376811504364, 'test/loss': 0.16564740985631943, '_timestamp': 1762896985.5088773}).
Epoch: 2, Steps: 132 | Train Loss: 0.3017399 Vali Loss: 0.2042866 Test Loss: 0.2153123
Validation loss decreased (0.217238 --> 0.204287).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2714833 Vali Loss: 0.2342884 Test Loss: 0.1784704
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3017398974660671, 'val/loss': 0.20428655793269476, 'test/loss': 0.21531234433253607, '_timestamp': 1762896996.04636}).
Epoch: 4, Steps: 132 | Train Loss: 0.2628299 Vali Loss: 0.2250245 Test Loss: 0.2721223
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2572692 Vali Loss: 0.2089935 Test Loss: 0.2043050
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2529968 Vali Loss: 0.2163057 Test Loss: 0.2403852
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2509823 Vali Loss: 0.2154389 Test Loss: 0.2342387
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2534318 Vali Loss: 0.2157956 Test Loss: 0.2422941
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2502095 Vali Loss: 0.2165530 Test Loss: 0.2491660
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2503648 Vali Loss: 0.2153328 Test Loss: 0.2487834
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2522967 Vali Loss: 0.2160175 Test Loss: 0.2401001
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2536295 Vali Loss: 0.2262515 Test Loss: 0.2606129
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_DRD_GOLD_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.0009450804209336638, mae:0.021808575838804245, rmse:0.030742160975933075, r2:-0.01492464542388916, dtw:Not calculated


VAL - MSE: 0.0009, MAE: 0.0218, RMSE: 0.0307, RÂ²: -0.0149, MAPE: 1611423.38%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.708 MB of 0.710 MB uploadedwandb: \ 0.708 MB of 0.710 MB uploadedwandb: | 0.708 MB of 0.710 MB uploadedwandb: / 0.710 MB of 0.710 MB uploadedwandb: - 0.710 MB of 0.710 MB uploadedwandb: \ 0.710 MB of 0.997 MB uploadedwandb: | 0.997 MB of 0.997 MB uploadedwandb: / 0.997 MB of 0.997 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–ƒâ–†â–…â–†â–†â–†â–†â–‡
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–‚â–â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.26061
wandb:                 train/loss 0.25363
wandb:   val/directional_accuracy 51.05263
wandb:                   val/loss 0.22625
wandb:                    val/mae 0.02181
wandb:                   val/mape 161142337.5
wandb:                    val/mse 0.00095
wandb:                     val/r2 -0.01492
wandb:                   val/rmse 0.03074
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/2upy1tda
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_233555-2upy1tda/logs
Completed: DRD_GOLD H=50

Training: Informer on DRD_GOLD for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.23.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_233905-822hfgqc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/822hfgqc
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_DRD_GOLD_H100Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/822hfgqc
>>>>>>>start training : long_term_forecast_Informer_DRD_GOLD_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
Overriding target from 'OT' to 'close' for stock data
val 140
Overriding target from 'OT' to 'close' for stock data
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.4156851 Vali Loss: 0.3542910 Test Loss: 0.3096122
Validation loss decreased (inf --> 0.354291).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.41568514704704285, 'val/loss': 0.354291045665741, 'test/loss': 0.30961220264434813, '_timestamp': 1762897172.2558813}).
Epoch: 2, Steps: 130 | Train Loss: 0.3253225 Vali Loss: 0.2607402 Test Loss: 0.2695390
Validation loss decreased (0.354291 --> 0.260740).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.2930435 Vali Loss: 0.3205568 Test Loss: 0.4077951
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3253224729345395, 'val/loss': 0.26074018478393557, 'test/loss': 0.26953904032707215, '_timestamp': 1762897182.315359}).
Epoch: 4, Steps: 130 | Train Loss: 0.2794438 Vali Loss: 0.3218060 Test Loss: 0.4612745
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2714346 Vali Loss: 0.3003306 Test Loss: 0.4399497
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2689049 Vali Loss: 0.3368962 Test Loss: 0.4692175
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2677696 Vali Loss: 0.3175458 Test Loss: 0.4285858
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2678211 Vali Loss: 0.3260745 Test Loss: 0.4801066
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2657815 Vali Loss: 0.3227372 Test Loss: 0.4285980
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2671060 Vali Loss: 0.3085705 Test Loss: 0.4376023
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2662628 Vali Loss: 0.3408380 Test Loss: 0.4654798
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2662724 Vali Loss: 0.3064618 Test Loss: 0.4399458
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_DRD_GOLD_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.000924518855754286, mae:0.021393395960330963, rmse:0.03040590137243271, r2:-0.02321171760559082, dtw:Not calculated


VAL - MSE: 0.0009, MAE: 0.0214, RMSE: 0.0304, RÂ²: -0.0232, MAPE: 1058001.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.797 MB of 0.802 MB uploadedwandb: \ 0.797 MB of 0.802 MB uploadedwandb: | 0.802 MB of 0.802 MB uploadedwandb: / 0.802 MB of 1.088 MB uploadedwandb: - 1.088 MB of 1.088 MB uploadedwandb: \ 1.088 MB of 1.088 MB uploadedwandb: | 1.088 MB of 1.088 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–†â–„â–‡â–ƒâ–ˆâ–ƒâ–„â–‡â–„
wandb:                 train/loss â–ˆâ–…â–‚â–‚â–‚â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–…â–â–‡â–„â–…â–…â–‚â–ˆâ–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.43995
wandb:                 train/loss 0.26627
wandb:   val/directional_accuracy 50.80808
wandb:                   val/loss 0.30646
wandb:                    val/mae 0.02139
wandb:                   val/mape 105800125.0
wandb:                    val/mse 0.00092
wandb:                     val/r2 -0.02321
wandb:                   val/rmse 0.03041
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/822hfgqc
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_233905-822hfgqc/logs
Completed: DRD_GOLD H=100

Training: Informer on ANGLO_AMERICAN for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.23.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_234151-2fzk91jx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/2fzk91jx
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ANGLO_AMERICAN_H3Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/2fzk91jx
>>>>>>>start training : long_term_forecast_Informer_ANGLO_AMERICAN_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 3
============================================================
train 778
Overriding target from 'OT' to 'close' for stock data
val 44
Overriding target from 'OT' to 'close' for stock data
test 46
Epoch: 1, Steps: 25 | Train Loss: 0.4375062 Vali Loss: 0.6689023 Test Loss: 2.3742881
Validation loss decreased (inf --> 0.668902).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 25 | Train Loss: 0.3041823 Vali Loss: 0.6393887 Test Loss: 2.3225467
Validation loss decreased (0.668902 --> 0.639389).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 25 | Train Loss: 0.2835395 Vali Loss: 0.7047613 Test Loss: 2.5942901
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 25 | Train Loss: 0.2745409 Vali Loss: 0.6678547 Test Loss: 2.4788842
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.43750621676445006, 'val/loss': 0.6689022779464722, 'test/loss': 2.3742880821228027, '_timestamp': 1762897330.850144}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3041823333501816, 'val/loss': 0.6393887400627136, 'test/loss': 2.3225467205047607, '_timestamp': 1762897335.635572}).
Epoch: 5, Steps: 25 | Train Loss: 0.2666735 Vali Loss: 0.6710245 Test Loss: 2.4665288
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 25 | Train Loss: 0.2653530 Vali Loss: 0.6383657 Test Loss: 2.4174496
Validation loss decreased (0.639389 --> 0.638366).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 25 | Train Loss: 0.2665638 Vali Loss: 0.6908712 Test Loss: 2.4704051
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 25 | Train Loss: 0.2571151 Vali Loss: 0.7158170 Test Loss: 2.4702172
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 25 | Train Loss: 0.2706678 Vali Loss: 0.6814551 Test Loss: 2.4675210
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 25 | Train Loss: 0.2565666 Vali Loss: 0.6745108 Test Loss: 2.4679494
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 25 | Train Loss: 0.2581059 Vali Loss: 0.7101482 Test Loss: 2.4649144
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 25 | Train Loss: 0.2648269 Vali Loss: 0.6893998 Test Loss: 2.4965546
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 25 | Train Loss: 0.2574007 Vali Loss: 0.7282622 Test Loss: 2.4904734
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 25 | Train Loss: 0.2618249 Vali Loss: 0.6591797 Test Loss: 2.4591424
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 25 | Train Loss: 0.2582573 Vali Loss: 0.6465666 Test Loss: 2.4711415
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 25 | Train Loss: 0.2585107 Vali Loss: 0.6793249 Test Loss: 2.4700464
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ANGLO_AMERICAN_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 46
test shape: (46, 3, 1) (46, 3, 1)
test shape: (46, 3, 1) (46, 3, 1)


	mse:0.006717241369187832, mae:0.05642788112163544, rmse:0.08195877820253372, r2:0.008509039878845215, dtw:Not calculated


VAL - MSE: 0.0067, MAE: 0.0564, RMSE: 0.0820, RÂ²: 0.0085, MAPE: 13237631.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.434 MB of 0.434 MB uploadedwandb: \ 0.434 MB of 0.434 MB uploadedwandb: | 0.434 MB of 0.434 MB uploadedwandb: / 0.434 MB of 0.434 MB uploadedwandb: - 0.434 MB of 0.434 MB uploadedwandb: \ 0.434 MB of 0.434 MB uploadedwandb: | 0.434 MB of 0.434 MB uploadedwandb: / 0.434 MB of 0.434 MB uploadedwandb: - 0.616 MB of 0.914 MB uploaded (0.002 MB deduped)wandb: \ 0.914 MB of 0.914 MB uploaded (0.002 MB deduped)wandb: | 0.914 MB of 0.914 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–†â–„â–ƒâ–„â–â–…â–â–â–ƒâ–â–‚â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–ƒâ–„â–â–…â–‡â–„â–„â–‡â–…â–ˆâ–ƒâ–‚â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 2.47005
wandb:                 train/loss 0.25851
wandb:   val/directional_accuracy 56.52174
wandb:                   val/loss 0.67932
wandb:                    val/mae 0.05643
wandb:                   val/mape 1323763100.0
wandb:                    val/mse 0.00672
wandb:                     val/r2 0.00851
wandb:                   val/rmse 0.08196
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/2fzk91jx
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_234151-2fzk91jx/logs
Completed: ANGLO_AMERICAN H=3

Training: Informer on ANGLO_AMERICAN for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.23.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_234339-0uvo0a9f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/0uvo0a9f
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ANGLO_AMERICAN_H5Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/0uvo0a9f
>>>>>>>start training : long_term_forecast_Informer_ANGLO_AMERICAN_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 5
============================================================
train 776
Overriding target from 'OT' to 'close' for stock data
val 42
Overriding target from 'OT' to 'close' for stock data
test 44
Epoch: 1, Steps: 25 | Train Loss: 0.4483235 Vali Loss: 0.6849545 Test Loss: 2.5633457
Validation loss decreased (inf --> 0.684954).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 25 | Train Loss: 0.3235099 Vali Loss: 0.6924397 Test Loss: 2.5232476
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 25 | Train Loss: 0.2960735 Vali Loss: 0.6474488 Test Loss: 2.3738599
Validation loss decreased (0.684954 --> 0.647449).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 25 | Train Loss: 0.2852723 Vali Loss: 0.6585164 Test Loss: 2.5214961
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
Epoch: 5, Steps: 25 | Train Loss: 0.2751642 Vali Loss: 0.6208865 Test Loss: 2.4792627
Validation loss decreased (0.647449 --> 0.620887).  Saving model ...
Updating learning rate to 6.25e-06
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.44832348465919497, 'val/loss': 0.6849544942378998, 'test/loss': 2.5633456707000732, '_timestamp': 1762897441.2352865}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3235099327564239, 'val/loss': 0.6924396753311157, 'test/loss': 2.5232475996017456, '_timestamp': 1762897445.2898932}).
Epoch: 6, Steps: 25 | Train Loss: 0.2706190 Vali Loss: 0.7097792 Test Loss: 2.5428544
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 25 | Train Loss: 0.2765843 Vali Loss: 0.7201194 Test Loss: 2.5545487
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 25 | Train Loss: 0.2679020 Vali Loss: 0.7071918 Test Loss: 2.5452391
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 25 | Train Loss: 0.2691134 Vali Loss: 0.7173908 Test Loss: 2.5556406
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 25 | Train Loss: 0.2702946 Vali Loss: 0.6736712 Test Loss: 2.5647194
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 25 | Train Loss: 0.2661006 Vali Loss: 0.6504942 Test Loss: 2.5516102
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 25 | Train Loss: 0.2695022 Vali Loss: 0.6808894 Test Loss: 2.5420985
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 25 | Train Loss: 0.2711814 Vali Loss: 0.6990630 Test Loss: 2.5347513
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 25 | Train Loss: 0.2733785 Vali Loss: 0.7191949 Test Loss: 2.5398588
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 25 | Train Loss: 0.2725739 Vali Loss: 0.6904512 Test Loss: 2.5683784
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ANGLO_AMERICAN_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 44
test shape: (44, 5, 1) (44, 5, 1)
test shape: (44, 5, 1) (44, 5, 1)


	mse:0.007054346147924662, mae:0.058057621121406555, rmse:0.08399015665054321, r2:-0.00034439563751220703, dtw:Not calculated


VAL - MSE: 0.0071, MAE: 0.0581, RMSE: 0.0840, RÂ²: -0.0003, MAPE: 7139543.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.476 MB of 0.476 MB uploadedwandb: \ 0.476 MB of 0.476 MB uploadedwandb: | 0.476 MB of 0.476 MB uploadedwandb: / 0.476 MB of 0.476 MB uploadedwandb: - 0.476 MB of 0.763 MB uploadedwandb: \ 0.564 MB of 0.763 MB uploadedwandb: | 0.763 MB of 0.763 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–†â–…â–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–ˆ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–ƒâ–â–‚â–‚â–â–‚â–‚â–ƒâ–ƒ
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–„â–â–‡â–ˆâ–‡â–ˆâ–…â–ƒâ–…â–‡â–ˆâ–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 2.56838
wandb:                 train/loss 0.27257
wandb:   val/directional_accuracy 56.81818
wandb:                   val/loss 0.69045
wandb:                    val/mae 0.05806
wandb:                   val/mape 713954350.0
wandb:                    val/mse 0.00705
wandb:                     val/r2 -0.00034
wandb:                   val/rmse 0.08399
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/0uvo0a9f
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_234339-0uvo0a9f/logs
Completed: ANGLO_AMERICAN H=5

Training: Informer on ANGLO_AMERICAN for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.23.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_234552-mjsrm5o0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/mjsrm5o0
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ANGLO_AMERICAN_H10Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/mjsrm5o0
>>>>>>>start training : long_term_forecast_Informer_ANGLO_AMERICAN_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 10
============================================================
train 771
Overriding target from 'OT' to 'close' for stock data
val 37
Overriding target from 'OT' to 'close' for stock data
test 39
Epoch: 1, Steps: 25 | Train Loss: 0.4641726 Vali Loss: 0.7153983 Test Loss: 2.5200431
Validation loss decreased (inf --> 0.715398).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 25 | Train Loss: 0.3359034 Vali Loss: 0.6795576 Test Loss: 2.3886452
Validation loss decreased (0.715398 --> 0.679558).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.4641725730895996, 'val/loss': 0.7153982520103455, 'test/loss': 2.520043134689331, '_timestamp': 1762897577.635849}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.33590341091156006, 'val/loss': 0.6795576214790344, 'test/loss': 2.3886451721191406, '_timestamp': 1762897585.088707}).
Epoch: 3, Steps: 25 | Train Loss: 0.3083069 Vali Loss: 0.7067878 Test Loss: 2.5403342
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 25 | Train Loss: 0.3011587 Vali Loss: 0.6875542 Test Loss: 2.5349437
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 25 | Train Loss: 0.2856859 Vali Loss: 0.8557274 Test Loss: 2.6741698
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 25 | Train Loss: 0.2889876 Vali Loss: 0.7089470 Test Loss: 2.5611601
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 25 | Train Loss: 0.2848988 Vali Loss: 0.6827223 Test Loss: 2.5810513
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 25 | Train Loss: 0.2830539 Vali Loss: 0.7470856 Test Loss: 2.5859845
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 25 | Train Loss: 0.2847614 Vali Loss: 0.6553006 Test Loss: 2.5942246
Validation loss decreased (0.679558 --> 0.655301).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 25 | Train Loss: 0.2823494 Vali Loss: 0.7239070 Test Loss: 2.6083999
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 25 | Train Loss: 0.2837034 Vali Loss: 0.7127230 Test Loss: 2.5774077
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 25 | Train Loss: 0.2801289 Vali Loss: 0.7388713 Test Loss: 2.5952466
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 25 | Train Loss: 0.2833228 Vali Loss: 0.7461099 Test Loss: 2.5792279
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 25 | Train Loss: 0.2800824 Vali Loss: 0.7265786 Test Loss: 2.5928024
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 25 | Train Loss: 0.2869091 Vali Loss: 0.7505666 Test Loss: 2.6148978
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 25 | Train Loss: 0.2810939 Vali Loss: 0.7343245 Test Loss: 2.6014515
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 25 | Train Loss: 0.2828458 Vali Loss: 0.7034263 Test Loss: 2.5962843
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 25 | Train Loss: 0.2812236 Vali Loss: 0.7341430 Test Loss: 2.5714064
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 25 | Train Loss: 0.2794344 Vali Loss: 0.7425898 Test Loss: 2.5967933
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ANGLO_AMERICAN_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 39
test shape: (39, 10, 1) (39, 10, 1)
test shape: (39, 10, 1) (39, 10, 1)


	mse:0.007757370825856924, mae:0.061286360025405884, rmse:0.08807593584060669, r2:-0.0011442899703979492, dtw:Not calculated


VAL - MSE: 0.0078, MAE: 0.0613, RMSE: 0.0881, RÂ²: -0.0011, MAPE: 1740809.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.497 MB of 0.498 MB uploadedwandb: \ 0.497 MB of 0.498 MB uploadedwandb: | 0.498 MB of 0.498 MB uploadedwandb: / 0.498 MB of 0.498 MB uploadedwandb: - 0.498 MB of 0.498 MB uploadedwandb: \ 0.498 MB of 0.785 MB uploadedwandb: | 0.774 MB of 0.785 MB uploadedwandb: / 0.785 MB of 0.785 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–â–ˆâ–‚â–ƒâ–„â–„â–…â–ƒâ–„â–ƒâ–„â–…â–„â–„â–ƒâ–„
wandb:                 train/loss â–ˆâ–†â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–ƒâ–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–‚â–ˆâ–ƒâ–‚â–„â–â–ƒâ–ƒâ–„â–„â–ƒâ–„â–„â–ƒâ–„â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 18
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 2.59679
wandb:                 train/loss 0.27943
wandb:   val/directional_accuracy 50.42735
wandb:                   val/loss 0.74259
wandb:                    val/mae 0.06129
wandb:                   val/mape 174080950.0
wandb:                    val/mse 0.00776
wandb:                     val/r2 -0.00114
wandb:                   val/rmse 0.08808
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/mjsrm5o0
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_234552-mjsrm5o0/logs
Completed: ANGLO_AMERICAN H=10

Training: Informer on ANGLO_AMERICAN for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.23.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_234908-4tcehslr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/4tcehslr
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ANGLO_AMERICAN_H22Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/4tcehslr
>>>>>>>start training : long_term_forecast_Informer_ANGLO_AMERICAN_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 22
============================================================
train 759
Overriding target from 'OT' to 'close' for stock data
val 25
Overriding target from 'OT' to 'close' for stock data
test 27
Epoch: 1, Steps: 24 | Train Loss: 0.4813023 Vali Loss: 0.7042817 Test Loss: 2.7127376
Validation loss decreased (inf --> 0.704282).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 24 | Train Loss: 0.3388965 Vali Loss: 0.8102657 Test Loss: 2.9545462
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 24 | Train Loss: 0.3209940 Vali Loss: 0.7623702 Test Loss: 2.7860703
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 24 | Train Loss: 0.3069700 Vali Loss: 0.7421575 Test Loss: 2.7549400
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 24 | Train Loss: 0.3005767 Vali Loss: 0.7362452 Test Loss: 2.7441623
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 24 | Train Loss: 0.2989951 Vali Loss: 0.7582552 Test Loss: 2.7826703
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.4813023159901301, 'val/loss': 0.7042816877365112, 'test/loss': 2.712737560272217, '_timestamp': 1762897769.9459798}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3388964707652728, 'val/loss': 0.8102656602859497, 'test/loss': 2.9545462131500244, '_timestamp': 1762897774.5586162}).
Epoch: 7, Steps: 24 | Train Loss: 0.2960435 Vali Loss: 0.7395701 Test Loss: 2.7468534
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 24 | Train Loss: 0.2968334 Vali Loss: 0.7405074 Test Loss: 2.7374392
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 24 | Train Loss: 0.2943350 Vali Loss: 0.7364416 Test Loss: 2.7425086
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 24 | Train Loss: 0.2965813 Vali Loss: 0.7424889 Test Loss: 2.7533901
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 24 | Train Loss: 0.2945941 Vali Loss: 0.7420158 Test Loss: 2.7323108
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ANGLO_AMERICAN_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 27
test shape: (27, 22, 1) (27, 22, 1)
test shape: (27, 22, 1) (27, 22, 1)


	mse:0.007917601615190506, mae:0.06318578124046326, rmse:0.08898090571165085, r2:-0.008464455604553223, dtw:Not calculated


VAL - MSE: 0.0079, MAE: 0.0632, RMSE: 0.0890, RÂ²: -0.0085, MAPE: 227882.09%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.484 MB of 0.485 MB uploadedwandb: \ 0.484 MB of 0.485 MB uploadedwandb: | 0.485 MB of 0.485 MB uploadedwandb: / 0.485 MB of 0.485 MB uploadedwandb: - 0.485 MB of 0.773 MB uploadedwandb: \ 0.773 MB of 0.773 MB uploadedwandb: | 0.773 MB of 0.773 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–ƒâ–ˆâ–ƒâ–‚â–‚â–„â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–â–‚â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–â–‡â–‚â–‚â–â–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 2.73231
wandb:                 train/loss 0.29459
wandb:   val/directional_accuracy 50.61728
wandb:                   val/loss 0.74202
wandb:                    val/mae 0.06319
wandb:                   val/mape 22788209.375
wandb:                    val/mse 0.00792
wandb:                     val/r2 -0.00846
wandb:                   val/rmse 0.08898
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/4tcehslr
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_234908-4tcehslr/logs
Completed: ANGLO_AMERICAN H=22

Training: Informer on ANGLO_AMERICAN for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.23.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_235112-d8fm6p5u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/d8fm6p5u
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ANGLO_AMERICAN_H50Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/d8fm6p5u
>>>>>>>start training : long_term_forecast_Informer_ANGLO_AMERICAN_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 50
============================================================
train 731
Overriding target from 'OT' to 'close' for stock data
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.005 MB of 0.009 MB uploadedwandb: / 0.005 MB of 0.012 MB uploadedwandb: - 0.012 MB of 0.012 MB uploadedwandb: ðŸš€ View run Informer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/d8fm6p5u
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_235112-d8fm6p5u/logs
Traceback (most recent call last):
  File "run.py", line 207, in <module>
    exp.train(setting)
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 93, in train
    vali_data, vali_loader = self._get_data(flag='val')
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 43, in _get_data
    data_set, data_loader = data_provider(self.args, flag)
  File "/home-mscluster/bkodze/research_project/forecast-research/data_provider/data_factory.py", line 79, in data_provider
    print(flag, len(data_set))
ValueError: __len__() should return >= 0
Completed: ANGLO_AMERICAN H=50

Training: Informer on ANGLO_AMERICAN for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.23.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_235217-v1zabcjv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/v1zabcjv
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ANGLO_AMERICAN_H100Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/v1zabcjv
>>>>>>>start training : long_term_forecast_Informer_ANGLO_AMERICAN_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 100
============================================================
train 681
Overriding target from 'OT' to 'close' for stock data
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.005 MB of 0.005 MB uploadedwandb: / 0.005 MB of 0.005 MB uploadedwandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.005 MB of 0.005 MB uploadedwandb: / 0.005 MB of 0.005 MB uploadedwandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.024 MB uploadedwandb: | 0.012 MB of 0.024 MB uploadedwandb: / 0.024 MB of 0.024 MB uploadedwandb: ðŸš€ View run Informer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/v1zabcjv
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_235217-v1zabcjv/logs
Traceback (most recent call last):
  File "run.py", line 207, in <module>
    exp.train(setting)
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 93, in train
    vali_data, vali_loader = self._get_data(flag='val')
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 43, in _get_data
    data_set, data_loader = data_provider(self.args, flag)
  File "/home-mscluster/bkodze/research_project/forecast-research/data_provider/data_factory.py", line 79, in data_provider
    print(flag, len(data_set))
ValueError: __len__() should return >= 0
Completed: ANGLO_AMERICAN H=100

Informer training completed for all datasets!
