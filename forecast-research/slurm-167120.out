##############################################################################
# Training Mamba Model on All Datasets
##############################################################################
Training: Mamba on NVIDIA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_142838-3ekoqdnl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/3ekoqdnl
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H3     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/3ekoqdnl
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.2398016 Vali Loss: 0.1788270 Test Loss: 0.3172470
Validation loss decreased (inf --> 0.178827).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2340902 Vali Loss: 0.1717735 Test Loss: 0.2828078
Validation loss decreased (0.178827 --> 0.171773).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2290828 Vali Loss: 0.1712544 Test Loss: 0.2625151
Validation loss decreased (0.171773 --> 0.171254).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2264387 Vali Loss: 0.1759266 Test Loss: 0.2518765
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2398015543034202, 'val/loss': 0.17882704734802246, 'test/loss': 0.31724697072058916, '_timestamp': 1762777747.5298545}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2340902467643408, 'val/loss': 0.1717734532430768, 'test/loss': 0.2828077971935272, '_timestamp': 1762777749.4503856}).
Epoch: 5, Steps: 133 | Train Loss: 0.2253185 Vali Loss: 0.1716171 Test Loss: 0.2501391
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2247877 Vali Loss: 0.1717962 Test Loss: 0.2484813
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2251847 Vali Loss: 0.1693215 Test Loss: 0.2476090
Validation loss decreased (0.171254 --> 0.169321).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2247420 Vali Loss: 0.1769274 Test Loss: 0.2472675
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2251137 Vali Loss: 0.1815853 Test Loss: 0.2472048
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2247136 Vali Loss: 0.1705476 Test Loss: 0.2471542
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2252591 Vali Loss: 0.1676287 Test Loss: 0.2471129
Validation loss decreased (0.169321 --> 0.167629).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2247293 Vali Loss: 0.1677361 Test Loss: 0.2471004
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2250578 Vali Loss: 0.1710783 Test Loss: 0.2470911
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2246744 Vali Loss: 0.1748466 Test Loss: 0.2470886
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2249781 Vali Loss: 0.1721212 Test Loss: 0.2470872
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2251096 Vali Loss: 0.1717082 Test Loss: 0.2470870
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2249535 Vali Loss: 0.1699347 Test Loss: 0.2470868
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2246095 Vali Loss: 0.1766164 Test Loss: 0.2470868
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2249725 Vali Loss: 0.1710211 Test Loss: 0.2470868
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2249478 Vali Loss: 0.1887673 Test Loss: 0.2470868
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2251654 Vali Loss: 0.1919309 Test Loss: 0.2470868
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.0011408244026824832, mae:0.02526126615703106, rmse:0.033776093274354935, r2:-0.01697099208831787, dtw:Not calculated


VAL - MSE: 0.0011, MAE: 0.0253, RMSE: 0.0338, RÂ²: -0.0170, MAPE: 505264.03%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.469 MB of 0.469 MB uploadedwandb: \ 0.469 MB of 0.469 MB uploadedwandb: | 0.469 MB of 0.469 MB uploadedwandb: / 0.469 MB of 0.469 MB uploadedwandb: - 0.469 MB of 0.469 MB uploadedwandb: \ 0.469 MB of 0.469 MB uploadedwandb: | 0.469 MB of 0.469 MB uploadedwandb: / 0.469 MB of 0.469 MB uploadedwandb: - 0.518 MB of 0.579 MB uploaded (0.002 MB deduped)wandb: \ 0.579 MB of 0.579 MB uploaded (0.002 MB deduped)wandb: | 0.579 MB of 0.579 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–‚â–â–‚â–â–‚â–â–‚â–â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–ƒâ–‚â–‚â–â–„â–…â–‚â–â–â–‚â–ƒâ–‚â–‚â–‚â–„â–‚â–‡â–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35218
wandb:     model/trainable_params 35218
wandb:                  test/loss 0.24709
wandb:                 train/loss 0.22517
wandb:   val/directional_accuracy 48.94515
wandb:                   val/loss 0.19193
wandb:                    val/mae 0.02526
wandb:                   val/mape 50526403.125
wandb:                    val/mse 0.00114
wandb:                     val/r2 -0.01697
wandb:                   val/rmse 0.03378
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/3ekoqdnl
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_142838-3ekoqdnl/logs
Completed: NVIDIA H=3

Training: Mamba on NVIDIA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_143027-jpc70cjv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/jpc70cjv
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H5     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/jpc70cjv
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.2424326 Vali Loss: 0.1815101 Test Loss: 0.3337510
Validation loss decreased (inf --> 0.181510).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2366367 Vali Loss: 0.1743817 Test Loss: 0.3049187
Validation loss decreased (0.181510 --> 0.174382).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2347172 Vali Loss: 0.1725042 Test Loss: 0.2926634
Validation loss decreased (0.174382 --> 0.172504).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2320447 Vali Loss: 0.1878856 Test Loss: 0.2857704
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2318516 Vali Loss: 0.1769637 Test Loss: 0.2840420
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2312570 Vali Loss: 0.1784714 Test Loss: 0.2823206
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2313774 Vali Loss: 0.1706789 Test Loss: 0.2816602
Validation loss decreased (0.172504 --> 0.170679).  Saving model ...
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.24243259149834626, 'val/loss': 0.18151008803397417, 'test/loss': 0.3337509948760271, '_timestamp': 1762777835.5346506}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2366366570157216, 'val/loss': 0.1743817087262869, 'test/loss': 0.30491867382079363, '_timestamp': 1762777837.4092045}).
Epoch: 8, Steps: 133 | Train Loss: 0.2302592 Vali Loss: 0.1780427 Test Loss: 0.2811970
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2304898 Vali Loss: 0.1744161 Test Loss: 0.2810155
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2304819 Vali Loss: 0.1722462 Test Loss: 0.2809382
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2300859 Vali Loss: 0.1941548 Test Loss: 0.2808755
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2305348 Vali Loss: 0.1756295 Test Loss: 0.2808554
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2305901 Vali Loss: 0.1960458 Test Loss: 0.2808451
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2304982 Vali Loss: 0.1722645 Test Loss: 0.2808399
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2306097 Vali Loss: 0.1938578 Test Loss: 0.2808385
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2304665 Vali Loss: 0.1859047 Test Loss: 0.2808378
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2308349 Vali Loss: 0.1710094 Test Loss: 0.2808376
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.0011469967430457473, mae:0.025410864502191544, rmse:0.03386734053492546, r2:-0.015868306159973145, dtw:Not calculated


VAL - MSE: 0.0011, MAE: 0.0254, RMSE: 0.0339, RÂ²: -0.0159, MAPE: 603449.81%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.532 MB of 0.532 MB uploadedwandb: \ 0.532 MB of 0.532 MB uploadedwandb: | 0.532 MB of 0.532 MB uploadedwandb: / 0.532 MB of 0.604 MB uploadedwandb: - 0.604 MB of 0.604 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–„â–ƒâ–ƒâ–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–†â–ƒâ–ƒâ–â–ƒâ–‚â–â–‡â–‚â–ˆâ–â–‡â–…â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35998
wandb:     model/trainable_params 35998
wandb:                  test/loss 0.28084
wandb:                 train/loss 0.23083
wandb:   val/directional_accuracy 50.31915
wandb:                   val/loss 0.17101
wandb:                    val/mae 0.02541
wandb:                   val/mape 60344981.25
wandb:                    val/mse 0.00115
wandb:                     val/r2 -0.01587
wandb:                   val/rmse 0.03387
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/jpc70cjv
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_143027-jpc70cjv/logs
Completed: NVIDIA H=5

Training: Mamba on NVIDIA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_143130-qklrj8y6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/qklrj8y6
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H10    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/qklrj8y6
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2457423 Vali Loss: 0.2138988 Test Loss: 0.3566367
Validation loss decreased (inf --> 0.213899).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2416601 Vali Loss: 0.1772527 Test Loss: 0.3360414
Validation loss decreased (0.213899 --> 0.177253).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2391447 Vali Loss: 0.1828327 Test Loss: 0.3260374
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2384738 Vali Loss: 0.1785791 Test Loss: 0.3218473
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2386500 Vali Loss: 0.1921391 Test Loss: 0.3196857
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2381857 Vali Loss: 0.1808511 Test Loss: 0.3187064
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2371379 Vali Loss: 0.1813596 Test Loss: 0.3182844
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.24574230095945804, 'val/loss': 0.2138987798243761, 'test/loss': 0.35663665644824505, '_timestamp': 1762777897.366849}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24166011944749302, 'val/loss': 0.1772526940330863, 'test/loss': 0.3360414132475853, '_timestamp': 1762777899.2450259}).
Epoch: 8, Steps: 133 | Train Loss: 0.2375710 Vali Loss: 0.1792200 Test Loss: 0.3180966
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2379339 Vali Loss: 0.1772256 Test Loss: 0.3179489
Validation loss decreased (0.177253 --> 0.177226).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2368558 Vali Loss: 0.1753297 Test Loss: 0.3178935
Validation loss decreased (0.177226 --> 0.175330).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2374540 Vali Loss: 0.1730128 Test Loss: 0.3178691
Validation loss decreased (0.175330 --> 0.173013).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2382330 Vali Loss: 0.2002591 Test Loss: 0.3178570
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2369997 Vali Loss: 0.1940116 Test Loss: 0.3178498
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2369992 Vali Loss: 0.1938522 Test Loss: 0.3178466
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2370086 Vali Loss: 0.1771567 Test Loss: 0.3178446
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2373629 Vali Loss: 0.1752111 Test Loss: 0.3178442
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2373955 Vali Loss: 0.1733002 Test Loss: 0.3178440
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2373358 Vali Loss: 0.1965321 Test Loss: 0.3178440
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2372083 Vali Loss: 0.1978634 Test Loss: 0.3178440
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2371697 Vali Loss: 0.1805365 Test Loss: 0.3178440
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2375547 Vali Loss: 0.1826738 Test Loss: 0.3178440
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.0011708087986335158, mae:0.025738045573234558, rmse:0.03421708196401596, r2:-0.020646095275878906, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0257, RMSE: 0.0342, RÂ²: -0.0206, MAPE: 555485.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.563 MB of 0.563 MB uploadedwandb: \ 0.563 MB of 0.563 MB uploadedwandb: | 0.563 MB of 0.563 MB uploadedwandb: / 0.563 MB of 0.563 MB uploadedwandb: - 0.563 MB of 0.636 MB uploadedwandb: \ 0.636 MB of 0.636 MB uploadedwandb: | 0.636 MB of 0.636 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–†â–†â–…â–‚â–ƒâ–„â–â–ƒâ–…â–â–â–â–ƒâ–ƒâ–‚â–‚â–‚â–ƒ
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–‚â–†â–ƒâ–ƒâ–ƒâ–‚â–‚â–â–ˆâ–†â–†â–‚â–‚â–â–‡â–‡â–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 37948
wandb:     model/trainable_params 37948
wandb:                  test/loss 0.31784
wandb:                 train/loss 0.23755
wandb:   val/directional_accuracy 49.85507
wandb:                   val/loss 0.18267
wandb:                    val/mae 0.02574
wandb:                   val/mape 55548550.0
wandb:                    val/mse 0.00117
wandb:                     val/r2 -0.02065
wandb:                   val/rmse 0.03422
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/qklrj8y6
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_143130-qklrj8y6/logs
Completed: NVIDIA H=10

Training: Mamba on NVIDIA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_143241-6fxlwigp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/6fxlwigp
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H22    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/6fxlwigp
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2501167 Vali Loss: 0.1871742 Test Loss: 0.4230973
Validation loss decreased (inf --> 0.187174).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2471064 Vali Loss: 0.1857132 Test Loss: 0.3981702
Validation loss decreased (0.187174 --> 0.185713).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2452490 Vali Loss: 0.1856090 Test Loss: 0.3879586
Validation loss decreased (0.185713 --> 0.185609).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2441747 Vali Loss: 0.1848800 Test Loss: 0.3816607
Validation loss decreased (0.185609 --> 0.184880).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2437045 Vali Loss: 0.1826753 Test Loss: 0.3793149
Validation loss decreased (0.184880 --> 0.182675).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2435026 Vali Loss: 0.1845022 Test Loss: 0.3779257
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2435205 Vali Loss: 0.1839220 Test Loss: 0.3774701
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2501167233920459, 'val/loss': 0.18717419036797114, 'test/loss': 0.4230973337377821, '_timestamp': 1762777969.9313817}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24710635829604033, 'val/loss': 0.18571320814745768, 'test/loss': 0.3981701838118689, '_timestamp': 1762777971.7838824}).
Epoch: 8, Steps: 132 | Train Loss: 0.2433110 Vali Loss: 0.1841608 Test Loss: 0.3772473
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2433337 Vali Loss: 0.1841746 Test Loss: 0.3771422
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2434458 Vali Loss: 0.1848292 Test Loss: 0.3770866
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2433054 Vali Loss: 0.1837328 Test Loss: 0.3770610
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2432216 Vali Loss: 0.1833400 Test Loss: 0.3770456
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2431975 Vali Loss: 0.1830692 Test Loss: 0.3770384
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2432275 Vali Loss: 0.1840516 Test Loss: 0.3770352
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2432920 Vali Loss: 0.1846509 Test Loss: 0.3770336
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.0011992166982963681, mae:0.025938289240002632, rmse:0.034629710018634796, r2:-0.016409754753112793, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0259, RMSE: 0.0346, RÂ²: -0.0164, MAPE: 547151.19%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.567 MB of 0.569 MB uploadedwandb: \ 0.567 MB of 0.569 MB uploadedwandb: | 0.569 MB of 0.569 MB uploadedwandb: / 0.569 MB of 0.569 MB uploadedwandb: - 0.569 MB of 0.569 MB uploadedwandb: \ 0.569 MB of 0.641 MB uploadedwandb: | 0.641 MB of 0.641 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–â–…â–„â–…â–…â–†â–„â–ƒâ–‚â–„â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 42628
wandb:     model/trainable_params 42628
wandb:                  test/loss 0.37703
wandb:                 train/loss 0.24329
wandb:   val/directional_accuracy 49.56313
wandb:                   val/loss 0.18465
wandb:                    val/mae 0.02594
wandb:                   val/mape 54715118.75
wandb:                    val/mse 0.0012
wandb:                     val/r2 -0.01641
wandb:                   val/rmse 0.03463
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/6fxlwigp
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_143241-6fxlwigp/logs
Completed: NVIDIA H=22

Training: Mamba on NVIDIA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_143343-kqhh6qct
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/kqhh6qct
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H50    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/kqhh6qct
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.2609774 Vali Loss: 0.2055285 Test Loss: 0.5615093
Validation loss decreased (inf --> 0.205529).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2583421 Vali Loss: 0.2029993 Test Loss: 0.5265764
Validation loss decreased (0.205529 --> 0.202999).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2557309 Vali Loss: 0.2005865 Test Loss: 0.5118629
Validation loss decreased (0.202999 --> 0.200587).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2559916 Vali Loss: 0.1995617 Test Loss: 0.5072306
Validation loss decreased (0.200587 --> 0.199562).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2555839 Vali Loss: 0.1991070 Test Loss: 0.5048560
Validation loss decreased (0.199562 --> 0.199107).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2535598 Vali Loss: 0.1983880 Test Loss: 0.5033350
Validation loss decreased (0.199107 --> 0.198388).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2546222 Vali Loss: 0.1982716 Test Loss: 0.5027114
Validation loss decreased (0.198388 --> 0.198272).  Saving model ...
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.260977441444993, 'val/loss': 0.20552852749824524, 'test/loss': 0.5615093012650808, '_timestamp': 1762778031.7204697}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2583421198933413, 'val/loss': 0.20299925406773886, 'test/loss': 0.5265764271219572, '_timestamp': 1762778033.582401}).
Epoch: 8, Steps: 132 | Train Loss: 0.2536155 Vali Loss: 0.1981789 Test Loss: 0.5023946
Validation loss decreased (0.198272 --> 0.198179).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2538505 Vali Loss: 0.1982814 Test Loss: 0.5023131
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2565145 Vali Loss: 0.1983837 Test Loss: 0.5022464
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2584966 Vali Loss: 0.1981944 Test Loss: 0.5022154
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2544651 Vali Loss: 0.1982273 Test Loss: 0.5021967
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2542696 Vali Loss: 0.1982298 Test Loss: 0.5021892
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2542662 Vali Loss: 0.1982615 Test Loss: 0.5021883
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2570956 Vali Loss: 0.1981046 Test Loss: 0.5021867
Validation loss decreased (0.198179 --> 0.198105).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2585314 Vali Loss: 0.1983968 Test Loss: 0.5021861
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2557014 Vali Loss: 0.1982756 Test Loss: 0.5021859
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2541440 Vali Loss: 0.1985174 Test Loss: 0.5021858
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2539374 Vali Loss: 0.1982229 Test Loss: 0.5021858
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2538533 Vali Loss: 0.1984642 Test Loss: 0.5021858
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2555684 Vali Loss: 0.1985563 Test Loss: 0.5021858
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2537539 Vali Loss: 0.1985420 Test Loss: 0.5021858
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.2533504 Vali Loss: 0.1980295 Test Loss: 0.5021858
Validation loss decreased (0.198105 --> 0.198030).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 132 | Train Loss: 0.2548678 Vali Loss: 0.1982520 Test Loss: 0.5021858
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 132 | Train Loss: 0.2541494 Vali Loss: 0.1984278 Test Loss: 0.5021858
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 132 | Train Loss: 0.2536177 Vali Loss: 0.1982958 Test Loss: 0.5021858
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 132 | Train Loss: 0.2537178 Vali Loss: 0.1988859 Test Loss: 0.5021858
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 132 | Train Loss: 0.2539799 Vali Loss: 0.1979333 Test Loss: 0.5021858
Validation loss decreased (0.198030 --> 0.197933).  Saving model ...
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 132 | Train Loss: 0.2559412 Vali Loss: 0.1983606 Test Loss: 0.5021858
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 132 | Train Loss: 0.2534573 Vali Loss: 0.1981071 Test Loss: 0.5021858
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 132 | Train Loss: 0.2553585 Vali Loss: 0.1983380 Test Loss: 0.5021858
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 132 | Train Loss: 0.2545096 Vali Loss: 0.1983388 Test Loss: 0.5021858
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 132 | Train Loss: 0.2544068 Vali Loss: 0.1981609 Test Loss: 0.5021858
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 132 | Train Loss: 0.2558657 Vali Loss: 0.1981020 Test Loss: 0.5021858
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.1641532182693482e-14
Epoch: 35, Steps: 132 | Train Loss: 0.2540222 Vali Loss: 0.1983757 Test Loss: 0.5021858
EarlyStopping counter: 7 out of 10
Updating learning rate to 5.820766091346741e-15
Epoch: 36, Steps: 132 | Train Loss: 0.2573964 Vali Loss: 0.1981125 Test Loss: 0.5021858
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.9103830456733705e-15
Epoch: 37, Steps: 132 | Train Loss: 0.2546385 Vali Loss: 0.1984919 Test Loss: 0.5021858
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.4551915228366853e-15
Epoch: 38, Steps: 132 | Train Loss: 0.2554921 Vali Loss: 0.1982513 Test Loss: 0.5021858
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.0012001310242339969, mae:0.026252716779708862, rmse:0.03464290872216225, r2:-0.008520245552062988, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0263, RMSE: 0.0346, RÂ²: -0.0085, MAPE: 418686.69%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.631 MB of 0.634 MB uploadedwandb: \ 0.631 MB of 0.634 MB uploadedwandb: | 0.631 MB of 0.634 MB uploadedwandb: / 0.634 MB of 0.634 MB uploadedwandb: - 0.634 MB of 0.710 MB uploadedwandb: \ 0.710 MB of 0.710 MB uploadedwandb: | 0.710 MB of 0.710 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–„â–…â–„â–â–ƒâ–â–‚â–…â–ˆâ–ƒâ–‚â–‚â–†â–ˆâ–„â–‚â–‚â–‚â–„â–‚â–â–ƒâ–‚â–â–â–‚â–…â–â–„â–ƒâ–‚â–„â–‚â–†â–ƒâ–„
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–â–‚â–‚â–‚â–„â–â–‚â–â–‚â–‚â–‚â–â–‚â–â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 37
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 53548
wandb:     model/trainable_params 53548
wandb:                  test/loss 0.50219
wandb:                 train/loss 0.25549
wandb:   val/directional_accuracy 49.69925
wandb:                   val/loss 0.19825
wandb:                    val/mae 0.02625
wandb:                   val/mape 41868668.75
wandb:                    val/mse 0.0012
wandb:                     val/r2 -0.00852
wandb:                   val/rmse 0.03464
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/kqhh6qct
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_143343-kqhh6qct/logs
Completed: NVIDIA H=50

Training: Mamba on NVIDIA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_143530-pc8dfr0v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/pc8dfr0v
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H100   Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/pc8dfr0v
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.2819586 Vali Loss: 0.2433210 Test Loss: 0.8356802
Validation loss decreased (inf --> 0.243321).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2791050 Vali Loss: 0.2421639 Test Loss: 0.7755031
Validation loss decreased (0.243321 --> 0.242164).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.2756288 Vali Loss: 0.2417649 Test Loss: 0.7484781
Validation loss decreased (0.242164 --> 0.241765).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2740870 Vali Loss: 0.2334515 Test Loss: 0.7389238
Validation loss decreased (0.241765 --> 0.233451).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2734116 Vali Loss: 0.2301229 Test Loss: 0.7343592
Validation loss decreased (0.233451 --> 0.230123).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2733760 Vali Loss: 0.2247877 Test Loss: 0.7322787
Validation loss decreased (0.230123 --> 0.224788).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2729584 Vali Loss: 0.2307942 Test Loss: 0.7311626
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.28195859824235625, 'val/loss': 0.24332099854946138, 'test/loss': 0.835680240392685, '_timestamp': 1762778138.8985512}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2791050176207836, 'val/loss': 0.24216385781764985, 'test/loss': 0.7755030870437623, '_timestamp': 1762778140.7650747}).
Epoch: 8, Steps: 130 | Train Loss: 0.2731243 Vali Loss: 0.2357922 Test Loss: 0.7308600
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2729324 Vali Loss: 0.2318946 Test Loss: 0.7306715
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2726480 Vali Loss: 0.2234940 Test Loss: 0.7305548
Validation loss decreased (0.224788 --> 0.223494).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2729899 Vali Loss: 0.2295666 Test Loss: 0.7305134
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2726425 Vali Loss: 0.2302296 Test Loss: 0.7304782
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2727210 Vali Loss: 0.2316004 Test Loss: 0.7304683
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.2728953 Vali Loss: 0.2299142 Test Loss: 0.7304628
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.2729657 Vali Loss: 0.2280555 Test Loss: 0.7304601
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 130 | Train Loss: 0.2728701 Vali Loss: 0.2294913 Test Loss: 0.7304589
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 130 | Train Loss: 0.2730381 Vali Loss: 0.2306316 Test Loss: 0.7304587
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 130 | Train Loss: 0.2728308 Vali Loss: 0.2310511 Test Loss: 0.7304586
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 130 | Train Loss: 0.2730787 Vali Loss: 0.2335847 Test Loss: 0.7304586
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 130 | Train Loss: 0.2727354 Vali Loss: 0.2344110 Test Loss: 0.7304585
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0012996223522350192, mae:0.0275521669536829, rmse:0.03605027496814728, r2:-0.00948178768157959, dtw:Not calculated


VAL - MSE: 0.0013, MAE: 0.0276, RMSE: 0.0361, RÂ²: -0.0095, MAPE: 261134.03%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.709 MB of 0.714 MB uploadedwandb: \ 0.709 MB of 0.714 MB uploadedwandb: | 0.714 MB of 0.714 MB uploadedwandb: / 0.714 MB of 0.787 MB uploadedwandb: - 0.787 MB of 0.787 MB uploadedwandb: \ 0.787 MB of 0.787 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–â–â–‚â–‚â–‚â–‚â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–„â–â–„â–†â–„â–â–ƒâ–„â–„â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 73048
wandb:     model/trainable_params 73048
wandb:                  test/loss 0.73046
wandb:                 train/loss 0.27274
wandb:   val/directional_accuracy 50.03608
wandb:                   val/loss 0.23441
wandb:                    val/mae 0.02755
wandb:                   val/mape 26113403.125
wandb:                    val/mse 0.0013
wandb:                     val/r2 -0.00948
wandb:                   val/rmse 0.03605
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/pc8dfr0v
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_143530-pc8dfr0v/logs
Completed: NVIDIA H=100

Training: Mamba on APPLE for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_143638-omt3bqb5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/omt3bqb5
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H3      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/omt3bqb5
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.2306321 Vali Loss: 0.0865742 Test Loss: 0.1276309
Validation loss decreased (inf --> 0.086574).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2244123 Vali Loss: 0.0848729 Test Loss: 0.1264926
Validation loss decreased (0.086574 --> 0.084873).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2178714 Vali Loss: 0.0832271 Test Loss: 0.1243467
Validation loss decreased (0.084873 --> 0.083227).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2159199 Vali Loss: 0.0825583 Test Loss: 0.1244670
Validation loss decreased (0.083227 --> 0.082558).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2150647 Vali Loss: 0.0856008 Test Loss: 0.1245723
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2139222 Vali Loss: 0.0845905 Test Loss: 0.1245845
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2136695 Vali Loss: 0.0834685 Test Loss: 0.1245466
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23063214885112934, 'val/loss': 0.08657418703660369, 'test/loss': 0.1276308805681765, '_timestamp': 1762778205.7914865}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.22441228047797554, 'val/loss': 0.08487292099744081, 'test/loss': 0.12649256037548184, '_timestamp': 1762778207.6654654}).
Epoch: 8, Steps: 133 | Train Loss: 0.2138680 Vali Loss: 0.0825934 Test Loss: 0.1245345
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2140596 Vali Loss: 0.0825583 Test Loss: 0.1245307
Validation loss decreased (0.082558 --> 0.082558).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2141282 Vali Loss: 0.0882854 Test Loss: 0.1245323
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2146020 Vali Loss: 0.0861095 Test Loss: 0.1245324
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2141011 Vali Loss: 0.0834767 Test Loss: 0.1245329
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2137223 Vali Loss: 0.0849476 Test Loss: 0.1245330
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2135679 Vali Loss: 0.0854076 Test Loss: 0.1245325
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2138018 Vali Loss: 0.0828035 Test Loss: 0.1245325
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2141291 Vali Loss: 0.0855946 Test Loss: 0.1245324
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2142324 Vali Loss: 0.0860878 Test Loss: 0.1245324
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2141182 Vali Loss: 0.0866261 Test Loss: 0.1245324
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2141335 Vali Loss: 0.0845877 Test Loss: 0.1245324
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.00020185259927529842, mae:0.010253838263452053, rmse:0.01420748420059681, r2:-0.009552001953125, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0103, RMSE: 0.0142, RÂ²: -0.0096, MAPE: 416669.53%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.457 MB of 0.457 MB uploadedwandb: \ 0.457 MB of 0.457 MB uploadedwandb: | 0.457 MB of 0.457 MB uploadedwandb: / 0.457 MB of 0.457 MB uploadedwandb: - 0.457 MB of 0.457 MB uploadedwandb: \ 0.457 MB of 0.457 MB uploadedwandb: | 0.457 MB of 0.457 MB uploadedwandb: / 0.457 MB of 0.457 MB uploadedwandb: - 0.506 MB of 0.578 MB uploaded (0.002 MB deduped)wandb: \ 0.578 MB of 0.578 MB uploaded (0.002 MB deduped)wandb: | 0.578 MB of 0.578 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–…â–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–â–‚â–‚â–ƒâ–‚â–â–â–â–‚â–‚â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–â–…â–ƒâ–‚â–â–â–ˆâ–…â–‚â–„â–„â–â–…â–…â–†â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 18
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35218
wandb:     model/trainable_params 35218
wandb:                  test/loss 0.12453
wandb:                 train/loss 0.21413
wandb:   val/directional_accuracy 45.99156
wandb:                   val/loss 0.08459
wandb:                    val/mae 0.01025
wandb:                   val/mape 41666953.125
wandb:                    val/mse 0.0002
wandb:                     val/r2 -0.00955
wandb:                   val/rmse 0.01421
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/omt3bqb5
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_143638-omt3bqb5/logs
Completed: APPLE H=3

Training: Mamba on APPLE for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_143750-3crtjzkb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/3crtjzkb
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H5      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/3crtjzkb
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.2332394 Vali Loss: 0.0876458 Test Loss: 0.1290435
Validation loss decreased (inf --> 0.087646).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2262634 Vali Loss: 0.0895127 Test Loss: 0.1295921
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2232746 Vali Loss: 0.0881311 Test Loss: 0.1281856
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2201150 Vali Loss: 0.0859996 Test Loss: 0.1282271
Validation loss decreased (0.087646 --> 0.086000).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2193164 Vali Loss: 0.0850845 Test Loss: 0.1280628
Validation loss decreased (0.086000 --> 0.085084).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2184932 Vali Loss: 0.0868933 Test Loss: 0.1280244
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2184791 Vali Loss: 0.0856328 Test Loss: 0.1279776
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23323937723959298, 'val/loss': 0.08764575514942408, 'test/loss': 0.1290434869006276, '_timestamp': 1762778277.442946}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.22626342786882156, 'val/loss': 0.0895126722753048, 'test/loss': 0.1295921360142529, '_timestamp': 1762778279.3237545}).
Epoch: 8, Steps: 133 | Train Loss: 0.2183375 Vali Loss: 0.0879256 Test Loss: 0.1279697
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2184246 Vali Loss: 0.0836315 Test Loss: 0.1279564
Validation loss decreased (0.085084 --> 0.083631).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2176737 Vali Loss: 0.0843063 Test Loss: 0.1279550
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2178978 Vali Loss: 0.0882346 Test Loss: 0.1279553
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2178139 Vali Loss: 0.0840760 Test Loss: 0.1279541
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2177517 Vali Loss: 0.0838510 Test Loss: 0.1279529
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2179621 Vali Loss: 0.0849664 Test Loss: 0.1279525
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2181831 Vali Loss: 0.0861977 Test Loss: 0.1279523
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2180874 Vali Loss: 0.0842490 Test Loss: 0.1279522
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2180789 Vali Loss: 0.0885176 Test Loss: 0.1279522
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2185116 Vali Loss: 0.0850095 Test Loss: 0.1279522
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2174872 Vali Loss: 0.0852745 Test Loss: 0.1279522
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.00020312986453063786, mae:0.01026515755802393, rmse:0.014252363704144955, r2:-0.011302828788757324, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0103, RMSE: 0.0143, RÂ²: -0.0113, MAPE: 250383.80%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.491 MB of 0.491 MB uploadedwandb: \ 0.491 MB of 0.491 MB uploadedwandb: | 0.491 MB of 0.491 MB uploadedwandb: / 0.491 MB of 0.491 MB uploadedwandb: - 0.491 MB of 0.563 MB uploadedwandb: \ 0.563 MB of 0.563 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‡â–ˆâ–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–„â–ƒâ–†â–„â–‡â–â–‚â–ˆâ–‚â–â–ƒâ–…â–‚â–ˆâ–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 18
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35998
wandb:     model/trainable_params 35998
wandb:                  test/loss 0.12795
wandb:                 train/loss 0.21749
wandb:   val/directional_accuracy 48.51064
wandb:                   val/loss 0.08527
wandb:                    val/mae 0.01027
wandb:                   val/mape 25038379.6875
wandb:                    val/mse 0.0002
wandb:                     val/r2 -0.0113
wandb:                   val/rmse 0.01425
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/3crtjzkb
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_143750-3crtjzkb/logs
Completed: APPLE H=5

Training: Mamba on APPLE for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_143856-cpd5ughd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/cpd5ughd
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H10     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/cpd5ughd
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2358432 Vali Loss: 0.0850431 Test Loss: 0.1311064
Validation loss decreased (inf --> 0.085043).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2314479 Vali Loss: 0.0908981 Test Loss: 0.1312727
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2276617 Vali Loss: 0.0890594 Test Loss: 0.1315967
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2267411 Vali Loss: 0.0869244 Test Loss: 0.1310316
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2252383 Vali Loss: 0.0884522 Test Loss: 0.1310597
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2260602 Vali Loss: 0.0938946 Test Loss: 0.1310123
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2249073 Vali Loss: 0.0908851 Test Loss: 0.1309889
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23584323426834622, 'val/loss': 0.08504310622811317, 'test/loss': 0.13110639061778784, '_timestamp': 1762778345.5209343}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2314479142651522, 'val/loss': 0.09089808259159327, 'test/loss': 0.131272716447711, '_timestamp': 1762778347.3815432}).
Epoch: 8, Steps: 133 | Train Loss: 0.2247821 Vali Loss: 0.0893364 Test Loss: 0.1309646
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2256609 Vali Loss: 0.0886136 Test Loss: 0.1309586
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2250164 Vali Loss: 0.0888672 Test Loss: 0.1309525
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2246693 Vali Loss: 0.0891355 Test Loss: 0.1309511
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.00020767757087014616, mae:0.010369696654379368, rmse:0.014411022886633873, r2:-0.02573680877685547, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0104, RMSE: 0.0144, RÂ²: -0.0257, MAPE: 357095.31%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.567 MB of 0.568 MB uploadedwandb: \ 0.567 MB of 0.568 MB uploadedwandb: | 0.568 MB of 0.568 MB uploadedwandb: / 0.568 MB of 0.639 MB uploadedwandb: - 0.568 MB of 0.639 MB uploadedwandb: \ 0.639 MB of 0.639 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–‚â–‚â–â–â–â–â–
wandb:                 train/loss â–ˆâ–†â–‚â–„â–‚â–â–ƒâ–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–â–ƒâ–ˆâ–…â–ƒâ–ƒâ–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 37948
wandb:     model/trainable_params 37948
wandb:                  test/loss 0.13095
wandb:                 train/loss 0.22467
wandb:   val/directional_accuracy 49.37198
wandb:                   val/loss 0.08914
wandb:                    val/mae 0.01037
wandb:                   val/mape 35709531.25
wandb:                    val/mse 0.00021
wandb:                     val/r2 -0.02574
wandb:                   val/rmse 0.01441
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/cpd5ughd
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_143856-cpd5ughd/logs
Completed: APPLE H=10

Training: Mamba on APPLE for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_143947-3tdtxbsm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/3tdtxbsm
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H22     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/3tdtxbsm
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2427358 Vali Loss: 0.0888050 Test Loss: 0.1336867
Validation loss decreased (inf --> 0.088805).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2378967 Vali Loss: 0.0896493 Test Loss: 0.1352285
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2343187 Vali Loss: 0.0893510 Test Loss: 0.1351604
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2328132 Vali Loss: 0.0895479 Test Loss: 0.1355640
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2321174 Vali Loss: 0.0891334 Test Loss: 0.1354729
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2317955 Vali Loss: 0.0890905 Test Loss: 0.1353644
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2316720 Vali Loss: 0.0891826 Test Loss: 0.1353413
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.24273581780267484, 'val/loss': 0.08880498260259628, 'test/loss': 0.13368669471570424, '_timestamp': 1762778393.4781387}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2378967141337467, 'val/loss': 0.08964925046477999, 'test/loss': 0.13522852531501225, '_timestamp': 1762778395.3632553}).
Epoch: 8, Steps: 132 | Train Loss: 0.2317506 Vali Loss: 0.0891290 Test Loss: 0.1353300
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2317673 Vali Loss: 0.0891110 Test Loss: 0.1353179
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2317733 Vali Loss: 0.0892190 Test Loss: 0.1353192
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2315469 Vali Loss: 0.0890250 Test Loss: 0.1353184
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.00021331568132154644, mae:0.010486758314073086, rmse:0.014605330303311348, r2:-0.028005123138427734, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0105, RMSE: 0.0146, RÂ²: -0.0280, MAPE: 344432.72%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.645 MB of 0.646 MB uploadedwandb: \ 0.645 MB of 0.646 MB uploadedwandb: | 0.646 MB of 0.646 MB uploadedwandb: / 0.646 MB of 0.646 MB uploadedwandb: - 0.646 MB of 0.717 MB uploadedwandb: \ 0.717 MB of 0.717 MB uploadedwandb: | 0.717 MB of 0.717 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–†â–…â–„â–„â–„â–„â–„
wandb:                 train/loss â–ˆâ–„â–‚â–‚â–â–‚â–‚â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–ˆâ–‚â–‚â–ƒâ–‚â–‚â–„â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 42628
wandb:     model/trainable_params 42628
wandb:                  test/loss 0.13532
wandb:                 train/loss 0.23155
wandb:   val/directional_accuracy 49.08257
wandb:                   val/loss 0.08903
wandb:                    val/mae 0.01049
wandb:                   val/mape 34443271.875
wandb:                    val/mse 0.00021
wandb:                     val/r2 -0.02801
wandb:                   val/rmse 0.01461
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/3tdtxbsm
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_143947-3tdtxbsm/logs
Completed: APPLE H=22

Training: Mamba on APPLE for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_144040-ojv9ypie
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/ojv9ypie
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H50     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/ojv9ypie
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.2553219 Vali Loss: 0.0915600 Test Loss: 0.1509035
Validation loss decreased (inf --> 0.091560).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2511482 Vali Loss: 0.0921618 Test Loss: 0.1547728
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2465104 Vali Loss: 0.0921926 Test Loss: 0.1564678
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2445080 Vali Loss: 0.0916750 Test Loss: 0.1554230
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2446846 Vali Loss: 0.0915324 Test Loss: 0.1555552
Validation loss decreased (0.091560 --> 0.091532).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2435390 Vali Loss: 0.0915143 Test Loss: 0.1556869
Validation loss decreased (0.091532 --> 0.091514).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2429801 Vali Loss: 0.0915053 Test Loss: 0.1558494
Validation loss decreased (0.091514 --> 0.091505).  Saving model ...
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2553218696266413, 'val/loss': 0.09156000862518947, 'test/loss': 0.15090352793534598, '_timestamp': 1762778447.006896}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25114819892879686, 'val/loss': 0.09216180567940076, 'test/loss': 0.15477279325326285, '_timestamp': 1762778448.9498463}).
Epoch: 8, Steps: 132 | Train Loss: 0.2426997 Vali Loss: 0.0914323 Test Loss: 0.1557836
Validation loss decreased (0.091505 --> 0.091432).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2431474 Vali Loss: 0.0914770 Test Loss: 0.1557750
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2444575 Vali Loss: 0.0915328 Test Loss: 0.1557706
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2521760 Vali Loss: 0.0914962 Test Loss: 0.1557677
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2428106 Vali Loss: 0.0915041 Test Loss: 0.1557660
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2441897 Vali Loss: 0.0914922 Test Loss: 0.1557663
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2436674 Vali Loss: 0.0914864 Test Loss: 0.1557656
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2443314 Vali Loss: 0.0914942 Test Loss: 0.1557653
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2473584 Vali Loss: 0.0914898 Test Loss: 0.1557653
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2429994 Vali Loss: 0.0914606 Test Loss: 0.1557653
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2429153 Vali Loss: 0.0914675 Test Loss: 0.1557653
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.00022734566300641745, mae:0.010877016931772232, rmse:0.01507798582315445, r2:-0.02465641498565674, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0109, RMSE: 0.0151, RÂ²: -0.0247, MAPE: 236704.92%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.692 MB of 0.694 MB uploadedwandb: \ 0.694 MB of 0.694 MB uploadedwandb: | 0.694 MB of 0.694 MB uploadedwandb: / 0.694 MB of 0.766 MB uploadedwandb: - 0.766 MB of 0.766 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–‚â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–„â–‚â–‚â–‚â–â–â–â–‚â–ˆâ–â–‚â–‚â–‚â–„â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–‚â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 17
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 53548
wandb:     model/trainable_params 53548
wandb:                  test/loss 0.15577
wandb:                 train/loss 0.24292
wandb:   val/directional_accuracy 49.73147
wandb:                   val/loss 0.09147
wandb:                    val/mae 0.01088
wandb:                   val/mape 23670492.1875
wandb:                    val/mse 0.00023
wandb:                     val/r2 -0.02466
wandb:                   val/rmse 0.01508
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/ojv9ypie
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_144040-ojv9ypie/logs
Completed: APPLE H=50

Training: Mamba on APPLE for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_144144-ers6h82t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/ers6h82t
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H100    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/ers6h82t
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.2730915 Vali Loss: 0.0992124 Test Loss: 0.1673684
Validation loss decreased (inf --> 0.099212).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2675001 Vali Loss: 0.1015967 Test Loss: 0.1749921
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.2622638 Vali Loss: 0.1019466 Test Loss: 0.1774007
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2606317 Vali Loss: 0.1012554 Test Loss: 0.1765848
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2599002 Vali Loss: 0.1003493 Test Loss: 0.1771528
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2595520 Vali Loss: 0.1000575 Test Loss: 0.1773552
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2594822 Vali Loss: 0.1011277 Test Loss: 0.1773335
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2730914832307742, 'val/loss': 0.09921240955591201, 'test/loss': 0.16736838817596436, '_timestamp': 1762778511.321476}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26750013530254363, 'val/loss': 0.10159672200679778, 'test/loss': 0.1749920815229416, '_timestamp': 1762778513.1888223}).
Epoch: 8, Steps: 130 | Train Loss: 0.2596374 Vali Loss: 0.1011331 Test Loss: 0.1773401
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2596852 Vali Loss: 0.1013497 Test Loss: 0.1773388
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2592885 Vali Loss: 0.0994727 Test Loss: 0.1773459
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2597082 Vali Loss: 0.1011307 Test Loss: 0.1773489
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.00024629756808280945, mae:0.011310148984193802, rmse:0.015693871304392815, r2:-0.04313957691192627, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0113, RMSE: 0.0157, RÂ²: -0.0431, MAPE: 674458.62%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.713 MB of 0.717 MB uploadedwandb: \ 0.713 MB of 0.717 MB uploadedwandb: | 0.713 MB of 0.717 MB uploadedwandb: / 0.717 MB of 0.717 MB uploadedwandb: - 0.717 MB of 0.717 MB uploadedwandb: \ 0.717 MB of 0.789 MB uploadedwandb: | 0.789 MB of 0.789 MB uploadedwandb: / 0.789 MB of 0.789 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–†â–ˆâ–‡â–‡â–‡â–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–„â–‚â–‚â–â–‚â–‚â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–ƒâ–ƒâ–†â–†â–†â–â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 73048
wandb:     model/trainable_params 73048
wandb:                  test/loss 0.17735
wandb:                 train/loss 0.25971
wandb:   val/directional_accuracy 49.55988
wandb:                   val/loss 0.10113
wandb:                    val/mae 0.01131
wandb:                   val/mape 67445862.5
wandb:                    val/mse 0.00025
wandb:                     val/r2 -0.04314
wandb:                   val/rmse 0.01569
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/ers6h82t
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_144144-ers6h82t/logs
Completed: APPLE H=100

Training: Mamba on SP500 for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_144238-sxq3ln3b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/sxq3ln3b
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H3      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/sxq3ln3b
>>>>>>>start training : long_term_forecast_Mamba_SP500_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 3
============================================================
train 4241
val 237
test 238
Epoch: 1, Steps: 133 | Train Loss: 0.1788524 Vali Loss: 0.0678778 Test Loss: 0.0751026
Validation loss decreased (inf --> 0.067878).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.1776037 Vali Loss: 0.0684688 Test Loss: 0.0742747
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.1769178 Vali Loss: 0.0667518 Test Loss: 0.0737335
Validation loss decreased (0.067878 --> 0.066752).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1743766 Vali Loss: 0.0673000 Test Loss: 0.0735021
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1744497 Vali Loss: 0.0666213 Test Loss: 0.0733418
Validation loss decreased (0.066752 --> 0.066621).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1743111 Vali Loss: 0.0678516 Test Loss: 0.0732678
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1742530 Vali Loss: 0.0656145 Test Loss: 0.0732280
Validation loss decreased (0.066621 --> 0.065614).  Saving model ...
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.17885241411010125, 'val/loss': 0.06787781789898872, 'test/loss': 0.0751025932841003, '_timestamp': 1762778564.679654}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.1776037023479777, 'val/loss': 0.06846880447119474, 'test/loss': 0.07427474297583103, '_timestamp': 1762778566.6138613}).
Epoch: 8, Steps: 133 | Train Loss: 0.1739880 Vali Loss: 0.0682379 Test Loss: 0.0732114
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1743990 Vali Loss: 0.0679404 Test Loss: 0.0732028
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1750076 Vali Loss: 0.0668560 Test Loss: 0.0731982
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1741783 Vali Loss: 0.0643423 Test Loss: 0.0731958
Validation loss decreased (0.065614 --> 0.064342).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1741164 Vali Loss: 0.0674902 Test Loss: 0.0731946
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1736123 Vali Loss: 0.0678885 Test Loss: 0.0731941
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1741792 Vali Loss: 0.0675878 Test Loss: 0.0731938
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1742104 Vali Loss: 0.0678191 Test Loss: 0.0731937
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1734958 Vali Loss: 0.0664307 Test Loss: 0.0731936
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1736178 Vali Loss: 0.0662938 Test Loss: 0.0731936
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1737632 Vali Loss: 0.0676386 Test Loss: 0.0731936
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1736723 Vali Loss: 0.0680269 Test Loss: 0.0731936
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1743744 Vali Loss: 0.0676261 Test Loss: 0.0731936
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1740358 Vali Loss: 0.0671668 Test Loss: 0.0731936
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SP500_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 238
test shape: (238, 3, 1) (238, 3, 1)
test shape: (238, 3, 1) (238, 3, 1)


	mse:6.494968693004921e-05, mae:0.005942987743765116, rmse:0.00805913656949997, r2:0.000947117805480957, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0059, RMSE: 0.0081, RÂ²: 0.0009, MAPE: 1.82%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.472 MB of 0.473 MB uploadedwandb: \ 0.473 MB of 0.473 MB uploadedwandb: | 0.473 MB of 0.473 MB uploadedwandb: / 0.473 MB of 0.473 MB uploadedwandb: - 0.473 MB of 0.473 MB uploadedwandb: \ 0.521 MB of 0.594 MB uploaded (0.002 MB deduped)wandb: | 0.594 MB of 0.594 MB uploaded (0.002 MB deduped)wandb: / 0.594 MB of 0.594 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–„â–‚â–‚â–â–‚â–‚â–â–â–‚â–â–ƒâ–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–†â–…â–‡â–ƒâ–ˆâ–‡â–†â–â–‡â–‡â–‡â–‡â–…â–…â–‡â–ˆâ–‡â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35218
wandb:     model/trainable_params 35218
wandb:                  test/loss 0.07319
wandb:                 train/loss 0.17404
wandb:   val/directional_accuracy 44.53782
wandb:                   val/loss 0.06717
wandb:                    val/mae 0.00594
wandb:                   val/mape 181.55607
wandb:                    val/mse 6e-05
wandb:                     val/r2 0.00095
wandb:                   val/rmse 0.00806
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/sxq3ln3b
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_144238-sxq3ln3b/logs
Completed: SP500 H=3

Training: Mamba on SP500 for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_144351-w1p9z3vg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/w1p9z3vg
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H5      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/w1p9z3vg
>>>>>>>start training : long_term_forecast_Mamba_SP500_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 5
============================================================
train 4239
val 235
test 236
Epoch: 1, Steps: 133 | Train Loss: 0.1801395 Vali Loss: 0.0713168 Test Loss: 0.0770384
Validation loss decreased (inf --> 0.071317).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.1785941 Vali Loss: 0.0672549 Test Loss: 0.0762159
Validation loss decreased (0.071317 --> 0.067255).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.1762495 Vali Loss: 0.0694855 Test Loss: 0.0756848
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1774779 Vali Loss: 0.0686490 Test Loss: 0.0753421
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1750272 Vali Loss: 0.0699714 Test Loss: 0.0751547
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1751934 Vali Loss: 0.0681550 Test Loss: 0.0750647
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1767405 Vali Loss: 0.0653954 Test Loss: 0.0750229
Validation loss decreased (0.067255 --> 0.065395).  Saving model ...
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.18013953820879297, 'val/loss': 0.07131676049903035, 'test/loss': 0.07703842362388968, '_timestamp': 1762778639.0432084}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.17859408483469397, 'val/loss': 0.06725488184019923, 'test/loss': 0.07621585112065077, '_timestamp': 1762778641.0133455}).
Epoch: 8, Steps: 133 | Train Loss: 0.1751583 Vali Loss: 0.0676073 Test Loss: 0.0749996
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1752959 Vali Loss: 0.0676637 Test Loss: 0.0749887
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1748698 Vali Loss: 0.0696925 Test Loss: 0.0749834
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1759585 Vali Loss: 0.0701257 Test Loss: 0.0749805
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1753220 Vali Loss: 0.0686201 Test Loss: 0.0749792
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1756526 Vali Loss: 0.0684123 Test Loss: 0.0749785
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1771436 Vali Loss: 0.0687310 Test Loss: 0.0749782
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1752538 Vali Loss: 0.0688444 Test Loss: 0.0749780
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1754403 Vali Loss: 0.0686093 Test Loss: 0.0749780
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1749983 Vali Loss: 0.0676177 Test Loss: 0.0749779
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SP500_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
test shape: (236, 5, 1) (236, 5, 1)
test shape: (236, 5, 1) (236, 5, 1)


	mse:6.54492323519662e-05, mae:0.00596848176792264, rmse:0.008090070448815823, r2:-0.007511258125305176, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0060, RMSE: 0.0081, RÂ²: -0.0075, MAPE: 1.51%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.523 MB of 0.523 MB uploadedwandb: \ 0.523 MB of 0.523 MB uploadedwandb: | 0.523 MB of 0.523 MB uploadedwandb: / 0.523 MB of 0.523 MB uploadedwandb: - 0.523 MB of 0.596 MB uploadedwandb: \ 0.596 MB of 0.596 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–…â–ˆâ–â–‚â–†â–‚â–‚â–â–„â–‚â–ƒâ–‡â–‚â–ƒâ–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–†â–ˆâ–…â–â–„â–„â–‡â–ˆâ–†â–…â–†â–†â–†â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35998
wandb:     model/trainable_params 35998
wandb:                  test/loss 0.07498
wandb:                 train/loss 0.175
wandb:   val/directional_accuracy 49.89407
wandb:                   val/loss 0.06762
wandb:                    val/mae 0.00597
wandb:                   val/mape 150.76141
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.00751
wandb:                   val/rmse 0.00809
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/w1p9z3vg
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_144351-w1p9z3vg/logs
Completed: SP500 H=5

Training: Mamba on SP500 for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_144456-2e0sxedf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/2e0sxedf
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H10     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/2e0sxedf
>>>>>>>start training : long_term_forecast_Mamba_SP500_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 10
============================================================
train 4234
val 230
test 231
Epoch: 1, Steps: 133 | Train Loss: 0.1812602 Vali Loss: 0.0692117 Test Loss: 0.0800250
Validation loss decreased (inf --> 0.069212).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.1790277 Vali Loss: 0.0703385 Test Loss: 0.0792416
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.1782398 Vali Loss: 0.0707564 Test Loss: 0.0786521
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1772740 Vali Loss: 0.0707266 Test Loss: 0.0783108
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1766916 Vali Loss: 0.0685502 Test Loss: 0.0781182
Validation loss decreased (0.069212 --> 0.068550).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1764892 Vali Loss: 0.0679444 Test Loss: 0.0780245
Validation loss decreased (0.068550 --> 0.067944).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1768823 Vali Loss: 0.0685678 Test Loss: 0.0779713
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.18126020160384645, 'val/loss': 0.06921170884743333, 'test/loss': 0.08002498792484403, '_timestamp': 1762778703.7067149}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.17902769753359316, 'val/loss': 0.07033848715946078, 'test/loss': 0.07924160920083523, '_timestamp': 1762778705.6289191}).
Epoch: 8, Steps: 133 | Train Loss: 0.1767534 Vali Loss: 0.0711418 Test Loss: 0.0779477
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1768152 Vali Loss: 0.0672876 Test Loss: 0.0779361
Validation loss decreased (0.067944 --> 0.067288).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1766193 Vali Loss: 0.0706952 Test Loss: 0.0779305
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1763620 Vali Loss: 0.0689263 Test Loss: 0.0779274
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1765967 Vali Loss: 0.0710142 Test Loss: 0.0779259
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1767955 Vali Loss: 0.0700138 Test Loss: 0.0779252
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1765360 Vali Loss: 0.0723019 Test Loss: 0.0779248
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1762669 Vali Loss: 0.0682031 Test Loss: 0.0779246
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1768613 Vali Loss: 0.0673729 Test Loss: 0.0779246
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1762402 Vali Loss: 0.0687916 Test Loss: 0.0779245
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1763440 Vali Loss: 0.0673098 Test Loss: 0.0779245
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1762044 Vali Loss: 0.0689646 Test Loss: 0.0779245
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SP500_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 231
test shape: (231, 10, 1) (231, 10, 1)
test shape: (231, 10, 1) (231, 10, 1)


	mse:6.543735071318224e-05, mae:0.00597123010084033, rmse:0.008089335635304451, r2:-0.006685137748718262, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0060, RMSE: 0.0081, RÂ²: -0.0067, MAPE: 1.74%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.533 MB of 0.533 MB uploadedwandb: \ 0.533 MB of 0.533 MB uploadedwandb: | 0.533 MB of 0.533 MB uploadedwandb: / 0.533 MB of 0.533 MB uploadedwandb: - 0.533 MB of 0.606 MB uploadedwandb: \ 0.606 MB of 0.606 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–â–ƒâ–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–†â–ƒâ–‚â–ƒâ–†â–â–†â–ƒâ–†â–…â–ˆâ–‚â–â–ƒâ–â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 18
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 37948
wandb:     model/trainable_params 37948
wandb:                  test/loss 0.07792
wandb:                 train/loss 0.1762
wandb:   val/directional_accuracy 50.45695
wandb:                   val/loss 0.06896
wandb:                    val/mae 0.00597
wandb:                   val/mape 173.52263
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.00669
wandb:                   val/rmse 0.00809
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/2e0sxedf
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_144456-2e0sxedf/logs
Completed: SP500 H=10

Training: Mamba on SP500 for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_144603-jy2a3ssm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/jy2a3ssm
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H22     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/jy2a3ssm
>>>>>>>start training : long_term_forecast_Mamba_SP500_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 22
============================================================
train 4222
val 218
test 219
Epoch: 1, Steps: 132 | Train Loss: 0.1838718 Vali Loss: 0.0727697 Test Loss: 0.0741248
Validation loss decreased (inf --> 0.072770).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.1820881 Vali Loss: 0.0718392 Test Loss: 0.0732928
Validation loss decreased (0.072770 --> 0.071839).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.1812293 Vali Loss: 0.0712448 Test Loss: 0.0726151
Validation loss decreased (0.071839 --> 0.071245).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.1806712 Vali Loss: 0.0711438 Test Loss: 0.0722045
Validation loss decreased (0.071245 --> 0.071144).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.1800631 Vali Loss: 0.0709258 Test Loss: 0.0719719
Validation loss decreased (0.071144 --> 0.070926).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.1801269 Vali Loss: 0.0706116 Test Loss: 0.0718512
Validation loss decreased (0.070926 --> 0.070612).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.1798420 Vali Loss: 0.0705399 Test Loss: 0.0717912
Validation loss decreased (0.070612 --> 0.070540).  Saving model ...
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.18387176452035253, 'val/loss': 0.07276968551533562, 'test/loss': 0.07412483862468175, '_timestamp': 1762778770.387174}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.18208812165892485, 'val/loss': 0.0718391654746873, 'test/loss': 0.07329283654689789, '_timestamp': 1762778772.3566506}).
Epoch: 8, Steps: 132 | Train Loss: 0.1798233 Vali Loss: 0.0706261 Test Loss: 0.0717610
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.1795399 Vali Loss: 0.0707133 Test Loss: 0.0717464
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.1796737 Vali Loss: 0.0708524 Test Loss: 0.0717391
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.1797595 Vali Loss: 0.0704680 Test Loss: 0.0717356
Validation loss decreased (0.070540 --> 0.070468).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.1796197 Vali Loss: 0.0708428 Test Loss: 0.0717338
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.1796181 Vali Loss: 0.0706283 Test Loss: 0.0717329
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.1795456 Vali Loss: 0.0707461 Test Loss: 0.0717325
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.1797274 Vali Loss: 0.0709394 Test Loss: 0.0717322
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.1795886 Vali Loss: 0.0705833 Test Loss: 0.0717322
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.1798254 Vali Loss: 0.0703931 Test Loss: 0.0717321
Validation loss decreased (0.070468 --> 0.070393).  Saving model ...
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.1797048 Vali Loss: 0.0705817 Test Loss: 0.0717321
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.1796639 Vali Loss: 0.0706966 Test Loss: 0.0717321
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.1797819 Vali Loss: 0.0706321 Test Loss: 0.0717321
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.1796644 Vali Loss: 0.0706061 Test Loss: 0.0717321
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.1797142 Vali Loss: 0.0706213 Test Loss: 0.0717321
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.1796190 Vali Loss: 0.0705104 Test Loss: 0.0717321
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 132 | Train Loss: 0.1798180 Vali Loss: 0.0708201 Test Loss: 0.0717321
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 132 | Train Loss: 0.1796751 Vali Loss: 0.0708165 Test Loss: 0.0717321
EarlyStopping counter: 8 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 132 | Train Loss: 0.1796470 Vali Loss: 0.0706025 Test Loss: 0.0717321
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 132 | Train Loss: 0.1797789 Vali Loss: 0.0707732 Test Loss: 0.0717321
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SP500_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 219
test shape: (219, 22, 1) (219, 22, 1)
test shape: (219, 22, 1) (219, 22, 1)


	mse:6.433633097913116e-05, mae:0.005940946750342846, rmse:0.008020993322134018, r2:-0.007688760757446289, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0059, RMSE: 0.0080, RÂ²: -0.0077, MAPE: 1.88%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.619 MB of 0.620 MB uploadedwandb: \ 0.619 MB of 0.620 MB uploadedwandb: | 0.620 MB of 0.620 MB uploadedwandb: / 0.620 MB of 0.694 MB uploadedwandb: - 0.694 MB of 0.694 MB uploadedwandb: \ 0.694 MB of 0.694 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–†â–ƒâ–ƒâ–‚â–‚â–â–‚â–‚â–â–â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‡â–…â–ƒâ–‚â–ƒâ–„â–…â–‚â–…â–ƒâ–„â–…â–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–…â–„â–ƒâ–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 26
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 42628
wandb:     model/trainable_params 42628
wandb:                  test/loss 0.07173
wandb:                 train/loss 0.17978
wandb:   val/directional_accuracy 50.38052
wandb:                   val/loss 0.07077
wandb:                    val/mae 0.00594
wandb:                   val/mape 188.32815
wandb:                    val/mse 6e-05
wandb:                     val/r2 -0.00769
wandb:                   val/rmse 0.00802
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/jy2a3ssm
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_144603-jy2a3ssm/logs
Completed: SP500 H=22

Training: Mamba on SP500 for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_144727-rmq93ybc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/rmq93ybc
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H50     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/rmq93ybc
>>>>>>>start training : long_term_forecast_Mamba_SP500_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 50
============================================================
train 4194
val 190
test 191
Epoch: 1, Steps: 132 | Train Loss: 0.1882779 Vali Loss: 0.0732229 Test Loss: 0.0797900
Validation loss decreased (inf --> 0.073223).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.1866174 Vali Loss: 0.0724440 Test Loss: 0.0787599
Validation loss decreased (0.073223 --> 0.072444).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.1852844 Vali Loss: 0.0718302 Test Loss: 0.0776732
Validation loss decreased (0.072444 --> 0.071830).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.1848197 Vali Loss: 0.0714366 Test Loss: 0.0769436
Validation loss decreased (0.071830 --> 0.071437).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.1843428 Vali Loss: 0.0711600 Test Loss: 0.0765285
Validation loss decreased (0.071437 --> 0.071160).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.1841111 Vali Loss: 0.0710439 Test Loss: 0.0763178
Validation loss decreased (0.071160 --> 0.071044).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.1865495 Vali Loss: 0.0710157 Test Loss: 0.0762124
Validation loss decreased (0.071044 --> 0.071016).  Saving model ...
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.18827791269304175, 'val/loss': 0.07322291160623233, 'test/loss': 0.0797899601360162, '_timestamp': 1762778855.4166746}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.18661736589715336, 'val/loss': 0.07244398320714633, 'test/loss': 0.07875986024737358, '_timestamp': 1762778857.3328164}).
Epoch: 8, Steps: 132 | Train Loss: 0.1839381 Vali Loss: 0.0709337 Test Loss: 0.0761645
Validation loss decreased (0.071016 --> 0.070934).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.1835074 Vali Loss: 0.0709756 Test Loss: 0.0761399
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.1843002 Vali Loss: 0.0709289 Test Loss: 0.0761277
Validation loss decreased (0.070934 --> 0.070929).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.1838997 Vali Loss: 0.0709850 Test Loss: 0.0761218
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.1837931 Vali Loss: 0.0709315 Test Loss: 0.0761188
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.1832904 Vali Loss: 0.0709473 Test Loss: 0.0761173
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.1841660 Vali Loss: 0.0709817 Test Loss: 0.0761166
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.1890228 Vali Loss: 0.0709783 Test Loss: 0.0761163
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.1898655 Vali Loss: 0.0709671 Test Loss: 0.0761161
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.1834139 Vali Loss: 0.0709646 Test Loss: 0.0761161
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.1857521 Vali Loss: 0.0709510 Test Loss: 0.0761161
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.1836593 Vali Loss: 0.0709340 Test Loss: 0.0761161
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.1838441 Vali Loss: 0.0709383 Test Loss: 0.0761161
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SP500_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 191
test shape: (191, 50, 1) (191, 50, 1)
test shape: (191, 50, 1) (191, 50, 1)


	mse:6.568945536855608e-05, mae:0.00602900143712759, rmse:0.008104903623461723, r2:-0.010101318359375, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0060, RMSE: 0.0081, RÂ²: -0.0101, MAPE: 2.08%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.698 MB of 0.698 MB uploadedwandb: \ 0.698 MB of 0.698 MB uploadedwandb: | 0.698 MB of 0.698 MB uploadedwandb: / 0.698 MB of 0.698 MB uploadedwandb: - 0.698 MB of 0.698 MB uploadedwandb: \ 0.698 MB of 0.771 MB uploadedwandb: | 0.771 MB of 0.771 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ƒâ–ƒâ–‚â–‚â–„â–‚â–â–‚â–‚â–‚â–â–‚â–‡â–ˆâ–â–„â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 53548
wandb:     model/trainable_params 53548
wandb:                  test/loss 0.07612
wandb:                 train/loss 0.18384
wandb:   val/directional_accuracy 50.13356
wandb:                   val/loss 0.07094
wandb:                    val/mae 0.00603
wandb:                   val/mape 207.81503
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.0101
wandb:                   val/rmse 0.0081
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/rmq93ybc
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_144727-rmq93ybc/logs
Completed: SP500 H=50

Training: Mamba on SP500 for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_144837-uansd3gx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/uansd3gx
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H100    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/uansd3gx
>>>>>>>start training : long_term_forecast_Mamba_SP500_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 100
============================================================
train 4144
val 140
test 141
Epoch: 1, Steps: 130 | Train Loss: 0.1993215 Vali Loss: 0.0750596 Test Loss: 0.0946196
Validation loss decreased (inf --> 0.075060).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.1977835 Vali Loss: 0.0742972 Test Loss: 0.0917063
Validation loss decreased (0.075060 --> 0.074297).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.1957595 Vali Loss: 0.0727847 Test Loss: 0.0872238
Validation loss decreased (0.074297 --> 0.072785).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.1933947 Vali Loss: 0.0719779 Test Loss: 0.0842303
Validation loss decreased (0.072785 --> 0.071978).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.1927549 Vali Loss: 0.0709927 Test Loss: 0.0830149
Validation loss decreased (0.071978 --> 0.070993).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.1923292 Vali Loss: 0.0706676 Test Loss: 0.0825093
Validation loss decreased (0.070993 --> 0.070668).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.1919552 Vali Loss: 0.0711565 Test Loss: 0.0822827
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.19932149201631547, 'val/loss': 0.07505956441164016, 'test/loss': 0.09461957663297653, '_timestamp': 1762778925.7919512}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.1977834518139179, 'val/loss': 0.07429716885089874, 'test/loss': 0.0917062982916832, '_timestamp': 1762778927.630207}).
Epoch: 8, Steps: 130 | Train Loss: 0.1919829 Vali Loss: 0.0709879 Test Loss: 0.0821785
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.1928899 Vali Loss: 0.0713085 Test Loss: 0.0821289
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.1925838 Vali Loss: 0.0703712 Test Loss: 0.0821058
Validation loss decreased (0.070668 --> 0.070371).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.1920687 Vali Loss: 0.0708435 Test Loss: 0.0820940
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.1935131 Vali Loss: 0.0707716 Test Loss: 0.0820886
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.1926296 Vali Loss: 0.0712194 Test Loss: 0.0820859
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.1927257 Vali Loss: 0.0707402 Test Loss: 0.0820845
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.1921695 Vali Loss: 0.0708355 Test Loss: 0.0820838
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 130 | Train Loss: 0.1922373 Vali Loss: 0.0705928 Test Loss: 0.0820836
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 130 | Train Loss: 0.1919663 Vali Loss: 0.0708736 Test Loss: 0.0820835
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 130 | Train Loss: 0.1918621 Vali Loss: 0.0713177 Test Loss: 0.0820835
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 130 | Train Loss: 0.1918866 Vali Loss: 0.0711224 Test Loss: 0.0820835
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 130 | Train Loss: 0.1920840 Vali Loss: 0.0712145 Test Loss: 0.0820835
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SP500_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 141
test shape: (141, 100, 1) (141, 100, 1)
test shape: (141, 100, 1) (141, 100, 1)


	mse:6.905981717864051e-05, mae:0.006134733557701111, rmse:0.008310223929584026, r2:-0.008049726486206055, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0083, RÂ²: -0.0080, MAPE: 2.24%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.797 MB of 0.802 MB uploadedwandb: \ 0.802 MB of 0.802 MB uploadedwandb: | 0.802 MB of 0.875 MB uploadedwandb: / 0.802 MB of 0.875 MB uploadedwandb: - 0.875 MB of 0.875 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–â–â–ƒâ–‚â–â–„â–‚â–ƒâ–‚â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–ƒâ–‚â–ƒâ–ƒâ–„â–â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–„â–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 73048
wandb:     model/trainable_params 73048
wandb:                  test/loss 0.08208
wandb:                 train/loss 0.19208
wandb:   val/directional_accuracy 50.17551
wandb:                   val/loss 0.07121
wandb:                    val/mae 0.00613
wandb:                   val/mape 224.09401
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.00805
wandb:                   val/rmse 0.00831
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/uansd3gx
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_144837-uansd3gx/logs
Completed: SP500 H=100

Training: Mamba on NASDAQ for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_144945-yj8ho2l7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/yj8ho2l7
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H3     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/yj8ho2l7
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.2458621 Vali Loss: 0.1350313 Test Loss: 0.1263915
Validation loss decreased (inf --> 0.135031).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2389316 Vali Loss: 0.1296194 Test Loss: 0.1208975
Validation loss decreased (0.135031 --> 0.129619).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2331022 Vali Loss: 0.1284978 Test Loss: 0.1181132
Validation loss decreased (0.129619 --> 0.128498).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2297127 Vali Loss: 0.1388153 Test Loss: 0.1169119
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2289219 Vali Loss: 0.1269001 Test Loss: 0.1164269
Validation loss decreased (0.128498 --> 0.126900).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2277003 Vali Loss: 0.1274907 Test Loss: 0.1162542
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2270339 Vali Loss: 0.1272393 Test Loss: 0.1161851
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2458620938591491, 'val/loss': 0.13503132667392492, 'test/loss': 0.12639151653274894, '_timestamp': 1762778993.5002644}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23893158473914727, 'val/loss': 0.12961940374225378, 'test/loss': 0.12089754128828645, '_timestamp': 1762778995.4116833}).
Epoch: 8, Steps: 133 | Train Loss: 0.2274241 Vali Loss: 0.1254459 Test Loss: 0.1161386
Validation loss decreased (0.126900 --> 0.125446).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2270592 Vali Loss: 0.1282535 Test Loss: 0.1161228
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2276746 Vali Loss: 0.1286615 Test Loss: 0.1161134
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2280247 Vali Loss: 0.1368231 Test Loss: 0.1161082
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2269093 Vali Loss: 0.1258267 Test Loss: 0.1161063
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2272345 Vali Loss: 0.1270531 Test Loss: 0.1161052
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2270070 Vali Loss: 0.1287774 Test Loss: 0.1161048
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2274405 Vali Loss: 0.1426429 Test Loss: 0.1161045
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2285639 Vali Loss: 0.1266334 Test Loss: 0.1161044
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2273099 Vali Loss: 0.1257312 Test Loss: 0.1161044
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2270799 Vali Loss: 0.1273573 Test Loss: 0.1161044
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.00013761621084995568, mae:0.008526433259248734, rmse:0.011730993166565895, r2:-0.01107490062713623, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0085, RMSE: 0.0117, RÂ²: -0.0111, MAPE: 1860077.88%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.488 MB of 0.488 MB uploadedwandb: \ 0.488 MB of 0.488 MB uploadedwandb: | 0.488 MB of 0.488 MB uploadedwandb: / 0.488 MB of 0.488 MB uploadedwandb: - 0.488 MB of 0.488 MB uploadedwandb: \ 0.488 MB of 0.488 MB uploadedwandb: | 0.488 MB of 0.488 MB uploadedwandb: / 0.537 MB of 0.609 MB uploaded (0.002 MB deduped)wandb: - 0.537 MB of 0.609 MB uploaded (0.002 MB deduped)wandb: \ 0.609 MB of 0.609 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–â–‚â–â–‚â–‚â–â–â–â–‚â–ƒâ–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–†â–‚â–‚â–‚â–â–‚â–‚â–†â–â–‚â–‚â–ˆâ–â–â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 17
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35218
wandb:     model/trainable_params 35218
wandb:                  test/loss 0.1161
wandb:                 train/loss 0.22708
wandb:   val/directional_accuracy 52.32068
wandb:                   val/loss 0.12736
wandb:                    val/mae 0.00853
wandb:                   val/mape 186007787.5
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.01107
wandb:                   val/rmse 0.01173
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/yj8ho2l7
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_144945-yj8ho2l7/logs
Completed: NASDAQ H=3

Training: Mamba on NASDAQ for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_145054-k1fc21ju
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/k1fc21ju
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H5     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/k1fc21ju
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.2478552 Vali Loss: 0.1398998 Test Loss: 0.1303058
Validation loss decreased (inf --> 0.139900).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2428223 Vali Loss: 0.1462959 Test Loss: 0.1270275
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2398199 Vali Loss: 0.1356710 Test Loss: 0.1250118
Validation loss decreased (0.139900 --> 0.135671).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2369165 Vali Loss: 0.1326666 Test Loss: 0.1239373
Validation loss decreased (0.135671 --> 0.132667).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2351199 Vali Loss: 0.1359565 Test Loss: 0.1234447
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2345554 Vali Loss: 0.1301606 Test Loss: 0.1232350
Validation loss decreased (0.132667 --> 0.130161).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2347062 Vali Loss: 0.1317360 Test Loss: 0.1231353
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2478551536350322, 'val/loss': 0.13989978190511465, 'test/loss': 0.13030576659366488, '_timestamp': 1762779061.9758923}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2428223059365624, 'val/loss': 0.1462958613410592, 'test/loss': 0.12702753441408277, '_timestamp': 1762779063.9735394}).
Epoch: 8, Steps: 133 | Train Loss: 0.2343217 Vali Loss: 0.1312940 Test Loss: 0.1230874
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2356780 Vali Loss: 0.1306391 Test Loss: 0.1230646
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2343812 Vali Loss: 0.1317401 Test Loss: 0.1230557
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2339061 Vali Loss: 0.1416795 Test Loss: 0.1230496
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2338154 Vali Loss: 0.1380549 Test Loss: 0.1230465
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2347230 Vali Loss: 0.1356406 Test Loss: 0.1230453
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2335757 Vali Loss: 0.1313571 Test Loss: 0.1230443
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2345264 Vali Loss: 0.1313152 Test Loss: 0.1230440
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2348076 Vali Loss: 0.1325225 Test Loss: 0.1230438
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.00013998738722875714, mae:0.00859509315341711, rmse:0.011831626296043396, r2:-0.023277878761291504, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0086, RMSE: 0.0118, RÂ²: -0.0233, MAPE: 2300979.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.518 MB of 0.518 MB uploadedwandb: \ 0.518 MB of 0.518 MB uploadedwandb: | 0.518 MB of 0.518 MB uploadedwandb: / 0.518 MB of 0.590 MB uploadedwandb: - 0.590 MB of 0.590 MB uploadedwandb: \ 0.590 MB of 0.590 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–â–â–‚â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–ƒâ–…â–â–‚â–‚â–â–‚â–ˆâ–†â–„â–‚â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35998
wandb:     model/trainable_params 35998
wandb:                  test/loss 0.12304
wandb:                 train/loss 0.23481
wandb:   val/directional_accuracy 51.80851
wandb:                   val/loss 0.13252
wandb:                    val/mae 0.0086
wandb:                   val/mape 230097975.0
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.02328
wandb:                   val/rmse 0.01183
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/k1fc21ju
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_145054-k1fc21ju/logs
Completed: NASDAQ H=5

Training: Mamba on NASDAQ for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_145155-69auke49
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/69auke49
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H10    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/69auke49
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2510863 Vali Loss: 0.1557969 Test Loss: 0.1330072
Validation loss decreased (inf --> 0.155797).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2481563 Vali Loss: 0.1431543 Test Loss: 0.1318262
Validation loss decreased (0.155797 --> 0.143154).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2458481 Vali Loss: 0.1397991 Test Loss: 0.1310069
Validation loss decreased (0.143154 --> 0.139799).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2453734 Vali Loss: 0.1365723 Test Loss: 0.1304867
Validation loss decreased (0.139799 --> 0.136572).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2438168 Vali Loss: 0.1421161 Test Loss: 0.1301667
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2447364 Vali Loss: 0.1369689 Test Loss: 0.1300153
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2433778 Vali Loss: 0.1404324 Test Loss: 0.1299354
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.25108633128772107, 'val/loss': 0.1557969292625785, 'test/loss': 0.13300724001601338, '_timestamp': 1762779122.7821455}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2481563334402285, 'val/loss': 0.14315427280962467, 'test/loss': 0.13182620098814368, '_timestamp': 1762779124.7073166}).
Epoch: 8, Steps: 133 | Train Loss: 0.2428096 Vali Loss: 0.1395540 Test Loss: 0.1299010
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2439006 Vali Loss: 0.1500483 Test Loss: 0.1298812
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2429639 Vali Loss: 0.1402433 Test Loss: 0.1298717
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2429978 Vali Loss: 0.1375659 Test Loss: 0.1298671
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2433817 Vali Loss: 0.1403225 Test Loss: 0.1298648
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2436838 Vali Loss: 0.1415152 Test Loss: 0.1298637
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2427569 Vali Loss: 0.1430254 Test Loss: 0.1298631
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.00014151899085845798, mae:0.008648729883134365, rmse:0.01189617533236742, r2:-0.02234339714050293, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0086, RMSE: 0.0119, RÂ²: -0.0223, MAPE: 3120336.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.575 MB of 0.575 MB uploadedwandb: \ 0.575 MB of 0.575 MB uploadedwandb: | 0.575 MB of 0.575 MB uploadedwandb: / 0.575 MB of 0.647 MB uploadedwandb: - 0.647 MB of 0.647 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–‡â–ƒâ–…â–‚â–â–„â–â–‚â–‚â–ƒâ–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–â–„â–â–ƒâ–ƒâ–ˆâ–ƒâ–‚â–ƒâ–„â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 13
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 37948
wandb:     model/trainable_params 37948
wandb:                  test/loss 0.12986
wandb:                 train/loss 0.24276
wandb:   val/directional_accuracy 50.14493
wandb:                   val/loss 0.14303
wandb:                    val/mae 0.00865
wandb:                   val/mape 312033675.0
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.02234
wandb:                   val/rmse 0.0119
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/69auke49
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_145155-69auke49/logs
Completed: NASDAQ H=10

Training: Mamba on NASDAQ for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_145248-mqgk8cdk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/mqgk8cdk
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H22    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/mqgk8cdk
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2563020 Vali Loss: 0.1558620 Test Loss: 0.1346381
Validation loss decreased (inf --> 0.155862).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2543674 Vali Loss: 0.1557751 Test Loss: 0.1337934
Validation loss decreased (0.155862 --> 0.155775).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2530264 Vali Loss: 0.1550150 Test Loss: 0.1332167
Validation loss decreased (0.155775 --> 0.155015).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2520332 Vali Loss: 0.1544907 Test Loss: 0.1328998
Validation loss decreased (0.155015 --> 0.154491).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2515358 Vali Loss: 0.1537359 Test Loss: 0.1327009
Validation loss decreased (0.154491 --> 0.153736).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2513640 Vali Loss: 0.1550709 Test Loss: 0.1326076
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2514011 Vali Loss: 0.1538473 Test Loss: 0.1325596
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.256302046956438, 'val/loss': 0.15586199079241073, 'test/loss': 0.1346380753176553, '_timestamp': 1762779174.6888907}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2543674199418588, 'val/loss': 0.15577509786401475, 'test/loss': 0.13379337745053427, '_timestamp': 1762779176.6199124}).
Epoch: 8, Steps: 132 | Train Loss: 0.2512388 Vali Loss: 0.1544327 Test Loss: 0.1325382
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2512583 Vali Loss: 0.1538791 Test Loss: 0.1325277
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2511768 Vali Loss: 0.1546133 Test Loss: 0.1325216
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2511310 Vali Loss: 0.1543489 Test Loss: 0.1325186
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2511022 Vali Loss: 0.1536543 Test Loss: 0.1325172
Validation loss decreased (0.153736 --> 0.153654).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2512009 Vali Loss: 0.1546343 Test Loss: 0.1325166
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2511068 Vali Loss: 0.1545709 Test Loss: 0.1325162
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2510871 Vali Loss: 0.1549676 Test Loss: 0.1325161
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2510399 Vali Loss: 0.1543680 Test Loss: 0.1325160
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2510683 Vali Loss: 0.1543520 Test Loss: 0.1325160
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2512153 Vali Loss: 0.1547643 Test Loss: 0.1325160
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2512075 Vali Loss: 0.1542513 Test Loss: 0.1325160
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2511316 Vali Loss: 0.1541746 Test Loss: 0.1325160
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2509509 Vali Loss: 0.1538635 Test Loss: 0.1325160
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2511469 Vali Loss: 0.1538452 Test Loss: 0.1325160
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.00014363547961693257, mae:0.00869136955589056, rmse:0.011984801851212978, r2:-0.026514172554016113, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0087, RMSE: 0.0120, RÂ²: -0.0265, MAPE: 3264882.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.629 MB of 0.630 MB uploadedwandb: \ 0.629 MB of 0.630 MB uploadedwandb: | 0.629 MB of 0.630 MB uploadedwandb: / 0.630 MB of 0.630 MB uploadedwandb: - 0.630 MB of 0.630 MB uploadedwandb: \ 0.630 MB of 0.703 MB uploadedwandb: | 0.703 MB of 0.703 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–‚â–‚â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–â–ˆâ–‚â–…â–‚â–†â–„â–â–†â–†â–‡â–…â–„â–†â–„â–„â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 42628
wandb:     model/trainable_params 42628
wandb:                  test/loss 0.13252
wandb:                 train/loss 0.25115
wandb:   val/directional_accuracy 50.39318
wandb:                   val/loss 0.15385
wandb:                    val/mae 0.00869
wandb:                   val/mape 326488225.0
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.02651
wandb:                   val/rmse 0.01198
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/mqgk8cdk
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_145248-mqgk8cdk/logs
Completed: NASDAQ H=22

Training: Mamba on NASDAQ for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_145400-j8dffrss
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/j8dffrss
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H50    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/j8dffrss
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.2633448 Vali Loss: 0.1647384 Test Loss: 0.1453047
Validation loss decreased (inf --> 0.164738).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2617571 Vali Loss: 0.1641074 Test Loss: 0.1444465
Validation loss decreased (0.164738 --> 0.164107).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2603926 Vali Loss: 0.1637150 Test Loss: 0.1437629
Validation loss decreased (0.164107 --> 0.163715).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2607785 Vali Loss: 0.1638224 Test Loss: 0.1433320
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2621631 Vali Loss: 0.1637647 Test Loss: 0.1430936
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2594096 Vali Loss: 0.1633791 Test Loss: 0.1429730
Validation loss decreased (0.163715 --> 0.163379).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2584487 Vali Loss: 0.1635604 Test Loss: 0.1429129
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.26334482397545467, 'val/loss': 0.16473844399054846, 'test/loss': 0.1453047307829062, '_timestamp': 1762779247.8689592}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2617571089755405, 'val/loss': 0.16410738229751587, 'test/loss': 0.1444464549422264, '_timestamp': 1762779249.786251}).
Epoch: 8, Steps: 132 | Train Loss: 0.2586080 Vali Loss: 0.1633181 Test Loss: 0.1428834
Validation loss decreased (0.163379 --> 0.163318).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2588499 Vali Loss: 0.1634043 Test Loss: 0.1428684
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2656944 Vali Loss: 0.1635040 Test Loss: 0.1428610
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2690370 Vali Loss: 0.1634185 Test Loss: 0.1428575
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2607939 Vali Loss: 0.1635077 Test Loss: 0.1428558
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2595174 Vali Loss: 0.1634340 Test Loss: 0.1428549
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2597784 Vali Loss: 0.1635763 Test Loss: 0.1428545
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2650994 Vali Loss: 0.1633336 Test Loss: 0.1428543
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2700028 Vali Loss: 0.1635212 Test Loss: 0.1428542
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2589055 Vali Loss: 0.1636624 Test Loss: 0.1428542
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2599041 Vali Loss: 0.1637234 Test Loss: 0.1428542
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.00014797058247495443, mae:0.00870724767446518, rmse:0.012164316140115261, r2:-0.02157425880432129, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0087, RMSE: 0.0122, RÂ²: -0.0216, MAPE: 3436018.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.683 MB of 0.686 MB uploadedwandb: \ 0.683 MB of 0.686 MB uploadedwandb: | 0.686 MB of 0.686 MB uploadedwandb: / 0.686 MB of 0.758 MB uploadedwandb: - 0.686 MB of 0.758 MB uploadedwandb: \ 0.758 MB of 0.758 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–‚â–‚â–ƒâ–‚â–â–â–â–…â–‡â–‚â–‚â–‚â–…â–ˆâ–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–ˆâ–‡â–‚â–„â–â–‚â–„â–‚â–„â–ƒâ–…â–â–„â–†â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 17
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 53548
wandb:     model/trainable_params 53548
wandb:                  test/loss 0.14285
wandb:                 train/loss 0.2599
wandb:   val/directional_accuracy 50.09667
wandb:                   val/loss 0.16372
wandb:                    val/mae 0.00871
wandb:                   val/mape 343601850.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.02157
wandb:                   val/rmse 0.01216
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/j8dffrss
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_145400-j8dffrss/logs
Completed: NASDAQ H=50

Training: Mamba on NASDAQ for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_145504-iwvzqxpg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/iwvzqxpg
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H100   Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/iwvzqxpg
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.2739329 Vali Loss: 0.1780014 Test Loss: 0.1580433
Validation loss decreased (inf --> 0.178001).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2737412 Vali Loss: 0.1801911 Test Loss: 0.1572926
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.2721675 Vali Loss: 0.1819681 Test Loss: 0.1567285
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2711011 Vali Loss: 0.1804123 Test Loss: 0.1563165
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2707598 Vali Loss: 0.1787556 Test Loss: 0.1560625
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2704709 Vali Loss: 0.1770764 Test Loss: 0.1559211
Validation loss decreased (0.178001 --> 0.177076).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2700779 Vali Loss: 0.1791614 Test Loss: 0.1558539
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.27393289357423783, 'val/loss': 0.17800139188766478, 'test/loss': 0.15804333686828614, '_timestamp': 1762779311.7227726}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.273741194491203, 'val/loss': 0.18019107282161712, 'test/loss': 0.15729255974292755, '_timestamp': 1762779313.6086729}).
Epoch: 8, Steps: 130 | Train Loss: 0.2704936 Vali Loss: 0.1808058 Test Loss: 0.1558173
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2706454 Vali Loss: 0.1805615 Test Loss: 0.1558010
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2699987 Vali Loss: 0.1759377 Test Loss: 0.1557926
Validation loss decreased (0.177076 --> 0.175938).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2704972 Vali Loss: 0.1805846 Test Loss: 0.1557882
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2700388 Vali Loss: 0.1790901 Test Loss: 0.1557861
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2701012 Vali Loss: 0.1805151 Test Loss: 0.1557850
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.2704551 Vali Loss: 0.1806993 Test Loss: 0.1557845
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.2702697 Vali Loss: 0.1784789 Test Loss: 0.1557843
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 130 | Train Loss: 0.2704255 Vali Loss: 0.1779687 Test Loss: 0.1557842
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 130 | Train Loss: 0.2701056 Vali Loss: 0.1788799 Test Loss: 0.1557841
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 130 | Train Loss: 0.2703304 Vali Loss: 0.1795608 Test Loss: 0.1557841
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 130 | Train Loss: 0.2705349 Vali Loss: 0.1803920 Test Loss: 0.1557841
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 130 | Train Loss: 0.2700562 Vali Loss: 0.1811233 Test Loss: 0.1557841
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.00015605593216605484, mae:0.008722483180463314, rmse:0.012492234818637371, r2:-0.02889072895050049, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0087, RMSE: 0.0125, RÂ²: -0.0289, MAPE: 1755704.12%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.747 MB of 0.752 MB uploadedwandb: \ 0.747 MB of 0.752 MB uploadedwandb: | 0.747 MB of 0.752 MB uploadedwandb: / 0.752 MB of 0.752 MB uploadedwandb: - 0.752 MB of 0.825 MB uploadedwandb: \ 0.752 MB of 0.825 MB uploadedwandb: | 0.825 MB of 0.825 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–â–ƒâ–ƒâ–â–ƒâ–â–â–‚â–‚â–‚â–â–‚â–ƒâ–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–„â–‚â–…â–‡â–†â–â–†â–…â–†â–‡â–„â–ƒâ–„â–…â–†â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 73048
wandb:     model/trainable_params 73048
wandb:                  test/loss 0.15578
wandb:                 train/loss 0.27006
wandb:   val/directional_accuracy 50.64935
wandb:                   val/loss 0.18112
wandb:                    val/mae 0.00872
wandb:                   val/mape 175570412.5
wandb:                    val/mse 0.00016
wandb:                     val/r2 -0.02889
wandb:                   val/rmse 0.01249
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/iwvzqxpg
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_145504-iwvzqxpg/logs
Completed: NASDAQ H=100

Training: Mamba on ABSA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_145613-unm35zsx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/unm35zsx
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H3       Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/unm35zsx
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 238
Epoch: 1, Steps: 133 | Train Loss: 0.3031713 Vali Loss: 0.1671447 Test Loss: 0.1571212
Validation loss decreased (inf --> 0.167145).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2948470 Vali Loss: 0.1612477 Test Loss: 0.1554512
Validation loss decreased (0.167145 --> 0.161248).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2863578 Vali Loss: 0.1635859 Test Loss: 0.1540404
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2818141 Vali Loss: 0.1635645 Test Loss: 0.1534657
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2794987 Vali Loss: 0.1605247 Test Loss: 0.1533132
Validation loss decreased (0.161248 --> 0.160525).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2781753 Vali Loss: 0.1659011 Test Loss: 0.1531356
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2778112 Vali Loss: 0.1676159 Test Loss: 0.1531009
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3031712922834812, 'val/loss': 0.16714470461010933, 'test/loss': 0.15712117217481136, '_timestamp': 1762779380.5766108}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2948470236663532, 'val/loss': 0.16124766878783703, 'test/loss': 0.1554512046277523, '_timestamp': 1762779382.522922}).
Epoch: 8, Steps: 133 | Train Loss: 0.2776133 Vali Loss: 0.1687020 Test Loss: 0.1530809
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2771185 Vali Loss: 0.1700861 Test Loss: 0.1530631
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2798422 Vali Loss: 0.1659611 Test Loss: 0.1530558
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2787678 Vali Loss: 0.1630827 Test Loss: 0.1530504
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2774799 Vali Loss: 0.1666586 Test Loss: 0.1530494
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2773256 Vali Loss: 0.1658053 Test Loss: 0.1530482
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2775149 Vali Loss: 0.1656934 Test Loss: 0.1530476
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2778381 Vali Loss: 0.1665017 Test Loss: 0.1530474
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 238
test shape: (238, 3, 1) (238, 3, 1)
test shape: (238, 3, 1) (238, 3, 1)


	mse:0.0004496102628763765, mae:0.0161319300532341, rmse:0.021204015240073204, r2:0.012625336647033691, dtw:Not calculated


VAL - MSE: 0.0004, MAE: 0.0161, RMSE: 0.0212, RÂ²: 0.0126, MAPE: 1.34%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.492 MB of 0.492 MB uploadedwandb: \ 0.492 MB of 0.492 MB uploadedwandb: | 0.492 MB of 0.492 MB uploadedwandb: / 0.492 MB of 0.492 MB uploadedwandb: - 0.492 MB of 0.492 MB uploadedwandb: \ 0.492 MB of 0.492 MB uploadedwandb: | 0.492 MB of 0.492 MB uploadedwandb: / 0.541 MB of 0.613 MB uploaded (0.002 MB deduped)wandb: - 0.541 MB of 0.613 MB uploaded (0.002 MB deduped)wandb: \ 0.613 MB of 0.613 MB uploaded (0.002 MB deduped)wandb: | 0.613 MB of 0.613 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–ƒâ–‚â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–ƒâ–â–…â–†â–‡â–ˆâ–…â–ƒâ–…â–…â–…â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35218
wandb:     model/trainable_params 35218
wandb:                  test/loss 0.15305
wandb:                 train/loss 0.27784
wandb:   val/directional_accuracy 46.84874
wandb:                   val/loss 0.1665
wandb:                    val/mae 0.01613
wandb:                   val/mape 133.98544
wandb:                    val/mse 0.00045
wandb:                     val/r2 0.01263
wandb:                   val/rmse 0.0212
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/unm35zsx
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_145613-unm35zsx/logs
Completed: ABSA H=3

Training: Mamba on ABSA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_145717-avvpkj0k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/avvpkj0k
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H5       Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/avvpkj0k
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 236
Epoch: 1, Steps: 133 | Train Loss: 0.3088989 Vali Loss: 0.1685857 Test Loss: 0.1602641
Validation loss decreased (inf --> 0.168586).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.3012959 Vali Loss: 0.1665140 Test Loss: 0.1605071
Validation loss decreased (0.168586 --> 0.166514).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2950363 Vali Loss: 0.1635185 Test Loss: 0.1605325
Validation loss decreased (0.166514 --> 0.163518).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2898581 Vali Loss: 0.1612499 Test Loss: 0.1602611
Validation loss decreased (0.163518 --> 0.161250).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2878928 Vali Loss: 0.1753924 Test Loss: 0.1601379
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2867404 Vali Loss: 0.1723837 Test Loss: 0.1600261
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2859785 Vali Loss: 0.1658832 Test Loss: 0.1599716
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.30889892152377535, 'val/loss': 0.16858572326600552, 'test/loss': 0.16026407480239868, '_timestamp': 1762779445.6623783}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3012959457429728, 'val/loss': 0.16651403531432152, 'test/loss': 0.16050714626908302, '_timestamp': 1762779447.6667166}).
Epoch: 8, Steps: 133 | Train Loss: 0.2859113 Vali Loss: 0.1732481 Test Loss: 0.1599468
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2854085 Vali Loss: 0.1661213 Test Loss: 0.1599403
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2859459 Vali Loss: 0.1688849 Test Loss: 0.1599341
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2861650 Vali Loss: 0.1678732 Test Loss: 0.1599295
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2854408 Vali Loss: 0.1722674 Test Loss: 0.1599279
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2870865 Vali Loss: 0.1713215 Test Loss: 0.1599273
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2861529 Vali Loss: 0.1689958 Test Loss: 0.1599268
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
test shape: (236, 5, 1) (236, 5, 1)
test shape: (236, 5, 1) (236, 5, 1)


	mse:0.00045802516979165375, mae:0.01631116308271885, rmse:0.021401522681117058, r2:-6.079673767089844e-06, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0163, RMSE: 0.0214, RÂ²: -0.0000, MAPE: 1.27%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.527 MB of 0.527 MB uploadedwandb: \ 0.527 MB of 0.527 MB uploadedwandb: | 0.527 MB of 0.527 MB uploadedwandb: / 0.527 MB of 0.599 MB uploadedwandb: - 0.527 MB of 0.599 MB uploadedwandb: \ 0.599 MB of 0.599 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–â–â–â–â–‚â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–â–ˆâ–‡â–ƒâ–‡â–ƒâ–…â–„â–†â–†â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 13
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35998
wandb:     model/trainable_params 35998
wandb:                  test/loss 0.15993
wandb:                 train/loss 0.28615
wandb:   val/directional_accuracy 47.56356
wandb:                   val/loss 0.169
wandb:                    val/mae 0.01631
wandb:                   val/mape 126.52524
wandb:                    val/mse 0.00046
wandb:                     val/r2 -1e-05
wandb:                   val/rmse 0.0214
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/avvpkj0k
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_145717-avvpkj0k/logs
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread IntMsgThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 268, in check_network_status
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 300, in check_internal_messages
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 848, in deliver_network_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 856, in deliver_internal_messages
    return self._deliver_network_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 510, in _deliver_network_status
    return self._deliver_internal_messages(internal_message)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 516, in _deliver_internal_messages
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
        self._send_message(msg)sent = self._sock.send(data)

  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
BrokenPipeError: [Errno 32] Broken pipe
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: ABSA H=5

Training: Mamba on ABSA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_145817-txwzf67g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/txwzf67g
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H10      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/txwzf67g
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 231
Epoch: 1, Steps: 133 | Train Loss: 0.3146822 Vali Loss: 0.1780797 Test Loss: 0.1615114
Validation loss decreased (inf --> 0.178080).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.3112242 Vali Loss: 0.1799989 Test Loss: 0.1617071
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.3051082 Vali Loss: 0.1728824 Test Loss: 0.1616326
Validation loss decreased (0.178080 --> 0.172882).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.3045193 Vali Loss: 0.1671740 Test Loss: 0.1614908
Validation loss decreased (0.172882 --> 0.167174).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.3009685 Vali Loss: 0.1767034 Test Loss: 0.1614116
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.3015179 Vali Loss: 0.1673934 Test Loss: 0.1613463
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.3002735 Vali Loss: 0.1743524 Test Loss: 0.1613107
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.31468218789064795, 'val/loss': 0.17807969078421593, 'test/loss': 0.16151143237948418, '_timestamp': 1762779505.3796158}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3112242105311917, 'val/loss': 0.17999893985688686, 'test/loss': 0.16170713678002357, '_timestamp': 1762779507.3462577}).
Epoch: 8, Steps: 133 | Train Loss: 0.3003307 Vali Loss: 0.1754874 Test Loss: 0.1612986
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.3006189 Vali Loss: 0.1691038 Test Loss: 0.1612864
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2995427 Vali Loss: 0.1700090 Test Loss: 0.1612819
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2992003 Vali Loss: 0.1676447 Test Loss: 0.1612811
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.3006350 Vali Loss: 0.1726306 Test Loss: 0.1612797
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.3003520 Vali Loss: 0.1728412 Test Loss: 0.1612792
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2992817 Vali Loss: 0.1759864 Test Loss: 0.1612790
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 231
test shape: (231, 10, 1) (231, 10, 1)
test shape: (231, 10, 1) (231, 10, 1)


	mse:0.00046365297748707235, mae:0.01639004983007908, rmse:0.021532602608203888, r2:-0.004282236099243164, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0164, RMSE: 0.0215, RÂ²: -0.0043, MAPE: 1.47%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.561 MB of 0.561 MB uploadedwandb: \ 0.561 MB of 0.561 MB uploadedwandb: | 0.561 MB of 0.561 MB uploadedwandb: / 0.561 MB of 0.561 MB uploadedwandb: - 0.561 MB of 0.561 MB uploadedwandb: \ 0.561 MB of 0.561 MB uploadedwandb: | 0.561 MB of 0.633 MB uploadedwandb: / 0.565 MB of 0.633 MB uploadedwandb: - 0.633 MB of 0.633 MB uploadedwandb: \ 0.633 MB of 0.633 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–„â–‚â–‚â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–‡â–ƒâ–„â–‚â–‚â–ƒâ–â–â–ƒâ–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–â–ˆâ–â–†â–‡â–‚â–ƒâ–â–…â–…â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 13
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 37948
wandb:     model/trainable_params 37948
wandb:                  test/loss 0.16128
wandb:                 train/loss 0.29928
wandb:   val/directional_accuracy 49.25445
wandb:                   val/loss 0.17599
wandb:                    val/mae 0.01639
wandb:                   val/mape 147.29367
wandb:                    val/mse 0.00046
wandb:                     val/r2 -0.00428
wandb:                   val/rmse 0.02153
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/txwzf67g
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_145817-txwzf67g/logs
Completed: ABSA H=10

Training: Mamba on ABSA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_145918-f9k4mpfu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/f9k4mpfu
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H22      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/f9k4mpfu
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 219
Epoch: 1, Steps: 132 | Train Loss: 0.3252476 Vali Loss: 0.1801121 Test Loss: 0.1564813
Validation loss decreased (inf --> 0.180112).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.3219973 Vali Loss: 0.1796400 Test Loss: 0.1565815
Validation loss decreased (0.180112 --> 0.179640).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.3193969 Vali Loss: 0.1791019 Test Loss: 0.1567026
Validation loss decreased (0.179640 --> 0.179102).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.3174250 Vali Loss: 0.1793249 Test Loss: 0.1567201
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.3162839 Vali Loss: 0.1791598 Test Loss: 0.1567312
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.3155492 Vali Loss: 0.1790310 Test Loss: 0.1566913
Validation loss decreased (0.179102 --> 0.179031).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.3153521 Vali Loss: 0.1792155 Test Loss: 0.1566958
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.32524755348761875, 'val/loss': 0.18011213839054108, 'test/loss': 0.15648129263094493, '_timestamp': 1762779565.1670077}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.32199731503020634, 'val/loss': 0.17963998019695282, 'test/loss': 0.15658154977219446, '_timestamp': 1762779567.064602}).
Epoch: 8, Steps: 132 | Train Loss: 0.3150440 Vali Loss: 0.1794222 Test Loss: 0.1566934
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.3149097 Vali Loss: 0.1791190 Test Loss: 0.1566946
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.3147866 Vali Loss: 0.1790102 Test Loss: 0.1566948
Validation loss decreased (0.179031 --> 0.179010).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.3146910 Vali Loss: 0.1792507 Test Loss: 0.1566945
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.3148347 Vali Loss: 0.1788808 Test Loss: 0.1566943
Validation loss decreased (0.179010 --> 0.178881).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.3148400 Vali Loss: 0.1790965 Test Loss: 0.1566944
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.3147164 Vali Loss: 0.1792499 Test Loss: 0.1566943
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.3148088 Vali Loss: 0.1796081 Test Loss: 0.1566943
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.3148285 Vali Loss: 0.1791950 Test Loss: 0.1566943
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.3148559 Vali Loss: 0.1790680 Test Loss: 0.1566943
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.3149928 Vali Loss: 0.1793392 Test Loss: 0.1566943
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.3148222 Vali Loss: 0.1794479 Test Loss: 0.1566943
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.3146033 Vali Loss: 0.1791983 Test Loss: 0.1566943
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.3148737 Vali Loss: 0.1789630 Test Loss: 0.1566943
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.3147468 Vali Loss: 0.1790222 Test Loss: 0.1566943
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 219
test shape: (219, 22, 1) (219, 22, 1)
test shape: (219, 22, 1) (219, 22, 1)


	mse:0.0004709883942268789, mae:0.016539130359888077, rmse:0.021702267229557037, r2:-0.005265474319458008, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0165, RMSE: 0.0217, RÂ²: -0.0053, MAPE: 1.36%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.628 MB of 0.629 MB uploadedwandb: \ 0.628 MB of 0.629 MB uploadedwandb: | 0.629 MB of 0.629 MB uploadedwandb: / 0.629 MB of 0.702 MB uploadedwandb: - 0.629 MB of 0.702 MB uploadedwandb: \ 0.702 MB of 0.702 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ƒâ–†â–ˆâ–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–…â–„â–‚â–„â–†â–ƒâ–‚â–…â–â–ƒâ–…â–ˆâ–„â–ƒâ–…â–†â–„â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 42628
wandb:     model/trainable_params 42628
wandb:                  test/loss 0.15669
wandb:                 train/loss 0.31475
wandb:   val/directional_accuracy 50.33703
wandb:                   val/loss 0.17902
wandb:                    val/mae 0.01654
wandb:                   val/mape 135.80569
wandb:                    val/mse 0.00047
wandb:                     val/r2 -0.00527
wandb:                   val/rmse 0.0217
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/f9k4mpfu
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_145918-f9k4mpfu/logs
Completed: ABSA H=22

Training: Mamba on ABSA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_150032-3skrxqxx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/3skrxqxx
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H50      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/3skrxqxx
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 191
Epoch: 1, Steps: 132 | Train Loss: 0.3457125 Vali Loss: 0.1805737 Test Loss: 0.1557199
Validation loss decreased (inf --> 0.180574).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.3429137 Vali Loss: 0.1806220 Test Loss: 0.1553494
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.3393749 Vali Loss: 0.1809102 Test Loss: 0.1551849
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.3411828 Vali Loss: 0.1810224 Test Loss: 0.1550996
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.3376583 Vali Loss: 0.1810180 Test Loss: 0.1550674
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.3361183 Vali Loss: 0.1810989 Test Loss: 0.1550497
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.3355236 Vali Loss: 0.1811335 Test Loss: 0.1550435
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.34571249783039093, 'val/loss': 0.18057373662789664, 'test/loss': 0.15571987628936768, '_timestamp': 1762779641.2744424}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3429136649903023, 'val/loss': 0.18062202632427216, 'test/loss': 0.1553493564327558, '_timestamp': 1762779643.206426}).
Epoch: 8, Steps: 132 | Train Loss: 0.3358826 Vali Loss: 0.1811418 Test Loss: 0.1550396
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.3353288 Vali Loss: 0.1811416 Test Loss: 0.1550379
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.3479830 Vali Loss: 0.1811464 Test Loss: 0.1550369
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.3483835 Vali Loss: 0.1811540 Test Loss: 0.1550365
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 191
test shape: (191, 50, 1) (191, 50, 1)
test shape: (191, 50, 1) (191, 50, 1)


	mse:0.0004924027598462999, mae:0.016992047429084778, rmse:0.022190149873495102, r2:-0.012154102325439453, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0170, RMSE: 0.0222, RÂ²: -0.0122, MAPE: 1.21%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.737 MB of 0.740 MB uploadedwandb: \ 0.737 MB of 0.740 MB uploadedwandb: | 0.737 MB of 0.740 MB uploadedwandb: / 0.740 MB of 0.740 MB uploadedwandb: - 0.740 MB of 0.811 MB uploadedwandb: \ 0.740 MB of 0.811 MB uploadedwandb: | 0.811 MB of 0.811 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–
wandb:                 train/loss â–ƒâ–„â–‚â–â–â–â–â–ˆâ–ˆ
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–„â–„â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 53548
wandb:     model/trainable_params 53548
wandb:                  test/loss 0.15504
wandb:                 train/loss 0.34838
wandb:   val/directional_accuracy 49.58863
wandb:                   val/loss 0.18115
wandb:                    val/mae 0.01699
wandb:                   val/mape 121.13444
wandb:                    val/mse 0.00049
wandb:                     val/r2 -0.01215
wandb:                   val/rmse 0.02219
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/3skrxqxx
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_150032-3skrxqxx/logs
Completed: ABSA H=50

Training: Mamba on ABSA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_150126-cdv6ng1l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/cdv6ng1l
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H100     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/cdv6ng1l
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 141
Epoch: 1, Steps: 130 | Train Loss: 0.3825173 Vali Loss: 0.1949567 Test Loss: 0.1645957
Validation loss decreased (inf --> 0.194957).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.3819092 Vali Loss: 0.1939927 Test Loss: 0.1643275
Validation loss decreased (0.194957 --> 0.193993).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.3782293 Vali Loss: 0.1948356 Test Loss: 0.1642723
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.3758118 Vali Loss: 0.1965022 Test Loss: 0.1643067
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.3743647 Vali Loss: 0.1963244 Test Loss: 0.1643553
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.3741643 Vali Loss: 0.1970607 Test Loss: 0.1643677
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.3731485 Vali Loss: 0.1962737 Test Loss: 0.1643686
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.38251729905605314, 'val/loss': 0.1949567049741745, 'test/loss': 0.16459567844867706, '_timestamp': 1762779692.2444668}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.38190917384165984, 'val/loss': 0.19399268329143524, 'test/loss': 0.16432749927043916, '_timestamp': 1762779694.130322}).
Epoch: 8, Steps: 130 | Train Loss: 0.3741647 Vali Loss: 0.1963744 Test Loss: 0.1643721
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.3737353 Vali Loss: 0.1965617 Test Loss: 0.1643744
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.3729689 Vali Loss: 0.1982321 Test Loss: 0.1643757
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.3735393 Vali Loss: 0.1960339 Test Loss: 0.1643760
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.3732674 Vali Loss: 0.1960779 Test Loss: 0.1643763
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 141
test shape: (141, 100, 1) (141, 100, 1)
test shape: (141, 100, 1) (141, 100, 1)


	mse:0.0005207785870879889, mae:0.01743066869676113, rmse:0.022820573300123215, r2:-0.00919485092163086, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0174, RMSE: 0.0228, RÂ²: -0.0092, MAPE: 1.15%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.816 MB of 0.821 MB uploadedwandb: \ 0.816 MB of 0.821 MB uploadedwandb: | 0.821 MB of 0.821 MB uploadedwandb: / 0.821 MB of 0.821 MB uploadedwandb: - 0.821 MB of 0.893 MB uploadedwandb: \ 0.893 MB of 0.893 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ƒâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–â–ƒâ–‚â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–„â–„â–†â–„â–„â–…â–ˆâ–ƒâ–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 73048
wandb:     model/trainable_params 73048
wandb:                  test/loss 0.16438
wandb:                 train/loss 0.37327
wandb:   val/directional_accuracy 49.58808
wandb:                   val/loss 0.19608
wandb:                    val/mae 0.01743
wandb:                   val/mape 114.9825
wandb:                    val/mse 0.00052
wandb:                     val/r2 -0.00919
wandb:                   val/rmse 0.02282
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/cdv6ng1l
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_150126-cdv6ng1l/logs
Completed: ABSA H=100

Training: Mamba on SASOL for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_150220-fk8ya4c0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/fk8ya4c0
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H3      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/fk8ya4c0
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 3
============================================================
train 3772
val 211
test 212
Epoch: 1, Steps: 118 | Train Loss: 0.2304092 Vali Loss: 0.1088375 Test Loss: 0.1538957
Validation loss decreased (inf --> 0.108837).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2197946 Vali Loss: 0.1067985 Test Loss: 0.1492286
Validation loss decreased (0.108837 --> 0.106799).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 118 | Train Loss: 0.2100712 Vali Loss: 0.1081661 Test Loss: 0.1462707
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2049142 Vali Loss: 0.1034864 Test Loss: 0.1448475
Validation loss decreased (0.106799 --> 0.103486).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2024168 Vali Loss: 0.1050934 Test Loss: 0.1442229
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2013207 Vali Loss: 0.1034047 Test Loss: 0.1439393
Validation loss decreased (0.103486 --> 0.103405).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2005001 Vali Loss: 0.1034159 Test Loss: 0.1438064
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2002206 Vali Loss: 0.1059102 Test Loss: 0.1437410
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23040920634896067, 'val/loss': 0.10883748957089015, 'test/loss': 0.15389565591301238, '_timestamp': 1762779746.8308809}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.21979459665589413, 'val/loss': 0.10679853281804494, 'test/loss': 0.14922862499952316, '_timestamp': 1762779748.60562}).
Epoch: 9, Steps: 118 | Train Loss: 0.2003113 Vali Loss: 0.1046886 Test Loss: 0.1437086
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.1995655 Vali Loss: 0.1039094 Test Loss: 0.1436933
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2004139 Vali Loss: 0.1074579 Test Loss: 0.1436858
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2001765 Vali Loss: 0.1050249 Test Loss: 0.1436818
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.1994653 Vali Loss: 0.1033353 Test Loss: 0.1436800
Validation loss decreased (0.103405 --> 0.103335).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.1994486 Vali Loss: 0.1042860 Test Loss: 0.1436791
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.1995559 Vali Loss: 0.1072709 Test Loss: 0.1436787
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.1997363 Vali Loss: 0.1017164 Test Loss: 0.1436785
Validation loss decreased (0.103335 --> 0.101716).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.2001258 Vali Loss: 0.1052229 Test Loss: 0.1436784
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.1992748 Vali Loss: 0.1028518 Test Loss: 0.1436784
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.2000331 Vali Loss: 0.1038810 Test Loss: 0.1436784
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.1997282 Vali Loss: 0.1060273 Test Loss: 0.1436784
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 118 | Train Loss: 0.1996574 Vali Loss: 0.1067942 Test Loss: 0.1436784
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 118 | Train Loss: 0.1998879 Vali Loss: 0.1059075 Test Loss: 0.1436784
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 118 | Train Loss: 0.2001428 Vali Loss: 0.1028865 Test Loss: 0.1436784
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 118 | Train Loss: 0.1998924 Vali Loss: 0.1026650 Test Loss: 0.1436784
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 118 | Train Loss: 0.1995938 Vali Loss: 0.1079619 Test Loss: 0.1436784
EarlyStopping counter: 9 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 118 | Train Loss: 0.1992440 Vali Loss: 0.1024503 Test Loss: 0.1436784
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 212
test shape: (212, 3, 1) (212, 3, 1)
test shape: (212, 3, 1) (212, 3, 1)


	mse:0.0022293718066066504, mae:0.03516839072108269, rmse:0.047216225415468216, r2:-0.011969208717346191, dtw:Not calculated


VAL - MSE: 0.0022, MAE: 0.0352, RMSE: 0.0472, RÂ²: -0.0120, MAPE: 11297045.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.462 MB of 0.462 MB uploadedwandb: \ 0.462 MB of 0.462 MB uploadedwandb: | 0.462 MB of 0.462 MB uploadedwandb: / 0.462 MB of 0.462 MB uploadedwandb: - 0.462 MB of 0.462 MB uploadedwandb: \ 0.462 MB of 0.462 MB uploadedwandb: | 0.462 MB of 0.462 MB uploadedwandb: / 0.511 MB of 0.584 MB uploaded (0.002 MB deduped)wandb: - 0.573 MB of 0.584 MB uploaded (0.002 MB deduped)wandb: \ 0.584 MB of 0.584 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–‚â–â–‚â–â–â–â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–…â–ƒâ–ƒâ–†â–„â–ƒâ–‡â–…â–ƒâ–„â–‡â–â–…â–‚â–ƒâ–†â–‡â–†â–‚â–‚â–ˆâ–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 25
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35218
wandb:     model/trainable_params 35218
wandb:                  test/loss 0.14368
wandb:                 train/loss 0.19924
wandb:   val/directional_accuracy 53.30189
wandb:                   val/loss 0.10245
wandb:                    val/mae 0.03517
wandb:                   val/mape 1129704500.0
wandb:                    val/mse 0.00223
wandb:                     val/r2 -0.01197
wandb:                   val/rmse 0.04722
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/fk8ya4c0
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_150220-fk8ya4c0/logs
Completed: SASOL H=3

Training: Mamba on SASOL for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_150342-bhuvm17o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/bhuvm17o
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H5      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/bhuvm17o
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 5
============================================================
train 3770
val 209
test 210
Epoch: 1, Steps: 118 | Train Loss: 0.2333904 Vali Loss: 0.1103252 Test Loss: 0.1551043
Validation loss decreased (inf --> 0.110325).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2222215 Vali Loss: 0.1083017 Test Loss: 0.1496417
Validation loss decreased (0.110325 --> 0.108302).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 118 | Train Loss: 0.2107871 Vali Loss: 0.1052748 Test Loss: 0.1468132
Validation loss decreased (0.108302 --> 0.105275).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2060271 Vali Loss: 0.1060283 Test Loss: 0.1455563
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2031321 Vali Loss: 0.1074398 Test Loss: 0.1450451
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2019300 Vali Loss: 0.1046266 Test Loss: 0.1448327
Validation loss decreased (0.105275 --> 0.104627).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2014186 Vali Loss: 0.1071675 Test Loss: 0.1447373
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2007962 Vali Loss: 0.1061752 Test Loss: 0.1446935
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2333903520415395, 'val/loss': 0.11032524704933167, 'test/loss': 0.1551043369940349, '_timestamp': 1762779831.3169713}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.22222148728067592, 'val/loss': 0.10830173535006386, 'test/loss': 0.1496416511280196, '_timestamp': 1762779833.123137}).
Epoch: 9, Steps: 118 | Train Loss: 0.2013979 Vali Loss: 0.1059574 Test Loss: 0.1446705
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2007775 Vali Loss: 0.1071656 Test Loss: 0.1446600
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2013956 Vali Loss: 0.1052391 Test Loss: 0.1446543
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2010200 Vali Loss: 0.1069970 Test Loss: 0.1446516
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2009578 Vali Loss: 0.1055566 Test Loss: 0.1446502
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2015518 Vali Loss: 0.1029787 Test Loss: 0.1446495
Validation loss decreased (0.104627 --> 0.102979).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.2013543 Vali Loss: 0.1036263 Test Loss: 0.1446492
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.2012573 Vali Loss: 0.1043529 Test Loss: 0.1446491
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.2010570 Vali Loss: 0.1030791 Test Loss: 0.1446490
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.2009886 Vali Loss: 0.1032048 Test Loss: 0.1446490
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.2011293 Vali Loss: 0.1025418 Test Loss: 0.1446490
Validation loss decreased (0.102979 --> 0.102542).  Saving model ...
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.2012142 Vali Loss: 0.1054519 Test Loss: 0.1446490
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 118 | Train Loss: 0.2009836 Vali Loss: 0.1076001 Test Loss: 0.1446490
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 118 | Train Loss: 0.2009421 Vali Loss: 0.1059951 Test Loss: 0.1446490
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 118 | Train Loss: 0.2012936 Vali Loss: 0.1063864 Test Loss: 0.1446490
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 118 | Train Loss: 0.2014692 Vali Loss: 0.1033929 Test Loss: 0.1446490
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 118 | Train Loss: 0.2009177 Vali Loss: 0.1044855 Test Loss: 0.1446490
EarlyStopping counter: 6 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 118 | Train Loss: 0.2014363 Vali Loss: 0.1043715 Test Loss: 0.1446490
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 118 | Train Loss: 0.2010664 Vali Loss: 0.1058889 Test Loss: 0.1446490
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 118 | Train Loss: 0.2013669 Vali Loss: 0.1035854 Test Loss: 0.1446490
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 118 | Train Loss: 0.2008433 Vali Loss: 0.1064139 Test Loss: 0.1446490
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 210
test shape: (210, 5, 1) (210, 5, 1)
test shape: (210, 5, 1) (210, 5, 1)


	mse:0.0022311913780868053, mae:0.03523603454232216, rmse:0.04723548889160156, r2:-0.005312919616699219, dtw:Not calculated


VAL - MSE: 0.0022, MAE: 0.0352, RMSE: 0.0472, RÂ²: -0.0053, MAPE: 10999160.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.497 MB of 0.497 MB uploadedwandb: \ 0.497 MB of 0.497 MB uploadedwandb: | 0.497 MB of 0.497 MB uploadedwandb: / 0.497 MB of 0.497 MB uploadedwandb: - 0.497 MB of 0.571 MB uploadedwandb: \ 0.571 MB of 0.571 MB uploadedwandb: | 0.571 MB of 0.571 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–†â–ˆâ–„â–‡â–†â–†â–‡â–…â–‡â–…â–‚â–ƒâ–„â–‚â–‚â–â–…â–ˆâ–†â–†â–‚â–„â–„â–†â–‚â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 28
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35998
wandb:     model/trainable_params 35998
wandb:                  test/loss 0.14465
wandb:                 train/loss 0.20084
wandb:   val/directional_accuracy 51.19048
wandb:                   val/loss 0.10641
wandb:                    val/mae 0.03524
wandb:                   val/mape 1099916000.0
wandb:                    val/mse 0.00223
wandb:                     val/r2 -0.00531
wandb:                   val/rmse 0.04724
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/bhuvm17o
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_150342-bhuvm17o/logs
Completed: SASOL H=5

Training: Mamba on SASOL for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_150506-hihhf1x8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/hihhf1x8
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H10     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/hihhf1x8
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 10
============================================================
train 3765
val 204
test 205
Epoch: 1, Steps: 118 | Train Loss: 0.2406788 Vali Loss: 0.1123103 Test Loss: 0.1581212
Validation loss decreased (inf --> 0.112310).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2331267 Vali Loss: 0.1101183 Test Loss: 0.1537539
Validation loss decreased (0.112310 --> 0.110118).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 118 | Train Loss: 0.2211032 Vali Loss: 0.1117146 Test Loss: 0.1496626
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2135842 Vali Loss: 0.1072034 Test Loss: 0.1478940
Validation loss decreased (0.110118 --> 0.107203).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2101773 Vali Loss: 0.1100801 Test Loss: 0.1472060
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2090008 Vali Loss: 0.1105404 Test Loss: 0.1469323
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2090440 Vali Loss: 0.1090248 Test Loss: 0.1468108
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2087125 Vali Loss: 0.1125299 Test Loss: 0.1467541
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2406788300407135, 'val/loss': 0.11231031588145665, 'test/loss': 0.15812123779739654, '_timestamp': 1762779913.1173592}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23312674734299466, 'val/loss': 0.11011828907898494, 'test/loss': 0.15375390968152455, '_timestamp': 1762779914.8978105}).
Epoch: 9, Steps: 118 | Train Loss: 0.2092606 Vali Loss: 0.1102721 Test Loss: 0.1467260
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2084380 Vali Loss: 0.1077550 Test Loss: 0.1467126
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2081059 Vali Loss: 0.1084380 Test Loss: 0.1467060
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2087330 Vali Loss: 0.1092026 Test Loss: 0.1467027
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2084189 Vali Loss: 0.1113582 Test Loss: 0.1467011
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2081596 Vali Loss: 0.1113855 Test Loss: 0.1467003
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 205
test shape: (205, 10, 1) (205, 10, 1)
test shape: (205, 10, 1) (205, 10, 1)


	mse:0.0022425202187150717, mae:0.03514242172241211, rmse:0.04735525697469711, r2:-0.010280370712280273, dtw:Not calculated


VAL - MSE: 0.0022, MAE: 0.0351, RMSE: 0.0474, RÂ²: -0.0103, MAPE: 9834387.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.535 MB of 0.535 MB uploadedwandb: \ 0.535 MB of 0.535 MB uploadedwandb: | 0.535 MB of 0.535 MB uploadedwandb: / 0.535 MB of 0.607 MB uploadedwandb: - 0.607 MB of 0.607 MB uploadedwandb: \ 0.607 MB of 0.607 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–‚â–â–‚â–â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–â–…â–…â–ƒâ–ˆâ–…â–‚â–ƒâ–„â–†â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 13
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 37948
wandb:     model/trainable_params 37948
wandb:                  test/loss 0.1467
wandb:                 train/loss 0.20816
wandb:   val/directional_accuracy 50.1897
wandb:                   val/loss 0.11139
wandb:                    val/mae 0.03514
wandb:                   val/mape 983438700.0
wandb:                    val/mse 0.00224
wandb:                     val/r2 -0.01028
wandb:                   val/rmse 0.04736
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/hihhf1x8
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_150506-hihhf1x8/logs
Completed: SASOL H=10

Training: Mamba on SASOL for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_150556-hylfgv00
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/hylfgv00
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H22     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/hylfgv00
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 22
============================================================
train 3753
val 192
test 193
Epoch: 1, Steps: 118 | Train Loss: 0.2556521 Vali Loss: 0.1134161 Test Loss: 0.1605874
Validation loss decreased (inf --> 0.113416).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2454627 Vali Loss: 0.1135860 Test Loss: 0.1558338
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 118 | Train Loss: 0.2312844 Vali Loss: 0.1133688 Test Loss: 0.1522444
Validation loss decreased (0.113416 --> 0.113369).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2238785 Vali Loss: 0.1123986 Test Loss: 0.1510098
Validation loss decreased (0.113369 --> 0.112399).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2213870 Vali Loss: 0.1119008 Test Loss: 0.1506809
Validation loss decreased (0.112399 --> 0.111901).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2209606 Vali Loss: 0.1116716 Test Loss: 0.1505292
Validation loss decreased (0.111901 --> 0.111672).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2222231 Vali Loss: 0.1115431 Test Loss: 0.1504657
Validation loss decreased (0.111672 --> 0.111543).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2202505 Vali Loss: 0.1114933 Test Loss: 0.1504345
Validation loss decreased (0.111543 --> 0.111493).  Saving model ...
Updating learning rate to 7.8125e-07
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2556520702980332, 'val/loss': 0.11341614897052447, 'test/loss': 0.16058742042098725, '_timestamp': 1762779963.4003801}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2454626632191367, 'val/loss': 0.11358603710929553, 'test/loss': 0.15583375096321106, '_timestamp': 1762779965.1893978}).
Epoch: 9, Steps: 118 | Train Loss: 0.2206099 Vali Loss: 0.1114681 Test Loss: 0.1504188
Validation loss decreased (0.111493 --> 0.111468).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2213211 Vali Loss: 0.1114554 Test Loss: 0.1504107
Validation loss decreased (0.111468 --> 0.111455).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2200246 Vali Loss: 0.1114490 Test Loss: 0.1504075
Validation loss decreased (0.111455 --> 0.111449).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2200071 Vali Loss: 0.1114459 Test Loss: 0.1504055
Validation loss decreased (0.111449 --> 0.111446).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2218701 Vali Loss: 0.1114442 Test Loss: 0.1504046
Validation loss decreased (0.111446 --> 0.111444).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2200917 Vali Loss: 0.1114434 Test Loss: 0.1504041
Validation loss decreased (0.111444 --> 0.111443).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.2205108 Vali Loss: 0.1114430 Test Loss: 0.1504039
Validation loss decreased (0.111443 --> 0.111443).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.2203431 Vali Loss: 0.1114428 Test Loss: 0.1504038
Validation loss decreased (0.111443 --> 0.111443).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.2197627 Vali Loss: 0.1114427 Test Loss: 0.1504038
Validation loss decreased (0.111443 --> 0.111443).  Saving model ...
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.2201483 Vali Loss: 0.1114427 Test Loss: 0.1504038
Validation loss decreased (0.111443 --> 0.111443).  Saving model ...
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.2203204 Vali Loss: 0.1114427 Test Loss: 0.1504038
Validation loss decreased (0.111443 --> 0.111443).  Saving model ...
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.2197809 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 118 | Train Loss: 0.2199068 Vali Loss: 0.1114427 Test Loss: 0.1504038
Validation loss decreased (0.111443 --> 0.111443).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 118 | Train Loss: 0.2197476 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 118 | Train Loss: 0.2211213 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 118 | Train Loss: 0.2211593 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 118 | Train Loss: 0.2199434 Vali Loss: 0.1114427 Test Loss: 0.1504038
Validation loss decreased (0.111443 --> 0.111443).  Saving model ...
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 118 | Train Loss: 0.2198541 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 118 | Train Loss: 0.2203777 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 118 | Train Loss: 0.2208747 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 118 | Train Loss: 0.2205625 Vali Loss: 0.1114427 Test Loss: 0.1504038
Validation loss decreased (0.111443 --> 0.111443).  Saving model ...
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 118 | Train Loss: 0.2200783 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 118 | Train Loss: 0.2200577 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 118 | Train Loss: 0.2204037 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 118 | Train Loss: 0.2194825 Vali Loss: 0.1114427 Test Loss: 0.1504038
Validation loss decreased (0.111443 --> 0.111443).  Saving model ...
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 118 | Train Loss: 0.2195189 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.1641532182693482e-14
Epoch: 35, Steps: 118 | Train Loss: 0.2211314 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.820766091346741e-15
Epoch: 36, Steps: 118 | Train Loss: 0.2201101 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.9103830456733705e-15
Epoch: 37, Steps: 118 | Train Loss: 0.2199343 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.4551915228366853e-15
Epoch: 38, Steps: 118 | Train Loss: 0.2205444 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.275957614183426e-16
Epoch: 39, Steps: 118 | Train Loss: 0.2207301 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.637978807091713e-16
Epoch: 40, Steps: 118 | Train Loss: 0.2198356 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.8189894035458566e-16
Epoch: 41, Steps: 118 | Train Loss: 0.2197414 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.094947017729283e-17
Epoch: 42, Steps: 118 | Train Loss: 0.2198937 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.5474735088646414e-17
Epoch: 43, Steps: 118 | Train Loss: 0.2206122 Vali Loss: 0.1114427 Test Loss: 0.1504038
Validation loss decreased (0.111443 --> 0.111443).  Saving model ...
Updating learning rate to 2.2737367544323207e-17
Epoch: 44, Steps: 118 | Train Loss: 0.2204464 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.1368683772161604e-17
Epoch: 45, Steps: 118 | Train Loss: 0.2213022 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.684341886080802e-18
Epoch: 46, Steps: 118 | Train Loss: 0.2212659 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.842170943040401e-18
Epoch: 47, Steps: 118 | Train Loss: 0.2199207 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.4210854715202004e-18
Epoch: 48, Steps: 118 | Train Loss: 0.2201337 Vali Loss: 0.1114427 Test Loss: 0.1504038
Validation loss decreased (0.111443 --> 0.111443).  Saving model ...
Updating learning rate to 7.105427357601002e-19
Epoch: 49, Steps: 118 | Train Loss: 0.2207330 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.552713678800501e-19
Epoch: 50, Steps: 118 | Train Loss: 0.2199597 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.7763568394002505e-19
Epoch: 51, Steps: 118 | Train Loss: 0.2199504 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 3 out of 10
Updating learning rate to 8.881784197001253e-20
Epoch: 52, Steps: 118 | Train Loss: 0.2198265 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.4408920985006264e-20
Epoch: 53, Steps: 118 | Train Loss: 0.2198828 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.2204460492503132e-20
Epoch: 54, Steps: 118 | Train Loss: 0.2198188 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.1102230246251566e-20
Epoch: 55, Steps: 118 | Train Loss: 0.2207456 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 7 out of 10
Updating learning rate to 5.551115123125783e-21
Epoch: 56, Steps: 118 | Train Loss: 0.2198634 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.7755575615628915e-21
Epoch: 57, Steps: 118 | Train Loss: 0.2198661 Vali Loss: 0.1114427 Test Loss: 0.1504038
Validation loss decreased (0.111443 --> 0.111443).  Saving model ...
Updating learning rate to 1.3877787807814457e-21
Epoch: 58, Steps: 118 | Train Loss: 0.2204132 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.938893903907229e-22
Epoch: 59, Steps: 118 | Train Loss: 0.2200901 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.4694469519536144e-22
Epoch: 60, Steps: 118 | Train Loss: 0.2202992 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.7347234759768072e-22
Epoch: 61, Steps: 118 | Train Loss: 0.2203818 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 4 out of 10
Updating learning rate to 8.673617379884036e-23
Epoch: 62, Steps: 118 | Train Loss: 0.2198596 Vali Loss: 0.1114427 Test Loss: 0.1504038
Validation loss decreased (0.111443 --> 0.111443).  Saving model ...
Updating learning rate to 4.336808689942018e-23
Epoch: 63, Steps: 118 | Train Loss: 0.2204734 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.168404344971009e-23
Epoch: 64, Steps: 118 | Train Loss: 0.2203872 Vali Loss: 0.1114427 Test Loss: 0.1504038
Validation loss decreased (0.111443 --> 0.111443).  Saving model ...
Updating learning rate to 1.0842021724855045e-23
Epoch: 65, Steps: 118 | Train Loss: 0.2196949 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.4210108624275224e-24
Epoch: 66, Steps: 118 | Train Loss: 0.2199553 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.7105054312137612e-24
Epoch: 67, Steps: 118 | Train Loss: 0.2203626 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.3552527156068806e-24
Epoch: 68, Steps: 118 | Train Loss: 0.2197669 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.776263578034403e-25
Epoch: 69, Steps: 118 | Train Loss: 0.2197279 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.3881317890172015e-25
Epoch: 70, Steps: 118 | Train Loss: 0.2201110 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.6940658945086008e-25
Epoch: 71, Steps: 118 | Train Loss: 0.2209499 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 7 out of 10
Updating learning rate to 8.470329472543004e-26
Epoch: 72, Steps: 118 | Train Loss: 0.2201028 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.235164736271502e-26
Epoch: 73, Steps: 118 | Train Loss: 0.2202050 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.117582368135751e-26
Epoch: 74, Steps: 118 | Train Loss: 0.2204123 Vali Loss: 0.1114427 Test Loss: 0.1504038
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 193
test shape: (193, 22, 1) (193, 22, 1)
test shape: (193, 22, 1) (193, 22, 1)


	mse:0.0022675301879644394, mae:0.035104043781757355, rmse:0.04761859029531479, r2:-0.010428190231323242, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0351, RMSE: 0.0476, RÂ²: -0.0104, MAPE: 8188428.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.592 MB of 0.593 MB uploadedwandb: \ 0.592 MB of 0.593 MB uploadedwandb: | 0.592 MB of 0.593 MB uploadedwandb: / 0.593 MB of 0.593 MB uploadedwandb: - 0.593 MB of 0.593 MB uploadedwandb: \ 0.593 MB of 0.675 MB uploadedwandb: | 0.675 MB of 0.675 MB uploadedwandb: / 0.675 MB of 0.675 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–‚â–â–‚â–â–‚â–‚â–â–â–â–‚â–‚â–â–‚â–â–‚â–â–‚â–â–‚â–â–‚â–‚â–‚â–â–â–â–â–‚â–â–â–‚â–‚â–‚â–â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 73
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 42628
wandb:     model/trainable_params 42628
wandb:                  test/loss 0.1504
wandb:                 train/loss 0.22041
wandb:   val/directional_accuracy 50.16038
wandb:                   val/loss 0.11144
wandb:                    val/mae 0.0351
wandb:                   val/mape 818842800.0
wandb:                    val/mse 0.00227
wandb:                     val/r2 -0.01043
wandb:                   val/rmse 0.04762
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/hylfgv00
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_150556-hylfgv00/logs
Completed: SASOL H=22

Training: Mamba on SASOL for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_150840-icpqlbtk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/icpqlbtk
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H50     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/icpqlbtk
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 50
============================================================
train 3725
val 164
test 165
Epoch: 1, Steps: 117 | Train Loss: 0.2879258 Vali Loss: 0.1196701 Test Loss: 0.1652908
Validation loss decreased (inf --> 0.119670).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 117 | Train Loss: 0.2798147 Vali Loss: 0.1224882 Test Loss: 0.1621795
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 117 | Train Loss: 0.2640119 Vali Loss: 0.1179545 Test Loss: 0.1613773
Validation loss decreased (0.119670 --> 0.117954).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 117 | Train Loss: 0.2564386 Vali Loss: 0.1154840 Test Loss: 0.1614184
Validation loss decreased (0.117954 --> 0.115484).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 117 | Train Loss: 0.2527200 Vali Loss: 0.1145394 Test Loss: 0.1615200
Validation loss decreased (0.115484 --> 0.114539).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 117 | Train Loss: 0.2517435 Vali Loss: 0.1145927 Test Loss: 0.1615821
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 117 | Train Loss: 0.2510648 Vali Loss: 0.1124983 Test Loss: 0.1616304
Validation loss decreased (0.114539 --> 0.112498).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 117 | Train Loss: 0.2512406 Vali Loss: 0.1091539 Test Loss: 0.1616403
Validation loss decreased (0.112498 --> 0.109154).  Saving model ...
Updating learning rate to 7.8125e-07
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.28792577128634494, 'val/loss': 0.11967014148831367, 'test/loss': 0.16529079029957452, '_timestamp': 1762780128.6221845}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2798147234651778, 'val/loss': 0.12248819693922997, 'test/loss': 0.1621795172492663, '_timestamp': 1762780130.442987}).
Epoch: 9, Steps: 117 | Train Loss: 0.2505070 Vali Loss: 0.1100802 Test Loss: 0.1616443
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 117 | Train Loss: 0.2512545 Vali Loss: 0.1162158 Test Loss: 0.1616491
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 117 | Train Loss: 0.2505639 Vali Loss: 0.1143381 Test Loss: 0.1616503
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 117 | Train Loss: 0.2507978 Vali Loss: 0.1123553 Test Loss: 0.1616519
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 117 | Train Loss: 0.2511379 Vali Loss: 0.1150365 Test Loss: 0.1616522
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 117 | Train Loss: 0.2505010 Vali Loss: 0.1092295 Test Loss: 0.1616525
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 117 | Train Loss: 0.2504112 Vali Loss: 0.1150372 Test Loss: 0.1616527
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 117 | Train Loss: 0.2509028 Vali Loss: 0.1190745 Test Loss: 0.1616527
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 117 | Train Loss: 0.2512469 Vali Loss: 0.1099135 Test Loss: 0.1616527
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 117 | Train Loss: 0.2521809 Vali Loss: 0.1174420 Test Loss: 0.1616527
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 165
test shape: (165, 50, 1) (165, 50, 1)
test shape: (165, 50, 1) (165, 50, 1)


	mse:0.002066132379695773, mae:0.03380599990487099, rmse:0.0454547293484211, r2:-0.008478164672851562, dtw:Not calculated


VAL - MSE: 0.0021, MAE: 0.0338, RMSE: 0.0455, RÂ²: -0.0085, MAPE: 7459518.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.630 MB of 0.632 MB uploadedwandb: \ 0.630 MB of 0.632 MB uploadedwandb: | 0.630 MB of 0.632 MB uploadedwandb: / 0.632 MB of 0.632 MB uploadedwandb: - 0.632 MB of 0.632 MB uploadedwandb: \ 0.632 MB of 0.705 MB uploadedwandb: | 0.705 MB of 0.705 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–‚â–…â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–…â–…â–…â–ƒâ–â–‚â–†â–…â–ƒâ–…â–â–…â–ˆâ–‚â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 17
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 53548
wandb:     model/trainable_params 53548
wandb:                  test/loss 0.16165
wandb:                 train/loss 0.25218
wandb:   val/directional_accuracy 49.40012
wandb:                   val/loss 0.11744
wandb:                    val/mae 0.03381
wandb:                   val/mape 745951850.0
wandb:                    val/mse 0.00207
wandb:                     val/r2 -0.00848
wandb:                   val/rmse 0.04545
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/icpqlbtk
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_150840-icpqlbtk/logs
Completed: SASOL H=50

Training: Mamba on SASOL for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_150945-mxe4zkgh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/mxe4zkgh
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H100    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/mxe4zkgh
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 100
============================================================
train 3675
val 114
test 115
Epoch: 1, Steps: 115 | Train Loss: 0.3502283 Vali Loss: 0.1349269 Test Loss: 0.1729450
Validation loss decreased (inf --> 0.134927).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 115 | Train Loss: 0.3393322 Vali Loss: 0.1358073 Test Loss: 0.1727349
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 115 | Train Loss: 0.3184477 Vali Loss: 0.1313805 Test Loss: 0.1744700
Validation loss decreased (0.134927 --> 0.131381).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 115 | Train Loss: 0.3117521 Vali Loss: 0.1297756 Test Loss: 0.1752041
Validation loss decreased (0.131381 --> 0.129776).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 115 | Train Loss: 0.3094026 Vali Loss: 0.1293053 Test Loss: 0.1753702
Validation loss decreased (0.129776 --> 0.129305).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 115 | Train Loss: 0.3081623 Vali Loss: 0.1288759 Test Loss: 0.1754912
Validation loss decreased (0.129305 --> 0.128876).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 115 | Train Loss: 0.3090302 Vali Loss: 0.1281108 Test Loss: 0.1755394
Validation loss decreased (0.128876 --> 0.128111).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 115 | Train Loss: 0.3082744 Vali Loss: 0.1276743 Test Loss: 0.1755531
Validation loss decreased (0.128111 --> 0.127674).  Saving model ...
Updating learning rate to 7.8125e-07
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3502283440983814, 'val/loss': 0.13492687046527863, 'test/loss': 0.172945037484169, '_timestamp': 1762780192.9091303}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.339332247816998, 'val/loss': 0.1358073130249977, 'test/loss': 0.17273494973778725, '_timestamp': 1762780194.6597757}).
Epoch: 9, Steps: 115 | Train Loss: 0.3082769 Vali Loss: 0.1288245 Test Loss: 0.1755688
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 115 | Train Loss: 0.3081176 Vali Loss: 0.1277128 Test Loss: 0.1755725
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 115 | Train Loss: 0.3083311 Vali Loss: 0.1261496 Test Loss: 0.1755763
Validation loss decreased (0.127674 --> 0.126150).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 115 | Train Loss: 0.3089729 Vali Loss: 0.1277878 Test Loss: 0.1755775
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 115 | Train Loss: 0.3084225 Vali Loss: 0.1284459 Test Loss: 0.1755785
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 115 | Train Loss: 0.3082564 Vali Loss: 0.1284188 Test Loss: 0.1755788
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 115 | Train Loss: 0.3080096 Vali Loss: 0.1278930 Test Loss: 0.1755790
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 115 | Train Loss: 0.3078651 Vali Loss: 0.1293649 Test Loss: 0.1755791
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 115 | Train Loss: 0.3078617 Vali Loss: 0.1270288 Test Loss: 0.1755791
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 115 | Train Loss: 0.3081300 Vali Loss: 0.1289014 Test Loss: 0.1755791
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 115 | Train Loss: 0.3083811 Vali Loss: 0.1286056 Test Loss: 0.1755791
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 115 | Train Loss: 0.3081079 Vali Loss: 0.1292680 Test Loss: 0.1755791
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 115 | Train Loss: 0.3076080 Vali Loss: 0.1292471 Test Loss: 0.1755791
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 115
test shape: (115, 100, 1) (115, 100, 1)
test shape: (115, 100, 1) (115, 100, 1)


	mse:0.002001367509365082, mae:0.03322107344865799, rmse:0.04473664611577988, r2:-0.008540511131286621, dtw:Not calculated


VAL - MSE: 0.0020, MAE: 0.0332, RMSE: 0.0447, RÂ²: -0.0085, MAPE: 7615238.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.674 MB of 0.679 MB uploadedwandb: \ 0.674 MB of 0.679 MB uploadedwandb: | 0.679 MB of 0.679 MB uploadedwandb: / 0.679 MB of 0.752 MB uploadedwandb: - 0.752 MB of 0.752 MB uploadedwandb: \ 0.752 MB of 0.752 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–„â–‚â–â–‚â–â–â–â–â–‚â–‚â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–…â–…â–„â–ƒâ–…â–ƒâ–â–ƒâ–„â–„â–ƒâ–…â–‚â–…â–„â–…â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 73048
wandb:     model/trainable_params 73048
wandb:                  test/loss 0.17558
wandb:                 train/loss 0.30761
wandb:   val/directional_accuracy 49.39833
wandb:                   val/loss 0.12925
wandb:                    val/mae 0.03322
wandb:                   val/mape 761523850.0
wandb:                    val/mse 0.002
wandb:                     val/r2 -0.00854
wandb:                   val/rmse 0.04474
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/mxe4zkgh
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_150945-mxe4zkgh/logs
Completed: SASOL H=100

Training: Mamba on DRD_GOLD for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_151053-doh57c58
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/doh57c58
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_DRD_GOLD_H3   Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/doh57c58
>>>>>>>start training : long_term_forecast_Mamba_DRD_GOLD_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.2734321 Vali Loss: 0.1572794 Test Loss: 0.1402273
Validation loss decreased (inf --> 0.157279).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2459828 Vali Loss: 0.1359802 Test Loss: 0.1299705
Validation loss decreased (0.157279 --> 0.135980).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2269720 Vali Loss: 0.1284391 Test Loss: 0.1273841
Validation loss decreased (0.135980 --> 0.128439).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2223442 Vali Loss: 0.1275708 Test Loss: 0.1267054
Validation loss decreased (0.128439 --> 0.127571).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2209330 Vali Loss: 0.1282278 Test Loss: 0.1264594
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2194951 Vali Loss: 0.1244679 Test Loss: 0.1263881
Validation loss decreased (0.127571 --> 0.124468).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2205185 Vali Loss: 0.1243684 Test Loss: 0.1263469
Validation loss decreased (0.124468 --> 0.124368).  Saving model ...
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2734320624206299, 'val/loss': 0.15727943275123835, 'test/loss': 0.14022729080170393, '_timestamp': 1762780261.4075556}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2459827825090939, 'val/loss': 0.1359801832586527, 'test/loss': 0.12997049931436777, '_timestamp': 1762780263.3148694}).
Epoch: 8, Steps: 133 | Train Loss: 0.2196708 Vali Loss: 0.1302554 Test Loss: 0.1263225
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2195837 Vali Loss: 0.1279418 Test Loss: 0.1263105
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2204229 Vali Loss: 0.1301016 Test Loss: 0.1263068
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2197433 Vali Loss: 0.1279536 Test Loss: 0.1263033
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2197035 Vali Loss: 0.1340431 Test Loss: 0.1263014
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2191491 Vali Loss: 0.1284985 Test Loss: 0.1263008
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2194676 Vali Loss: 0.1331416 Test Loss: 0.1263004
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2194894 Vali Loss: 0.1282340 Test Loss: 0.1263002
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2193786 Vali Loss: 0.1268214 Test Loss: 0.1263001
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2195070 Vali Loss: 0.1305440 Test Loss: 0.1263001
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_DRD_GOLD_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.0009296228527091444, mae:0.021527385339140892, rmse:0.030489716678857803, r2:-0.0024253129959106445, dtw:Not calculated


VAL - MSE: 0.0009, MAE: 0.0215, RMSE: 0.0305, RÂ²: -0.0024, MAPE: 1146722.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.480 MB of 0.480 MB uploadedwandb: \ 0.480 MB of 0.480 MB uploadedwandb: | 0.480 MB of 0.480 MB uploadedwandb: / 0.480 MB of 0.480 MB uploadedwandb: - 0.480 MB of 0.480 MB uploadedwandb: \ 0.480 MB of 0.480 MB uploadedwandb: | 0.480 MB of 0.480 MB uploadedwandb: / 0.480 MB of 0.480 MB uploadedwandb: - 0.529 MB of 0.601 MB uploadedwandb: \ 0.597 MB of 0.601 MB uploadedwandb: | 0.601 MB of 0.601 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–â–‚â–â–â–‚â–‚â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–ƒâ–„â–â–â–…â–„â–…â–„â–ˆâ–„â–‡â–„â–ƒâ–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35218
wandb:     model/trainable_params 35218
wandb:                  test/loss 0.1263
wandb:                 train/loss 0.21951
wandb:   val/directional_accuracy 53.16456
wandb:                   val/loss 0.13054
wandb:                    val/mae 0.02153
wandb:                   val/mape 114672225.0
wandb:                    val/mse 0.00093
wandb:                     val/r2 -0.00243
wandb:                   val/rmse 0.03049
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/doh57c58
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_151053-doh57c58/logs
Completed: DRD_GOLD H=3

Training: Mamba on DRD_GOLD for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_151203-7hyugydy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/7hyugydy
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_DRD_GOLD_H5   Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/7hyugydy
>>>>>>>start training : long_term_forecast_Mamba_DRD_GOLD_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.2781651 Vali Loss: 0.1658982 Test Loss: 0.1423318
Validation loss decreased (inf --> 0.165898).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2494266 Vali Loss: 0.1379366 Test Loss: 0.1322230
Validation loss decreased (0.165898 --> 0.137937).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2312090 Vali Loss: 0.1348380 Test Loss: 0.1307273
Validation loss decreased (0.137937 --> 0.134838).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2276783 Vali Loss: 0.1334121 Test Loss: 0.1300795
Validation loss decreased (0.134838 --> 0.133412).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2267368 Vali Loss: 0.1423528 Test Loss: 0.1299050
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2263517 Vali Loss: 0.1316751 Test Loss: 0.1297782
Validation loss decreased (0.133412 --> 0.131675).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2255759 Vali Loss: 0.1330422 Test Loss: 0.1297343
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2781651422269362, 'val/loss': 0.16589822433888912, 'test/loss': 0.14233179856091738, '_timestamp': 1762780332.2117589}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24942659670697118, 'val/loss': 0.1379365874454379, 'test/loss': 0.1322230063378811, '_timestamp': 1762780334.1273713}).
Epoch: 8, Steps: 133 | Train Loss: 0.2258768 Vali Loss: 0.1325085 Test Loss: 0.1297193
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2255472 Vali Loss: 0.1321851 Test Loss: 0.1297080
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2256401 Vali Loss: 0.1356625 Test Loss: 0.1297026
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2258459 Vali Loss: 0.1428589 Test Loss: 0.1297003
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2252352 Vali Loss: 0.1384731 Test Loss: 0.1296994
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2255654 Vali Loss: 0.1319988 Test Loss: 0.1296988
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2256387 Vali Loss: 0.1387015 Test Loss: 0.1296985
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2258285 Vali Loss: 0.1312498 Test Loss: 0.1296984
Validation loss decreased (0.131675 --> 0.131250).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2258729 Vali Loss: 0.1355046 Test Loss: 0.1296983
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2256779 Vali Loss: 0.1380692 Test Loss: 0.1296983
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2256017 Vali Loss: 0.1345534 Test Loss: 0.1296983
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2254677 Vali Loss: 0.1357717 Test Loss: 0.1296983
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2253099 Vali Loss: 0.1367219 Test Loss: 0.1296983
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2253436 Vali Loss: 0.1337621 Test Loss: 0.1296983
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2255292 Vali Loss: 0.1403453 Test Loss: 0.1296983
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2256470 Vali Loss: 0.1335355 Test Loss: 0.1296983
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2252367 Vali Loss: 0.1369272 Test Loss: 0.1296983
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2261341 Vali Loss: 0.1357600 Test Loss: 0.1296983
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_DRD_GOLD_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.0009373463690280914, mae:0.02176186442375183, rmse:0.030616112053394318, r2:-0.0038042068481445312, dtw:Not calculated


VAL - MSE: 0.0009, MAE: 0.0218, RMSE: 0.0306, RÂ²: -0.0038, MAPE: 1282656.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.507 MB of 0.507 MB uploadedwandb: \ 0.507 MB of 0.507 MB uploadedwandb: | 0.507 MB of 0.507 MB uploadedwandb: / 0.507 MB of 0.507 MB uploadedwandb: - 0.507 MB of 0.581 MB uploadedwandb: \ 0.581 MB of 0.581 MB uploadedwandb: | 0.581 MB of 0.581 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–â–‚â–â–â–‚â–â–â–â–‚â–‚â–‚â–â–â–â–â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–‚â–ˆâ–â–‚â–‚â–‚â–„â–ˆâ–…â–â–…â–â–„â–…â–ƒâ–„â–„â–ƒâ–†â–‚â–„â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 24
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35998
wandb:     model/trainable_params 35998
wandb:                  test/loss 0.1297
wandb:                 train/loss 0.22613
wandb:   val/directional_accuracy 51.2766
wandb:                   val/loss 0.13576
wandb:                    val/mae 0.02176
wandb:                   val/mape 128265625.0
wandb:                    val/mse 0.00094
wandb:                     val/r2 -0.0038
wandb:                   val/rmse 0.03062
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/7hyugydy
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_151203-7hyugydy/logs
Completed: DRD_GOLD H=5

Training: Mamba on DRD_GOLD for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_151324-r3gwdjbj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/r3gwdjbj
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_DRD_GOLD_H10  Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/r3gwdjbj
>>>>>>>start training : long_term_forecast_Mamba_DRD_GOLD_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2892367 Vali Loss: 0.1735752 Test Loss: 0.1479459
Validation loss decreased (inf --> 0.173575).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2646224 Vali Loss: 0.1562136 Test Loss: 0.1398014
Validation loss decreased (0.173575 --> 0.156214).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2412225 Vali Loss: 0.1433775 Test Loss: 0.1387203
Validation loss decreased (0.156214 --> 0.143378).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2377290 Vali Loss: 0.1394989 Test Loss: 0.1380222
Validation loss decreased (0.143378 --> 0.139499).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2371423 Vali Loss: 0.1417367 Test Loss: 0.1378305
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2368699 Vali Loss: 0.1379665 Test Loss: 0.1377486
Validation loss decreased (0.139499 --> 0.137966).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2363106 Vali Loss: 0.1422153 Test Loss: 0.1376936
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.28923672426463964, 'val/loss': 0.17357520759105682, 'test/loss': 0.14794593583792448, '_timestamp': 1762780411.7538857}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2646223744727615, 'val/loss': 0.156213553622365, 'test/loss': 0.13980139419436455, '_timestamp': 1762780413.7165442}).
Epoch: 8, Steps: 133 | Train Loss: 0.2358563 Vali Loss: 0.1422646 Test Loss: 0.1376752
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2369945 Vali Loss: 0.1385707 Test Loss: 0.1376684
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2363880 Vali Loss: 0.1384836 Test Loss: 0.1376623
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2357010 Vali Loss: 0.1378573 Test Loss: 0.1376623
Validation loss decreased (0.137966 --> 0.137857).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2358396 Vali Loss: 0.1442848 Test Loss: 0.1376610
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2361510 Vali Loss: 0.1414348 Test Loss: 0.1376602
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2364178 Vali Loss: 0.1454400 Test Loss: 0.1376599
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2359714 Vali Loss: 0.1443080 Test Loss: 0.1376598
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2365636 Vali Loss: 0.1418353 Test Loss: 0.1376597
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2363979 Vali Loss: 0.1399880 Test Loss: 0.1376597
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2357172 Vali Loss: 0.1401642 Test Loss: 0.1376597
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2365641 Vali Loss: 0.1374179 Test Loss: 0.1376597
Validation loss decreased (0.137857 --> 0.137418).  Saving model ...
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2360924 Vali Loss: 0.1497763 Test Loss: 0.1376597
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2355934 Vali Loss: 0.1441385 Test Loss: 0.1376597
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2359438 Vali Loss: 0.1383554 Test Loss: 0.1376597
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2358915 Vali Loss: 0.1424390 Test Loss: 0.1376597
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2356790 Vali Loss: 0.1424573 Test Loss: 0.1376597
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2356396 Vali Loss: 0.1385909 Test Loss: 0.1376597
EarlyStopping counter: 6 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2356905 Vali Loss: 0.1435929 Test Loss: 0.1376597
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.2361944 Vali Loss: 0.1426133 Test Loss: 0.1376597
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.2359824 Vali Loss: 0.1471186 Test Loss: 0.1376597
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.2363043 Vali Loss: 0.1388225 Test Loss: 0.1376597
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_DRD_GOLD_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.0009561037877574563, mae:0.022024597972631454, rmse:0.030920928344130516, r2:-0.008268952369689941, dtw:Not calculated


VAL - MSE: 0.0010, MAE: 0.0220, RMSE: 0.0309, RÂ²: -0.0083, MAPE: 1186357.88%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.528 MB of 0.529 MB uploadedwandb: \ 0.528 MB of 0.529 MB uploadedwandb: | 0.528 MB of 0.529 MB uploadedwandb: / 0.529 MB of 0.529 MB uploadedwandb: - 0.529 MB of 0.603 MB uploadedwandb: \ 0.603 MB of 0.603 MB uploadedwandb: | 0.603 MB of 0.603 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–ƒâ–‚â–â–ƒâ–‚â–â–â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–‚â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–‚â–ƒâ–â–„â–„â–‚â–‚â–â–…â–ƒâ–†â–…â–„â–‚â–ƒâ–â–ˆâ–…â–‚â–„â–„â–‚â–„â–„â–†â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 28
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 37948
wandb:     model/trainable_params 37948
wandb:                  test/loss 0.13766
wandb:                 train/loss 0.2363
wandb:   val/directional_accuracy 50.43478
wandb:                   val/loss 0.13882
wandb:                    val/mae 0.02202
wandb:                   val/mape 118635787.5
wandb:                    val/mse 0.00096
wandb:                     val/r2 -0.00827
wandb:                   val/rmse 0.03092
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/r3gwdjbj
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_151324-r3gwdjbj/logs
Completed: DRD_GOLD H=10

Training: Mamba on DRD_GOLD for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_151456-or0bbjai
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/or0bbjai
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_DRD_GOLD_H22  Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/or0bbjai
>>>>>>>start training : long_term_forecast_Mamba_DRD_GOLD_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.3107670 Vali Loss: 0.1937038 Test Loss: 0.1513037
Validation loss decreased (inf --> 0.193704).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2844957 Vali Loss: 0.1663783 Test Loss: 0.1498850
Validation loss decreased (0.193704 --> 0.166378).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2636476 Vali Loss: 0.1628377 Test Loss: 0.1494486
Validation loss decreased (0.166378 --> 0.162838).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2607494 Vali Loss: 0.1628110 Test Loss: 0.1493954
Validation loss decreased (0.162838 --> 0.162811).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2600352 Vali Loss: 0.1624132 Test Loss: 0.1493202
Validation loss decreased (0.162811 --> 0.162413).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2599828 Vali Loss: 0.1624759 Test Loss: 0.1493513
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2592471 Vali Loss: 0.1618181 Test Loss: 0.1493676
Validation loss decreased (0.162413 --> 0.161818).  Saving model ...
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.31076699822689546, 'val/loss': 0.19370376638003758, 'test/loss': 0.15130368726594107, '_timestamp': 1762780505.0297468}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.28449565409259364, 'val/loss': 0.1663782915898732, 'test/loss': 0.14988498176847184, '_timestamp': 1762780506.930938}).
Epoch: 8, Steps: 132 | Train Loss: 0.2593058 Vali Loss: 0.1625451 Test Loss: 0.1493971
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2592984 Vali Loss: 0.1623799 Test Loss: 0.1493895
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2591398 Vali Loss: 0.1624859 Test Loss: 0.1494007
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2591017 Vali Loss: 0.1622395 Test Loss: 0.1494013
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2591441 Vali Loss: 0.1617214 Test Loss: 0.1494031
Validation loss decreased (0.161818 --> 0.161721).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2593316 Vali Loss: 0.1618339 Test Loss: 0.1494037
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2589728 Vali Loss: 0.1625242 Test Loss: 0.1494037
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2591766 Vali Loss: 0.1621609 Test Loss: 0.1494038
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2592236 Vali Loss: 0.1620270 Test Loss: 0.1494038
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2588785 Vali Loss: 0.1621293 Test Loss: 0.1494038
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2593907 Vali Loss: 0.1619431 Test Loss: 0.1494038
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2591038 Vali Loss: 0.1624139 Test Loss: 0.1494038
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2591526 Vali Loss: 0.1622144 Test Loss: 0.1494038
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2590995 Vali Loss: 0.1619531 Test Loss: 0.1494038
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2591071 Vali Loss: 0.1622077 Test Loss: 0.1494038
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_DRD_GOLD_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.0009587820968590677, mae:0.02201380766928196, rmse:0.030964206904172897, r2:-0.006827950477600098, dtw:Not calculated


VAL - MSE: 0.0010, MAE: 0.0220, RMSE: 0.0310, RÂ²: -0.0068, MAPE: 1386772.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.589 MB of 0.590 MB uploadedwandb: \ 0.589 MB of 0.590 MB uploadedwandb: | 0.589 MB of 0.590 MB uploadedwandb: / 0.590 MB of 0.590 MB uploadedwandb: - 0.590 MB of 0.663 MB uploadedwandb: \ 0.590 MB of 0.663 MB uploadedwandb: | 0.590 MB of 0.663 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–â–ƒâ–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†
wandb:                 train/loss â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–‚â–â–â–‚â–â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ˆâ–…â–†â–‚â–†â–…â–†â–„â–â–‚â–†â–„â–ƒâ–„â–‚â–…â–„â–‚â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 42628
wandb:     model/trainable_params 42628
wandb:                  test/loss 0.1494
wandb:                 train/loss 0.25911
wandb:   val/directional_accuracy 49.6505
wandb:                   val/loss 0.16221
wandb:                    val/mae 0.02201
wandb:                   val/mape 138677275.0
wandb:                    val/mse 0.00096
wandb:                     val/r2 -0.00683
wandb:                   val/rmse 0.03096
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/or0bbjai
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_151456-or0bbjai/logs
Completed: DRD_GOLD H=22

Training: Mamba on DRD_GOLD for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_151610-ziza8yxk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/ziza8yxk
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_DRD_GOLD_H50  Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/ziza8yxk
>>>>>>>start training : long_term_forecast_Mamba_DRD_GOLD_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.3572320 Vali Loss: 0.2341919 Test Loss: 0.1529089
Validation loss decreased (inf --> 0.234192).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.3335380 Vali Loss: 0.2025179 Test Loss: 0.1545302
Validation loss decreased (0.234192 --> 0.202518).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.3087652 Vali Loss: 0.1981068 Test Loss: 0.1545260
Validation loss decreased (0.202518 --> 0.198107).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.3061613 Vali Loss: 0.1972799 Test Loss: 0.1550056
Validation loss decreased (0.198107 --> 0.197280).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.3044847 Vali Loss: 0.1968424 Test Loss: 0.1550586
Validation loss decreased (0.197280 --> 0.196842).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.3043948 Vali Loss: 0.1964552 Test Loss: 0.1552330
Validation loss decreased (0.196842 --> 0.196455).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.3050927 Vali Loss: 0.1963862 Test Loss: 0.1553008
Validation loss decreased (0.196455 --> 0.196386).  Saving model ...
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3572319813750007, 'val/loss': 0.23419192681709924, 'test/loss': 0.15290886412064233, '_timestamp': 1762780577.5095496}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3335380194087823, 'val/loss': 0.20251790434122086, 'test/loss': 0.1545301948984464, '_timestamp': 1762780579.4659686}).
Epoch: 8, Steps: 132 | Train Loss: 0.3042120 Vali Loss: 0.1961392 Test Loss: 0.1553021
Validation loss decreased (0.196386 --> 0.196139).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.3036642 Vali Loss: 0.1964334 Test Loss: 0.1553132
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.3061764 Vali Loss: 0.1961868 Test Loss: 0.1553117
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.3092957 Vali Loss: 0.1963628 Test Loss: 0.1553167
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.3050376 Vali Loss: 0.1965211 Test Loss: 0.1553178
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.3079773 Vali Loss: 0.1961895 Test Loss: 0.1553176
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.3041015 Vali Loss: 0.1967241 Test Loss: 0.1553182
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.3052218 Vali Loss: 0.1964926 Test Loss: 0.1553183
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.3084260 Vali Loss: 0.1964285 Test Loss: 0.1553184
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.3034082 Vali Loss: 0.1963159 Test Loss: 0.1553184
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.3038773 Vali Loss: 0.1965261 Test Loss: 0.1553184
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_DRD_GOLD_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.0009362294804304838, mae:0.021652288734912872, rmse:0.0305978674441576, r2:-0.005419611930847168, dtw:Not calculated


VAL - MSE: 0.0009, MAE: 0.0217, RMSE: 0.0306, RÂ²: -0.0054, MAPE: 1142138.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.707 MB of 0.709 MB uploadedwandb: \ 0.707 MB of 0.709 MB uploadedwandb: | 0.709 MB of 0.709 MB uploadedwandb: / 0.709 MB of 0.709 MB uploadedwandb: - 0.709 MB of 0.781 MB uploadedwandb: \ 0.709 MB of 0.781 MB uploadedwandb: | 0.781 MB of 0.781 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–…â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–‡â–„â–‚â–‚â–ƒâ–‚â–â–„â–ˆâ–ƒâ–†â–‚â–ƒâ–‡â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–„â–‚â–‚â–â–‚â–â–‚â–‚â–â–ƒâ–‚â–‚â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 17
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 53548
wandb:     model/trainable_params 53548
wandb:                  test/loss 0.15532
wandb:                 train/loss 0.30388
wandb:   val/directional_accuracy 50.25779
wandb:                   val/loss 0.19653
wandb:                    val/mae 0.02165
wandb:                   val/mape 114213850.0
wandb:                    val/mse 0.00094
wandb:                     val/r2 -0.00542
wandb:                   val/rmse 0.0306
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/ziza8yxk
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_151610-ziza8yxk/logs
Completed: DRD_GOLD H=50

Training: Mamba on DRD_GOLD for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_151715-drjc4cko
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/drjc4cko
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_DRD_GOLD_H100 Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/drjc4cko
>>>>>>>start training : long_term_forecast_Mamba_DRD_GOLD_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.4419043 Vali Loss: 0.2762365 Test Loss: 0.1630071
Validation loss decreased (inf --> 0.276236).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.4129031 Vali Loss: 0.2630877 Test Loss: 0.1591004
Validation loss decreased (0.276236 --> 0.263088).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.3885833 Vali Loss: 0.2585274 Test Loss: 0.1585442
Validation loss decreased (0.263088 --> 0.258527).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.3846529 Vali Loss: 0.2604236 Test Loss: 0.1584906
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.3840553 Vali Loss: 0.2587269 Test Loss: 0.1582412
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.3836849 Vali Loss: 0.2541564 Test Loss: 0.1582494
Validation loss decreased (0.258527 --> 0.254156).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.3828946 Vali Loss: 0.2579712 Test Loss: 0.1582575
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.44190433552631964, 'val/loss': 0.27623648643493653, 'test/loss': 0.16300712823867797, '_timestamp': 1762780642.4218793}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.4129031254695012, 'val/loss': 0.2630877286195755, 'test/loss': 0.1591004192829132, '_timestamp': 1762780644.3310628}).
Epoch: 8, Steps: 130 | Train Loss: 0.3832432 Vali Loss: 0.2580348 Test Loss: 0.1582743
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.3833798 Vali Loss: 0.2573487 Test Loss: 0.1582688
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.3826193 Vali Loss: 0.2549959 Test Loss: 0.1582733
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.3835268 Vali Loss: 0.2575981 Test Loss: 0.1582728
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.3835231 Vali Loss: 0.2579975 Test Loss: 0.1582717
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.3829964 Vali Loss: 0.2617892 Test Loss: 0.1582719
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.3835488 Vali Loss: 0.2583308 Test Loss: 0.1582719
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.3829983 Vali Loss: 0.2547398 Test Loss: 0.1582719
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 130 | Train Loss: 0.3834716 Vali Loss: 0.2542694 Test Loss: 0.1582719
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_DRD_GOLD_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0009108818485401571, mae:0.021200433373451233, rmse:0.03018081933259964, r2:-0.008118987083435059, dtw:Not calculated


VAL - MSE: 0.0009, MAE: 0.0212, RMSE: 0.0302, RÂ²: -0.0081, MAPE: 922282.69%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.786 MB of 0.791 MB uploadedwandb: \ 0.786 MB of 0.791 MB uploadedwandb: | 0.791 MB of 0.791 MB uploadedwandb: / 0.791 MB of 0.791 MB uploadedwandb: - 0.791 MB of 0.791 MB uploadedwandb: \ 0.791 MB of 0.863 MB uploadedwandb: | 0.863 MB of 0.863 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‡â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–ƒâ–ƒâ–‚â–â–‚â–‚â–â–‚â–‚â–â–‚â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–‡â–…â–â–„â–…â–„â–‚â–„â–…â–ˆâ–…â–‚â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 73048
wandb:     model/trainable_params 73048
wandb:                  test/loss 0.15827
wandb:                 train/loss 0.38347
wandb:   val/directional_accuracy 49.94228
wandb:                   val/loss 0.25427
wandb:                    val/mae 0.0212
wandb:                   val/mape 92228268.75
wandb:                    val/mse 0.00091
wandb:                     val/r2 -0.00812
wandb:                   val/rmse 0.03018
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/drjc4cko
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_151715-drjc4cko/logs
Completed: DRD_GOLD H=100

Training: Mamba on ANGLO_AMERICAN for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_151816-skrosm1w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/skrosm1w
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ANGLO_AMERICAN_H3Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/skrosm1w
>>>>>>>start training : long_term_forecast_Mamba_ANGLO_AMERICAN_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 3
============================================================
train 778
val 44
test 46
Epoch: 1, Steps: 25 | Train Loss: 0.3418667 Vali Loss: 0.6427545 Test Loss: 0.8930266
Validation loss decreased (inf --> 0.642754).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 25 | Train Loss: 0.3380760 Vali Loss: 0.6315382 Test Loss: 0.8820098
Validation loss decreased (0.642754 --> 0.631538).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 25 | Train Loss: 0.3277508 Vali Loss: 0.6492002 Test Loss: 0.8748530
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 25 | Train Loss: 0.3243149 Vali Loss: 0.6329926 Test Loss: 0.8708914
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 25 | Train Loss: 0.3238789 Vali Loss: 0.6525088 Test Loss: 0.8686915
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 25 | Train Loss: 0.3261858 Vali Loss: 0.6117403 Test Loss: 0.8675074
Validation loss decreased (0.631538 --> 0.611740).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 25 | Train Loss: 0.3251231 Vali Loss: 0.6318278 Test Loss: 0.8669384
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 25 | Train Loss: 0.3253989 Vali Loss: 0.6235069 Test Loss: 0.8666323
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 25 | Train Loss: 0.3260947 Vali Loss: 0.6164562 Test Loss: 0.8664897
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 25 | Train Loss: 0.3234638 Vali Loss: 0.5942003 Test Loss: 0.8664170
Validation loss decreased (0.611740 --> 0.594200).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 25 | Train Loss: 0.3264794 Vali Loss: 0.6538367 Test Loss: 0.8663824
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 25 | Train Loss: 0.3226090 Vali Loss: 0.6579851 Test Loss: 0.8663636
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 25 | Train Loss: 0.3199306 Vali Loss: 0.6512333 Test Loss: 0.8663546
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 25 | Train Loss: 0.3240689 Vali Loss: 0.6184392 Test Loss: 0.8663503
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 25 | Train Loss: 0.3230884 Vali Loss: 0.6148795 Test Loss: 0.8663483
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 25 | Train Loss: 0.3207465 Vali Loss: 0.6324060 Test Loss: 0.8663472
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 25 | Train Loss: 0.3250499 Vali Loss: 0.6203389 Test Loss: 0.8663470
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 25 | Train Loss: 0.3210947 Vali Loss: 0.6480060 Test Loss: 0.8663468
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3418667423725128, 'val/loss': 0.642754465341568, 'test/loss': 0.8930265754461288, '_timestamp': 1762780704.3143609}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.33807596743106844, 'val/loss': 0.6315381824970245, 'test/loss': 0.8820098489522934, '_timestamp': 1762780705.1466503}).
Epoch: 19, Steps: 25 | Train Loss: 0.3335534 Vali Loss: 0.6555866 Test Loss: 0.8663468
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 25 | Train Loss: 0.3233503 Vali Loss: 0.6242616 Test Loss: 0.8663468
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_ANGLO_AMERICAN_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 46
test shape: (46, 3, 1) (46, 3, 1)
test shape: (46, 3, 1) (46, 3, 1)


	mse:0.006846006494015455, mae:0.05740407481789589, rmse:0.08274059742689133, r2:-0.010497212409973145, dtw:Not calculated


VAL - MSE: 0.0068, MAE: 0.0574, RMSE: 0.0827, RÂ²: -0.0105, MAPE: 50433100.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.439 MB of 0.439 MB uploadedwandb: \ 0.439 MB of 0.439 MB uploadedwandb: | 0.439 MB of 0.439 MB uploadedwandb: / 0.439 MB of 0.439 MB uploadedwandb: - 0.439 MB of 0.439 MB uploadedwandb: \ 0.439 MB of 0.439 MB uploadedwandb: | 0.439 MB of 0.439 MB uploadedwandb: / 0.488 MB of 0.560 MB uploadedwandb: - 0.488 MB of 0.560 MB uploadedwandb: \ 0.560 MB of 0.560 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–…â–ƒâ–ƒâ–„â–„â–„â–„â–ƒâ–„â–‚â–â–ƒâ–ƒâ–â–„â–‚â–ˆâ–ƒ
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–…â–‡â–ƒâ–…â–„â–ƒâ–â–ˆâ–ˆâ–‡â–„â–ƒâ–…â–„â–‡â–ˆâ–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35218
wandb:     model/trainable_params 35218
wandb:                  test/loss 0.86635
wandb:                 train/loss 0.32335
wandb:   val/directional_accuracy 52.17391
wandb:                   val/loss 0.62426
wandb:                    val/mae 0.0574
wandb:                   val/mape 5043310000.0
wandb:                    val/mse 0.00685
wandb:                     val/r2 -0.0105
wandb:                   val/rmse 0.08274
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/skrosm1w
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_151816-skrosm1w/logs
Completed: ANGLO_AMERICAN H=3

Training: Mamba on ANGLO_AMERICAN for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_151905-6qfhginj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/6qfhginj
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ANGLO_AMERICAN_H5Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/6qfhginj
>>>>>>>start training : long_term_forecast_Mamba_ANGLO_AMERICAN_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 5
============================================================
train 776
val 42
test 44
Epoch: 1, Steps: 25 | Train Loss: 0.3470364 Vali Loss: 0.6772174 Test Loss: 0.9145996
Validation loss decreased (inf --> 0.677217).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 25 | Train Loss: 0.3410973 Vali Loss: 0.6680882 Test Loss: 0.9009152
Validation loss decreased (0.677217 --> 0.668088).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 25 | Train Loss: 0.3397651 Vali Loss: 0.6320401 Test Loss: 0.8924080
Validation loss decreased (0.668088 --> 0.632040).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 25 | Train Loss: 0.3374046 Vali Loss: 0.6212578 Test Loss: 0.8873093
Validation loss decreased (0.632040 --> 0.621258).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 25 | Train Loss: 0.3353868 Vali Loss: 0.6674888 Test Loss: 0.8844737
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 25 | Train Loss: 0.3358910 Vali Loss: 0.6499809 Test Loss: 0.8828902
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 25 | Train Loss: 0.3356908 Vali Loss: 0.6729205 Test Loss: 0.8822346
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 25 | Train Loss: 0.3312563 Vali Loss: 0.6497303 Test Loss: 0.8818546
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 25 | Train Loss: 0.3329845 Vali Loss: 0.6362870 Test Loss: 0.8816673
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 25 | Train Loss: 0.3336596 Vali Loss: 0.6770463 Test Loss: 0.8815809
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 25 | Train Loss: 0.3265271 Vali Loss: 0.6322293 Test Loss: 0.8815390
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 25 | Train Loss: 0.3333608 Vali Loss: 0.6111100 Test Loss: 0.8815176
Validation loss decreased (0.621258 --> 0.611110).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 25 | Train Loss: 0.3346055 Vali Loss: 0.6459394 Test Loss: 0.8815068
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 25 | Train Loss: 0.3310774 Vali Loss: 0.6511968 Test Loss: 0.8815015
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 25 | Train Loss: 0.3317035 Vali Loss: 0.6395231 Test Loss: 0.8814989
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 25 | Train Loss: 0.3256787 Vali Loss: 0.6260574 Test Loss: 0.8814975
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 25 | Train Loss: 0.3328943 Vali Loss: 0.6486502 Test Loss: 0.8814972
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 25 | Train Loss: 0.3325085 Vali Loss: 0.6255104 Test Loss: 0.8814971
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.34703644514083865, 'val/loss': 0.6772173643112183, 'test/loss': 0.9145996272563934, '_timestamp': 1762780750.457821}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.34109726071357727, 'val/loss': 0.6680882275104523, 'test/loss': 0.9009151607751846, '_timestamp': 1762780751.2286284}).
Epoch: 19, Steps: 25 | Train Loss: 0.3381504 Vali Loss: 0.6483335 Test Loss: 0.8814971
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 25 | Train Loss: 0.3308404 Vali Loss: 0.6211321 Test Loss: 0.8814971
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 25 | Train Loss: 0.3314503 Vali Loss: 0.6573738 Test Loss: 0.8814971
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 25 | Train Loss: 0.3304201 Vali Loss: 0.6049605 Test Loss: 0.8814971
Validation loss decreased (0.611110 --> 0.604961).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 25 | Train Loss: 0.3301780 Vali Loss: 0.6461183 Test Loss: 0.8814971
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 25 | Train Loss: 0.3285001 Vali Loss: 0.6680032 Test Loss: 0.8814971
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 25 | Train Loss: 0.3349931 Vali Loss: 0.6543008 Test Loss: 0.8814971
EarlyStopping counter: 3 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 25 | Train Loss: 0.3285001 Vali Loss: 0.6409099 Test Loss: 0.8814971
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 25 | Train Loss: 0.3269685 Vali Loss: 0.6092219 Test Loss: 0.8814971
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 25 | Train Loss: 0.3334533 Vali Loss: 0.6825451 Test Loss: 0.8814971
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 25 | Train Loss: 0.3321833 Vali Loss: 0.6568184 Test Loss: 0.8814971
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 25 | Train Loss: 0.3261965 Vali Loss: 0.6211299 Test Loss: 0.8814971
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 25 | Train Loss: 0.3343446 Vali Loss: 0.6646296 Test Loss: 0.8814971
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 25 | Train Loss: 0.3288094 Vali Loss: 0.5924687 Test Loss: 0.8814971
Validation loss decreased (0.604961 --> 0.592469).  Saving model ...
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 25 | Train Loss: 0.3321713 Vali Loss: 0.6575133 Test Loss: 0.8814971
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 25 | Train Loss: 0.3289093 Vali Loss: 0.6534909 Test Loss: 0.8814971
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.1641532182693482e-14
Epoch: 35, Steps: 25 | Train Loss: 0.3388217 Vali Loss: 0.6193121 Test Loss: 0.8814971
EarlyStopping counter: 3 out of 10
Updating learning rate to 5.820766091346741e-15
Epoch: 36, Steps: 25 | Train Loss: 0.3271443 Vali Loss: 0.6336232 Test Loss: 0.8814971
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.9103830456733705e-15
Epoch: 37, Steps: 25 | Train Loss: 0.3296974 Vali Loss: 0.6441936 Test Loss: 0.8814971
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.4551915228366853e-15
Epoch: 38, Steps: 25 | Train Loss: 0.3289209 Vali Loss: 0.6403539 Test Loss: 0.8814971
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.275957614183426e-16
Epoch: 39, Steps: 25 | Train Loss: 0.3292403 Vali Loss: 0.6299154 Test Loss: 0.8814971
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.637978807091713e-16
Epoch: 40, Steps: 25 | Train Loss: 0.3287715 Vali Loss: 0.6432241 Test Loss: 0.8814971
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.8189894035458566e-16
Epoch: 41, Steps: 25 | Train Loss: 0.3311975 Vali Loss: 0.6112430 Test Loss: 0.8814971
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.094947017729283e-17
Epoch: 42, Steps: 25 | Train Loss: 0.3310907 Vali Loss: 0.6306750 Test Loss: 0.8814971
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_ANGLO_AMERICAN_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 44
test shape: (44, 5, 1) (44, 5, 1)
test shape: (44, 5, 1) (44, 5, 1)


	mse:0.007153111044317484, mae:0.058994486927986145, rmse:0.0845760703086853, r2:-0.014349818229675293, dtw:Not calculated


VAL - MSE: 0.0072, MAE: 0.0590, RMSE: 0.0846, RÂ²: -0.0143, MAPE: 44340128.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.482 MB of 0.482 MB uploadedwandb: \ 0.482 MB of 0.482 MB uploadedwandb: | 0.482 MB of 0.482 MB uploadedwandb: / 0.482 MB of 0.482 MB uploadedwandb: - 0.482 MB of 0.558 MB uploadedwandb: \ 0.482 MB of 0.558 MB uploadedwandb: | 0.558 MB of 0.558 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–‡â–†â–†â–†â–„â–…â–…â–â–…â–…â–„â–„â–â–…â–„â–‡â–„â–„â–ƒâ–ƒâ–‚â–†â–‚â–‚â–…â–„â–â–…â–ƒâ–„â–ƒâ–ˆâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–ƒâ–‡â–…â–‡â–…â–„â–ˆâ–„â–‚â–…â–†â–…â–„â–…â–„â–…â–ƒâ–†â–‚â–…â–‡â–†â–…â–‚â–ˆâ–†â–ƒâ–‡â–â–†â–†â–ƒâ–„â–…â–…â–„â–…â–‚â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 41
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 35998
wandb:     model/trainable_params 35998
wandb:                  test/loss 0.8815
wandb:                 train/loss 0.33109
wandb:   val/directional_accuracy 48.86364
wandb:                   val/loss 0.63067
wandb:                    val/mae 0.05899
wandb:                   val/mape 4434012800.0
wandb:                    val/mse 0.00715
wandb:                     val/r2 -0.01435
wandb:                   val/rmse 0.08458
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/6qfhginj
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_151905-6qfhginj/logs
Completed: ANGLO_AMERICAN H=5

Training: Mamba on ANGLO_AMERICAN for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_152009-o4648orw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/o4648orw
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ANGLO_AMERICAN_H10Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/o4648orw
>>>>>>>start training : long_term_forecast_Mamba_ANGLO_AMERICAN_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 10
============================================================
train 771
val 37
test 39
Epoch: 1, Steps: 25 | Train Loss: 0.3623398 Vali Loss: 0.7709913 Test Loss: 0.9514444
Validation loss decreased (inf --> 0.770991).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 25 | Train Loss: 0.3542058 Vali Loss: 0.7179777 Test Loss: 0.9432954
Validation loss decreased (0.770991 --> 0.717978).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 25 | Train Loss: 0.3635122 Vali Loss: 0.7004871 Test Loss: 0.9384782
Validation loss decreased (0.717978 --> 0.700487).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 25 | Train Loss: 0.3537262 Vali Loss: 0.7302069 Test Loss: 0.9354312
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 25 | Train Loss: 0.3541937 Vali Loss: 0.8158480 Test Loss: 0.9337507
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 25 | Train Loss: 0.3579315 Vali Loss: 0.7277872 Test Loss: 0.9328288
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 25 | Train Loss: 0.3478992 Vali Loss: 0.6866156 Test Loss: 0.9323743
Validation loss decreased (0.700487 --> 0.686616).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 25 | Train Loss: 0.3501638 Vali Loss: 0.7321821 Test Loss: 0.9321561
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 25 | Train Loss: 0.3461496 Vali Loss: 0.6935820 Test Loss: 0.9320416
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 25 | Train Loss: 0.3714235 Vali Loss: 0.6861365 Test Loss: 0.9319851
Validation loss decreased (0.686616 --> 0.686136).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 25 | Train Loss: 0.3473574 Vali Loss: 0.7154317 Test Loss: 0.9319567
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 25 | Train Loss: 0.3498952 Vali Loss: 0.7022824 Test Loss: 0.9319438
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 25 | Train Loss: 0.3560919 Vali Loss: 0.7620425 Test Loss: 0.9319368
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 25 | Train Loss: 0.3492503 Vali Loss: 0.7458663 Test Loss: 0.9319335
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 25 | Train Loss: 0.3458240 Vali Loss: 0.7208205 Test Loss: 0.9319319
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 25 | Train Loss: 0.3693433 Vali Loss: 0.6711384 Test Loss: 0.9319311
Validation loss decreased (0.686136 --> 0.671138).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 25 | Train Loss: 0.3499284 Vali Loss: 0.6883377 Test Loss: 0.9319309
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.36233983278274534, 'val/loss': 0.7709912955760956, 'test/loss': 0.9514443725347519, '_timestamp': 1762780815.385112}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3542058080434799, 'val/loss': 0.7179776728153229, 'test/loss': 0.9432953894138336, '_timestamp': 1762780816.280824}).
Epoch: 18, Steps: 25 | Train Loss: 0.3507151 Vali Loss: 0.6928626 Test Loss: 0.9319308
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 25 | Train Loss: 0.3576474 Vali Loss: 0.7052113 Test Loss: 0.9319308
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 25 | Train Loss: 0.3517527 Vali Loss: 0.7220249 Test Loss: 0.9319308
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 25 | Train Loss: 0.3594035 Vali Loss: 0.7356721 Test Loss: 0.9319308
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 25 | Train Loss: 0.3451137 Vali Loss: 0.7079472 Test Loss: 0.9319308
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 25 | Train Loss: 0.3647181 Vali Loss: 0.7489456 Test Loss: 0.9319308
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 25 | Train Loss: 0.3499708 Vali Loss: 0.7097922 Test Loss: 0.9319308
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 25 | Train Loss: 0.3470823 Vali Loss: 0.7517158 Test Loss: 0.9319308
EarlyStopping counter: 9 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 25 | Train Loss: 0.3498059 Vali Loss: 0.7246813 Test Loss: 0.9319308
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Mamba_ANGLO_AMERICAN_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 39
test shape: (39, 10, 1) (39, 10, 1)
test shape: (39, 10, 1) (39, 10, 1)


	mse:0.007859921082854271, mae:0.062283359467983246, rmse:0.08865619450807571, r2:-0.014379024505615234, dtw:Not calculated


VAL - MSE: 0.0079, MAE: 0.0623, RMSE: 0.0887, RÂ²: -0.0144, MAPE: 21995148.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.493 MB of 0.494 MB uploadedwandb: \ 0.493 MB of 0.494 MB uploadedwandb: | 0.493 MB of 0.494 MB uploadedwandb: / 0.494 MB of 0.494 MB uploadedwandb: - 0.494 MB of 0.567 MB uploadedwandb: \ 0.567 MB of 0.567 MB uploadedwandb: | 0.567 MB of 0.567 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–†â–ƒâ–ƒâ–„â–‚â–‚â–â–ˆâ–‚â–‚â–„â–‚â–â–‡â–‚â–‚â–„â–ƒâ–…â–â–†â–‚â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–„â–ˆâ–„â–‚â–„â–‚â–‚â–ƒâ–ƒâ–…â–…â–ƒâ–â–‚â–‚â–ƒâ–ƒâ–„â–ƒâ–…â–ƒâ–…â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 25
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 37948
wandb:     model/trainable_params 37948
wandb:                  test/loss 0.93193
wandb:                 train/loss 0.34981
wandb:   val/directional_accuracy 45.29915
wandb:                   val/loss 0.72468
wandb:                    val/mae 0.06228
wandb:                   val/mape 2199514800.0
wandb:                    val/mse 0.00786
wandb:                     val/r2 -0.01438
wandb:                   val/rmse 0.08866
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/o4648orw
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_152009-o4648orw/logs
Completed: ANGLO_AMERICAN H=10

Training: Mamba on ANGLO_AMERICAN for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_152100-yb5j9dy7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/yb5j9dy7
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ANGLO_AMERICAN_H22Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/yb5j9dy7
>>>>>>>start training : long_term_forecast_Mamba_ANGLO_AMERICAN_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 22
============================================================
train 759
val 25
test 27
Epoch: 1, Steps: 24 | Train Loss: 0.3946683 Vali Loss: 0.8803831 Test Loss: 1.4487017
Validation loss decreased (inf --> 0.880383).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 24 | Train Loss: 0.3917009 Vali Loss: 0.8695480 Test Loss: 1.4370993
Validation loss decreased (0.880383 --> 0.869548).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 24 | Train Loss: 0.3880674 Vali Loss: 0.8633906 Test Loss: 1.4303827
Validation loss decreased (0.869548 --> 0.863391).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 24 | Train Loss: 0.3878287 Vali Loss: 0.8595348 Test Loss: 1.4262569
Validation loss decreased (0.863391 --> 0.859535).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 24 | Train Loss: 0.3864279 Vali Loss: 0.8572880 Test Loss: 1.4238133
Validation loss decreased (0.859535 --> 0.857288).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 24 | Train Loss: 0.3867506 Vali Loss: 0.8560809 Test Loss: 1.4225072
Validation loss decreased (0.857288 --> 0.856081).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 24 | Train Loss: 0.3870493 Vali Loss: 0.8554739 Test Loss: 1.4218413
Validation loss decreased (0.856081 --> 0.855474).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 24 | Train Loss: 0.3862379 Vali Loss: 0.8551796 Test Loss: 1.4215194
Validation loss decreased (0.855474 --> 0.855180).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 24 | Train Loss: 0.3852523 Vali Loss: 0.8550304 Test Loss: 1.4213581
Validation loss decreased (0.855180 --> 0.855030).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 24 | Train Loss: 0.3853618 Vali Loss: 0.8549562 Test Loss: 1.4212756
Validation loss decreased (0.855030 --> 0.854956).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 24 | Train Loss: 0.3849136 Vali Loss: 0.8549182 Test Loss: 1.4212351
Validation loss decreased (0.854956 --> 0.854918).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 24 | Train Loss: 0.3852470 Vali Loss: 0.8549000 Test Loss: 1.4212146
Validation loss decreased (0.854918 --> 0.854900).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 24 | Train Loss: 0.3859704 Vali Loss: 0.8548909 Test Loss: 1.4212048
Validation loss decreased (0.854900 --> 0.854891).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 24 | Train Loss: 0.3864583 Vali Loss: 0.8548864 Test Loss: 1.4211999
Validation loss decreased (0.854891 --> 0.854886).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 24 | Train Loss: 0.3855638 Vali Loss: 0.8548843 Test Loss: 1.4211974
Validation loss decreased (0.854886 --> 0.854884).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 24 | Train Loss: 0.3856857 Vali Loss: 0.8548831 Test Loss: 1.4211961
Validation loss decreased (0.854884 --> 0.854883).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 24 | Train Loss: 0.3848552 Vali Loss: 0.8548827 Test Loss: 1.4211957
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 24 | Train Loss: 0.3855451 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 7.62939453125e-10
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3946682699024677, 'val/loss': 0.8803830742835999, 'test/loss': 1.4487017393112183, '_timestamp': 1762780867.1858933}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.39170091847578686, 'val/loss': 0.8695480227470398, 'test/loss': 1.4370993375778198, '_timestamp': 1762780867.9509466}).
Epoch: 19, Steps: 24 | Train Loss: 0.3843532 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 24 | Train Loss: 0.3842716 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 24 | Train Loss: 0.3844478 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 24 | Train Loss: 0.3849718 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 24 | Train Loss: 0.3860480 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 24 | Train Loss: 0.3852194 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 24 | Train Loss: 0.3848231 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 24 | Train Loss: 0.3854107 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 24 | Train Loss: 0.3851923 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 24 | Train Loss: 0.3848370 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 24 | Train Loss: 0.3844021 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 24 | Train Loss: 0.3861903 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 24 | Train Loss: 0.3850840 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 24 | Train Loss: 0.3851000 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 24 | Train Loss: 0.3854811 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 24 | Train Loss: 0.3854543 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 1.1641532182693482e-14
Epoch: 35, Steps: 24 | Train Loss: 0.3854541 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 5.820766091346741e-15
Epoch: 36, Steps: 24 | Train Loss: 0.3848865 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 2.9103830456733705e-15
Epoch: 37, Steps: 24 | Train Loss: 0.3846838 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 1.4551915228366853e-15
Epoch: 38, Steps: 24 | Train Loss: 0.3850685 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 7.275957614183426e-16
Epoch: 39, Steps: 24 | Train Loss: 0.3851604 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 3.637978807091713e-16
Epoch: 40, Steps: 24 | Train Loss: 0.3862187 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 1.8189894035458566e-16
Epoch: 41, Steps: 24 | Train Loss: 0.3864279 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 9.094947017729283e-17
Epoch: 42, Steps: 24 | Train Loss: 0.3861043 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 4.5474735088646414e-17
Epoch: 43, Steps: 24 | Train Loss: 0.3861855 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 2.2737367544323207e-17
Epoch: 44, Steps: 24 | Train Loss: 0.3850753 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 1.1368683772161604e-17
Epoch: 45, Steps: 24 | Train Loss: 0.3849880 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 5.684341886080802e-18
Epoch: 46, Steps: 24 | Train Loss: 0.3858700 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 2.842170943040401e-18
Epoch: 47, Steps: 24 | Train Loss: 0.3859735 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 1.4210854715202004e-18
Epoch: 48, Steps: 24 | Train Loss: 0.3852037 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 7.105427357601002e-19
Epoch: 49, Steps: 24 | Train Loss: 0.3852662 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 3.552713678800501e-19
Epoch: 50, Steps: 24 | Train Loss: 0.3857206 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 1.7763568394002505e-19
Epoch: 51, Steps: 24 | Train Loss: 0.3848050 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 8.881784197001253e-20
Epoch: 52, Steps: 24 | Train Loss: 0.3855232 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 4.4408920985006264e-20
Epoch: 53, Steps: 24 | Train Loss: 0.3850163 Vali Loss: 0.8548827 Test Loss: 1.4211956
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.2204460492503132e-20
Epoch: 54, Steps: 24 | Train Loss: 0.3856659 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 1.1102230246251566e-20
Epoch: 55, Steps: 24 | Train Loss: 0.3851096 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 5.551115123125783e-21
Epoch: 56, Steps: 24 | Train Loss: 0.3852552 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 2.7755575615628915e-21
Epoch: 57, Steps: 24 | Train Loss: 0.3864480 Vali Loss: 0.8548827 Test Loss: 1.4211956
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.3877787807814457e-21
Epoch: 58, Steps: 24 | Train Loss: 0.3846134 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 6.938893903907229e-22
Epoch: 59, Steps: 24 | Train Loss: 0.3861842 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 3.4694469519536144e-22
Epoch: 60, Steps: 24 | Train Loss: 0.3843241 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 1.7347234759768072e-22
Epoch: 61, Steps: 24 | Train Loss: 0.3860074 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 8.673617379884036e-23
Epoch: 62, Steps: 24 | Train Loss: 0.3849510 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 4.336808689942018e-23
Epoch: 63, Steps: 24 | Train Loss: 0.3857600 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 2.168404344971009e-23
Epoch: 64, Steps: 24 | Train Loss: 0.3857405 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 1.0842021724855045e-23
Epoch: 65, Steps: 24 | Train Loss: 0.3853594 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 5.4210108624275224e-24
Epoch: 66, Steps: 24 | Train Loss: 0.3855046 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 2.7105054312137612e-24
Epoch: 67, Steps: 24 | Train Loss: 0.3851221 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 1.3552527156068806e-24
Epoch: 68, Steps: 24 | Train Loss: 0.3847227 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 6.776263578034403e-25
Epoch: 69, Steps: 24 | Train Loss: 0.3847608 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 3.3881317890172015e-25
Epoch: 70, Steps: 24 | Train Loss: 0.3862508 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 1.6940658945086008e-25
Epoch: 71, Steps: 24 | Train Loss: 0.3858454 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 8.470329472543004e-26
Epoch: 72, Steps: 24 | Train Loss: 0.3870549 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 4.235164736271502e-26
Epoch: 73, Steps: 24 | Train Loss: 0.3850789 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 2.117582368135751e-26
Epoch: 74, Steps: 24 | Train Loss: 0.3851878 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 1.0587911840678755e-26
Epoch: 75, Steps: 24 | Train Loss: 0.3849631 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 5.2939559203393774e-27
Epoch: 76, Steps: 24 | Train Loss: 0.3853886 Vali Loss: 0.8548827 Test Loss: 1.4211956
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.6469779601696887e-27
Epoch: 77, Steps: 24 | Train Loss: 0.3847134 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 1.3234889800848443e-27
Epoch: 78, Steps: 24 | Train Loss: 0.3850251 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 6.617444900424222e-28
Epoch: 79, Steps: 24 | Train Loss: 0.3868109 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 3.308722450212111e-28
Epoch: 80, Steps: 24 | Train Loss: 0.3853466 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 1.6543612251060554e-28
Epoch: 81, Steps: 24 | Train Loss: 0.3863534 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 8.271806125530277e-29
Epoch: 82, Steps: 24 | Train Loss: 0.3855450 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 4.1359030627651386e-29
Epoch: 83, Steps: 24 | Train Loss: 0.3850023 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 2.0679515313825693e-29
Epoch: 84, Steps: 24 | Train Loss: 0.3851913 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 1.0339757656912846e-29
Epoch: 85, Steps: 24 | Train Loss: 0.3852521 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 5.169878828456423e-30
Epoch: 86, Steps: 24 | Train Loss: 0.3855957 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 2.5849394142282116e-30
Epoch: 87, Steps: 24 | Train Loss: 0.3853761 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 1.2924697071141058e-30
Epoch: 88, Steps: 24 | Train Loss: 0.3863713 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 6.462348535570529e-31
Epoch: 89, Steps: 24 | Train Loss: 0.3849940 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 3.2311742677852645e-31
Epoch: 90, Steps: 24 | Train Loss: 0.3859867 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 1.6155871338926323e-31
Epoch: 91, Steps: 24 | Train Loss: 0.3843354 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 8.077935669463161e-32
Epoch: 92, Steps: 24 | Train Loss: 0.3855945 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 4.0389678347315806e-32
Epoch: 93, Steps: 24 | Train Loss: 0.3856092 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 2.0194839173657903e-32
Epoch: 94, Steps: 24 | Train Loss: 0.3866962 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 1.0097419586828952e-32
Epoch: 95, Steps: 24 | Train Loss: 0.3849357 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 5.048709793414476e-33
Epoch: 96, Steps: 24 | Train Loss: 0.3854584 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 2.524354896707238e-33
Epoch: 97, Steps: 24 | Train Loss: 0.3851042 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 1.262177448353619e-33
Epoch: 98, Steps: 24 | Train Loss: 0.3851346 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 6.310887241768095e-34
Epoch: 99, Steps: 24 | Train Loss: 0.3854876 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 3.1554436208840474e-34
Epoch: 100, Steps: 24 | Train Loss: 0.3848683 Vali Loss: 0.8548827 Test Loss: 1.4211956
Validation loss decreased (0.854883 --> 0.854883).  Saving model ...
Updating learning rate to 1.5777218104420237e-34
>>>>>>>testing : long_term_forecast_Mamba_ANGLO_AMERICAN_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 27
test shape: (27, 22, 1) (27, 22, 1)
test shape: (27, 22, 1) (27, 22, 1)


	mse:0.008060870692133904, mae:0.06436202675104141, rmse:0.08978234976530075, r2:-0.026712656021118164, dtw:Not calculated


VAL - MSE: 0.0081, MAE: 0.0644, RMSE: 0.0898, RÂ²: -0.0267, MAPE: 14427948.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.481 MB of 0.482 MB uploadedwandb: \ 0.481 MB of 0.482 MB uploadedwandb: | 0.482 MB of 0.482 MB uploadedwandb: / 0.482 MB of 0.571 MB uploadedwandb: - 0.482 MB of 0.571 MB uploadedwandb: \ 0.571 MB of 0.571 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–†â–ƒâ–ƒâ–ƒâ–‚â–â–‚â–‚â–ƒâ–…â–ƒâ–ƒâ–‚â–…â–„â–‚â–„â–„â–ƒâ–ƒâ–…â–â–‚â–ƒâ–ƒâ–…â–†â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 99
wandb:         model/architecture Model(
wandb:   (embedding)...
wandb: model/non_trainable_params 0
wandb:         model/total_params 42628
wandb:     model/trainable_params 42628
wandb:                  test/loss 1.4212
wandb:                 train/loss 0.38487
wandb:   val/directional_accuracy 52.02822
wandb:                   val/loss 0.85488
wandb:                    val/mae 0.06436
wandb:                   val/mape 1442794800.0
wandb:                    val/mse 0.00806
wandb:                     val/r2 -0.02671
wandb:                   val/rmse 0.08978
wandb: 
wandb: ðŸš€ View run Mamba_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/yb5j9dy7
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_152100-yb5j9dy7/logs
Completed: ANGLO_AMERICAN H=22

Training: Mamba on ANGLO_AMERICAN for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_152251-1skip38i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/1skip38i
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ANGLO_AMERICAN_H50Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/1skip38i
>>>>>>>start training : long_term_forecast_Mamba_ANGLO_AMERICAN_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 50
============================================================
train 731
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.005 MB of 0.005 MB uploadedwandb: / 0.005 MB of 0.005 MB uploadedwandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.015 MB of 0.034 MB uploaded (0.002 MB deduped)wandb: | 0.015 MB of 0.034 MB uploaded (0.002 MB deduped)wandb: / 0.034 MB of 0.034 MB uploaded (0.002 MB deduped)wandb: ðŸš€ View run Mamba_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/1skip38i
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_152251-1skip38i/logs
Traceback (most recent call last):
  File "run.py", line 207, in <module>
    exp.train(setting)
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 93, in train
    vali_data, vali_loader = self._get_data(flag='val')
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 43, in _get_data
    data_set, data_loader = data_provider(self.args, flag)
  File "/home-mscluster/bkodze/research_project/forecast-research/data_provider/data_factory.py", line 79, in data_provider
    print(flag, len(data_set))
ValueError: __len__() should return >= 0
Completed: ANGLO_AMERICAN H=50

Training: Mamba on ANGLO_AMERICAN for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_152318-y6z69tuk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Mamba_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/y6z69tuk
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ANGLO_AMERICAN_H100Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
âœ… W&B initialized: Mamba_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/y6z69tuk
>>>>>>>start training : long_term_forecast_Mamba_ANGLO_AMERICAN_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 100
============================================================
train 681
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.005 MB of 0.020 MB uploadedwandb: / 0.024 MB of 0.024 MB uploadedwandb: ðŸš€ View run Mamba_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/y6z69tuk
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_152318-y6z69tuk/logs
Traceback (most recent call last):
  File "run.py", line 207, in <module>
    exp.train(setting)
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 93, in train
    vali_data, vali_loader = self._get_data(flag='val')
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 43, in _get_data
    data_set, data_loader = data_provider(self.args, flag)
  File "/home-mscluster/bkodze/research_project/forecast-research/data_provider/data_factory.py", line 79, in data_provider
    print(flag, len(data_set))
ValueError: __len__() should return >= 0
Completed: ANGLO_AMERICAN H=100

Mamba training completed for all datasets!
