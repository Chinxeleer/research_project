##############################################################################
# Training Mamba Model on All Datasets
##############################################################################
Training: Mamba on NVIDIA for H=3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H3     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 3
============================================================
train 2201
val 123
test 125
Validation loss decreased (inf --> 0.047165).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.047165 --> 0.046995).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.046995 --> 0.046034).  Saving model ...
Updating learning rate to 2.5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 125
test shape: (125, 3, 1) (125, 3, 1)
test shape: (125, 3, 1) (125, 3, 1)


	mse:0.003835655516013503, mae:0.039634186774492264, rmse:0.06193266808986664, r2:-0.020053863525390625, dtw:Not calculated


VAL - MSE: 0.0038, MAE: 0.0396, RMSE: 0.0619, RÂ²: -0.0201, MAPE: 3830192.25%
Completed: NVIDIA H=3

Training: Mamba on NVIDIA for H=5
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H5     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 5
============================================================
train 2199
val 121
test 123
Validation loss decreased (inf --> 0.047706).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.047706 --> 0.047366).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.047366 --> 0.046836).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.046836 --> 0.046462).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (0.046462 --> 0.045903).  Saving model ...
Updating learning rate to 6.25e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 3 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.90625e-07
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 123
test shape: (123, 5, 1) (123, 5, 1)
test shape: (123, 5, 1) (123, 5, 1)


	mse:0.0038419864140450954, mae:0.039877790957689285, rmse:0.06198376044631004, r2:-0.010977864265441895, dtw:Not calculated


VAL - MSE: 0.0038, MAE: 0.0399, RMSE: 0.0620, RÂ²: -0.0110, MAPE: 4164984.25%
Completed: NVIDIA H=5

Training: Mamba on NVIDIA for H=10
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H10    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 10
============================================================
train 2194
val 116
test 118
Validation loss decreased (inf --> 0.048391).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.048391 --> 0.047688).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.047688 --> 0.047409).  Saving model ...
Updating learning rate to 2.5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
Validation loss decreased (0.047409 --> 0.047112).  Saving model ...
Updating learning rate to 7.8125e-07
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.90625e-07
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.953125e-07
Validation loss decreased (0.047112 --> 0.046399).  Saving model ...
Updating learning rate to 9.765625e-08
EarlyStopping counter: 1 out of 5
Updating learning rate to 4.8828125e-08
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.44140625e-08
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.220703125e-08
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.103515625e-09
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 118
test shape: (118, 10, 1) (118, 10, 1)
test shape: (118, 10, 1) (118, 10, 1)


	mse:0.0039306264370679855, mae:0.04006973281502724, rmse:0.06269470602273941, r2:-0.016428828239440918, dtw:Not calculated


VAL - MSE: 0.0039, MAE: 0.0401, RMSE: 0.0627, RÂ²: -0.0164, MAPE: 3562456.50%
Completed: NVIDIA H=10

Training: Mamba on NVIDIA for H=22
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H22    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 22
============================================================
train 2182
val 104
test 106
Validation loss decreased (inf --> 0.048980).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
Validation loss decreased (0.048980 --> 0.048877).  Saving model ...
Updating learning rate to 3.125e-06
Validation loss decreased (0.048877 --> 0.048812).  Saving model ...
Updating learning rate to 1.5625e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.90625e-07
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.953125e-07
EarlyStopping counter: 4 out of 5
Updating learning rate to 9.765625e-08
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 106
test shape: (106, 22, 1) (106, 22, 1)
test shape: (106, 22, 1) (106, 22, 1)


	mse:0.004017998464405537, mae:0.040283288806676865, rmse:0.0633876845240593, r2:-0.020828962326049805, dtw:Not calculated


VAL - MSE: 0.0040, MAE: 0.0403, RMSE: 0.0634, RÂ²: -0.0208, MAPE: 3470717.00%
Completed: NVIDIA H=22

Training: Mamba on NVIDIA for H=50
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H50    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 50
============================================================
train 2154
val 76
test 78
Validation loss decreased (inf --> 0.049739).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.049739 --> 0.049646).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.049646 --> 0.049462).  Saving model ...
Updating learning rate to 2.5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
Validation loss decreased (0.049462 --> 0.049133).  Saving model ...
Updating learning rate to 3.125e-06
Validation loss decreased (0.049133 --> 0.048767).  Saving model ...
Updating learning rate to 1.5625e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.90625e-07
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.953125e-07
EarlyStopping counter: 4 out of 5
Updating learning rate to 9.765625e-08
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 78
test shape: (78, 50, 1) (78, 50, 1)
test shape: (78, 50, 1) (78, 50, 1)


	mse:0.0038321621250361204, mae:0.03956127166748047, rmse:0.061904460191726685, r2:-0.02676224708557129, dtw:Not calculated


VAL - MSE: 0.0038, MAE: 0.0396, RMSE: 0.0619, RÂ²: -0.0268, MAPE: 2879532.00%
Completed: NVIDIA H=50

Training: Mamba on NVIDIA for H=100
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H100   Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 100
============================================================
train 2104
val 26
test 28
Validation loss decreased (inf --> 0.049304).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.049304 --> 0.049304).  Saving model ...
Updating learning rate to 5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 28
test shape: (28, 100, 1) (28, 100, 1)
test shape: (28, 100, 1) (28, 100, 1)


	mse:0.004102472681552172, mae:0.04012592136859894, rmse:0.06405054777860641, r2:-0.037271976470947266, dtw:Not calculated


VAL - MSE: 0.0041, MAE: 0.0401, RMSE: 0.0641, RÂ²: -0.0373, MAPE: 2534242.50%
Completed: NVIDIA H=100

Training: Mamba on APPLE for H=3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H3      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 3
============================================================
train 2201
val 123
test 125
Validation loss decreased (inf --> 0.075342).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.075342 --> 0.075154).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.075154 --> 0.075069).  Saving model ...
Updating learning rate to 2.5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
Validation loss decreased (0.075069 --> 0.074692).  Saving model ...
Updating learning rate to 1.5625e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.90625e-07
Validation loss decreased (0.074692 --> 0.074650).  Saving model ...
Updating learning rate to 1.953125e-07
EarlyStopping counter: 1 out of 5
Updating learning rate to 9.765625e-08
Validation loss decreased (0.074650 --> 0.074193).  Saving model ...
Updating learning rate to 4.8828125e-08
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.44140625e-08
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.220703125e-08
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.103515625e-09
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.0517578125e-09
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 125
test shape: (125, 3, 1) (125, 3, 1)
test shape: (125, 3, 1) (125, 3, 1)


	mse:3.2016614568419755e-05, mae:0.004252259619534016, rmse:0.005658322479575872, r2:-0.021785616874694824, dtw:Not calculated


VAL - MSE: 0.0000, MAE: 0.0043, RMSE: 0.0057, RÂ²: -0.0218, MAPE: 66382.17%
Completed: APPLE H=3

Training: Mamba on APPLE for H=5
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H5      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 5
============================================================
train 2199
val 121
test 123
Validation loss decreased (inf --> 0.074067).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
Validation loss decreased (0.074067 --> 0.073041).  Saving model ...
Updating learning rate to 2.5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 123
test shape: (123, 5, 1) (123, 5, 1)
test shape: (123, 5, 1) (123, 5, 1)


	mse:3.222898158128373e-05, mae:0.004276340827345848, rmse:0.005677057430148125, r2:-0.016283154487609863, dtw:Not calculated


VAL - MSE: 0.0000, MAE: 0.0043, RMSE: 0.0057, RÂ²: -0.0163, MAPE: 89528.02%
Completed: APPLE H=5

Training: Mamba on APPLE for H=10
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H10     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 10
============================================================
train 2194
val 116
test 118
Validation loss decreased (inf --> 0.074173).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
Validation loss decreased (0.074173 --> 0.073596).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.073596 --> 0.073458).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (0.073458 --> 0.073316).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (0.073316 --> 0.072454).  Saving model ...
Updating learning rate to 3.125e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.90625e-07
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.953125e-07
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 118
test shape: (118, 10, 1) (118, 10, 1)
test shape: (118, 10, 1) (118, 10, 1)


	mse:3.277361247455701e-05, mae:0.004305967129766941, rmse:0.005724824033677578, r2:-0.019135475158691406, dtw:Not calculated


VAL - MSE: 0.0000, MAE: 0.0043, RMSE: 0.0057, RÂ²: -0.0191, MAPE: 107574.29%
Completed: APPLE H=10

Training: Mamba on APPLE for H=22
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H22     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 22
============================================================
train 2182
val 104
test 106
Validation loss decreased (inf --> 0.071634).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.071634 --> 0.069289).  Saving model ...
Updating learning rate to 5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
Validation loss decreased (0.069289 --> 0.068634).  Saving model ...
Updating learning rate to 3.125e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.90625e-07
Validation loss decreased (0.068634 --> 0.066721).  Saving model ...
Updating learning rate to 1.953125e-07
EarlyStopping counter: 1 out of 5
Updating learning rate to 9.765625e-08
EarlyStopping counter: 2 out of 5
Updating learning rate to 4.8828125e-08
EarlyStopping counter: 3 out of 5
Updating learning rate to 2.44140625e-08
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.220703125e-08
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 106
test shape: (106, 22, 1) (106, 22, 1)
test shape: (106, 22, 1) (106, 22, 1)


	mse:3.371772982063703e-05, mae:0.0043327054008841515, rmse:0.005806697066873312, r2:-0.018278956413269043, dtw:Not calculated


VAL - MSE: 0.0000, MAE: 0.0043, RMSE: 0.0058, RÂ²: -0.0183, MAPE: 122023.74%
Completed: APPLE H=22

Training: Mamba on APPLE for H=50
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H50     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 50
============================================================
train 2154
val 76
test 78
Validation loss decreased (inf --> 0.067372).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.067372 --> 0.065227).  Saving model ...
Updating learning rate to 5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 78
test shape: (78, 50, 1) (78, 50, 1)
test shape: (78, 50, 1) (78, 50, 1)


	mse:3.402173751965165e-05, mae:0.0043611605651676655, rmse:0.005832815542817116, r2:-0.016137003898620605, dtw:Not calculated


VAL - MSE: 0.0000, MAE: 0.0044, RMSE: 0.0058, RÂ²: -0.0161, MAPE: 185547.47%
Completed: APPLE H=50

Training: Mamba on APPLE for H=100
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H100    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 100
============================================================
train 2104
val 26
test 28
Validation loss decreased (inf --> 0.069858).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.069858 --> 0.069682).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.069682 --> 0.069629).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.069629 --> 0.069603).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (0.069603 --> 0.069593).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (0.069593 --> 0.069588).  Saving model ...
Updating learning rate to 3.125e-06
Validation loss decreased (0.069588 --> 0.069585).  Saving model ...
Updating learning rate to 1.5625e-06
Validation loss decreased (0.069585 --> 0.069584).  Saving model ...
Updating learning rate to 7.8125e-07
Validation loss decreased (0.069584 --> 0.069583).  Saving model ...
Updating learning rate to 3.90625e-07
Validation loss decreased (0.069583 --> 0.069583).  Saving model ...
Updating learning rate to 1.953125e-07
Validation loss decreased (0.069583 --> 0.069583).  Saving model ...
Updating learning rate to 9.765625e-08
Validation loss decreased (0.069583 --> 0.069583).  Saving model ...
Updating learning rate to 4.8828125e-08
Validation loss decreased (0.069583 --> 0.069583).  Saving model ...
Updating learning rate to 2.44140625e-08
Validation loss decreased (0.069583 --> 0.069583).  Saving model ...
Updating learning rate to 1.220703125e-08
Validation loss decreased (0.069583 --> 0.069583).  Saving model ...
Updating learning rate to 6.103515625e-09
Validation loss decreased (0.069583 --> 0.069583).  Saving model ...
Updating learning rate to 3.0517578125e-09
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.52587890625e-09
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.62939453125e-10
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.814697265625e-10
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.9073486328125e-10
Validation loss decreased (0.069583 --> 0.069583).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Validation loss decreased (0.069583 --> 0.069583).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Validation loss decreased (0.069583 --> 0.069583).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Validation loss decreased (0.069583 --> 0.069583).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Validation loss decreased (0.069583 --> 0.069583).  Saving model ...
Updating learning rate to 5.960464477539063e-12
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.9802322387695314e-12
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.4901161193847657e-12
Validation loss decreased (0.069583 --> 0.069583).  Saving model ...
Updating learning rate to 7.450580596923828e-13
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.725290298461914e-13
Validation loss decreased (0.069583 --> 0.069583).  Saving model ...
Updating learning rate to 1.862645149230957e-13
Validation loss decreased (0.069583 --> 0.069583).  Saving model ...
Updating learning rate to 9.313225746154786e-14
Validation loss decreased (0.069583 --> 0.069583).  Saving model ...
Updating learning rate to 4.656612873077393e-14
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.3283064365386964e-14
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.1641532182693482e-14
EarlyStopping counter: 3 out of 5
Updating learning rate to 5.820766091346741e-15
Validation loss decreased (0.069583 --> 0.069583).  Saving model ...
Updating learning rate to 2.9103830456733705e-15
Validation loss decreased (0.069583 --> 0.069583).  Saving model ...
Updating learning rate to 1.4551915228366853e-15
Validation loss decreased (0.069583 --> 0.069583).  Saving model ...
Updating learning rate to 7.275957614183426e-16
Validation loss decreased (0.069583 --> 0.069583).  Saving model ...
Updating learning rate to 3.637978807091713e-16
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.8189894035458566e-16
Validation loss decreased (0.069583 --> 0.069583).  Saving model ...
Updating learning rate to 9.094947017729283e-17
Validation loss decreased (0.069583 --> 0.069583).  Saving model ...
Updating learning rate to 4.5474735088646414e-17
Validation loss decreased (0.069583 --> 0.069583).  Saving model ...
Updating learning rate to 2.2737367544323207e-17
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.1368683772161604e-17
EarlyStopping counter: 2 out of 5
Updating learning rate to 5.684341886080802e-18
Validation loss decreased (0.069583 --> 0.069583).  Saving model ...
Updating learning rate to 2.842170943040401e-18
Validation loss decreased (0.069583 --> 0.069583).  Saving model ...
Updating learning rate to 1.4210854715202004e-18
EarlyStopping counter: 1 out of 5
Updating learning rate to 7.105427357601002e-19
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.552713678800501e-19
Validation loss decreased (0.069583 --> 0.069583).  Saving model ...
Updating learning rate to 1.7763568394002505e-19
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 28
test shape: (28, 100, 1) (28, 100, 1)
test shape: (28, 100, 1) (28, 100, 1)


	mse:3.4011427487712353e-05, mae:0.004345170687884092, rmse:0.005831931717693806, r2:-0.0034241676330566406, dtw:Not calculated


VAL - MSE: 0.0000, MAE: 0.0043, RMSE: 0.0058, RÂ²: -0.0034, MAPE: 63915.62%
Completed: APPLE H=100

Training: Mamba on SP500 for H=3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H3      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_SP500_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 3
============================================================
train 2201
val 123
test 125
Validation loss decreased (inf --> 0.277496).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
Validation loss decreased (0.277496 --> 0.276975).  Saving model ...
Updating learning rate to 2.5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
Validation loss decreased (0.276975 --> 0.274859).  Saving model ...
Updating learning rate to 7.8125e-07
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.90625e-07
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.953125e-07
EarlyStopping counter: 3 out of 5
Updating learning rate to 9.765625e-08
EarlyStopping counter: 4 out of 5
Updating learning rate to 4.8828125e-08
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_SP500_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 125
test shape: (125, 3, 1) (125, 3, 1)
test shape: (125, 3, 1) (125, 3, 1)


	mse:2.2494430140795885e-06, mae:0.0010657659731805325, rmse:0.001499814330600202, r2:-0.014298319816589355, dtw:Not calculated


VAL - MSE: 0.0000, MAE: 0.0011, RMSE: 0.0015, RÂ²: -0.0143, MAPE: 1.18%
Completed: SP500 H=3

Training: Mamba on SP500 for H=5
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H5      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_SP500_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 5
============================================================
train 2199
val 121
test 123
Validation loss decreased (inf --> 0.275814).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.275814 --> 0.272130).  Saving model ...
Updating learning rate to 5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
Validation loss decreased (0.272130 --> 0.271196).  Saving model ...
Updating learning rate to 3.125e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.90625e-07
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.953125e-07
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_SP500_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 123
test shape: (123, 5, 1) (123, 5, 1)
test shape: (123, 5, 1) (123, 5, 1)


	mse:2.2438314317696495e-06, mae:0.0010621498804539442, rmse:0.001497942372225225, r2:-0.006914615631103516, dtw:Not calculated


VAL - MSE: 0.0000, MAE: 0.0011, RMSE: 0.0015, RÂ²: -0.0069, MAPE: 1.19%
Completed: SP500 H=5

Training: Mamba on SP500 for H=10
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H10     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_SP500_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 10
============================================================
train 2194
val 116
test 118
Validation loss decreased (inf --> 0.274604).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_SP500_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 118
test shape: (118, 10, 1) (118, 10, 1)
test shape: (118, 10, 1) (118, 10, 1)


	mse:2.292267708980944e-06, mae:0.0010745961917564273, rmse:0.0015140236355364323, r2:-0.014477372169494629, dtw:Not calculated


VAL - MSE: 0.0000, MAE: 0.0011, RMSE: 0.0015, RÂ²: -0.0145, MAPE: 1.21%
Completed: SP500 H=10

Training: Mamba on SP500 for H=22
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H22     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_SP500_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 22
============================================================
train 2182
val 104
test 106
Validation loss decreased (inf --> 0.261532).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
Validation loss decreased (0.261532 --> 0.255966).  Saving model ...
Updating learning rate to 3.125e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.90625e-07
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.953125e-07
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_SP500_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 106
test shape: (106, 22, 1) (106, 22, 1)
test shape: (106, 22, 1) (106, 22, 1)


	mse:2.3725406208541244e-06, mae:0.001085421652533114, rmse:0.0015403053257614374, r2:-0.014472365379333496, dtw:Not calculated


VAL - MSE: 0.0000, MAE: 0.0011, RMSE: 0.0015, RÂ²: -0.0145, MAPE: 1.20%
Completed: SP500 H=22

Training: Mamba on SP500 for H=50
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H50     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_SP500_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 50
============================================================
train 2154
val 76
test 78
Validation loss decreased (inf --> 0.260182).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
Validation loss decreased (0.260182 --> 0.258099).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (0.258099 --> 0.255862).  Saving model ...
Updating learning rate to 3.125e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.90625e-07
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.953125e-07
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_SP500_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 78
test shape: (78, 50, 1) (78, 50, 1)
test shape: (78, 50, 1) (78, 50, 1)


	mse:2.7729352041205857e-06, mae:0.0011789402924478054, rmse:0.001665213261730969, r2:-0.012076616287231445, dtw:Not calculated


VAL - MSE: 0.0000, MAE: 0.0012, RMSE: 0.0017, RÂ²: -0.0121, MAPE: 1.21%
Completed: SP500 H=50

Training: Mamba on SP500 for H=100
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H100    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_SP500_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 100
============================================================
train 2104
val 26
test 28
Validation loss decreased (inf --> 0.272803).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.272803 --> 0.272452).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.272452 --> 0.272268).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.272268 --> 0.272177).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (0.272177 --> 0.272135).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (0.272135 --> 0.272116).  Saving model ...
Updating learning rate to 3.125e-06
Validation loss decreased (0.272116 --> 0.272105).  Saving model ...
Updating learning rate to 1.5625e-06
Validation loss decreased (0.272105 --> 0.272099).  Saving model ...
Updating learning rate to 7.8125e-07
Validation loss decreased (0.272099 --> 0.272096).  Saving model ...
Updating learning rate to 3.90625e-07
Validation loss decreased (0.272096 --> 0.272095).  Saving model ...
Updating learning rate to 1.953125e-07
Validation loss decreased (0.272095 --> 0.272095).  Saving model ...
Updating learning rate to 9.765625e-08
Validation loss decreased (0.272095 --> 0.272094).  Saving model ...
Updating learning rate to 4.8828125e-08
Validation loss decreased (0.272094 --> 0.272094).  Saving model ...
Updating learning rate to 2.44140625e-08
Validation loss decreased (0.272094 --> 0.272094).  Saving model ...
Updating learning rate to 1.220703125e-08
Validation loss decreased (0.272094 --> 0.272094).  Saving model ...
Updating learning rate to 6.103515625e-09
Validation loss decreased (0.272094 --> 0.272094).  Saving model ...
Updating learning rate to 3.0517578125e-09
Validation loss decreased (0.272094 --> 0.272094).  Saving model ...
Updating learning rate to 1.52587890625e-09
Validation loss decreased (0.272094 --> 0.272094).  Saving model ...
Updating learning rate to 7.62939453125e-10
Validation loss decreased (0.272094 --> 0.272094).  Saving model ...
Updating learning rate to 3.814697265625e-10
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.9073486328125e-10
EarlyStopping counter: 2 out of 5
Updating learning rate to 9.5367431640625e-11
EarlyStopping counter: 3 out of 5
Updating learning rate to 4.76837158203125e-11
EarlyStopping counter: 4 out of 5
Updating learning rate to 2.384185791015625e-11
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_SP500_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 28
test shape: (28, 100, 1) (28, 100, 1)
test shape: (28, 100, 1) (28, 100, 1)


	mse:2.409236685707583e-06, mae:0.0010922657093033195, rmse:0.0015521716559305787, r2:-0.002444028854370117, dtw:Not calculated


VAL - MSE: 0.0000, MAE: 0.0011, RMSE: 0.0016, RÂ²: -0.0024, MAPE: 1.15%
Completed: SP500 H=100

Training: Mamba on NASDAQ for H=3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H3     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 3
============================================================
train 2201
val 123
test 125
Validation loss decreased (inf --> 0.102888).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.102888 --> 0.102616).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.102616 --> 0.102113).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.102113 --> 0.101614).  Saving model ...
Updating learning rate to 1.25e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
Validation loss decreased (0.101614 --> 0.101267).  Saving model ...
Updating learning rate to 3.90625e-07
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.953125e-07
EarlyStopping counter: 2 out of 5
Updating learning rate to 9.765625e-08
EarlyStopping counter: 3 out of 5
Updating learning rate to 4.8828125e-08
EarlyStopping counter: 4 out of 5
Updating learning rate to 2.44140625e-08
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 125
test shape: (125, 3, 1) (125, 3, 1)
test shape: (125, 3, 1) (125, 3, 1)


	mse:2.9212042136350647e-05, mae:0.004075230564922094, rmse:0.005404816474765539, r2:-0.01412820816040039, dtw:Not calculated


VAL - MSE: 0.0000, MAE: 0.0041, RMSE: 0.0054, RÂ²: -0.0141, MAPE: 1207573.38%
Completed: NASDAQ H=3

Training: Mamba on NASDAQ for H=5
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H5     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 5
============================================================
train 2199
val 121
test 123
Validation loss decreased (inf --> 0.101926).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.101926 --> 0.101607).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.101607 --> 0.101503).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.101503 --> 0.101318).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (0.101318 --> 0.101077).  Saving model ...
Updating learning rate to 6.25e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 3 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.90625e-07
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 123
test shape: (123, 5, 1) (123, 5, 1)
test shape: (123, 5, 1) (123, 5, 1)


	mse:2.9615488529088907e-05, mae:0.004113267175853252, rmse:0.005442011635750532, r2:-0.01775217056274414, dtw:Not calculated


VAL - MSE: 0.0000, MAE: 0.0041, RMSE: 0.0054, RÂ²: -0.0178, MAPE: 849343.50%
Completed: NASDAQ H=5

Training: Mamba on NASDAQ for H=10
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H10    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 10
============================================================
train 2194
val 116
test 118
Validation loss decreased (inf --> 0.101749).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.101749 --> 0.100041).  Saving model ...
Updating learning rate to 5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 118
test shape: (118, 10, 1) (118, 10, 1)
test shape: (118, 10, 1) (118, 10, 1)


	mse:2.9609289413201623e-05, mae:0.004126748535782099, rmse:0.0054414416663348675, r2:-0.017267227172851562, dtw:Not calculated


VAL - MSE: 0.0000, MAE: 0.0041, RMSE: 0.0054, RÂ²: -0.0173, MAPE: 920365.25%
Completed: NASDAQ H=10

Training: Mamba on NASDAQ for H=22
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H22    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 22
============================================================
train 2182
val 104
test 106
Validation loss decreased (inf --> 0.101668).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.101668 --> 0.098143).  Saving model ...
Updating learning rate to 5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 106
test shape: (106, 22, 1) (106, 22, 1)
test shape: (106, 22, 1) (106, 22, 1)


	mse:3.0025756132090464e-05, mae:0.004153505899012089, rmse:0.00547957606613636, r2:-0.009406089782714844, dtw:Not calculated


VAL - MSE: 0.0000, MAE: 0.0042, RMSE: 0.0055, RÂ²: -0.0094, MAPE: 1407756.75%
Completed: NASDAQ H=22

Training: Mamba on NASDAQ for H=50
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H50    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 50
============================================================
train 2154
val 76
test 78
Validation loss decreased (inf --> 0.101425).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.101425 --> 0.099805).  Saving model ...
Updating learning rate to 5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 78
test shape: (78, 50, 1) (78, 50, 1)
test shape: (78, 50, 1) (78, 50, 1)


	mse:3.25496876030229e-05, mae:0.004314771853387356, rmse:0.005705233197659254, r2:-0.008873343467712402, dtw:Not calculated


VAL - MSE: 0.0000, MAE: 0.0043, RMSE: 0.0057, RÂ²: -0.0089, MAPE: 1455434.88%
Completed: NASDAQ H=50

Training: Mamba on NASDAQ for H=100
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H100   Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 100
============================================================
train 2104
val 26
test 28
Validation loss decreased (inf --> 0.100189).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.100189 --> 0.100012).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.100012 --> 0.099933).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.099933 --> 0.099889).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (0.099889 --> 0.099867).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (0.099867 --> 0.099856).  Saving model ...
Updating learning rate to 3.125e-06
Validation loss decreased (0.099856 --> 0.099851).  Saving model ...
Updating learning rate to 1.5625e-06
Validation loss decreased (0.099851 --> 0.099848).  Saving model ...
Updating learning rate to 7.8125e-07
Validation loss decreased (0.099848 --> 0.099847).  Saving model ...
Updating learning rate to 3.90625e-07
Validation loss decreased (0.099847 --> 0.099846).  Saving model ...
Updating learning rate to 1.953125e-07
Validation loss decreased (0.099846 --> 0.099846).  Saving model ...
Updating learning rate to 9.765625e-08
Validation loss decreased (0.099846 --> 0.099846).  Saving model ...
Updating learning rate to 4.8828125e-08
Validation loss decreased (0.099846 --> 0.099846).  Saving model ...
Updating learning rate to 2.44140625e-08
Validation loss decreased (0.099846 --> 0.099846).  Saving model ...
Updating learning rate to 1.220703125e-08
Validation loss decreased (0.099846 --> 0.099846).  Saving model ...
Updating learning rate to 6.103515625e-09
Validation loss decreased (0.099846 --> 0.099846).  Saving model ...
Updating learning rate to 3.0517578125e-09
Validation loss decreased (0.099846 --> 0.099846).  Saving model ...
Updating learning rate to 1.52587890625e-09
EarlyStopping counter: 1 out of 5
Updating learning rate to 7.62939453125e-10
Validation loss decreased (0.099846 --> 0.099846).  Saving model ...
Updating learning rate to 3.814697265625e-10
Validation loss decreased (0.099846 --> 0.099846).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Validation loss decreased (0.099846 --> 0.099846).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Validation loss decreased (0.099846 --> 0.099846).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Validation loss decreased (0.099846 --> 0.099846).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Validation loss decreased (0.099846 --> 0.099846).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
EarlyStopping counter: 1 out of 5
Updating learning rate to 5.960464477539063e-12
Validation loss decreased (0.099846 --> 0.099846).  Saving model ...
Updating learning rate to 2.9802322387695314e-12
Validation loss decreased (0.099846 --> 0.099846).  Saving model ...
Updating learning rate to 1.4901161193847657e-12
EarlyStopping counter: 1 out of 5
Updating learning rate to 7.450580596923828e-13
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.725290298461914e-13
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.862645149230957e-13
EarlyStopping counter: 4 out of 5
Updating learning rate to 9.313225746154786e-14
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 28
test shape: (28, 100, 1) (28, 100, 1)
test shape: (28, 100, 1) (28, 100, 1)


	mse:3.052584725082852e-05, mae:0.004181812983006239, rmse:0.0055250199511647224, r2:-0.006812930107116699, dtw:Not calculated


VAL - MSE: 0.0000, MAE: 0.0042, RMSE: 0.0055, RÂ²: -0.0068, MAPE: 1098602.75%
Completed: NASDAQ H=100

Training: Mamba on ABSA for H=3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H3       Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 3
============================================================
train 2201
val 123
test 125
Validation loss decreased (inf --> 0.017804).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.017804 --> 0.017655).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.017655 --> 0.017654).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.017654 --> 0.017621).  Saving model ...
Updating learning rate to 1.25e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 125
test shape: (125, 3, 1) (125, 3, 1)
test shape: (125, 3, 1) (125, 3, 1)


	mse:2.6288122171536088e-05, mae:0.0037551545538008213, rmse:0.005127194337546825, r2:-0.02442002296447754, dtw:Not calculated


VAL - MSE: 0.0000, MAE: 0.0038, RMSE: 0.0051, RÂ²: -0.0244, MAPE: 1.14%
Completed: ABSA H=3

Training: Mamba on ABSA for H=5
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H5       Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 5
============================================================
train 2199
val 121
test 123
Validation loss decreased (inf --> 0.018073).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.018073 --> 0.017789).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.017789 --> 0.017714).  Saving model ...
Updating learning rate to 2.5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 123
test shape: (123, 5, 1) (123, 5, 1)
test shape: (123, 5, 1) (123, 5, 1)


	mse:2.6382283977000043e-05, mae:0.0037961378693580627, rmse:0.0051363687962293625, r2:-0.014786005020141602, dtw:Not calculated


VAL - MSE: 0.0000, MAE: 0.0038, RMSE: 0.0051, RÂ²: -0.0148, MAPE: 1.21%
Completed: ABSA H=5

Training: Mamba on ABSA for H=10
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H10      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 10
============================================================
train 2194
val 116
test 118
Validation loss decreased (inf --> 0.017825).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.017825 --> 0.017617).  Saving model ...
Updating learning rate to 5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
Validation loss decreased (0.017617 --> 0.017409).  Saving model ...
Updating learning rate to 1.25e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
Validation loss decreased (0.017409 --> 0.017228).  Saving model ...
Updating learning rate to 3.125e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.90625e-07
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.953125e-07
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 118
test shape: (118, 10, 1) (118, 10, 1)
test shape: (118, 10, 1) (118, 10, 1)


	mse:2.7125019187224098e-05, mae:0.0038451291620731354, rmse:0.0052081686444580555, r2:-0.02135777473449707, dtw:Not calculated


VAL - MSE: 0.0000, MAE: 0.0038, RMSE: 0.0052, RÂ²: -0.0214, MAPE: 1.26%
Completed: ABSA H=10

Training: Mamba on ABSA for H=22
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H22      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 22
============================================================
train 2182
val 104
test 106
Validation loss decreased (inf --> 0.016675).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
Validation loss decreased (0.016675 --> 0.016304).  Saving model ...
Updating learning rate to 3.125e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.90625e-07
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.953125e-07
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 106
test shape: (106, 22, 1) (106, 22, 1)
test shape: (106, 22, 1) (106, 22, 1)


	mse:2.7765885533881374e-05, mae:0.00387365254573524, rmse:0.005269334651529789, r2:-0.014160752296447754, dtw:Not calculated


VAL - MSE: 0.0000, MAE: 0.0039, RMSE: 0.0053, RÂ²: -0.0142, MAPE: 1.23%
Completed: ABSA H=22

Training: Mamba on ABSA for H=50
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H50      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 50
============================================================
train 2154
val 76
test 78
Validation loss decreased (inf --> 0.016096).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.016096 --> 0.015734).  Saving model ...
Updating learning rate to 5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
Validation loss decreased (0.015734 --> 0.015648).  Saving model ...
Updating learning rate to 6.25e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 3 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.90625e-07
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 78
test shape: (78, 50, 1) (78, 50, 1)
test shape: (78, 50, 1) (78, 50, 1)


	mse:2.9258719223435037e-05, mae:0.003874464426189661, rmse:0.005409133154898882, r2:-0.006205320358276367, dtw:Not calculated


VAL - MSE: 0.0000, MAE: 0.0039, RMSE: 0.0054, RÂ²: -0.0062, MAPE: 1.27%
Completed: ABSA H=50

Training: Mamba on ABSA for H=100
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H100     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 100
============================================================
train 2104
val 26
test 28
Validation loss decreased (inf --> 0.016857).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.016857 --> 0.016814).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.016814 --> 0.016805).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.016805 --> 0.016798).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (0.016798 --> 0.016796).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (0.016796 --> 0.016795).  Saving model ...
Updating learning rate to 3.125e-06
Validation loss decreased (0.016795 --> 0.016794).  Saving model ...
Updating learning rate to 1.5625e-06
Validation loss decreased (0.016794 --> 0.016794).  Saving model ...
Updating learning rate to 7.8125e-07
Validation loss decreased (0.016794 --> 0.016794).  Saving model ...
Updating learning rate to 3.90625e-07
Validation loss decreased (0.016794 --> 0.016794).  Saving model ...
Updating learning rate to 1.953125e-07
Validation loss decreased (0.016794 --> 0.016794).  Saving model ...
Updating learning rate to 9.765625e-08
Validation loss decreased (0.016794 --> 0.016794).  Saving model ...
Updating learning rate to 4.8828125e-08
Validation loss decreased (0.016794 --> 0.016793).  Saving model ...
Updating learning rate to 2.44140625e-08
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 1.220703125e-08
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 6.103515625e-09
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 3.0517578125e-09
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 1.52587890625e-09
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 7.62939453125e-10
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 3.814697265625e-10
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 5.960464477539063e-12
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 2.9802322387695314e-12
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 1.4901161193847657e-12
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 7.450580596923828e-13
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 3.725290298461914e-13
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 1.862645149230957e-13
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 9.313225746154786e-14
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 4.656612873077393e-14
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 2.3283064365386964e-14
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 1.1641532182693482e-14
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 5.820766091346741e-15
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 2.9103830456733705e-15
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 1.4551915228366853e-15
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 7.275957614183426e-16
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 3.637978807091713e-16
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 1.8189894035458566e-16
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 9.094947017729283e-17
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 4.5474735088646414e-17
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 2.2737367544323207e-17
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 1.1368683772161604e-17
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 5.684341886080802e-18
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 2.842170943040401e-18
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 1.4210854715202004e-18
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 7.105427357601002e-19
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 3.552713678800501e-19
Validation loss decreased (0.016793 --> 0.016793).  Saving model ...
Updating learning rate to 1.7763568394002505e-19
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 28
test shape: (28, 100, 1) (28, 100, 1)
test shape: (28, 100, 1) (28, 100, 1)


	mse:2.8177895728731528e-05, mae:0.0038439580239355564, rmse:0.005308285355567932, r2:-0.014958739280700684, dtw:Not calculated


VAL - MSE: 0.0000, MAE: 0.0038, RMSE: 0.0053, RÂ²: -0.0150, MAPE: 1.16%
Completed: ABSA H=100

Training: Mamba on SASOL for H=3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H3      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 3
============================================================
train 2201
val 123
test 125
Validation loss decreased (inf --> 2.198589).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
Validation loss decreased (2.198589 --> 2.167305).  Saving model ...
Updating learning rate to 3.125e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.90625e-07
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.953125e-07
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 125
test shape: (125, 3, 1) (125, 3, 1)
test shape: (125, 3, 1) (125, 3, 1)


	mse:0.0014603999443352222, mae:0.02779996208846569, rmse:0.03821517899632454, r2:-0.011824727058410645, dtw:Not calculated


VAL - MSE: 0.0015, MAE: 0.0278, RMSE: 0.0382, RÂ²: -0.0118, MAPE: 24177270.00%
Completed: SASOL H=3

Training: Mamba on SASOL for H=5
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H5      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 5
============================================================
train 2199
val 121
test 123
Validation loss decreased (inf --> 2.214954).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (2.214954 --> 2.214036).  Saving model ...
Updating learning rate to 5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
Validation loss decreased (2.214036 --> 2.210056).  Saving model ...
Updating learning rate to 6.25e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 3 out of 5
Updating learning rate to 7.8125e-07
Validation loss decreased (2.210056 --> 2.182299).  Saving model ...
Updating learning rate to 3.90625e-07
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.953125e-07
EarlyStopping counter: 2 out of 5
Updating learning rate to 9.765625e-08
EarlyStopping counter: 3 out of 5
Updating learning rate to 4.8828125e-08
EarlyStopping counter: 4 out of 5
Updating learning rate to 2.44140625e-08
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 123
test shape: (123, 5, 1) (123, 5, 1)
test shape: (123, 5, 1) (123, 5, 1)


	mse:0.0014612271916121244, mae:0.027433102950453758, rmse:0.03822600096464157, r2:-0.00985407829284668, dtw:Not calculated


VAL - MSE: 0.0015, MAE: 0.0274, RMSE: 0.0382, RÂ²: -0.0099, MAPE: 16050268.00%
Completed: SASOL H=5

Training: Mamba on SASOL for H=10
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H10     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 10
============================================================
train 2194
val 116
test 118
Validation loss decreased (inf --> 2.248778).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
Validation loss decreased (2.248778 --> 2.244164).  Saving model ...
Updating learning rate to 2.5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 118
test shape: (118, 10, 1) (118, 10, 1)
test shape: (118, 10, 1) (118, 10, 1)


	mse:0.0014877835055813193, mae:0.027595791965723038, rmse:0.038571797311306, r2:-0.007639527320861816, dtw:Not calculated


VAL - MSE: 0.0015, MAE: 0.0276, RMSE: 0.0386, RÂ²: -0.0076, MAPE: 17317214.00%
Completed: SASOL H=10

Training: Mamba on SASOL for H=22
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H22     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 22
============================================================
train 2182
val 104
test 106
Validation loss decreased (inf --> 2.660218).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (2.660218 --> 2.344559).  Saving model ...
Updating learning rate to 5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 106
test shape: (106, 22, 1) (106, 22, 1)
test shape: (106, 22, 1) (106, 22, 1)


	mse:0.00151257892139256, mae:0.02771497704088688, rmse:0.03889188915491104, r2:-0.011608004570007324, dtw:Not calculated


VAL - MSE: 0.0015, MAE: 0.0277, RMSE: 0.0389, RÂ²: -0.0116, MAPE: 23384812.00%
Completed: SASOL H=22

Training: Mamba on SASOL for H=50
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H50     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 50
============================================================
train 2154
val 76
test 78
Validation loss decreased (inf --> 2.463131).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (2.463131 --> 2.440703).  Saving model ...
Updating learning rate to 5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 78
test shape: (78, 50, 1) (78, 50, 1)
test shape: (78, 50, 1) (78, 50, 1)


	mse:0.0016489004483446479, mae:0.028702333569526672, rmse:0.040606655180454254, r2:-0.012944459915161133, dtw:Not calculated


VAL - MSE: 0.0016, MAE: 0.0287, RMSE: 0.0406, RÂ²: -0.0129, MAPE: 26488588.00%
Completed: SASOL H=50

Training: Mamba on SASOL for H=100
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H100    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 100
============================================================
train 2104
val 26
test 28
Validation loss decreased (inf --> 2.450067).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (2.450067 --> 2.446844).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (2.446844 --> 2.445460).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (2.445460 --> 2.444660).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (2.444660 --> 2.444322).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (2.444322 --> 2.444142).  Saving model ...
Updating learning rate to 3.125e-06
Validation loss decreased (2.444142 --> 2.444047).  Saving model ...
Updating learning rate to 1.5625e-06
Validation loss decreased (2.444047 --> 2.443995).  Saving model ...
Updating learning rate to 7.8125e-07
Validation loss decreased (2.443995 --> 2.443972).  Saving model ...
Updating learning rate to 3.90625e-07
Validation loss decreased (2.443972 --> 2.443961).  Saving model ...
Updating learning rate to 1.953125e-07
Validation loss decreased (2.443961 --> 2.443954).  Saving model ...
Updating learning rate to 9.765625e-08
Validation loss decreased (2.443954 --> 2.443952).  Saving model ...
Updating learning rate to 4.8828125e-08
Validation loss decreased (2.443952 --> 2.443950).  Saving model ...
Updating learning rate to 2.44140625e-08
Validation loss decreased (2.443950 --> 2.443949).  Saving model ...
Updating learning rate to 1.220703125e-08
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 6.103515625e-09
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 3.0517578125e-09
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 1.52587890625e-09
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 7.62939453125e-10
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 3.814697265625e-10
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 5.960464477539063e-12
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 2.9802322387695314e-12
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.4901161193847657e-12
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 7.450580596923828e-13
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 3.725290298461914e-13
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 1.862645149230957e-13
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 9.313225746154786e-14
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 4.656612873077393e-14
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 2.3283064365386964e-14
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 1.1641532182693482e-14
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 5.820766091346741e-15
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 2.9103830456733705e-15
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 1.4551915228366853e-15
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 7.275957614183426e-16
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 3.637978807091713e-16
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 1.8189894035458566e-16
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 9.094947017729283e-17
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 4.5474735088646414e-17
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 2.2737367544323207e-17
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 1.1368683772161604e-17
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 5.684341886080802e-18
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.842170943040401e-18
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 1.4210854715202004e-18
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 7.105427357601002e-19
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 3.552713678800501e-19
Validation loss decreased (2.443949 --> 2.443949).  Saving model ...
Updating learning rate to 1.7763568394002505e-19
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 28
test shape: (28, 100, 1) (28, 100, 1)
test shape: (28, 100, 1) (28, 100, 1)


	mse:0.0015350813046097755, mae:0.027780555188655853, rmse:0.03918011486530304, r2:-0.008001923561096191, dtw:Not calculated


VAL - MSE: 0.0015, MAE: 0.0278, RMSE: 0.0392, RÂ²: -0.0080, MAPE: 18593004.00%
Completed: SASOL H=100

Mamba training completed for all datasets!
