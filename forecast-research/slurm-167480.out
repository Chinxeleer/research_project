##############################################################################
# Training iTransformer Model on All Datasets
##############################################################################
Training: iTransformer on NVIDIA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_193303-lfigwvap
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/lfigwvap
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_NVIDIA_H3Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/lfigwvap
>>>>>>>start training : long_term_forecast_iTransformer_NVIDIA_H3_iTransformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.2583118 Vali Loss: 0.1896148 Test Loss: 0.2570041
Validation loss decreased (inf --> 0.189615).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.25831180037860585, 'val/loss': 0.18961483426392078, 'test/loss': 0.2570040598511696, '_timestamp': 1762882433.6212556}).
Epoch: 2, Steps: 133 | Train Loss: 0.2325065 Vali Loss: 0.1971426 Test Loss: 0.2602984
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2137340 Vali Loss: 0.1907470 Test Loss: 0.2506275
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2017216 Vali Loss: 0.1922514 Test Loss: 0.2543822
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1940085 Vali Loss: 0.2124250 Test Loss: 0.2510350
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1896551 Vali Loss: 0.1829569 Test Loss: 0.2505477
Validation loss decreased (0.189615 --> 0.182957).  Saving model ...
Updating learning rate to 3.125e-06
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23250646071326464, 'val/loss': 0.1971426047384739, 'test/loss': 0.26029836386442184, '_timestamp': 1762882438.7019923}).
Epoch: 7, Steps: 133 | Train Loss: 0.1876729 Vali Loss: 0.1860456 Test Loss: 0.2497783
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1861874 Vali Loss: 0.1868236 Test Loss: 0.2498113
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1857012 Vali Loss: 0.1835138 Test Loss: 0.2502073
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1853568 Vali Loss: 0.1852354 Test Loss: 0.2503345
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1846948 Vali Loss: 0.1896385 Test Loss: 0.2503443
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1847437 Vali Loss: 0.1851997 Test Loss: 0.2503447
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1852160 Vali Loss: 0.1882999 Test Loss: 0.2503417
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1858742 Vali Loss: 0.1866371 Test Loss: 0.2503426
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1856413 Vali Loss: 0.1912335 Test Loss: 0.2503440
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1854949 Vali Loss: 0.2074247 Test Loss: 0.2503427
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_NVIDIA_H3_iTransformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.0012519097654148936, mae:0.026739466935396194, rmse:0.035382337868213654, r2:-0.1159963607788086, dtw:Not calculated


VAL - MSE: 0.0013, MAE: 0.0267, RMSE: 0.0354, RÂ²: -0.1160, MAPE: 596243.94%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.490 MB of 0.490 MB uploadedwandb: \ 0.490 MB of 0.490 MB uploadedwandb: | 0.490 MB of 0.490 MB uploadedwandb: / 0.490 MB of 0.490 MB uploadedwandb: - 0.490 MB of 0.490 MB uploadedwandb: \ 0.490 MB of 0.490 MB uploadedwandb: | 0.490 MB of 0.490 MB uploadedwandb: / 0.490 MB of 0.490 MB uploadedwandb: - 0.490 MB of 0.490 MB uploadedwandb: \ 0.490 MB of 0.490 MB uploadedwandb: | 0.595 MB of 0.752 MB uploaded (0.002 MB deduped)wandb: / 0.752 MB of 0.752 MB uploaded (0.002 MB deduped)wandb: - 0.752 MB of 0.752 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‚â–ˆâ–ƒâ–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–ƒâ–ˆâ–â–‚â–‚â–â–‚â–ƒâ–‚â–‚â–‚â–ƒâ–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6338563
wandb:     model/trainable_params 6338563
wandb:                  test/loss 0.25034
wandb:                 train/loss 0.18549
wandb:   val/directional_accuracy 48.10127
wandb:                   val/loss 0.20742
wandb:                    val/mae 0.02674
wandb:                   val/mape 59624393.75
wandb:                    val/mse 0.00125
wandb:                     val/r2 -0.116
wandb:                   val/rmse 0.03538
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/lfigwvap
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_193303-lfigwvap/logs
Completed: NVIDIA H=3

Training: iTransformer on NVIDIA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_193611-pazunmuh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/pazunmuh
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_NVIDIA_H5Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/pazunmuh
>>>>>>>start training : long_term_forecast_iTransformer_NVIDIA_H5_iTransformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.2599049 Vali Loss: 0.1881770 Test Loss: 0.2653353
Validation loss decreased (inf --> 0.188177).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2341833 Vali Loss: 0.2085087 Test Loss: 0.2752061
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2191885 Vali Loss: 0.1895927 Test Loss: 0.2576921
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.25990488430611175, 'val/loss': 0.18817702494561672, 'test/loss': 0.26533532701432705, '_timestamp': 1762882598.9725661}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23418330933366502, 'val/loss': 0.20850868709385395, 'test/loss': 0.2752061262726784, '_timestamp': 1762882603.4113774}).
Epoch: 4, Steps: 133 | Train Loss: 0.2091372 Vali Loss: 0.1826140 Test Loss: 0.2582329
Validation loss decreased (0.188177 --> 0.182614).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2024948 Vali Loss: 0.1970936 Test Loss: 0.2557411
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1994623 Vali Loss: 0.1830376 Test Loss: 0.2571272
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1975221 Vali Loss: 0.1974466 Test Loss: 0.2568628
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1968436 Vali Loss: 0.1822709 Test Loss: 0.2571976
Validation loss decreased (0.182614 --> 0.182271).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1970234 Vali Loss: 0.1852842 Test Loss: 0.2572477
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1966778 Vali Loss: 0.1831714 Test Loss: 0.2572488
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1956112 Vali Loss: 0.1825107 Test Loss: 0.2572652
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1954002 Vali Loss: 0.1816619 Test Loss: 0.2572755
Validation loss decreased (0.182271 --> 0.181662).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1958283 Vali Loss: 0.1826147 Test Loss: 0.2572727
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1951583 Vali Loss: 0.1847076 Test Loss: 0.2572747
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1955672 Vali Loss: 0.1884194 Test Loss: 0.2572765
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1952808 Vali Loss: 0.1808212 Test Loss: 0.2572775
Validation loss decreased (0.181662 --> 0.180821).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1958460 Vali Loss: 0.1822096 Test Loss: 0.2572774
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1964163 Vali Loss: 0.1953923 Test Loss: 0.2572778
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1956613 Vali Loss: 0.1815369 Test Loss: 0.2572778
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1962578 Vali Loss: 0.2028864 Test Loss: 0.2572776
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1948617 Vali Loss: 0.2026921 Test Loss: 0.2572775
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1949548 Vali Loss: 0.1994315 Test Loss: 0.2572776
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1956968 Vali Loss: 0.2023378 Test Loss: 0.2572776
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.1962362 Vali Loss: 0.1815607 Test Loss: 0.2572776
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.1954386 Vali Loss: 0.1815469 Test Loss: 0.2572775
EarlyStopping counter: 9 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.1959758 Vali Loss: 0.1835155 Test Loss: 0.2572776
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_NVIDIA_H5_iTransformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.0012371230404824018, mae:0.026719197630882263, rmse:0.03517276048660278, r2:-0.09569108486175537, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0267, RMSE: 0.0352, RÂ²: -0.0957, MAPE: 503426.62%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.523 MB of 0.523 MB uploadedwandb: \ 0.523 MB of 0.523 MB uploadedwandb: | 0.523 MB of 0.523 MB uploadedwandb: / 0.523 MB of 0.523 MB uploadedwandb: - 0.523 MB of 0.523 MB uploadedwandb: \ 0.523 MB of 0.523 MB uploadedwandb: | 0.523 MB of 0.682 MB uploadedwandb: / 0.682 MB of 0.682 MB uploadedwandb: - 0.682 MB of 0.682 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–†â–ˆâ–â–…â–„â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–‚â–†â–‚â–†â–â–‚â–‚â–‚â–â–‚â–‚â–ƒâ–â–â–†â–â–ˆâ–ˆâ–‡â–ˆâ–â–â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 25
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6339589
wandb:     model/trainable_params 6339589
wandb:                  test/loss 0.25728
wandb:                 train/loss 0.19598
wandb:   val/directional_accuracy 46.48936
wandb:                   val/loss 0.18352
wandb:                    val/mae 0.02672
wandb:                   val/mape 50342662.5
wandb:                    val/mse 0.00124
wandb:                     val/r2 -0.09569
wandb:                   val/rmse 0.03517
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/pazunmuh
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_193611-pazunmuh/logs
Completed: NVIDIA H=5

Training: iTransformer on NVIDIA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_193924-8orayco8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/8orayco8
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_NVIDIA_H10Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/8orayco8
>>>>>>>start training : long_term_forecast_iTransformer_NVIDIA_H10_iTransformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2601322 Vali Loss: 0.2060866 Test Loss: 0.2893011
Validation loss decreased (inf --> 0.206087).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2403920 Vali Loss: 0.1922853 Test Loss: 0.2881165
Validation loss decreased (0.206087 --> 0.192285).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2254269 Vali Loss: 0.2021755 Test Loss: 0.2786655
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.26013216925294774, 'val/loss': 0.20608657412230968, 'test/loss': 0.28930113930255175, '_timestamp': 1762882788.9755945}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24039197326602793, 'val/loss': 0.19228528812527657, 'test/loss': 0.28811645321547985, '_timestamp': 1762882793.98334}).
Epoch: 4, Steps: 133 | Train Loss: 0.2176707 Vali Loss: 0.2049233 Test Loss: 0.2756295
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2131573 Vali Loss: 0.1812963 Test Loss: 0.2748178
Validation loss decreased (0.192285 --> 0.181296).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2099400 Vali Loss: 0.2073234 Test Loss: 0.2745618
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2091538 Vali Loss: 0.1800235 Test Loss: 0.2749343
Validation loss decreased (0.181296 --> 0.180023).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2089479 Vali Loss: 0.2024739 Test Loss: 0.2745497
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2073695 Vali Loss: 0.1779518 Test Loss: 0.2744844
Validation loss decreased (0.180023 --> 0.177952).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2067959 Vali Loss: 0.1893586 Test Loss: 0.2744067
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2078732 Vali Loss: 0.1819912 Test Loss: 0.2744102
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2076433 Vali Loss: 0.1798421 Test Loss: 0.2744070
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2072685 Vali Loss: 0.1984333 Test Loss: 0.2744045
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2070536 Vali Loss: 0.1799708 Test Loss: 0.2744099
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2075267 Vali Loss: 0.1774851 Test Loss: 0.2744094
Validation loss decreased (0.177952 --> 0.177485).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2071602 Vali Loss: 0.1940873 Test Loss: 0.2744088
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2074731 Vali Loss: 0.2166057 Test Loss: 0.2744091
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2078151 Vali Loss: 0.1809810 Test Loss: 0.2744089
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2075655 Vali Loss: 0.1807464 Test Loss: 0.2744090
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2075747 Vali Loss: 0.1815847 Test Loss: 0.2744090
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2072370 Vali Loss: 0.1782834 Test Loss: 0.2744089
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2074314 Vali Loss: 0.1919746 Test Loss: 0.2744089
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2073259 Vali Loss: 0.1774470 Test Loss: 0.2744090
Validation loss decreased (0.177485 --> 0.177447).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2075011 Vali Loss: 0.2210215 Test Loss: 0.2744090
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2074140 Vali Loss: 0.1820926 Test Loss: 0.2744090
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2068537 Vali Loss: 0.2158468 Test Loss: 0.2744090
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.2068773 Vali Loss: 0.1779160 Test Loss: 0.2744090
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.2072113 Vali Loss: 0.1793079 Test Loss: 0.2744090
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.2090313 Vali Loss: 0.2190137 Test Loss: 0.2744090
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 133 | Train Loss: 0.2068729 Vali Loss: 0.1782013 Test Loss: 0.2744090
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 133 | Train Loss: 0.2068260 Vali Loss: 0.1807533 Test Loss: 0.2744090
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 133 | Train Loss: 0.2076261 Vali Loss: 0.1783037 Test Loss: 0.2744090
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 133 | Train Loss: 0.2076874 Vali Loss: 0.1823074 Test Loss: 0.2744090
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_NVIDIA_H10_iTransformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.0012339205713942647, mae:0.02679450251162052, rmse:0.03512720391154289, r2:-0.07566344738006592, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0268, RMSE: 0.0351, RÂ²: -0.0757, MAPE: 309234.19%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.578 MB of 0.579 MB uploadedwandb: \ 0.578 MB of 0.579 MB uploadedwandb: | 0.579 MB of 0.579 MB uploadedwandb: / 0.579 MB of 0.579 MB uploadedwandb: - 0.579 MB of 0.739 MB uploadedwandb: \ 0.579 MB of 0.739 MB uploadedwandb: | 0.739 MB of 0.739 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–…â–‚â–†â–â–…â–â–ƒâ–‚â–â–„â–â–â–„â–‡â–‚â–‚â–‚â–â–ƒâ–â–ˆâ–‚â–‡â–â–â–ˆâ–â–‚â–â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 32
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6342154
wandb:     model/trainable_params 6342154
wandb:                  test/loss 0.27441
wandb:                 train/loss 0.20769
wandb:   val/directional_accuracy 45.65217
wandb:                   val/loss 0.18231
wandb:                    val/mae 0.02679
wandb:                   val/mape 30923418.75
wandb:                    val/mse 0.00123
wandb:                     val/r2 -0.07566
wandb:                   val/rmse 0.03513
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/8orayco8
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_193924-8orayco8/logs
Completed: NVIDIA H=10

Training: iTransformer on NVIDIA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_194321-wfbiqmc4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/wfbiqmc4
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_NVIDIA_H22Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/wfbiqmc4
>>>>>>>start training : long_term_forecast_iTransformer_NVIDIA_H22_iTransformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2641191 Vali Loss: 0.1915213 Test Loss: 0.3519974
Validation loss decreased (inf --> 0.191521).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2459343 Vali Loss: 0.1906515 Test Loss: 0.3479012
Validation loss decreased (0.191521 --> 0.190651).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2357431 Vali Loss: 0.1865240 Test Loss: 0.3398691
Validation loss decreased (0.190651 --> 0.186524).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2641191174360839, 'val/loss': 0.19152131038052694, 'test/loss': 0.3519974244492395, '_timestamp': 1762883029.7571871}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2459343129938299, 'val/loss': 0.19065147638320923, 'test/loss': 0.3479012208325522, '_timestamp': 1762883035.4652944}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2459343129938299, 'val/loss': 0.19065147638320923, 'test/loss': 0.3479012208325522, '_timestamp': 1762883035.4652944}).
Epoch: 4, Steps: 132 | Train Loss: 0.2291459 Vali Loss: 0.1873052 Test Loss: 0.3387126
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2262799 Vali Loss: 0.1857829 Test Loss: 0.3337672
Validation loss decreased (0.186524 --> 0.185783).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2237938 Vali Loss: 0.1862884 Test Loss: 0.3332375
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2231043 Vali Loss: 0.1862700 Test Loss: 0.3329795
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2225461 Vali Loss: 0.1859520 Test Loss: 0.3329191
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2223921 Vali Loss: 0.1857323 Test Loss: 0.3328615
Validation loss decreased (0.185783 --> 0.185732).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2220231 Vali Loss: 0.1859334 Test Loss: 0.3328465
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2221208 Vali Loss: 0.1867960 Test Loss: 0.3328046
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2220299 Vali Loss: 0.1869268 Test Loss: 0.3328062
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2221572 Vali Loss: 0.1870283 Test Loss: 0.3328030
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2225144 Vali Loss: 0.1861480 Test Loss: 0.3328010
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2223572 Vali Loss: 0.1864102 Test Loss: 0.3327994
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2220882 Vali Loss: 0.1861339 Test Loss: 0.3327987
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2221864 Vali Loss: 0.1865103 Test Loss: 0.3327986
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2222299 Vali Loss: 0.1874212 Test Loss: 0.3327985
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2222243 Vali Loss: 0.1856817 Test Loss: 0.3327984
Validation loss decreased (0.185732 --> 0.185682).  Saving model ...
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2222495 Vali Loss: 0.1874795 Test Loss: 0.3327984
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2220710 Vali Loss: 0.1859284 Test Loss: 0.3327983
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2218811 Vali Loss: 0.1873751 Test Loss: 0.3327984
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.2220146 Vali Loss: 0.1856872 Test Loss: 0.3327983
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 132 | Train Loss: 0.2215873 Vali Loss: 0.1864504 Test Loss: 0.3327984
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 132 | Train Loss: 0.2221115 Vali Loss: 0.1847680 Test Loss: 0.3327983
Validation loss decreased (0.185682 --> 0.184768).  Saving model ...
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 132 | Train Loss: 0.2219091 Vali Loss: 0.1856127 Test Loss: 0.3327983
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 132 | Train Loss: 0.2220953 Vali Loss: 0.1868714 Test Loss: 0.3327983
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 132 | Train Loss: 0.2222487 Vali Loss: 0.1864832 Test Loss: 0.3327983
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 132 | Train Loss: 0.2225208 Vali Loss: 0.1873463 Test Loss: 0.3327983
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 132 | Train Loss: 0.2219576 Vali Loss: 0.1864924 Test Loss: 0.3327983
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 132 | Train Loss: 0.2219784 Vali Loss: 0.1890364 Test Loss: 0.3327983
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 132 | Train Loss: 0.2224504 Vali Loss: 0.1864741 Test Loss: 0.3327983
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 132 | Train Loss: 0.2223568 Vali Loss: 0.1863679 Test Loss: 0.3327983
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 132 | Train Loss: 0.2223718 Vali Loss: 0.1854257 Test Loss: 0.3327983
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1641532182693482e-14
Epoch: 35, Steps: 132 | Train Loss: 0.2220180 Vali Loss: 0.1859558 Test Loss: 0.3327983
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_NVIDIA_H22_iTransformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.0012475940166041255, mae:0.026781752705574036, rmse:0.03532129526138306, r2:-0.05741250514984131, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0268, RMSE: 0.0353, RÂ²: -0.0574, MAPE: 587078.19%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.601 MB of 0.601 MB uploadedwandb: \ 0.601 MB of 0.601 MB uploadedwandb: | 0.601 MB of 0.601 MB uploadedwandb: / 0.601 MB of 0.601 MB uploadedwandb: - 0.601 MB of 0.762 MB uploadedwandb: \ 0.762 MB of 0.762 MB uploadedwandb: | 0.762 MB of 0.762 MB uploadedwandb: / 0.762 MB of 0.762 MB uploadedwandb: - 0.762 MB of 0.762 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‡â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–…â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–…â–…â–ƒâ–„â–ƒâ–„â–…â–‚â–…â–ƒâ–…â–ƒâ–„â–â–‚â–„â–„â–…â–„â–ˆâ–„â–„â–‚â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 34
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6348310
wandb:     model/trainable_params 6348310
wandb:                  test/loss 0.3328
wandb:                 train/loss 0.22202
wandb:   val/directional_accuracy 46.67977
wandb:                   val/loss 0.18596
wandb:                    val/mae 0.02678
wandb:                   val/mape 58707818.75
wandb:                    val/mse 0.00125
wandb:                     val/r2 -0.05741
wandb:                   val/rmse 0.03532
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/wfbiqmc4
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_194321-wfbiqmc4/logs
Completed: NVIDIA H=22

Training: iTransformer on NVIDIA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_194813-ojb1jov6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/ojb1jov6
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_NVIDIA_H50Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/ojb1jov6
>>>>>>>start training : long_term_forecast_iTransformer_NVIDIA_H50_iTransformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.2750434 Vali Loss: 0.2034818 Test Loss: 0.4956385
Validation loss decreased (inf --> 0.203482).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2561124 Vali Loss: 0.1998892 Test Loss: 0.4823108
Validation loss decreased (0.203482 --> 0.199889).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
Epoch: 3, Steps: 132 | Train Loss: 0.2506646 Vali Loss: 0.1978405 Test Loss: 0.4713143
Validation loss decreased (0.199889 --> 0.197840).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.27504344668352243, 'val/loss': 0.20348180830478668, 'test/loss': 0.49563852200905484, '_timestamp': 1762883317.807707}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2561123983539415, 'val/loss': 0.1998891606926918, 'test/loss': 0.48231084148089093, '_timestamp': 1762883323.6194546}).
Epoch: 4, Steps: 132 | Train Loss: 0.2470718 Vali Loss: 0.1973219 Test Loss: 0.4669083
Validation loss decreased (0.197840 --> 0.197322).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2444768 Vali Loss: 0.1973887 Test Loss: 0.4657349
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2446972 Vali Loss: 0.1969807 Test Loss: 0.4663371
Validation loss decreased (0.197322 --> 0.196981).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2436581 Vali Loss: 0.1968440 Test Loss: 0.4653196
Validation loss decreased (0.196981 --> 0.196844).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2432728 Vali Loss: 0.1971178 Test Loss: 0.4654728
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2441945 Vali Loss: 0.1971567 Test Loss: 0.4654194
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2421515 Vali Loss: 0.1968579 Test Loss: 0.4653983
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2447175 Vali Loss: 0.1968232 Test Loss: 0.4654095
Validation loss decreased (0.196844 --> 0.196823).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2431680 Vali Loss: 0.1969085 Test Loss: 0.4653985
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2426023 Vali Loss: 0.1968868 Test Loss: 0.4653983
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2424719 Vali Loss: 0.1967085 Test Loss: 0.4653992
Validation loss decreased (0.196823 --> 0.196709).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2432490 Vali Loss: 0.1971288 Test Loss: 0.4654009
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2424868 Vali Loss: 0.1970007 Test Loss: 0.4654005
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2431873 Vali Loss: 0.1970923 Test Loss: 0.4654005
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2427957 Vali Loss: 0.1969644 Test Loss: 0.4654006
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2430005 Vali Loss: 0.1967820 Test Loss: 0.4654006
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2424399 Vali Loss: 0.1966829 Test Loss: 0.4654006
Validation loss decreased (0.196709 --> 0.196683).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2422644 Vali Loss: 0.1971207 Test Loss: 0.4654007
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2426824 Vali Loss: 0.1969044 Test Loss: 0.4654007
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.2421109 Vali Loss: 0.1970654 Test Loss: 0.4654007
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 132 | Train Loss: 0.2422091 Vali Loss: 0.1966443 Test Loss: 0.4654007
Validation loss decreased (0.196683 --> 0.196644).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 132 | Train Loss: 0.2422524 Vali Loss: 0.1967536 Test Loss: 0.4654007
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 132 | Train Loss: 0.2421418 Vali Loss: 0.1968579 Test Loss: 0.4654007
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 132 | Train Loss: 0.2425756 Vali Loss: 0.1969424 Test Loss: 0.4654007
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 132 | Train Loss: 0.2420082 Vali Loss: 0.1968646 Test Loss: 0.4654006
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 132 | Train Loss: 0.2426305 Vali Loss: 0.1970749 Test Loss: 0.4654006
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 132 | Train Loss: 0.2435039 Vali Loss: 0.1968900 Test Loss: 0.4654006
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 132 | Train Loss: 0.2419572 Vali Loss: 0.1968000 Test Loss: 0.4654006
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 132 | Train Loss: 0.2425308 Vali Loss: 0.1970073 Test Loss: 0.4654006
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 132 | Train Loss: 0.2417204 Vali Loss: 0.1969350 Test Loss: 0.4654006
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 132 | Train Loss: 0.2428934 Vali Loss: 0.1970737 Test Loss: 0.4654006
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_NVIDIA_H50_iTransformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.0012280582450330257, mae:0.026852713897824287, rmse:0.035043660551309586, r2:-0.03198862075805664, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0269, RMSE: 0.0350, RÂ²: -0.0320, MAPE: 559747.38%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.653 MB of 0.655 MB uploadedwandb: \ 0.655 MB of 0.655 MB uploadedwandb: | 0.655 MB of 0.655 MB uploadedwandb: / 0.655 MB of 0.655 MB uploadedwandb: - 0.655 MB of 0.655 MB uploadedwandb: \ 0.655 MB of 0.816 MB uploadedwandb: | 0.816 MB of 0.816 MB uploadedwandb: / 0.816 MB of 0.816 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–‚â–â–‚â–‚â–â–‚â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–…â–ƒâ–‚â–„â–„â–‚â–‚â–ƒâ–‚â–â–„â–ƒâ–„â–ƒâ–‚â–â–„â–ƒâ–ƒâ–â–‚â–‚â–ƒâ–‚â–„â–‚â–‚â–ƒâ–ƒâ–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 33
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6362674
wandb:     model/trainable_params 6362674
wandb:                  test/loss 0.4654
wandb:                 train/loss 0.24289
wandb:   val/directional_accuracy 49.65628
wandb:                   val/loss 0.19707
wandb:                    val/mae 0.02685
wandb:                   val/mape 55974737.5
wandb:                    val/mse 0.00123
wandb:                     val/r2 -0.03199
wandb:                   val/rmse 0.03504
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/ojb1jov6
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_194813-ojb1jov6/logs
Completed: NVIDIA H=50

Training: iTransformer on NVIDIA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_195333-mqz0euef
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/mqz0euef
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_NVIDIA_H100Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/mqz0euef
>>>>>>>start training : long_term_forecast_iTransformer_NVIDIA_H100_iTransformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.2965499 Vali Loss: 0.2359933 Test Loss: 0.7524862
Validation loss decreased (inf --> 0.235993).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2761123 Vali Loss: 0.2231081 Test Loss: 0.7191257
Validation loss decreased (0.235993 --> 0.223108).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.2716331 Vali Loss: 0.2189076 Test Loss: 0.7180630
Validation loss decreased (0.223108 --> 0.218908).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2965498997614934, 'val/loss': 0.2359933376312256, 'test/loss': 0.7524862468242646, '_timestamp': 1762883641.647282}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2761123389005661, 'val/loss': 0.22310809493064881, 'test/loss': 0.7191257089376449, '_timestamp': 1762883647.3300807}).
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
Epoch: 4, Steps: 130 | Train Loss: 0.2699174 Vali Loss: 0.2195848 Test Loss: 0.7080348
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2680249 Vali Loss: 0.2251055 Test Loss: 0.7089471
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2671529 Vali Loss: 0.2248544 Test Loss: 0.7087167
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2668545 Vali Loss: 0.2249789 Test Loss: 0.7087179
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2662587 Vali Loss: 0.2179619 Test Loss: 0.7079545
Validation loss decreased (0.218908 --> 0.217962).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2670203 Vali Loss: 0.2250838 Test Loss: 0.7078649
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2665123 Vali Loss: 0.2186043 Test Loss: 0.7079829
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2666045 Vali Loss: 0.2191509 Test Loss: 0.7080338
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2664164 Vali Loss: 0.2211276 Test Loss: 0.7079946
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2663222 Vali Loss: 0.2202919 Test Loss: 0.7079994
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.2664869 Vali Loss: 0.2197817 Test Loss: 0.7080058
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.2663165 Vali Loss: 0.2161372 Test Loss: 0.7079988
Validation loss decreased (0.217962 --> 0.216137).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 130 | Train Loss: 0.2663860 Vali Loss: 0.2204944 Test Loss: 0.7079989
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 130 | Train Loss: 0.2666832 Vali Loss: 0.2211174 Test Loss: 0.7079991
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 130 | Train Loss: 0.2665048 Vali Loss: 0.2193937 Test Loss: 0.7079991
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 130 | Train Loss: 0.2663755 Vali Loss: 0.2234563 Test Loss: 0.7079990
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 130 | Train Loss: 0.2664048 Vali Loss: 0.2175231 Test Loss: 0.7079991
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 130 | Train Loss: 0.2664427 Vali Loss: 0.2236247 Test Loss: 0.7079991
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 130 | Train Loss: 0.2664265 Vali Loss: 0.2202076 Test Loss: 0.7079992
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 130 | Train Loss: 0.2663512 Vali Loss: 0.2162123 Test Loss: 0.7079991
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 130 | Train Loss: 0.2664784 Vali Loss: 0.2237000 Test Loss: 0.7079991
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 130 | Train Loss: 0.2664954 Vali Loss: 0.2231262 Test Loss: 0.7079992
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_NVIDIA_H100_iTransformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.001309285406023264, mae:0.027810323983430862, rmse:0.036184050142765045, r2:-0.01698756217956543, dtw:Not calculated


VAL - MSE: 0.0013, MAE: 0.0278, RMSE: 0.0362, RÂ²: -0.0170, MAPE: 342748.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.718 MB of 0.723 MB uploadedwandb: \ 0.723 MB of 0.723 MB uploadedwandb: | 0.723 MB of 0.723 MB uploadedwandb: / 0.723 MB of 0.723 MB uploadedwandb: - 0.723 MB of 0.882 MB uploadedwandb: \ 0.813 MB of 0.882 MB uploadedwandb: | 0.882 MB of 0.882 MB uploadedwandb: / 0.882 MB of 0.882 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–†â–ƒâ–‚â–‚â–â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–„â–ˆâ–ˆâ–ˆâ–‚â–ˆâ–ƒâ–ƒâ–…â–„â–„â–â–„â–…â–„â–‡â–‚â–‡â–„â–â–‡â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 24
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6388324
wandb:     model/trainable_params 6388324
wandb:                  test/loss 0.708
wandb:                 train/loss 0.2665
wandb:   val/directional_accuracy 50.17316
wandb:                   val/loss 0.22313
wandb:                    val/mae 0.02781
wandb:                   val/mape 34274800.0
wandb:                    val/mse 0.00131
wandb:                     val/r2 -0.01699
wandb:                   val/rmse 0.03618
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/mqz0euef
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_195333-mqz0euef/logs
Completed: NVIDIA H=100

Training: iTransformer on APPLE for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_195758-c6wihl62
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/c6wihl62
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_APPLE_H3Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/c6wihl62
>>>>>>>start training : long_term_forecast_iTransformer_APPLE_H3_iTransformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.2477157 Vali Loss: 0.0966151 Test Loss: 0.1406802
Validation loss decreased (inf --> 0.096615).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2213527 Vali Loss: 0.0897334 Test Loss: 0.1404280
Validation loss decreased (0.096615 --> 0.089733).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.24771568918586673, 'val/loss': 0.09661505650728941, 'test/loss': 0.14068022277206182, '_timestamp': 1762883903.8879445}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2213527016845861, 'val/loss': 0.08973341435194016, 'test/loss': 0.14042795449495316, '_timestamp': 1762883910.1117852}).
Epoch: 3, Steps: 133 | Train Loss: 0.2025960 Vali Loss: 0.0890430 Test Loss: 0.1323234
Validation loss decreased (0.089733 --> 0.089043).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1915375 Vali Loss: 0.0875871 Test Loss: 0.1296988
Validation loss decreased (0.089043 --> 0.087587).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1835458 Vali Loss: 0.0895213 Test Loss: 0.1309942
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1802533 Vali Loss: 0.0904250 Test Loss: 0.1310457
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1783175 Vali Loss: 0.0870764 Test Loss: 0.1309542
Validation loss decreased (0.087587 --> 0.087076).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1764793 Vali Loss: 0.0890990 Test Loss: 0.1309205
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1766802 Vali Loss: 0.0871004 Test Loss: 0.1309447
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1752047 Vali Loss: 0.0890084 Test Loss: 0.1309459
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1750441 Vali Loss: 0.0899606 Test Loss: 0.1309577
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1750416 Vali Loss: 0.0896501 Test Loss: 0.1309572
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1756184 Vali Loss: 0.0872359 Test Loss: 0.1309586
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1751754 Vali Loss: 0.0894840 Test Loss: 0.1309570
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1753761 Vali Loss: 0.0874473 Test Loss: 0.1309574
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1758920 Vali Loss: 0.0874908 Test Loss: 0.1309581
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1747488 Vali Loss: 0.0857336 Test Loss: 0.1309585
Validation loss decreased (0.087076 --> 0.085734).  Saving model ...
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1759068 Vali Loss: 0.0865279 Test Loss: 0.1309585
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1760342 Vali Loss: 0.0897645 Test Loss: 0.1309583
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1755490 Vali Loss: 0.0880551 Test Loss: 0.1309585
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1759256 Vali Loss: 0.0878228 Test Loss: 0.1309583
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1751615 Vali Loss: 0.0876534 Test Loss: 0.1309583
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1743625 Vali Loss: 0.0867998 Test Loss: 0.1309583
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.1757722 Vali Loss: 0.0855135 Test Loss: 0.1309583
Validation loss decreased (0.085734 --> 0.085514).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.1766901 Vali Loss: 0.0864914 Test Loss: 0.1309583
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.1763966 Vali Loss: 0.0892431 Test Loss: 0.1309584
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.1755123 Vali Loss: 0.0890501 Test Loss: 0.1309584
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.1749748 Vali Loss: 0.0867577 Test Loss: 0.1309584
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.1758886 Vali Loss: 0.0883857 Test Loss: 0.1309584
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 133 | Train Loss: 0.1755874 Vali Loss: 0.0902465 Test Loss: 0.1309584
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 133 | Train Loss: 0.1753887 Vali Loss: 0.0891193 Test Loss: 0.1309584
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 133 | Train Loss: 0.1748369 Vali Loss: 0.0875349 Test Loss: 0.1309584
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 133 | Train Loss: 0.1755376 Vali Loss: 0.0880171 Test Loss: 0.1309584
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 133 | Train Loss: 0.1751444 Vali Loss: 0.0863134 Test Loss: 0.1309584
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_APPLE_H3_iTransformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.0002226001670351252, mae:0.010904060676693916, rmse:0.014919791370630264, r2:-0.1133195161819458, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0109, RMSE: 0.0149, RÂ²: -0.1133, MAPE: 416200.59%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.486 MB of 0.486 MB uploadedwandb: \ 0.486 MB of 0.486 MB uploadedwandb: | 0.486 MB of 0.486 MB uploadedwandb: / 0.486 MB of 0.486 MB uploadedwandb: - 0.486 MB of 0.486 MB uploadedwandb: \ 0.486 MB of 0.486 MB uploadedwandb: | 0.486 MB of 0.486 MB uploadedwandb: / 0.486 MB of 0.486 MB uploadedwandb: - 0.486 MB of 0.486 MB uploadedwandb: \ 0.486 MB of 0.486 MB uploadedwandb: | 0.591 MB of 0.751 MB uploaded (0.002 MB deduped)wandb: / 0.751 MB of 0.751 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–„â–…â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–„â–‡â–ˆâ–ƒâ–†â–ƒâ–†â–‡â–‡â–ƒâ–‡â–„â–„â–â–‚â–‡â–…â–„â–„â–ƒâ–â–‚â–†â–†â–ƒâ–…â–ˆâ–†â–„â–…â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 33
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6338563
wandb:     model/trainable_params 6338563
wandb:                  test/loss 0.13096
wandb:                 train/loss 0.17514
wandb:   val/directional_accuracy 44.93671
wandb:                   val/loss 0.08631
wandb:                    val/mae 0.0109
wandb:                   val/mape 41620059.375
wandb:                    val/mse 0.00022
wandb:                     val/r2 -0.11332
wandb:                   val/rmse 0.01492
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/c6wihl62
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_195758-c6wihl62/logs
Completed: APPLE H=3

Training: iTransformer on APPLE for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_200234-3b5pe5ai
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/3b5pe5ai
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_APPLE_H5Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/3b5pe5ai
>>>>>>>start training : long_term_forecast_iTransformer_APPLE_H5_iTransformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.2469638 Vali Loss: 0.0930470 Test Loss: 0.1392085
Validation loss decreased (inf --> 0.093047).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2242605 Vali Loss: 0.0924441 Test Loss: 0.1368920
Validation loss decreased (0.093047 --> 0.092444).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.24696382869006997, 'val/loss': 0.09304699208587408, 'test/loss': 0.139208503998816, '_timestamp': 1762884178.3278868}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2242605268843192, 'val/loss': 0.09244409389793873, 'test/loss': 0.13689201325178146, '_timestamp': 1762884184.3736522}).
Epoch: 3, Steps: 133 | Train Loss: 0.2073322 Vali Loss: 0.0892219 Test Loss: 0.1333819
Validation loss decreased (0.092444 --> 0.089222).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1967727 Vali Loss: 0.0897305 Test Loss: 0.1310862
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1896048 Vali Loss: 0.0888203 Test Loss: 0.1313944
Validation loss decreased (0.089222 --> 0.088820).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1865634 Vali Loss: 0.0898265 Test Loss: 0.1315031
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1845064 Vali Loss: 0.0912595 Test Loss: 0.1315172
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1843645 Vali Loss: 0.0870277 Test Loss: 0.1315508
Validation loss decreased (0.088820 --> 0.087028).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1843090 Vali Loss: 0.0896593 Test Loss: 0.1315507
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1842798 Vali Loss: 0.0864968 Test Loss: 0.1315647
Validation loss decreased (0.087028 --> 0.086497).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1834943 Vali Loss: 0.0890654 Test Loss: 0.1315615
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1834130 Vali Loss: 0.0881468 Test Loss: 0.1315629
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1845940 Vali Loss: 0.0868264 Test Loss: 0.1315625
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1834001 Vali Loss: 0.0882006 Test Loss: 0.1315633
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1833143 Vali Loss: 0.0882006 Test Loss: 0.1315640
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1836670 Vali Loss: 0.0864038 Test Loss: 0.1315641
Validation loss decreased (0.086497 --> 0.086404).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1844348 Vali Loss: 0.0873259 Test Loss: 0.1315643
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1833205 Vali Loss: 0.0849682 Test Loss: 0.1315640
Validation loss decreased (0.086404 --> 0.084968).  Saving model ...
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1839006 Vali Loss: 0.0856054 Test Loss: 0.1315643
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1836107 Vali Loss: 0.0853906 Test Loss: 0.1315641
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1834617 Vali Loss: 0.0900865 Test Loss: 0.1315641
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1826702 Vali Loss: 0.0878318 Test Loss: 0.1315641
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1838496 Vali Loss: 0.0879140 Test Loss: 0.1315641
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.1839297 Vali Loss: 0.0878189 Test Loss: 0.1315641
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.1836819 Vali Loss: 0.0892210 Test Loss: 0.1315641
EarlyStopping counter: 7 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.1839085 Vali Loss: 0.0870855 Test Loss: 0.1315641
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.1838791 Vali Loss: 0.0867777 Test Loss: 0.1315641
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.1833268 Vali Loss: 0.0879104 Test Loss: 0.1315641
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_APPLE_H5_iTransformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.0002195507986471057, mae:0.010822916403412819, rmse:0.014817246235907078, r2:-0.09305620193481445, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0108, RMSE: 0.0148, RÂ²: -0.0931, MAPE: 221016.53%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.515 MB of 0.516 MB uploadedwandb: \ 0.516 MB of 0.516 MB uploadedwandb: | 0.516 MB of 0.516 MB uploadedwandb: / 0.516 MB of 0.516 MB uploadedwandb: - 0.516 MB of 0.516 MB uploadedwandb: \ 0.516 MB of 0.516 MB uploadedwandb: | 0.516 MB of 0.675 MB uploadedwandb: / 0.675 MB of 0.675 MB uploadedwandb: - 0.675 MB of 0.675 MB uploadedwandb: \ 0.675 MB of 0.675 MB uploadedwandb: | 0.675 MB of 0.675 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–†â–…â–†â–ˆâ–ƒâ–†â–ƒâ–†â–…â–ƒâ–…â–…â–ƒâ–„â–â–‚â–â–‡â–„â–„â–„â–†â–ƒâ–ƒâ–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 27
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6339589
wandb:     model/trainable_params 6339589
wandb:                  test/loss 0.13156
wandb:                 train/loss 0.18333
wandb:   val/directional_accuracy 45.21277
wandb:                   val/loss 0.08791
wandb:                    val/mae 0.01082
wandb:                   val/mape 22101653.125
wandb:                    val/mse 0.00022
wandb:                     val/r2 -0.09306
wandb:                   val/rmse 0.01482
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/3b5pe5ai
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_200234-3b5pe5ai/logs
Completed: APPLE H=5

Training: iTransformer on APPLE for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_200646-jg9y4in7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/jg9y4in7
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_APPLE_H10Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/jg9y4in7
>>>>>>>start training : long_term_forecast_iTransformer_APPLE_H10_iTransformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2496254 Vali Loss: 0.0912007 Test Loss: 0.1368925
Validation loss decreased (inf --> 0.091201).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2270842 Vali Loss: 0.0882168 Test Loss: 0.1352058
Validation loss decreased (0.091201 --> 0.088217).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2136081 Vali Loss: 0.0885972 Test Loss: 0.1331937
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2496254338805837, 'val/loss': 0.09120071586221457, 'test/loss': 0.13689249847084284, '_timestamp': 1762884435.253568}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2270842025378593, 'val/loss': 0.08821684122085571, 'test/loss': 0.1352057969197631, '_timestamp': 1762884441.1754012}).
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
Epoch: 4, Steps: 133 | Train Loss: 0.2061527 Vali Loss: 0.0893111 Test Loss: 0.1316385
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2013008 Vali Loss: 0.0879340 Test Loss: 0.1319883
Validation loss decreased (0.088217 --> 0.087934).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1990770 Vali Loss: 0.0858240 Test Loss: 0.1320861
Validation loss decreased (0.087934 --> 0.085824).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1973154 Vali Loss: 0.0873064 Test Loss: 0.1320315
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1966887 Vali Loss: 0.0886416 Test Loss: 0.1320908
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1962908 Vali Loss: 0.0842449 Test Loss: 0.1321121
Validation loss decreased (0.085824 --> 0.084245).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1956918 Vali Loss: 0.0871888 Test Loss: 0.1321235
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1965822 Vali Loss: 0.0852350 Test Loss: 0.1321211
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1962037 Vali Loss: 0.0871250 Test Loss: 0.1321226
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1963725 Vali Loss: 0.0836508 Test Loss: 0.1321234
Validation loss decreased (0.084245 --> 0.083651).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1961817 Vali Loss: 0.0892323 Test Loss: 0.1321238
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1963788 Vali Loss: 0.0869932 Test Loss: 0.1321237
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1957244 Vali Loss: 0.0871245 Test Loss: 0.1321239
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1960737 Vali Loss: 0.0854036 Test Loss: 0.1321240
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1958712 Vali Loss: 0.0866875 Test Loss: 0.1321239
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1967927 Vali Loss: 0.0904376 Test Loss: 0.1321239
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1956066 Vali Loss: 0.0908302 Test Loss: 0.1321239
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1960690 Vali Loss: 0.0865579 Test Loss: 0.1321239
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1961189 Vali Loss: 0.0897329 Test Loss: 0.1321239
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1955195 Vali Loss: 0.0856586 Test Loss: 0.1321239
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_APPLE_H10_iTransformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.00022052008716855198, mae:0.010844172909855843, rmse:0.014849918894469738, r2:-0.08916699886322021, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0108, RMSE: 0.0148, RÂ²: -0.0892, MAPE: 445675.84%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.567 MB of 0.568 MB uploadedwandb: \ 0.568 MB of 0.568 MB uploadedwandb: | 0.568 MB of 0.568 MB uploadedwandb: / 0.568 MB of 0.568 MB uploadedwandb: - 0.568 MB of 0.726 MB uploadedwandb: \ 0.726 MB of 0.726 MB uploadedwandb: | 0.726 MB of 0.726 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–‡â–…â–ƒâ–…â–†â–‚â–„â–ƒâ–„â–â–†â–„â–„â–ƒâ–„â–ˆâ–ˆâ–„â–‡â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6342154
wandb:     model/trainable_params 6342154
wandb:                  test/loss 0.13212
wandb:                 train/loss 0.19552
wandb:   val/directional_accuracy 45.99034
wandb:                   val/loss 0.08566
wandb:                    val/mae 0.01084
wandb:                   val/mape 44567584.375
wandb:                    val/mse 0.00022
wandb:                     val/r2 -0.08917
wandb:                   val/rmse 0.01485
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/jg9y4in7
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_200646-jg9y4in7/logs
Completed: APPLE H=10

Training: iTransformer on APPLE for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_201154-m3lyxvbg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/m3lyxvbg
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_APPLE_H22Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/m3lyxvbg
>>>>>>>start training : long_term_forecast_iTransformer_APPLE_H22_iTransformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2540406 Vali Loss: 0.0887726 Test Loss: 0.1374866
Validation loss decreased (inf --> 0.088773).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2359366 Vali Loss: 0.0879924 Test Loss: 0.1360977
Validation loss decreased (0.088773 --> 0.087992).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.25404058600013907, 'val/loss': 0.08877264601843697, 'test/loss': 0.13748660683631897, '_timestamp': 1762884741.2048504}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2359365759925409, 'val/loss': 0.08799235948494502, 'test/loss': 0.1360976876957076, '_timestamp': 1762884747.4569864}).
Epoch: 3, Steps: 132 | Train Loss: 0.2250386 Vali Loss: 0.0865322 Test Loss: 0.1344791
Validation loss decreased (0.087992 --> 0.086532).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2180670 Vali Loss: 0.0866735 Test Loss: 0.1335660
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2147054 Vali Loss: 0.0865757 Test Loss: 0.1339636
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2130259 Vali Loss: 0.0865798 Test Loss: 0.1339653
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2122969 Vali Loss: 0.0866325 Test Loss: 0.1339517
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2113697 Vali Loss: 0.0867929 Test Loss: 0.1339443
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2114526 Vali Loss: 0.0864619 Test Loss: 0.1339521
Validation loss decreased (0.086532 --> 0.086462).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2111607 Vali Loss: 0.0862914 Test Loss: 0.1339514
Validation loss decreased (0.086462 --> 0.086291).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2114024 Vali Loss: 0.0866406 Test Loss: 0.1339470
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2112888 Vali Loss: 0.0868025 Test Loss: 0.1339490
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2111981 Vali Loss: 0.0865090 Test Loss: 0.1339495
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2112612 Vali Loss: 0.0864063 Test Loss: 0.1339501
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2110862 Vali Loss: 0.0867338 Test Loss: 0.1339499
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2114763 Vali Loss: 0.0864575 Test Loss: 0.1339499
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2113172 Vali Loss: 0.0866420 Test Loss: 0.1339502
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2114050 Vali Loss: 0.0866102 Test Loss: 0.1339502
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2111123 Vali Loss: 0.0864770 Test Loss: 0.1339502
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2112449 Vali Loss: 0.0865317 Test Loss: 0.1339501
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_APPLE_H22_iTransformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.00022250344045460224, mae:0.010827557183802128, rmse:0.014916549436748028, r2:-0.07228255271911621, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0108, RMSE: 0.0149, RÂ²: -0.0723, MAPE: 487886.19%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.657 MB of 0.658 MB uploadedwandb: \ 0.657 MB of 0.658 MB uploadedwandb: | 0.657 MB of 0.658 MB uploadedwandb: / 0.658 MB of 0.658 MB uploadedwandb: - 0.658 MB of 0.658 MB uploadedwandb: \ 0.658 MB of 0.816 MB uploadedwandb: | 0.658 MB of 0.816 MB uploadedwandb: / 0.816 MB of 0.816 MB uploadedwandb: - 0.816 MB of 0.816 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–†â–…â–…â–†â–ˆâ–ƒâ–â–†â–ˆâ–„â–ƒâ–‡â–ƒâ–†â–…â–„â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6348310
wandb:     model/trainable_params 6348310
wandb:                  test/loss 0.13395
wandb:                 train/loss 0.21124
wandb:   val/directional_accuracy 46.89821
wandb:                   val/loss 0.08653
wandb:                    val/mae 0.01083
wandb:                   val/mape 48788618.75
wandb:                    val/mse 0.00022
wandb:                     val/r2 -0.07228
wandb:                   val/rmse 0.01492
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/m3lyxvbg
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_201154-m3lyxvbg/logs
Completed: APPLE H=22

Training: iTransformer on APPLE for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_201523-03kwdd9h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/03kwdd9h
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_APPLE_H50Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/03kwdd9h
>>>>>>>start training : long_term_forecast_iTransformer_APPLE_H50_iTransformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.2676767 Vali Loss: 0.0902747 Test Loss: 0.1496331
Validation loss decreased (inf --> 0.090275).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2489401 Vali Loss: 0.0895241 Test Loss: 0.1487906
Validation loss decreased (0.090275 --> 0.089524).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
Epoch: 3, Steps: 132 | Train Loss: 0.2399665 Vali Loss: 0.0891054 Test Loss: 0.1483588
Validation loss decreased (0.089524 --> 0.089105).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2676767419572129, 'val/loss': 0.09027469158172607, 'test/loss': 0.14963306983311972, '_timestamp': 1762884950.1326754}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24894011635897736, 'val/loss': 0.08952410519123077, 'test/loss': 0.14879061778386435, '_timestamp': 1762884955.7787151}).
Epoch: 4, Steps: 132 | Train Loss: 0.2359176 Vali Loss: 0.0892946 Test Loss: 0.1486556
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2337964 Vali Loss: 0.0891323 Test Loss: 0.1489692
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2332823 Vali Loss: 0.0894278 Test Loss: 0.1492473
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2321270 Vali Loss: 0.0895156 Test Loss: 0.1493364
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2370110 Vali Loss: 0.0894878 Test Loss: 0.1492728
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2329796 Vali Loss: 0.0895126 Test Loss: 0.1493466
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2309652 Vali Loss: 0.0895382 Test Loss: 0.1493238
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2309798 Vali Loss: 0.0895031 Test Loss: 0.1493154
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2325675 Vali Loss: 0.0894987 Test Loss: 0.1493135
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2311273 Vali Loss: 0.0895651 Test Loss: 0.1493188
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_APPLE_H50_iTransformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.00023328838869929314, mae:0.01101688388735056, rmse:0.01527378149330616, r2:-0.05144059658050537, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0110, RMSE: 0.0153, RÂ²: -0.0514, MAPE: 573280.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.725 MB of 0.728 MB uploadedwandb: \ 0.725 MB of 0.728 MB uploadedwandb: | 0.728 MB of 0.728 MB uploadedwandb: / 0.728 MB of 0.728 MB uploadedwandb: - 0.728 MB of 0.728 MB uploadedwandb: \ 0.728 MB of 0.885 MB uploadedwandb: | 0.885 MB of 0.885 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ƒâ–…â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–†â–ƒâ–â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–„â–â–†â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6362674
wandb:     model/trainable_params 6362674
wandb:                  test/loss 0.14932
wandb:                 train/loss 0.23113
wandb:   val/directional_accuracy 49.33405
wandb:                   val/loss 0.08957
wandb:                    val/mae 0.01102
wandb:                   val/mape 57328000.0
wandb:                    val/mse 0.00023
wandb:                     val/r2 -0.05144
wandb:                   val/rmse 0.01527
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/03kwdd9h
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_201523-03kwdd9h/logs
Completed: APPLE H=50

Training: iTransformer on APPLE for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_201821-rrjl80n0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/rrjl80n0
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_APPLE_H100Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/rrjl80n0
>>>>>>>start training : long_term_forecast_iTransformer_APPLE_H100_iTransformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.2878854 Vali Loss: 0.0968229 Test Loss: 0.1633588
Validation loss decreased (inf --> 0.096823).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2669303 Vali Loss: 0.0955897 Test Loss: 0.1642313
Validation loss decreased (0.096823 --> 0.095590).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.2613730 Vali Loss: 0.0945332 Test Loss: 0.1625588
Validation loss decreased (0.095590 --> 0.094533).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2878853548031587, 'val/loss': 0.09682293087244034, 'test/loss': 0.16335884034633635, '_timestamp': 1762885125.7263877}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26693030217519176, 'val/loss': 0.09558974504470825, 'test/loss': 0.16423129439353942, '_timestamp': 1762885131.5330381}).
Epoch: 4, Steps: 130 | Train Loss: 0.2594913 Vali Loss: 0.0957445 Test Loss: 0.1640125
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2573762 Vali Loss: 0.0963757 Test Loss: 0.1648840
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2563600 Vali Loss: 0.0968078 Test Loss: 0.1653508
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2562957 Vali Loss: 0.0967241 Test Loss: 0.1651593
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2555523 Vali Loss: 0.0957155 Test Loss: 0.1653278
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2560424 Vali Loss: 0.0962405 Test Loss: 0.1654140
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2557447 Vali Loss: 0.0955691 Test Loss: 0.1654305
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2558612 Vali Loss: 0.0954122 Test Loss: 0.1653957
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2558558 Vali Loss: 0.0960446 Test Loss: 0.1653783
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2557805 Vali Loss: 0.0961696 Test Loss: 0.1653794
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_APPLE_H100_iTransformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.00024498713901266456, mae:0.011285796761512756, rmse:0.015652064234018326, r2:-0.0375896692276001, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0113, RMSE: 0.0157, RÂ²: -0.0376, MAPE: 743179.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.740 MB of 0.745 MB uploadedwandb: \ 0.740 MB of 0.745 MB uploadedwandb: | 0.740 MB of 0.745 MB uploadedwandb: / 0.740 MB of 0.745 MB uploadedwandb: - 0.745 MB of 0.745 MB uploadedwandb: \ 0.745 MB of 0.745 MB uploadedwandb: | 0.745 MB of 0.902 MB uploadedwandb: / 0.902 MB of 0.902 MB uploadedwandb: - 0.902 MB of 0.902 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–…â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–†â–ƒâ–‚â–‚â–â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–…â–‡â–ˆâ–ˆâ–…â–†â–„â–„â–†â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6388324
wandb:     model/trainable_params 6388324
wandb:                  test/loss 0.16538
wandb:                 train/loss 0.25578
wandb:   val/directional_accuracy 50.88023
wandb:                   val/loss 0.09617
wandb:                    val/mae 0.01129
wandb:                   val/mape 74317975.0
wandb:                    val/mse 0.00024
wandb:                     val/r2 -0.03759
wandb:                   val/rmse 0.01565
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/rrjl80n0
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_201821-rrjl80n0/logs
Completed: APPLE H=100

Training: iTransformer on SP500 for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_202231-5tv9on2u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/5tv9on2u
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_SP500_H3Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/5tv9on2u
>>>>>>>start training : long_term_forecast_iTransformer_SP500_H3_iTransformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 3
============================================================
train 4241
val 237
test 238
Epoch: 1, Steps: 133 | Train Loss: 0.2169187 Vali Loss: 0.0821169 Test Loss: 0.0896225
Validation loss decreased (inf --> 0.082117).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.1859270 Vali Loss: 0.0766587 Test Loss: 0.0854207
Validation loss decreased (0.082117 --> 0.076659).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.1566724 Vali Loss: 0.0713075 Test Loss: 0.0792095
Validation loss decreased (0.076659 --> 0.071308).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2169186840380045, 'val/loss': 0.08211686089634895, 'test/loss': 0.08962254412472248, '_timestamp': 1762885379.6310945}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.1859269728113834, 'val/loss': 0.07665868662297726, 'test/loss': 0.08542068535462022, '_timestamp': 1762885385.331358}).
Epoch: 4, Steps: 133 | Train Loss: 0.1428438 Vali Loss: 0.0712719 Test Loss: 0.0769252
Validation loss decreased (0.071308 --> 0.071272).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1343395 Vali Loss: 0.0691827 Test Loss: 0.0770592
Validation loss decreased (0.071272 --> 0.069183).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1295734 Vali Loss: 0.0685369 Test Loss: 0.0770920
Validation loss decreased (0.069183 --> 0.068537).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1266141 Vali Loss: 0.0680937 Test Loss: 0.0770471
Validation loss decreased (0.068537 --> 0.068094).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1270741 Vali Loss: 0.0681218 Test Loss: 0.0769595
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1276190 Vali Loss: 0.0686662 Test Loss: 0.0769943
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1264105 Vali Loss: 0.0687919 Test Loss: 0.0770091
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1260964 Vali Loss: 0.0680943 Test Loss: 0.0770173
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1240735 Vali Loss: 0.0678847 Test Loss: 0.0770185
Validation loss decreased (0.068094 --> 0.067885).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1257113 Vali Loss: 0.0684918 Test Loss: 0.0770180
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1245882 Vali Loss: 0.0672514 Test Loss: 0.0770166
Validation loss decreased (0.067885 --> 0.067251).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1254691 Vali Loss: 0.0687874 Test Loss: 0.0770160
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1256227 Vali Loss: 0.0674034 Test Loss: 0.0770166
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1248560 Vali Loss: 0.0685125 Test Loss: 0.0770163
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1248735 Vali Loss: 0.0670369 Test Loss: 0.0770163
Validation loss decreased (0.067251 --> 0.067037).  Saving model ...
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1249443 Vali Loss: 0.0680149 Test Loss: 0.0770165
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1244050 Vali Loss: 0.0663402 Test Loss: 0.0770163
Validation loss decreased (0.067037 --> 0.066340).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1256879 Vali Loss: 0.0693067 Test Loss: 0.0770164
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1254787 Vali Loss: 0.0688671 Test Loss: 0.0770164
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1254382 Vali Loss: 0.0680100 Test Loss: 0.0770164
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.1251767 Vali Loss: 0.0680814 Test Loss: 0.0770164
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.1256093 Vali Loss: 0.0677879 Test Loss: 0.0770164
EarlyStopping counter: 5 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.1263502 Vali Loss: 0.0687819 Test Loss: 0.0770164
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.1250965 Vali Loss: 0.0685763 Test Loss: 0.0770164
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.1245349 Vali Loss: 0.0692154 Test Loss: 0.0770164
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.1251258 Vali Loss: 0.0685798 Test Loss: 0.0770164
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 133 | Train Loss: 0.1253383 Vali Loss: 0.0694002 Test Loss: 0.0770164
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_SP500_H3_iTransformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 238
test shape: (238, 3, 1) (238, 3, 1)
test shape: (238, 3, 1) (238, 3, 1)


	mse:6.99837037245743e-05, mae:0.006252633407711983, rmse:0.008365626446902752, r2:-0.07648599147796631, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0063, RMSE: 0.0084, RÂ²: -0.0765, MAPE: 5.35%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.516 MB of 0.516 MB uploadedwandb: \ 0.516 MB of 0.516 MB uploadedwandb: | 0.516 MB of 0.516 MB uploadedwandb: / 0.516 MB of 0.516 MB uploadedwandb: - 0.516 MB of 0.516 MB uploadedwandb: \ 0.516 MB of 0.516 MB uploadedwandb: | 0.516 MB of 0.516 MB uploadedwandb: / 0.622 MB of 0.781 MB uploaded (0.002 MB deduped)wandb: - 0.781 MB of 0.781 MB uploaded (0.002 MB deduped)wandb: \ 0.781 MB of 0.781 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ˆâ–…â–„â–ƒâ–„â–„â–„â–ƒâ–ƒâ–„â–‚â–„â–‚â–„â–‚â–ƒâ–â–…â–…â–ƒâ–ƒâ–ƒâ–„â–„â–…â–„â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 29
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6338563
wandb:     model/trainable_params 6338563
wandb:                  test/loss 0.07702
wandb:                 train/loss 0.12534
wandb:   val/directional_accuracy 46.84874
wandb:                   val/loss 0.0694
wandb:                    val/mae 0.00625
wandb:                   val/mape 534.81789
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.07649
wandb:                   val/rmse 0.00837
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/5tv9on2u
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_202231-5tv9on2u/logs
Completed: SP500 H=3

Training: iTransformer on SP500 for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_202710-j4k64yc2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/j4k64yc2
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_SP500_H5Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/j4k64yc2
>>>>>>>start training : long_term_forecast_iTransformer_SP500_H5_iTransformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 5
============================================================
train 4239
val 235
test 236
Epoch: 1, Steps: 133 | Train Loss: 0.2119004 Vali Loss: 0.0789706 Test Loss: 0.0834702
Validation loss decreased (inf --> 0.078971).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.1847458 Vali Loss: 0.0757470 Test Loss: 0.0844297
Validation loss decreased (0.078971 --> 0.075747).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.21190039863935986, 'val/loss': 0.07897062785923481, 'test/loss': 0.08347019040957093, '_timestamp': 1762885656.3832405}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.18474582201780232, 'val/loss': 0.07574695302173495, 'test/loss': 0.08442972740158439, '_timestamp': 1762885662.4725144}).
Epoch: 3, Steps: 133 | Train Loss: 0.1615261 Vali Loss: 0.0713271 Test Loss: 0.0796907
Validation loss decreased (0.075747 --> 0.071327).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1478156 Vali Loss: 0.0687781 Test Loss: 0.0786188
Validation loss decreased (0.071327 --> 0.068778).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1403417 Vali Loss: 0.0695666 Test Loss: 0.0779402
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1377501 Vali Loss: 0.0670635 Test Loss: 0.0779881
Validation loss decreased (0.068778 --> 0.067064).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1361068 Vali Loss: 0.0698898 Test Loss: 0.0779733
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1341276 Vali Loss: 0.0695147 Test Loss: 0.0779602
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1336115 Vali Loss: 0.0687990 Test Loss: 0.0779898
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1335856 Vali Loss: 0.0704220 Test Loss: 0.0779709
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1332247 Vali Loss: 0.0695221 Test Loss: 0.0779723
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1322914 Vali Loss: 0.0694446 Test Loss: 0.0779730
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1340026 Vali Loss: 0.0681528 Test Loss: 0.0779715
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1329251 Vali Loss: 0.0692087 Test Loss: 0.0779717
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1332765 Vali Loss: 0.0708950 Test Loss: 0.0779715
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1327694 Vali Loss: 0.0693680 Test Loss: 0.0779717
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_SP500_H5_iTransformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
test shape: (236, 5, 1) (236, 5, 1)
test shape: (236, 5, 1) (236, 5, 1)


	mse:6.980150646995753e-05, mae:0.006255507934838533, rmse:0.00835472997277975, r2:-0.07450926303863525, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0063, RMSE: 0.0084, RÂ²: -0.0745, MAPE: 5.16%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.544 MB of 0.544 MB uploadedwandb: \ 0.544 MB of 0.544 MB uploadedwandb: | 0.544 MB of 0.544 MB uploadedwandb: / 0.544 MB of 0.544 MB uploadedwandb: - 0.544 MB of 0.544 MB uploadedwandb: \ 0.544 MB of 0.544 MB uploadedwandb: | 0.544 MB of 0.701 MB uploadedwandb: / 0.701 MB of 0.701 MB uploadedwandb: - 0.701 MB of 0.701 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–…â–â–†â–…â–„â–‡â–…â–…â–ƒâ–…â–‡â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6339589
wandb:     model/trainable_params 6339589
wandb:                  test/loss 0.07797
wandb:                 train/loss 0.13277
wandb:   val/directional_accuracy 47.98729
wandb:                   val/loss 0.06937
wandb:                    val/mae 0.00626
wandb:                   val/mape 515.71794
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.07451
wandb:                   val/rmse 0.00835
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/j4k64yc2
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_202710-j4k64yc2/logs
Completed: SP500 H=5

Training: iTransformer on SP500 for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_203035-kwcb614z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/kwcb614z
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_SP500_H10Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/kwcb614z
>>>>>>>start training : long_term_forecast_iTransformer_SP500_H10_iTransformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 10
============================================================
train 4234
val 230
test 231
Epoch: 1, Steps: 133 | Train Loss: 0.2050618 Vali Loss: 0.0803825 Test Loss: 0.0871320
Validation loss decreased (inf --> 0.080382).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.1843443 Vali Loss: 0.0786982 Test Loss: 0.0841011
Validation loss decreased (0.080382 --> 0.078698).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.1640462 Vali Loss: 0.0717135 Test Loss: 0.0815188
Validation loss decreased (0.078698 --> 0.071713).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2050617737765599, 'val/loss': 0.08038248494267464, 'test/loss': 0.08713196264579892, '_timestamp': 1762885856.8721256}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.18434430882894903, 'val/loss': 0.07869823090732098, 'test/loss': 0.08410111116245389, '_timestamp': 1762885861.915733}).
Epoch: 4, Steps: 133 | Train Loss: 0.1538793 Vali Loss: 0.0733892 Test Loss: 0.0807272
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1478962 Vali Loss: 0.0728411 Test Loss: 0.0804684
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1460092 Vali Loss: 0.0723065 Test Loss: 0.0804587
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1438546 Vali Loss: 0.0719379 Test Loss: 0.0804272
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1436395 Vali Loss: 0.0707200 Test Loss: 0.0804280
Validation loss decreased (0.071713 --> 0.070720).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1439110 Vali Loss: 0.0690004 Test Loss: 0.0804311
Validation loss decreased (0.070720 --> 0.069000).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1423320 Vali Loss: 0.0709796 Test Loss: 0.0804288
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1423390 Vali Loss: 0.0706540 Test Loss: 0.0804278
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1430577 Vali Loss: 0.0691166 Test Loss: 0.0804265
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1424733 Vali Loss: 0.0717017 Test Loss: 0.0804257
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1427598 Vali Loss: 0.0723604 Test Loss: 0.0804254
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1426034 Vali Loss: 0.0702669 Test Loss: 0.0804253
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1426510 Vali Loss: 0.0716929 Test Loss: 0.0804254
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1417951 Vali Loss: 0.0736898 Test Loss: 0.0804254
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1419834 Vali Loss: 0.0708508 Test Loss: 0.0804254
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1430581 Vali Loss: 0.0700938 Test Loss: 0.0804254
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_SP500_H10_iTransformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 231
test shape: (231, 10, 1) (231, 10, 1)
test shape: (231, 10, 1) (231, 10, 1)


	mse:6.98659714544192e-05, mae:0.006256133317947388, rmse:0.008358586579561234, r2:-0.0748147964477539, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0063, RMSE: 0.0084, RÂ²: -0.0748, MAPE: 4.33%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.563 MB of 0.564 MB uploadedwandb: \ 0.563 MB of 0.564 MB uploadedwandb: | 0.564 MB of 0.564 MB uploadedwandb: / 0.564 MB of 0.564 MB uploadedwandb: - 0.564 MB of 0.564 MB uploadedwandb: \ 0.564 MB of 0.722 MB uploadedwandb: | 0.722 MB of 0.722 MB uploadedwandb: / 0.722 MB of 0.722 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–ˆâ–‡â–†â–…â–„â–â–„â–ƒâ–â–…â–†â–ƒâ–…â–ˆâ–„â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 18
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6342154
wandb:     model/trainable_params 6342154
wandb:                  test/loss 0.08043
wandb:                 train/loss 0.14306
wandb:   val/directional_accuracy 47.90765
wandb:                   val/loss 0.07009
wandb:                    val/mae 0.00626
wandb:                   val/mape 432.59025
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.07481
wandb:                   val/rmse 0.00836
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/kwcb614z
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_203035-kwcb614z/logs
Completed: SP500 H=10

Training: iTransformer on SP500 for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_203423-q6sa9gpx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/q6sa9gpx
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_SP500_H22Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/q6sa9gpx
>>>>>>>start training : long_term_forecast_iTransformer_SP500_H22_iTransformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 22
============================================================
train 4222
val 218
test 219
Epoch: 1, Steps: 132 | Train Loss: 0.2052636 Vali Loss: 0.0767814 Test Loss: 0.0759409
Validation loss decreased (inf --> 0.076781).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.1842300 Vali Loss: 0.0759721 Test Loss: 0.0746407
Validation loss decreased (0.076781 --> 0.075972).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2052635888032841, 'val/loss': 0.07678137719631195, 'test/loss': 0.07594093041760581, '_timestamp': 1762886091.8052373}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.18423004550012675, 'val/loss': 0.0759721313204084, 'test/loss': 0.07464074077350753, '_timestamp': 1762886097.4024441}).
Epoch: 3, Steps: 132 | Train Loss: 0.1716085 Vali Loss: 0.0747149 Test Loss: 0.0734680
Validation loss decreased (0.075972 --> 0.074715).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.1640850 Vali Loss: 0.0743615 Test Loss: 0.0731393
Validation loss decreased (0.074715 --> 0.074361).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.1599951 Vali Loss: 0.0741175 Test Loss: 0.0728665
Validation loss decreased (0.074361 --> 0.074117).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.1580677 Vali Loss: 0.0739333 Test Loss: 0.0726595
Validation loss decreased (0.074117 --> 0.073933).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.1574824 Vali Loss: 0.0738105 Test Loss: 0.0726180
Validation loss decreased (0.073933 --> 0.073811).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.1568166 Vali Loss: 0.0739085 Test Loss: 0.0726059
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.1564151 Vali Loss: 0.0737307 Test Loss: 0.0726059
Validation loss decreased (0.073811 --> 0.073731).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.1565377 Vali Loss: 0.0740167 Test Loss: 0.0726029
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.1566161 Vali Loss: 0.0739388 Test Loss: 0.0726037
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.1562755 Vali Loss: 0.0735483 Test Loss: 0.0726030
Validation loss decreased (0.073731 --> 0.073548).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.1563876 Vali Loss: 0.0737111 Test Loss: 0.0726034
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.1559906 Vali Loss: 0.0739250 Test Loss: 0.0726036
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.1558753 Vali Loss: 0.0736806 Test Loss: 0.0726036
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.1558206 Vali Loss: 0.0737310 Test Loss: 0.0726036
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.1564828 Vali Loss: 0.0736950 Test Loss: 0.0726036
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.1557030 Vali Loss: 0.0740101 Test Loss: 0.0726036
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.1562814 Vali Loss: 0.0739453 Test Loss: 0.0726037
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.1566212 Vali Loss: 0.0739014 Test Loss: 0.0726037
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.1558565 Vali Loss: 0.0737076 Test Loss: 0.0726036
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.1561172 Vali Loss: 0.0738078 Test Loss: 0.0726037
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_SP500_H22_iTransformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 219
test shape: (219, 22, 1) (219, 22, 1)
test shape: (219, 22, 1) (219, 22, 1)


	mse:6.733742338838056e-05, mae:0.006126113701611757, rmse:0.008205938152968884, r2:-0.054694414138793945, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0082, RÂ²: -0.0547, MAPE: 3.37%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.637 MB of 0.638 MB uploadedwandb: \ 0.637 MB of 0.638 MB uploadedwandb: | 0.638 MB of 0.638 MB uploadedwandb: / 0.638 MB of 0.638 MB uploadedwandb: - 0.638 MB of 0.797 MB uploadedwandb: \ 0.640 MB of 0.797 MB uploadedwandb: | 0.797 MB of 0.797 MB uploadedwandb: / 0.797 MB of 0.797 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–„â–ƒâ–ƒâ–ƒâ–‚â–„â–ƒâ–â–‚â–ƒâ–‚â–‚â–‚â–„â–ƒâ–ƒâ–‚â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6348310
wandb:     model/trainable_params 6348310
wandb:                  test/loss 0.0726
wandb:                 train/loss 0.15612
wandb:   val/directional_accuracy 48.24962
wandb:                   val/loss 0.07381
wandb:                    val/mae 0.00613
wandb:                   val/mape 336.5231
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.05469
wandb:                   val/rmse 0.00821
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/q6sa9gpx
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_203423-q6sa9gpx/logs
Completed: SP500 H=22

Training: iTransformer on SP500 for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_203809-ppbv9lx1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/ppbv9lx1
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_SP500_H50Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/ppbv9lx1
>>>>>>>start training : long_term_forecast_iTransformer_SP500_H50_iTransformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 50
============================================================
train 4194
val 190
test 191
Epoch: 1, Steps: 132 | Train Loss: 0.2088920 Vali Loss: 0.0770017 Test Loss: 0.0787717
Validation loss decreased (inf --> 0.077002).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.1879954 Vali Loss: 0.0752557 Test Loss: 0.0771810
Validation loss decreased (0.077002 --> 0.075256).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.1787689 Vali Loss: 0.0748471 Test Loss: 0.0763088
Validation loss decreased (0.075256 --> 0.074847).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.20889196618262565, 'val/loss': 0.07700169210632642, 'test/loss': 0.07877166879673798, '_timestamp': 1762886318.0360882}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.18799536703436664, 'val/loss': 0.07525572304924329, 'test/loss': 0.07718099902073543, '_timestamp': 1762886322.0160425}).
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.20889196618262565, 'val/loss': 0.07700169210632642, 'test/loss': 0.07877166879673798, '_timestamp': 1762886318.0360882}).
Epoch: 4, Steps: 132 | Train Loss: 0.1764982 Vali Loss: 0.0743524 Test Loss: 0.0757983
Validation loss decreased (0.074847 --> 0.074352).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.1723355 Vali Loss: 0.0741668 Test Loss: 0.0755758
Validation loss decreased (0.074352 --> 0.074167).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.1710257 Vali Loss: 0.0741278 Test Loss: 0.0755041
Validation loss decreased (0.074167 --> 0.074128).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.1704585 Vali Loss: 0.0741142 Test Loss: 0.0754668
Validation loss decreased (0.074128 --> 0.074114).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.1703609 Vali Loss: 0.0740701 Test Loss: 0.0754536
Validation loss decreased (0.074114 --> 0.074070).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.1741277 Vali Loss: 0.0740816 Test Loss: 0.0754351
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.1697299 Vali Loss: 0.0741426 Test Loss: 0.0754397
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.1701534 Vali Loss: 0.0741119 Test Loss: 0.0754368
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.1699765 Vali Loss: 0.0741236 Test Loss: 0.0754361
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.1695659 Vali Loss: 0.0741037 Test Loss: 0.0754351
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.1697808 Vali Loss: 0.0741160 Test Loss: 0.0754348
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.1703724 Vali Loss: 0.0740588 Test Loss: 0.0754346
Validation loss decreased (0.074070 --> 0.074059).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.1688780 Vali Loss: 0.0740942 Test Loss: 0.0754346
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.1761214 Vali Loss: 0.0740757 Test Loss: 0.0754346
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.1701723 Vali Loss: 0.0741752 Test Loss: 0.0754346
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.1706731 Vali Loss: 0.0740971 Test Loss: 0.0754346
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.1705471 Vali Loss: 0.0740683 Test Loss: 0.0754346
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.1692600 Vali Loss: 0.0740842 Test Loss: 0.0754346
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.1693570 Vali Loss: 0.0740836 Test Loss: 0.0754346
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.1705187 Vali Loss: 0.0740999 Test Loss: 0.0754346
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 132 | Train Loss: 0.1701493 Vali Loss: 0.0741376 Test Loss: 0.0754346
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 132 | Train Loss: 0.1702543 Vali Loss: 0.0740933 Test Loss: 0.0754346
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_SP500_H50_iTransformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 191
test shape: (191, 50, 1) (191, 50, 1)
test shape: (191, 50, 1) (191, 50, 1)


	mse:6.739294622093439e-05, mae:0.0061053503304719925, rmse:0.008209320716559887, r2:-0.03629577159881592, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0082, RÂ²: -0.0363, MAPE: 2.92%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.716 MB of 0.719 MB uploadedwandb: \ 0.716 MB of 0.719 MB uploadedwandb: | 0.716 MB of 0.719 MB uploadedwandb: / 0.716 MB of 0.719 MB uploadedwandb: - 0.719 MB of 0.719 MB uploadedwandb: \ 0.719 MB of 0.719 MB uploadedwandb: | 0.719 MB of 0.877 MB uploadedwandb: / 0.719 MB of 0.877 MB uploadedwandb: - 0.877 MB of 0.877 MB uploadedwandb: \ 0.877 MB of 0.877 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–†â–ƒâ–ƒâ–‚â–‚â–…â–‚â–‚â–‚â–â–‚â–‚â–â–†â–‚â–‚â–‚â–â–â–‚â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–‚â–‚â–â–â–â–‚â–â–‚â–â–‚â–â–â–â–‚â–â–â–â–â–â–‚â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 24
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6362674
wandb:     model/trainable_params 6362674
wandb:                  test/loss 0.07543
wandb:                 train/loss 0.17025
wandb:   val/directional_accuracy 48.68041
wandb:                   val/loss 0.07409
wandb:                    val/mae 0.00611
wandb:                   val/mape 291.74991
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.0363
wandb:                   val/rmse 0.00821
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/ppbv9lx1
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_203809-ppbv9lx1/logs
Completed: SP500 H=50

Training: iTransformer on SP500 for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_204159-1juwhfi0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/1juwhfi0
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_SP500_H100Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/1juwhfi0
>>>>>>>start training : long_term_forecast_iTransformer_SP500_H100_iTransformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 100
============================================================
train 4144
val 140
test 141
Epoch: 1, Steps: 130 | Train Loss: 0.2186092 Vali Loss: 0.0765500 Test Loss: 0.0896200
Validation loss decreased (inf --> 0.076550).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.1968408 Vali Loss: 0.0736619 Test Loss: 0.0851166
Validation loss decreased (0.076550 --> 0.073662).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.1912615 Vali Loss: 0.0722289 Test Loss: 0.0830561
Validation loss decreased (0.073662 --> 0.072229).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.21860923125193668, 'val/loss': 0.07655002325773239, 'test/loss': 0.08961998224258423, '_timestamp': 1762886540.465058}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19684080206430876, 'val/loss': 0.073661907017231, 'test/loss': 0.08511659055948258, '_timestamp': 1762886546.3453093}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19684080206430876, 'val/loss': 0.073661907017231, 'test/loss': 0.08511659055948258, '_timestamp': 1762886546.3453093}).
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
Epoch: 4, Steps: 130 | Train Loss: 0.1884133 Vali Loss: 0.0719432 Test Loss: 0.0823502
Validation loss decreased (0.072229 --> 0.071943).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.1871114 Vali Loss: 0.0720733 Test Loss: 0.0821253
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.1863546 Vali Loss: 0.0720969 Test Loss: 0.0820213
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.1860739 Vali Loss: 0.0718722 Test Loss: 0.0819641
Validation loss decreased (0.071943 --> 0.071872).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.1851920 Vali Loss: 0.0716063 Test Loss: 0.0819296
Validation loss decreased (0.071872 --> 0.071606).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.1860262 Vali Loss: 0.0718607 Test Loss: 0.0819311
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.1862006 Vali Loss: 0.0714634 Test Loss: 0.0819215
Validation loss decreased (0.071606 --> 0.071463).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.1851615 Vali Loss: 0.0713838 Test Loss: 0.0819200
Validation loss decreased (0.071463 --> 0.071384).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.1853559 Vali Loss: 0.0718777 Test Loss: 0.0819196
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.1851505 Vali Loss: 0.0717793 Test Loss: 0.0819186
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.1852422 Vali Loss: 0.0715967 Test Loss: 0.0819181
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.1855170 Vali Loss: 0.0712536 Test Loss: 0.0819179
Validation loss decreased (0.071384 --> 0.071254).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 130 | Train Loss: 0.1852207 Vali Loss: 0.0715108 Test Loss: 0.0819179
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 130 | Train Loss: 0.1859869 Vali Loss: 0.0718498 Test Loss: 0.0819179
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 130 | Train Loss: 0.1850339 Vali Loss: 0.0715074 Test Loss: 0.0819178
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 130 | Train Loss: 0.1851427 Vali Loss: 0.0715973 Test Loss: 0.0819179
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 130 | Train Loss: 0.1850923 Vali Loss: 0.0716269 Test Loss: 0.0819178
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 130 | Train Loss: 0.1854957 Vali Loss: 0.0719728 Test Loss: 0.0819178
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 130 | Train Loss: 0.1856544 Vali Loss: 0.0717306 Test Loss: 0.0819178
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 130 | Train Loss: 0.1854640 Vali Loss: 0.0713802 Test Loss: 0.0819178
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 130 | Train Loss: 0.1855493 Vali Loss: 0.0718118 Test Loss: 0.0819178
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 130 | Train Loss: 0.1854092 Vali Loss: 0.0716174 Test Loss: 0.0819178
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_SP500_H100_iTransformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 141
test shape: (141, 100, 1) (141, 100, 1)
test shape: (141, 100, 1) (141, 100, 1)


	mse:6.936158024473116e-05, mae:0.006161628756672144, rmse:0.008328359574079514, r2:-0.012454509735107422, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0062, RMSE: 0.0083, RÂ²: -0.0125, MAPE: 2.62%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.810 MB of 0.815 MB uploadedwandb: \ 0.810 MB of 0.815 MB uploadedwandb: | 0.815 MB of 0.815 MB uploadedwandb: / 0.815 MB of 0.815 MB uploadedwandb: - 0.815 MB of 0.975 MB uploadedwandb: \ 0.815 MB of 0.975 MB uploadedwandb: | 0.975 MB of 0.975 MB uploadedwandb: / 0.975 MB of 0.975 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–‚â–‚â–â–â–â–â–‚â–â–‚â–â–â–â–‚â–‚â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–‡â–‡â–…â–„â–…â–ƒâ–‚â–…â–…â–ƒâ–â–ƒâ–…â–ƒâ–ƒâ–„â–†â–„â–‚â–…â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 24
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6388324
wandb:     model/trainable_params 6388324
wandb:                  test/loss 0.08192
wandb:                 train/loss 0.18541
wandb:   val/directional_accuracy 49.14392
wandb:                   val/loss 0.07162
wandb:                    val/mae 0.00616
wandb:                   val/mape 261.68883
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.01245
wandb:                   val/rmse 0.00833
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/1juwhfi0
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_204159-1juwhfi0/logs
Completed: SP500 H=100

Training: iTransformer on NASDAQ for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_204630-ewrr3dtf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/ewrr3dtf
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_NASDAQ_H3Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/ewrr3dtf
>>>>>>>start training : long_term_forecast_iTransformer_NASDAQ_H3_iTransformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.2777822 Vali Loss: 0.1469078 Test Loss: 0.1359014
Validation loss decreased (inf --> 0.146908).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2372469 Vali Loss: 0.1406406 Test Loss: 0.1364761
Validation loss decreased (0.146908 --> 0.140641).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.27778221465143044, 'val/loss': 0.1469078278169036, 'test/loss': 0.13590136030688882, '_timestamp': 1762886819.9911718}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23724687614835294, 'val/loss': 0.14064062852412462, 'test/loss': 0.13647611998021603, '_timestamp': 1762886826.07758}).
Epoch: 3, Steps: 133 | Train Loss: 0.2111786 Vali Loss: 0.1300303 Test Loss: 0.1276729
Validation loss decreased (0.140641 --> 0.130030).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1924696 Vali Loss: 0.1329278 Test Loss: 0.1260555
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1831357 Vali Loss: 0.1414445 Test Loss: 0.1256718
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1792051 Vali Loss: 0.1280089 Test Loss: 0.1257292
Validation loss decreased (0.130030 --> 0.128009).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1770825 Vali Loss: 0.1304577 Test Loss: 0.1254152
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1756493 Vali Loss: 0.1304603 Test Loss: 0.1253342
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1746366 Vali Loss: 0.1420253 Test Loss: 0.1253501
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1756223 Vali Loss: 0.1406478 Test Loss: 0.1253683
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1741865 Vali Loss: 0.1319331 Test Loss: 0.1253752
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1743498 Vali Loss: 0.1286429 Test Loss: 0.1253734
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1737422 Vali Loss: 0.1313767 Test Loss: 0.1253739
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1746591 Vali Loss: 0.1302899 Test Loss: 0.1253726
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1745055 Vali Loss: 0.1279807 Test Loss: 0.1253734
Validation loss decreased (0.128009 --> 0.127981).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1740659 Vali Loss: 0.1301377 Test Loss: 0.1253731
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1748119 Vali Loss: 0.1425308 Test Loss: 0.1253735
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1732105 Vali Loss: 0.1300509 Test Loss: 0.1253734
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1741297 Vali Loss: 0.1311313 Test Loss: 0.1253734
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1741680 Vali Loss: 0.1282891 Test Loss: 0.1253733
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1734054 Vali Loss: 0.1275795 Test Loss: 0.1253734
Validation loss decreased (0.127981 --> 0.127580).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1750170 Vali Loss: 0.1328539 Test Loss: 0.1253733
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1736963 Vali Loss: 0.1524249 Test Loss: 0.1253734
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.1751547 Vali Loss: 0.1293225 Test Loss: 0.1253734
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.1739474 Vali Loss: 0.1446562 Test Loss: 0.1253735
EarlyStopping counter: 4 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.1750068 Vali Loss: 0.1311315 Test Loss: 0.1253734
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.1752627 Vali Loss: 0.1290133 Test Loss: 0.1253734
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.1740149 Vali Loss: 0.1288180 Test Loss: 0.1253734
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.1740987 Vali Loss: 0.1284640 Test Loss: 0.1253734
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 133 | Train Loss: 0.1750582 Vali Loss: 0.1321304 Test Loss: 0.1253734
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 133 | Train Loss: 0.1755616 Vali Loss: 0.1283839 Test Loss: 0.1253734
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_NASDAQ_H3_iTransformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.00014754643780179322, mae:0.00890523660928011, rmse:0.01214686967432499, r2:-0.08403289318084717, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0089, RMSE: 0.0121, RÂ²: -0.0840, MAPE: 7301682.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.493 MB of 0.493 MB uploadedwandb: \ 0.493 MB of 0.493 MB uploadedwandb: | 0.493 MB of 0.493 MB uploadedwandb: / 0.493 MB of 0.493 MB uploadedwandb: - 0.493 MB of 0.493 MB uploadedwandb: \ 0.493 MB of 0.493 MB uploadedwandb: | 0.493 MB of 0.493 MB uploadedwandb: / 0.493 MB of 0.493 MB uploadedwandb: - 0.493 MB of 0.493 MB uploadedwandb: \ 0.493 MB of 0.493 MB uploadedwandb: | 0.598 MB of 0.758 MB uploaded (0.002 MB deduped)wandb: / 0.751 MB of 0.758 MB uploaded (0.002 MB deduped)wandb: - 0.758 MB of 0.758 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–ƒâ–…â–â–‚â–‚â–…â–…â–‚â–â–‚â–‚â–â–‚â–…â–‚â–‚â–â–â–‚â–ˆâ–â–†â–‚â–â–â–â–‚â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 30
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6338563
wandb:     model/trainable_params 6338563
wandb:                  test/loss 0.12537
wandb:                 train/loss 0.17556
wandb:   val/directional_accuracy 52.95359
wandb:                   val/loss 0.12838
wandb:                    val/mae 0.00891
wandb:                   val/mape 730168250.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.08403
wandb:                   val/rmse 0.01215
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/ewrr3dtf
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_204630-ewrr3dtf/logs
Completed: NASDAQ H=3

Training: iTransformer on NASDAQ for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_205206-ry13dddf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/ry13dddf
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_NASDAQ_H5Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/ry13dddf
>>>>>>>start training : long_term_forecast_iTransformer_NASDAQ_H5_iTransformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.2757723 Vali Loss: 0.1426856 Test Loss: 0.1333859
Validation loss decreased (inf --> 0.142686).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2408314 Vali Loss: 0.1461671 Test Loss: 0.1348313
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2180137 Vali Loss: 0.1348752 Test Loss: 0.1319001
Validation loss decreased (0.142686 --> 0.134875).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.27577231342631175, 'val/loss': 0.14268558751791716, 'test/loss': 0.13338585756719112, '_timestamp': 1762887151.182039}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24083142240244643, 'val/loss': 0.14616707246750593, 'test/loss': 0.13483129395172, '_timestamp': 1762887156.3520644}).
Epoch: 4, Steps: 133 | Train Loss: 0.2041577 Vali Loss: 0.1433839 Test Loss: 0.1302541
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1953613 Vali Loss: 0.1427726 Test Loss: 0.1297936
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1916975 Vali Loss: 0.1320220 Test Loss: 0.1294669
Validation loss decreased (0.134875 --> 0.132022).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1896054 Vali Loss: 0.1333589 Test Loss: 0.1295700
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1888640 Vali Loss: 0.1319799 Test Loss: 0.1295907
Validation loss decreased (0.132022 --> 0.131980).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1878139 Vali Loss: 0.1354793 Test Loss: 0.1296027
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1891158 Vali Loss: 0.1351735 Test Loss: 0.1296123
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1875746 Vali Loss: 0.1349686 Test Loss: 0.1296326
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1880301 Vali Loss: 0.1356469 Test Loss: 0.1296326
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1882290 Vali Loss: 0.1354452 Test Loss: 0.1296341
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1875453 Vali Loss: 0.1360200 Test Loss: 0.1296363
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1869502 Vali Loss: 0.1357779 Test Loss: 0.1296368
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1870099 Vali Loss: 0.1321933 Test Loss: 0.1296364
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1883586 Vali Loss: 0.1454015 Test Loss: 0.1296365
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1875352 Vali Loss: 0.1330633 Test Loss: 0.1296362
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_NASDAQ_H5_iTransformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.00014612132508773357, mae:0.008884375914931297, rmse:0.012088065035641193, r2:-0.06811559200286865, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0089, RMSE: 0.0121, RÂ²: -0.0681, MAPE: 4666710.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.537 MB of 0.537 MB uploadedwandb: \ 0.537 MB of 0.537 MB uploadedwandb: | 0.537 MB of 0.537 MB uploadedwandb: / 0.537 MB of 0.537 MB uploadedwandb: - 0.537 MB of 0.537 MB uploadedwandb: \ 0.537 MB of 0.694 MB uploadedwandb: | 0.537 MB of 0.694 MB uploadedwandb: / 0.694 MB of 0.694 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–‡â–‡â–â–‚â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–ˆâ–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 17
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6339589
wandb:     model/trainable_params 6339589
wandb:                  test/loss 0.12964
wandb:                 train/loss 0.18754
wandb:   val/directional_accuracy 50.21277
wandb:                   val/loss 0.13306
wandb:                    val/mae 0.00888
wandb:                   val/mape 466671000.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.06812
wandb:                   val/rmse 0.01209
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/ry13dddf
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_205206-ry13dddf/logs
Completed: NASDAQ H=5

Training: iTransformer on NASDAQ for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_205451-yodk9f28
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/yodk9f28
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_NASDAQ_H10Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/yodk9f28
>>>>>>>start training : long_term_forecast_iTransformer_NASDAQ_H10_iTransformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2749085 Vali Loss: 0.1469320 Test Loss: 0.1390416
Validation loss decreased (inf --> 0.146932).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2470057 Vali Loss: 0.1485470 Test Loss: 0.1370464
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2290112 Vali Loss: 0.1518644 Test Loss: 0.1327176
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2749085416247074, 'val/loss': 0.14693202544003725, 'test/loss': 0.13904164172708988, '_timestamp': 1762887315.3927848}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2470057409508784, 'val/loss': 0.14854699652642012, 'test/loss': 0.13704638555645943, '_timestamp': 1762887321.3582115}).
Epoch: 4, Steps: 133 | Train Loss: 0.2171358 Vali Loss: 0.1425338 Test Loss: 0.1329552
Validation loss decreased (0.146932 --> 0.142534).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2114563 Vali Loss: 0.1412464 Test Loss: 0.1323923
Validation loss decreased (0.142534 --> 0.141246).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2085582 Vali Loss: 0.1535791 Test Loss: 0.1323278
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2061315 Vali Loss: 0.1531211 Test Loss: 0.1322582
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2060651 Vali Loss: 0.1398637 Test Loss: 0.1322583
Validation loss decreased (0.141246 --> 0.139864).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2049818 Vali Loss: 0.1514933 Test Loss: 0.1322797
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2038061 Vali Loss: 0.1387625 Test Loss: 0.1322861
Validation loss decreased (0.139864 --> 0.138763).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2048082 Vali Loss: 0.1536163 Test Loss: 0.1322832
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2040671 Vali Loss: 0.1383854 Test Loss: 0.1322878
Validation loss decreased (0.138763 --> 0.138385).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2042484 Vali Loss: 0.1524020 Test Loss: 0.1322874
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2043303 Vali Loss: 0.1412005 Test Loss: 0.1322877
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2041037 Vali Loss: 0.1420509 Test Loss: 0.1322880
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2038488 Vali Loss: 0.1398177 Test Loss: 0.1322880
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2056185 Vali Loss: 0.1387987 Test Loss: 0.1322879
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2052261 Vali Loss: 0.1409275 Test Loss: 0.1322880
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2048560 Vali Loss: 0.1394918 Test Loss: 0.1322878
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2052909 Vali Loss: 0.1409819 Test Loss: 0.1322878
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2053640 Vali Loss: 0.1407436 Test Loss: 0.1322879
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2043959 Vali Loss: 0.1388658 Test Loss: 0.1322879
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_NASDAQ_H10_iTransformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.0001451186544727534, mae:0.00885506346821785, rmse:0.012046520598232746, r2:-0.0483475923538208, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0089, RMSE: 0.0120, RÂ²: -0.0483, MAPE: 3412582.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.592 MB of 0.593 MB uploadedwandb: \ 0.592 MB of 0.593 MB uploadedwandb: | 0.592 MB of 0.593 MB uploadedwandb: / 0.593 MB of 0.593 MB uploadedwandb: - 0.593 MB of 0.593 MB uploadedwandb: \ 0.593 MB of 0.751 MB uploadedwandb: | 0.593 MB of 0.751 MB uploadedwandb: / 0.751 MB of 0.751 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–†â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–ƒâ–‚â–ˆâ–ˆâ–‚â–‡â–â–ˆâ–â–‡â–‚â–ƒâ–‚â–â–‚â–‚â–‚â–‚â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6342154
wandb:     model/trainable_params 6342154
wandb:                  test/loss 0.13229
wandb:                 train/loss 0.2044
wandb:   val/directional_accuracy 51.93237
wandb:                   val/loss 0.13887
wandb:                    val/mae 0.00886
wandb:                   val/mape 341258225.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.04835
wandb:                   val/rmse 0.01205
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/yodk9f28
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_205451-yodk9f28/logs
Completed: NASDAQ H=10

Training: iTransformer on NASDAQ for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_205837-phpyoxg7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/phpyoxg7
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_NASDAQ_H22Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/phpyoxg7
>>>>>>>start training : long_term_forecast_iTransformer_NASDAQ_H22_iTransformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2778892 Vali Loss: 0.1573361 Test Loss: 0.1401648
Validation loss decreased (inf --> 0.157336).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2549071 Vali Loss: 0.1559108 Test Loss: 0.1392429
Validation loss decreased (0.157336 --> 0.155911).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.27788924572594237, 'val/loss': 0.15733609667846135, 'test/loss': 0.1401647882802146, '_timestamp': 1762887539.8359437}).
Epoch: 3, Steps: 132 | Train Loss: 0.2398906 Vali Loss: 0.1530868 Test Loss: 0.1364400
Validation loss decreased (0.155911 --> 0.153087).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25490705680215, 'val/loss': 0.15591083467006683, 'test/loss': 0.13924294071538107, '_timestamp': 1762887545.7444577}).
Epoch: 4, Steps: 132 | Train Loss: 0.2312651 Vali Loss: 0.1522449 Test Loss: 0.1357500
Validation loss decreased (0.153087 --> 0.152245).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2278281 Vali Loss: 0.1523415 Test Loss: 0.1353059
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2248081 Vali Loss: 0.1523146 Test Loss: 0.1352509
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2245276 Vali Loss: 0.1519930 Test Loss: 0.1352594
Validation loss decreased (0.152245 --> 0.151993).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2236253 Vali Loss: 0.1510193 Test Loss: 0.1352519
Validation loss decreased (0.151993 --> 0.151019).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2234143 Vali Loss: 0.1524543 Test Loss: 0.1352438
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2229992 Vali Loss: 0.1525758 Test Loss: 0.1352404
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2232591 Vali Loss: 0.1517112 Test Loss: 0.1352362
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2233826 Vali Loss: 0.1512968 Test Loss: 0.1352365
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2230419 Vali Loss: 0.1520108 Test Loss: 0.1352368
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2230122 Vali Loss: 0.1520146 Test Loss: 0.1352367
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2235101 Vali Loss: 0.1520883 Test Loss: 0.1352366
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2233104 Vali Loss: 0.1527380 Test Loss: 0.1352366
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2227217 Vali Loss: 0.1522672 Test Loss: 0.1352366
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2228039 Vali Loss: 0.1527253 Test Loss: 0.1352366
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_NASDAQ_H22_iTransformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.00014691220712848008, mae:0.008860795758664608, rmse:0.01212073490023613, r2:-0.04993176460266113, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0089, RMSE: 0.0121, RÂ²: -0.0499, MAPE: 1937231.88%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.678 MB of 0.679 MB uploadedwandb: \ 0.678 MB of 0.679 MB uploadedwandb: | 0.679 MB of 0.679 MB uploadedwandb: / 0.679 MB of 0.679 MB uploadedwandb: - 0.679 MB of 0.679 MB uploadedwandb: \ 0.679 MB of 0.837 MB uploadedwandb: | 0.768 MB of 0.837 MB uploadedwandb: / 0.837 MB of 0.837 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–…â–…â–„â–â–†â–†â–ƒâ–‚â–„â–„â–…â–‡â–…â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 17
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6348310
wandb:     model/trainable_params 6348310
wandb:                  test/loss 0.13524
wandb:                 train/loss 0.2228
wandb:   val/directional_accuracy 52.73045
wandb:                   val/loss 0.15273
wandb:                    val/mae 0.00886
wandb:                   val/mape 193723187.5
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.04993
wandb:                   val/rmse 0.01212
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/phpyoxg7
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_205837-phpyoxg7/logs
Completed: NASDAQ H=22

Training: iTransformer on NASDAQ for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_210157-f7drpos3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/f7drpos3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_NASDAQ_H50Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/f7drpos3
>>>>>>>start training : long_term_forecast_iTransformer_NASDAQ_H50_iTransformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.2864487 Vali Loss: 0.1650824 Test Loss: 0.1482062
Validation loss decreased (inf --> 0.165082).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2634744 Vali Loss: 0.1641057 Test Loss: 0.1474745
Validation loss decreased (0.165082 --> 0.164106).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.28644867263960117, 'val/loss': 0.16508235285679498, 'test/loss': 0.1482061669230461, '_timestamp': 1762887742.5088043}).
Epoch: 3, Steps: 132 | Train Loss: 0.2528915 Vali Loss: 0.1625905 Test Loss: 0.1455479
Validation loss decreased (0.164106 --> 0.162591).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2634744264298316, 'val/loss': 0.16410565624634424, 'test/loss': 0.14747449507315954, '_timestamp': 1762887748.3962498}).
Epoch: 4, Steps: 132 | Train Loss: 0.2476942 Vali Loss: 0.1609092 Test Loss: 0.1450294
Validation loss decreased (0.162591 --> 0.160909).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2455101 Vali Loss: 0.1609327 Test Loss: 0.1447249
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2460399 Vali Loss: 0.1606130 Test Loss: 0.1446499
Validation loss decreased (0.160909 --> 0.160613).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2427178 Vali Loss: 0.1603930 Test Loss: 0.1445770
Validation loss decreased (0.160613 --> 0.160393).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2421776 Vali Loss: 0.1605374 Test Loss: 0.1445496
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2446444 Vali Loss: 0.1607063 Test Loss: 0.1445442
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2416393 Vali Loss: 0.1604967 Test Loss: 0.1445432
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2418809 Vali Loss: 0.1604951 Test Loss: 0.1445381
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2430746 Vali Loss: 0.1607264 Test Loss: 0.1445385
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2419840 Vali Loss: 0.1603780 Test Loss: 0.1445385
Validation loss decreased (0.160393 --> 0.160378).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2437194 Vali Loss: 0.1604480 Test Loss: 0.1445382
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2422165 Vali Loss: 0.1608382 Test Loss: 0.1445383
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2420097 Vali Loss: 0.1604995 Test Loss: 0.1445380
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2444774 Vali Loss: 0.1605179 Test Loss: 0.1445380
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2417734 Vali Loss: 0.1605698 Test Loss: 0.1445380
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2418380 Vali Loss: 0.1604559 Test Loss: 0.1445380
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2424676 Vali Loss: 0.1604736 Test Loss: 0.1445380
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2422238 Vali Loss: 0.1606133 Test Loss: 0.1445380
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2417008 Vali Loss: 0.1606314 Test Loss: 0.1445380
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.2416716 Vali Loss: 0.1606746 Test Loss: 0.1445380
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_NASDAQ_H50_iTransformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.0001504316460341215, mae:0.008885692805051804, rmse:0.012265058234333992, r2:-0.03856527805328369, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0089, RMSE: 0.0123, RÂ²: -0.0386, MAPE: 2587404.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.715 MB of 0.718 MB uploadedwandb: \ 0.715 MB of 0.718 MB uploadedwandb: | 0.715 MB of 0.718 MB uploadedwandb: / 0.718 MB of 0.718 MB uploadedwandb: - 0.718 MB of 0.718 MB uploadedwandb: \ 0.718 MB of 0.876 MB uploadedwandb: | 0.807 MB of 0.876 MB uploadedwandb: / 0.876 MB of 0.876 MB uploadedwandb: - 0.876 MB of 0.876 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–„â–‚â–â–ƒâ–â–â–‚â–â–‚â–â–â–ƒâ–â–â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–ƒâ–‚â–â–‚â–‚â–â–â–‚â–â–â–‚â–â–â–‚â–â–â–‚â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6362674
wandb:     model/trainable_params 6362674
wandb:                  test/loss 0.14454
wandb:                 train/loss 0.24167
wandb:   val/directional_accuracy 51.9334
wandb:                   val/loss 0.16067
wandb:                    val/mae 0.00889
wandb:                   val/mape 258740450.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.03857
wandb:                   val/rmse 0.01227
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/f7drpos3
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_210157-f7drpos3/logs
Completed: NASDAQ H=50

Training: iTransformer on NASDAQ for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_210542-v1l4cehc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/v1l4cehc
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_NASDAQ_H100Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/v1l4cehc
>>>>>>>start training : long_term_forecast_iTransformer_NASDAQ_H100_iTransformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.2976236 Vali Loss: 0.1794012 Test Loss: 0.1586454
Validation loss decreased (inf --> 0.179401).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2740485 Vali Loss: 0.1746111 Test Loss: 0.1588740
Validation loss decreased (0.179401 --> 0.174611).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2976236132475046, 'val/loss': 0.17940119802951812, 'test/loss': 0.15864540338516236, '_timestamp': 1762887971.2842731}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.27404851752978104, 'val/loss': 0.17461109757423401, 'test/loss': 0.15887404084205628, '_timestamp': 1762887977.5700803}).
Epoch: 3, Steps: 130 | Train Loss: 0.2685039 Vali Loss: 0.1719948 Test Loss: 0.1587158
Validation loss decreased (0.174611 --> 0.171995).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2660492 Vali Loss: 0.1715162 Test Loss: 0.1586339
Validation loss decreased (0.171995 --> 0.171516).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2639634 Vali Loss: 0.1760200 Test Loss: 0.1584705
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2629646 Vali Loss: 0.1747689 Test Loss: 0.1584008
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2625962 Vali Loss: 0.1747341 Test Loss: 0.1583279
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2618190 Vali Loss: 0.1714755 Test Loss: 0.1583389
Validation loss decreased (0.171516 --> 0.171476).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2624770 Vali Loss: 0.1749321 Test Loss: 0.1583430
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2622923 Vali Loss: 0.1706595 Test Loss: 0.1583444
Validation loss decreased (0.171476 --> 0.170660).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2623103 Vali Loss: 0.1721584 Test Loss: 0.1583436
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2620967 Vali Loss: 0.1720854 Test Loss: 0.1583444
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2620751 Vali Loss: 0.1724256 Test Loss: 0.1583442
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.2620756 Vali Loss: 0.1730157 Test Loss: 0.1583439
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.2620371 Vali Loss: 0.1697927 Test Loss: 0.1583438
Validation loss decreased (0.170660 --> 0.169793).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 130 | Train Loss: 0.2620861 Vali Loss: 0.1731257 Test Loss: 0.1583438
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 130 | Train Loss: 0.2625110 Vali Loss: 0.1732814 Test Loss: 0.1583437
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 130 | Train Loss: 0.2622911 Vali Loss: 0.1722456 Test Loss: 0.1583438
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 130 | Train Loss: 0.2618636 Vali Loss: 0.1728413 Test Loss: 0.1583438
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 130 | Train Loss: 0.2618377 Vali Loss: 0.1711733 Test Loss: 0.1583438
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 130 | Train Loss: 0.2618866 Vali Loss: 0.1742885 Test Loss: 0.1583438
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 130 | Train Loss: 0.2619531 Vali Loss: 0.1715969 Test Loss: 0.1583438
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 130 | Train Loss: 0.2618266 Vali Loss: 0.1702614 Test Loss: 0.1583438
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 130 | Train Loss: 0.2620130 Vali Loss: 0.1746612 Test Loss: 0.1583438
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 130 | Train Loss: 0.2619687 Vali Loss: 0.1734415 Test Loss: 0.1583438
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_NASDAQ_H100_iTransformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0001573579356772825, mae:0.008828293532133102, rmse:0.012544238939881325, r2:-0.037474870681762695, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0088, RMSE: 0.0125, RÂ²: -0.0375, MAPE: 2033990.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.774 MB of 0.779 MB uploadedwandb: \ 0.774 MB of 0.779 MB uploadedwandb: | 0.779 MB of 0.779 MB uploadedwandb: / 0.779 MB of 0.779 MB uploadedwandb: - 0.779 MB of 0.938 MB uploadedwandb: \ 0.938 MB of 0.938 MB uploadedwandb: | 0.938 MB of 0.938 MB uploadedwandb: / 0.938 MB of 0.938 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‡â–„â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–‚â–â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–ƒâ–ˆâ–‡â–‡â–ƒâ–‡â–‚â–„â–„â–„â–…â–â–…â–…â–„â–„â–ƒâ–†â–ƒâ–‚â–†â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 24
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6388324
wandb:     model/trainable_params 6388324
wandb:                  test/loss 0.15834
wandb:                 train/loss 0.26197
wandb:   val/directional_accuracy 49.87013
wandb:                   val/loss 0.17344
wandb:                    val/mae 0.00883
wandb:                   val/mape 203399025.0
wandb:                    val/mse 0.00016
wandb:                     val/r2 -0.03747
wandb:                   val/rmse 0.01254
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/v1l4cehc
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_210542-v1l4cehc/logs
Completed: NASDAQ H=100

Training: iTransformer on ABSA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_210858-5gktki0j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/5gktki0j
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_ABSA_H3Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/5gktki0j
>>>>>>>start training : long_term_forecast_iTransformer_ABSA_H3_iTransformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 238
Epoch: 1, Steps: 133 | Train Loss: 0.3291668 Vali Loss: 0.1813523 Test Loss: 0.1735908
Validation loss decreased (inf --> 0.181352).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.32916675676080515, 'val/loss': 0.18135233409702778, 'test/loss': 0.17359076533466578, '_timestamp': 1762888167.6811628}).
Epoch: 2, Steps: 133 | Train Loss: 0.2809232 Vali Loss: 0.1789691 Test Loss: 0.1709037
Validation loss decreased (0.181352 --> 0.178969).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.28092315255251143, 'val/loss': 0.17896906659007072, 'test/loss': 0.1709036948159337, '_timestamp': 1762888175.7553864}).
Epoch: 3, Steps: 133 | Train Loss: 0.2506082 Vali Loss: 0.1795691 Test Loss: 0.1678330
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2312036 Vali Loss: 0.1760964 Test Loss: 0.1672460
Validation loss decreased (0.178969 --> 0.176096).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2214094 Vali Loss: 0.1750151 Test Loss: 0.1673466
Validation loss decreased (0.176096 --> 0.175015).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2161198 Vali Loss: 0.1785781 Test Loss: 0.1672743
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2135674 Vali Loss: 0.1722469 Test Loss: 0.1671820
Validation loss decreased (0.175015 --> 0.172247).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2116269 Vali Loss: 0.1681991 Test Loss: 0.1674547
Validation loss decreased (0.172247 --> 0.168199).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2123004 Vali Loss: 0.1742271 Test Loss: 0.1674012
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2101666 Vali Loss: 0.1724624 Test Loss: 0.1674767
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2104258 Vali Loss: 0.1732024 Test Loss: 0.1674699
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2102636 Vali Loss: 0.1775670 Test Loss: 0.1674797
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2103321 Vali Loss: 0.1700506 Test Loss: 0.1674736
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2104001 Vali Loss: 0.1737255 Test Loss: 0.1674777
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2101115 Vali Loss: 0.1723469 Test Loss: 0.1674793
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2109551 Vali Loss: 0.1735547 Test Loss: 0.1674800
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2102704 Vali Loss: 0.1788733 Test Loss: 0.1674799
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2105433 Vali Loss: 0.1742385 Test Loss: 0.1674797
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_ABSA_H3_iTransformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 238
test shape: (238, 3, 1) (238, 3, 1)
test shape: (238, 3, 1) (238, 3, 1)


	mse:0.0005006106221117079, mae:0.016904016956686974, rmse:0.022374330088496208, r2:-0.09937500953674316, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0169, RMSE: 0.0224, RÂ²: -0.0994, MAPE: 2.42%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.499 MB of 0.500 MB uploadedwandb: \ 0.499 MB of 0.500 MB uploadedwandb: | 0.499 MB of 0.500 MB uploadedwandb: / 0.500 MB of 0.500 MB uploadedwandb: - 0.500 MB of 0.500 MB uploadedwandb: \ 0.500 MB of 0.500 MB uploadedwandb: | 0.500 MB of 0.500 MB uploadedwandb: / 0.500 MB of 0.500 MB uploadedwandb: - 0.500 MB of 0.500 MB uploadedwandb: \ 0.605 MB of 0.762 MB uploaded (0.002 MB deduped)wandb: | 0.762 MB of 0.762 MB uploaded (0.002 MB deduped)wandb: / 0.762 MB of 0.762 MB uploaded (0.002 MB deduped)wandb: - 0.762 MB of 0.762 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–ƒâ–‚â–â–„â–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–„
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–…â–‡â–ƒâ–â–…â–„â–„â–‡â–‚â–„â–„â–„â–ˆâ–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 17
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6338563
wandb:     model/trainable_params 6338563
wandb:                  test/loss 0.16748
wandb:                 train/loss 0.21054
wandb:   val/directional_accuracy 51.2605
wandb:                   val/loss 0.17424
wandb:                    val/mae 0.0169
wandb:                   val/mape 242.38775
wandb:                    val/mse 0.0005
wandb:                     val/r2 -0.09938
wandb:                   val/rmse 0.02237
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/5gktki0j
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_210858-5gktki0j/logs
Completed: ABSA H=3

Training: iTransformer on ABSA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_211350-qfwfwsiw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/qfwfwsiw
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_ABSA_H5Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/qfwfwsiw
>>>>>>>start training : long_term_forecast_iTransformer_ABSA_H5_iTransformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 236
Epoch: 1, Steps: 133 | Train Loss: 0.3247493 Vali Loss: 0.1927921 Test Loss: 0.1817052
Validation loss decreased (inf --> 0.192792).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2912788 Vali Loss: 0.1880542 Test Loss: 0.1774824
Validation loss decreased (0.192792 --> 0.188054).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.32474925296199053, 'val/loss': 0.19279210828244686, 'test/loss': 0.18170522805303335, '_timestamp': 1762888456.1277199}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.29127876281290127, 'val/loss': 0.18805416859686375, 'test/loss': 0.1774824419990182, '_timestamp': 1762888462.3082325}).
Epoch: 3, Steps: 133 | Train Loss: 0.2569387 Vali Loss: 0.1819010 Test Loss: 0.1746580
Validation loss decreased (0.188054 --> 0.181901).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2416130 Vali Loss: 0.1803141 Test Loss: 0.1713919
Validation loss decreased (0.181901 --> 0.180314).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2331646 Vali Loss: 0.1764024 Test Loss: 0.1711961
Validation loss decreased (0.180314 --> 0.176402).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2283891 Vali Loss: 0.1761859 Test Loss: 0.1709597
Validation loss decreased (0.176402 --> 0.176186).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2254735 Vali Loss: 0.1746723 Test Loss: 0.1710847
Validation loss decreased (0.176186 --> 0.174672).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2251864 Vali Loss: 0.1729228 Test Loss: 0.1710975
Validation loss decreased (0.174672 --> 0.172923).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2232996 Vali Loss: 0.1801640 Test Loss: 0.1711868
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2234554 Vali Loss: 0.1759506 Test Loss: 0.1712142
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2232207 Vali Loss: 0.1778010 Test Loss: 0.1712273
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2230013 Vali Loss: 0.1734549 Test Loss: 0.1712408
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2233171 Vali Loss: 0.1797100 Test Loss: 0.1712433
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2233480 Vali Loss: 0.1728229 Test Loss: 0.1712446
Validation loss decreased (0.172923 --> 0.172823).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2233878 Vali Loss: 0.1766623 Test Loss: 0.1712455
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2243529 Vali Loss: 0.1800629 Test Loss: 0.1712463
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2225752 Vali Loss: 0.1791921 Test Loss: 0.1712470
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2236460 Vali Loss: 0.1784593 Test Loss: 0.1712470
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2230045 Vali Loss: 0.1806561 Test Loss: 0.1712468
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2233437 Vali Loss: 0.1779867 Test Loss: 0.1712468
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2231765 Vali Loss: 0.1784866 Test Loss: 0.1712468
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2238676 Vali Loss: 0.1785528 Test Loss: 0.1712468
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2226233 Vali Loss: 0.1799774 Test Loss: 0.1712469
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2228950 Vali Loss: 0.1750404 Test Loss: 0.1712468
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_ABSA_H5_iTransformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
test shape: (236, 5, 1) (236, 5, 1)
test shape: (236, 5, 1) (236, 5, 1)


	mse:0.0005066614248789847, mae:0.01701672561466694, rmse:0.022509140893816948, r2:-0.1061936616897583, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0170, RMSE: 0.0225, RÂ²: -0.1062, MAPE: 2.18%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.536 MB of 0.536 MB uploadedwandb: \ 0.536 MB of 0.536 MB uploadedwandb: | 0.536 MB of 0.536 MB uploadedwandb: / 0.536 MB of 0.536 MB uploadedwandb: - 0.536 MB of 0.536 MB uploadedwandb: \ 0.536 MB of 0.536 MB uploadedwandb: | 0.536 MB of 0.536 MB uploadedwandb: / 0.536 MB of 0.694 MB uploadedwandb: - 0.694 MB of 0.694 MB uploadedwandb: \ 0.694 MB of 0.694 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‡â–„â–„â–‚â–â–‡â–ƒâ–…â–â–†â–â–„â–‡â–†â–…â–‡â–…â–…â–…â–‡â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 23
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6339589
wandb:     model/trainable_params 6339589
wandb:                  test/loss 0.17125
wandb:                 train/loss 0.2229
wandb:   val/directional_accuracy 50.84746
wandb:                   val/loss 0.17504
wandb:                    val/mae 0.01702
wandb:                   val/mape 218.29093
wandb:                    val/mse 0.00051
wandb:                     val/r2 -0.10619
wandb:                   val/rmse 0.02251
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/qfwfwsiw
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_211350-qfwfwsiw/logs
Completed: ABSA H=5

Training: iTransformer on ABSA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_211735-hl0tr38z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/hl0tr38z
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_ABSA_H10Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/hl0tr38z
>>>>>>>start training : long_term_forecast_iTransformer_ABSA_H10_iTransformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 231
Epoch: 1, Steps: 133 | Train Loss: 0.3275207 Vali Loss: 0.1909555 Test Loss: 0.1770051
Validation loss decreased (inf --> 0.190955).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2962093 Vali Loss: 0.1904032 Test Loss: 0.1753682
Validation loss decreased (0.190955 --> 0.190403).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2707914 Vali Loss: 0.1805791 Test Loss: 0.1730114
Validation loss decreased (0.190403 --> 0.180579).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3275207032386522, 'val/loss': 0.19095546938478947, 'test/loss': 0.1770050898194313, '_timestamp': 1762888680.6559174}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.29620928748657827, 'val/loss': 0.190403176471591, 'test/loss': 0.17536820098757744, '_timestamp': 1762888686.3172789}).
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3275207032386522, 'val/loss': 0.19095546938478947, 'test/loss': 0.1770050898194313, '_timestamp': 1762888680.6559174}).
Epoch: 4, Steps: 133 | Train Loss: 0.2578355 Vali Loss: 0.1840731 Test Loss: 0.1704452
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2509158 Vali Loss: 0.1770538 Test Loss: 0.1705887
Validation loss decreased (0.180579 --> 0.177054).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2464455 Vali Loss: 0.1835837 Test Loss: 0.1707679
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2440376 Vali Loss: 0.1807159 Test Loss: 0.1707431
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2430972 Vali Loss: 0.1782161 Test Loss: 0.1707223
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2437737 Vali Loss: 0.1851984 Test Loss: 0.1707711
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2419440 Vali Loss: 0.1784517 Test Loss: 0.1707759
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2450622 Vali Loss: 0.1775404 Test Loss: 0.1707662
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2421900 Vali Loss: 0.1779824 Test Loss: 0.1707639
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2418754 Vali Loss: 0.1857093 Test Loss: 0.1707623
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2412097 Vali Loss: 0.1830601 Test Loss: 0.1707632
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2425348 Vali Loss: 0.1846518 Test Loss: 0.1707628
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_ABSA_H10_iTransformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 231
test shape: (231, 10, 1) (231, 10, 1)
test shape: (231, 10, 1) (231, 10, 1)


	mse:0.0004997850046493113, mae:0.016881093382835388, rmse:0.02235587127506733, r2:-0.08254504203796387, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0169, RMSE: 0.0224, RÂ²: -0.0825, MAPE: 1.99%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.576 MB of 0.576 MB uploadedwandb: \ 0.576 MB of 0.576 MB uploadedwandb: | 0.576 MB of 0.576 MB uploadedwandb: / 0.576 MB of 0.576 MB uploadedwandb: - 0.576 MB of 0.576 MB uploadedwandb: \ 0.576 MB of 0.576 MB uploadedwandb: | 0.576 MB of 0.732 MB uploadedwandb: / 0.732 MB of 0.732 MB uploadedwandb: - 0.732 MB of 0.732 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–‚â–â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–‡â–â–†â–„â–‚â–ˆâ–‚â–â–‚â–ˆâ–†â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6342154
wandb:     model/trainable_params 6342154
wandb:                  test/loss 0.17076
wandb:                 train/loss 0.24253
wandb:   val/directional_accuracy 51.41895
wandb:                   val/loss 0.18465
wandb:                    val/mae 0.01688
wandb:                   val/mape 199.01788
wandb:                    val/mse 0.0005
wandb:                     val/r2 -0.08255
wandb:                   val/rmse 0.02236
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/hl0tr38z
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_211735-hl0tr38z/logs
Completed: ABSA H=10

Training: iTransformer on ABSA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_212046-i9fp8vs8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/i9fp8vs8
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_ABSA_H22Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/i9fp8vs8
>>>>>>>start training : long_term_forecast_iTransformer_ABSA_H22_iTransformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 219
Epoch: 1, Steps: 132 | Train Loss: 0.3402396 Vali Loss: 0.1877077 Test Loss: 0.1711772
Validation loss decreased (inf --> 0.187708).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.3110086 Vali Loss: 0.1879443 Test Loss: 0.1705572
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2910584 Vali Loss: 0.1877728 Test Loss: 0.1685870
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.34023963129430107, 'val/loss': 0.1877076838697706, 'test/loss': 0.17117724461214884, '_timestamp': 1762888873.1940536}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.31100859380129614, 'val/loss': 0.18794429302215576, 'test/loss': 0.17055719026497432, '_timestamp': 1762888879.4520519}).
Epoch: 4, Steps: 132 | Train Loss: 0.2807110 Vali Loss: 0.1861733 Test Loss: 0.1683664
Validation loss decreased (0.187708 --> 0.186173).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2751362 Vali Loss: 0.1862384 Test Loss: 0.1689250
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2717569 Vali Loss: 0.1862515 Test Loss: 0.1688034
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2705262 Vali Loss: 0.1861176 Test Loss: 0.1689367
Validation loss decreased (0.186173 --> 0.186118).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2696341 Vali Loss: 0.1863796 Test Loss: 0.1689617
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2692320 Vali Loss: 0.1863362 Test Loss: 0.1689456
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2687745 Vali Loss: 0.1865838 Test Loss: 0.1689387
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2688497 Vali Loss: 0.1866141 Test Loss: 0.1689379
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2692507 Vali Loss: 0.1861028 Test Loss: 0.1689377
Validation loss decreased (0.186118 --> 0.186103).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2685466 Vali Loss: 0.1863656 Test Loss: 0.1689364
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2689774 Vali Loss: 0.1868769 Test Loss: 0.1689371
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2683145 Vali Loss: 0.1859813 Test Loss: 0.1689371
Validation loss decreased (0.186103 --> 0.185981).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2689436 Vali Loss: 0.1864634 Test Loss: 0.1689373
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2680715 Vali Loss: 0.1864856 Test Loss: 0.1689372
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2689551 Vali Loss: 0.1866003 Test Loss: 0.1689373
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2687911 Vali Loss: 0.1867776 Test Loss: 0.1689373
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2692577 Vali Loss: 0.1863463 Test Loss: 0.1689373
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2693584 Vali Loss: 0.1862261 Test Loss: 0.1689373
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2688529 Vali Loss: 0.1864405 Test Loss: 0.1689373
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.2684869 Vali Loss: 0.1863595 Test Loss: 0.1689374
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 132 | Train Loss: 0.2691651 Vali Loss: 0.1861754 Test Loss: 0.1689373
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 132 | Train Loss: 0.2683945 Vali Loss: 0.1865704 Test Loss: 0.1689373
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_ABSA_H22_iTransformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 219
test shape: (219, 22, 1) (219, 22, 1)
test shape: (219, 22, 1) (219, 22, 1)


	mse:0.0005015438655391335, mae:0.017002400010824203, rmse:0.022395174950361252, r2:-0.07048225402832031, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0170, RMSE: 0.0224, RÂ²: -0.0705, MAPE: 2.18%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.667 MB of 0.668 MB uploadedwandb: \ 0.667 MB of 0.668 MB uploadedwandb: | 0.668 MB of 0.668 MB uploadedwandb: / 0.668 MB of 0.827 MB uploadedwandb: - 0.827 MB of 0.827 MB uploadedwandb: \ 0.827 MB of 0.827 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–„â–â–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–â–ƒâ–„â–â–ƒâ–ƒâ–ƒâ–„â–‚â–‚â–ƒâ–‚â–‚â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 24
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6348310
wandb:     model/trainable_params 6348310
wandb:                  test/loss 0.16894
wandb:                 train/loss 0.26839
wandb:   val/directional_accuracy 50.09785
wandb:                   val/loss 0.18657
wandb:                    val/mae 0.017
wandb:                   val/mape 217.75255
wandb:                    val/mse 0.0005
wandb:                     val/r2 -0.07048
wandb:                   val/rmse 0.0224
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/i9fp8vs8
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_212046-i9fp8vs8/logs
Completed: ABSA H=22

Training: iTransformer on ABSA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_212423-ap40yh5j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/ap40yh5j
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_ABSA_H50Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/ap40yh5j
>>>>>>>start training : long_term_forecast_iTransformer_ABSA_H50_iTransformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 191
Epoch: 1, Steps: 132 | Train Loss: 0.3659654 Vali Loss: 0.1853398 Test Loss: 0.1668341
Validation loss decreased (inf --> 0.185340).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.3365685 Vali Loss: 0.1823925 Test Loss: 0.1660846
Validation loss decreased (0.185340 --> 0.182393).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.3226871 Vali Loss: 0.1809045 Test Loss: 0.1666232
Validation loss decreased (0.182393 --> 0.180904).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.365965367831064, 'val/loss': 0.18533983578284582, 'test/loss': 0.16683407003680864, '_timestamp': 1762889088.5360692}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.33656851507046004, 'val/loss': 0.1823925276597341, 'test/loss': 0.16608460371692976, '_timestamp': 1762889094.2230582}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.33656851507046004, 'val/loss': 0.1823925276597341, 'test/loss': 0.16608460371692976, '_timestamp': 1762889094.2230582}).
Epoch: 4, Steps: 132 | Train Loss: 0.3150138 Vali Loss: 0.1812687 Test Loss: 0.1663422
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.3122298 Vali Loss: 0.1816003 Test Loss: 0.1663880
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.3092202 Vali Loss: 0.1818235 Test Loss: 0.1665136
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.3074317 Vali Loss: 0.1820660 Test Loss: 0.1665729
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.3073041 Vali Loss: 0.1820068 Test Loss: 0.1666595
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.3069260 Vali Loss: 0.1819305 Test Loss: 0.1666922
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.3059719 Vali Loss: 0.1820240 Test Loss: 0.1667004
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.3059608 Vali Loss: 0.1819560 Test Loss: 0.1667085
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.3066192 Vali Loss: 0.1820591 Test Loss: 0.1667148
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.3071241 Vali Loss: 0.1819237 Test Loss: 0.1667166
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_ABSA_H50_iTransformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 191
test shape: (191, 50, 1) (191, 50, 1)
test shape: (191, 50, 1) (191, 50, 1)


	mse:0.0005131588550284505, mae:0.017366718500852585, rmse:0.022653009742498398, r2:-0.05481910705566406, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0174, RMSE: 0.0227, RÂ²: -0.0548, MAPE: 1.65%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.777 MB of 0.780 MB uploadedwandb: \ 0.777 MB of 0.780 MB uploadedwandb: | 0.777 MB of 0.780 MB uploadedwandb: / 0.780 MB of 0.780 MB uploadedwandb: - 0.780 MB of 0.780 MB uploadedwandb: \ 0.780 MB of 0.936 MB uploadedwandb: | 0.936 MB of 0.936 MB uploadedwandb: / 0.936 MB of 0.936 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–†â–â–‚â–„â–…â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ƒâ–…â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6362674
wandb:     model/trainable_params 6362674
wandb:                  test/loss 0.16672
wandb:                 train/loss 0.30712
wandb:   val/directional_accuracy 50.09082
wandb:                   val/loss 0.18192
wandb:                    val/mae 0.01737
wandb:                   val/mape 165.2503
wandb:                    val/mse 0.00051
wandb:                     val/r2 -0.05482
wandb:                   val/rmse 0.02265
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/ap40yh5j
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_212423-ap40yh5j/logs
Completed: ABSA H=50

Training: iTransformer on ABSA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_212705-fa17a7j8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/fa17a7j8
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_ABSA_H100Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/fa17a7j8
>>>>>>>start training : long_term_forecast_iTransformer_ABSA_H100_iTransformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 141
Epoch: 1, Steps: 130 | Train Loss: 0.4021309 Vali Loss: 0.1913497 Test Loss: 0.1683399
Validation loss decreased (inf --> 0.191350).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.3734146 Vali Loss: 0.1947713 Test Loss: 0.1674120
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.3627711 Vali Loss: 0.1946649 Test Loss: 0.1683018
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.4021308850783568, 'val/loss': 0.19134969115257264, 'test/loss': 0.16833986043930055, '_timestamp': 1762889254.5715787}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3734146372630046, 'val/loss': 0.19477129876613616, 'test/loss': 0.1674119636416435, '_timestamp': 1762889259.921113}).
Epoch: 4, Steps: 130 | Train Loss: 0.3572069 Vali Loss: 0.1982272 Test Loss: 0.1682908
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.3530559 Vali Loss: 0.1967411 Test Loss: 0.1685012
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.3509598 Vali Loss: 0.1965522 Test Loss: 0.1686065
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.3497534 Vali Loss: 0.1983231 Test Loss: 0.1688187
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.3492736 Vali Loss: 0.1949202 Test Loss: 0.1687204
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.3504498 Vali Loss: 0.1961056 Test Loss: 0.1687761
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.3496487 Vali Loss: 0.1981752 Test Loss: 0.1687808
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.3491759 Vali Loss: 0.1971779 Test Loss: 0.1687976
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_ABSA_H100_iTransformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 141
test shape: (141, 100, 1) (141, 100, 1)
test shape: (141, 100, 1) (141, 100, 1)


	mse:0.0005402045790106058, mae:0.01769166626036167, rmse:0.023242302238941193, r2:-0.04683947563171387, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0177, RMSE: 0.0232, RÂ²: -0.0468, MAPE: 1.45%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.883 MB of 0.887 MB uploadedwandb: \ 0.883 MB of 0.887 MB uploadedwandb: | 0.887 MB of 0.887 MB uploadedwandb: / 0.887 MB of 0.887 MB uploadedwandb: - 0.887 MB of 1.044 MB uploadedwandb: \ 1.044 MB of 1.044 MB uploadedwandb: | 1.044 MB of 1.044 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–â–„â–…â–ˆâ–‡â–‡â–‡â–ˆ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ˆâ–…â–…â–ˆâ–â–„â–ˆâ–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6388324
wandb:     model/trainable_params 6388324
wandb:                  test/loss 0.1688
wandb:                 train/loss 0.34918
wandb:   val/directional_accuracy 49.66688
wandb:                   val/loss 0.19718
wandb:                    val/mae 0.01769
wandb:                   val/mape 144.50601
wandb:                    val/mse 0.00054
wandb:                     val/r2 -0.04684
wandb:                   val/rmse 0.02324
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/fa17a7j8
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_212705-fa17a7j8/logs
Exception in thread Exception in thread ChkStopThrIntMsgThr:
:
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 286, in check_stop_status
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 300, in check_internal_messages
Completed: ABSA H=100

Training: iTransformer on SASOL for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_212951-5hr61etz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/5hr61etz
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_SASOL_H3Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/5hr61etz
>>>>>>>start training : long_term_forecast_iTransformer_SASOL_H3_iTransformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 3
============================================================
train 3772
val 211
test 212
Epoch: 1, Steps: 118 | Train Loss: 0.2409880 Vali Loss: 0.1073708 Test Loss: 0.1628248
Validation loss decreased (inf --> 0.107371).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2086577 Vali Loss: 0.1084831 Test Loss: 0.1556139
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 118 | Train Loss: 0.1850795 Vali Loss: 0.1052625 Test Loss: 0.1494027
Validation loss decreased (0.107371 --> 0.105262).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.24098798828357357, 'val/loss': 0.1073707789182663, 'test/loss': 0.16282479677881515, '_timestamp': 1762889417.8367991}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.20865765655949964, 'val/loss': 0.10848310589790344, 'test/loss': 0.15561392584017344, '_timestamp': 1762889422.7420921}).
Epoch: 4, Steps: 118 | Train Loss: 0.1705256 Vali Loss: 0.1054335 Test Loss: 0.1484117
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.1633124 Vali Loss: 0.1018571 Test Loss: 0.1472444
Validation loss decreased (0.105262 --> 0.101857).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.1584930 Vali Loss: 0.1026007 Test Loss: 0.1474421
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.1566717 Vali Loss: 0.1036334 Test Loss: 0.1476567
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.1558048 Vali Loss: 0.1035997 Test Loss: 0.1477363
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.1551882 Vali Loss: 0.1006191 Test Loss: 0.1478109
Validation loss decreased (0.101857 --> 0.100619).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.1550274 Vali Loss: 0.1021396 Test Loss: 0.1478169
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.1542534 Vali Loss: 0.1015114 Test Loss: 0.1478335
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.1552438 Vali Loss: 0.1009033 Test Loss: 0.1478428
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.1552226 Vali Loss: 0.1025163 Test Loss: 0.1478467
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.1548333 Vali Loss: 0.1035486 Test Loss: 0.1478493
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.1549499 Vali Loss: 0.1026403 Test Loss: 0.1478506
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.1547662 Vali Loss: 0.1017488 Test Loss: 0.1478513
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.1547452 Vali Loss: 0.1005876 Test Loss: 0.1478511
Validation loss decreased (0.100619 --> 0.100588).  Saving model ...
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.1539255 Vali Loss: 0.1023608 Test Loss: 0.1478512
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.1557975 Vali Loss: 0.1025599 Test Loss: 0.1478515
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.1535453 Vali Loss: 0.1027961 Test Loss: 0.1478514
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 118 | Train Loss: 0.1552039 Vali Loss: 0.1053991 Test Loss: 0.1478514
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 118 | Train Loss: 0.1544653 Vali Loss: 0.1024481 Test Loss: 0.1478515
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 118 | Train Loss: 0.1541660 Vali Loss: 0.1044281 Test Loss: 0.1478515
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 118 | Train Loss: 0.1546285 Vali Loss: 0.0996231 Test Loss: 0.1478515
Validation loss decreased (0.100588 --> 0.099623).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 118 | Train Loss: 0.1553314 Vali Loss: 0.0992426 Test Loss: 0.1478515
Validation loss decreased (0.099623 --> 0.099243).  Saving model ...
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 118 | Train Loss: 0.1542661 Vali Loss: 0.0994897 Test Loss: 0.1478515
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 118 | Train Loss: 0.1553098 Vali Loss: 0.1005986 Test Loss: 0.1478515
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 118 | Train Loss: 0.1539992 Vali Loss: 0.1030041 Test Loss: 0.1478515
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 118 | Train Loss: 0.1551948 Vali Loss: 0.0992108 Test Loss: 0.1478515
Validation loss decreased (0.099243 --> 0.099211).  Saving model ...
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 118 | Train Loss: 0.1549796 Vali Loss: 0.0998312 Test Loss: 0.1478515
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 118 | Train Loss: 0.1554296 Vali Loss: 0.0993174 Test Loss: 0.1478515
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 118 | Train Loss: 0.1557476 Vali Loss: 0.0986235 Test Loss: 0.1478515
Validation loss decreased (0.099211 --> 0.098624).  Saving model ...
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 118 | Train Loss: 0.1540541 Vali Loss: 0.1005857 Test Loss: 0.1478515
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 118 | Train Loss: 0.1544868 Vali Loss: 0.1002369 Test Loss: 0.1478515
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.1641532182693482e-14
Epoch: 35, Steps: 118 | Train Loss: 0.1542418 Vali Loss: 0.1037076 Test Loss: 0.1478515
EarlyStopping counter: 3 out of 10
Updating learning rate to 5.820766091346741e-15
Epoch: 36, Steps: 118 | Train Loss: 0.1549096 Vali Loss: 0.1014539 Test Loss: 0.1478515
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.9103830456733705e-15
Epoch: 37, Steps: 118 | Train Loss: 0.1554783 Vali Loss: 0.1043956 Test Loss: 0.1478515
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.4551915228366853e-15
Epoch: 38, Steps: 118 | Train Loss: 0.1545762 Vali Loss: 0.1014377 Test Loss: 0.1478515
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.275957614183426e-16
Epoch: 39, Steps: 118 | Train Loss: 0.1549596 Vali Loss: 0.1001677 Test Loss: 0.1478515
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.637978807091713e-16
Epoch: 40, Steps: 118 | Train Loss: 0.1548001 Vali Loss: 0.0998394 Test Loss: 0.1478515
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.8189894035458566e-16
Epoch: 41, Steps: 118 | Train Loss: 0.1536135 Vali Loss: 0.0994642 Test Loss: 0.1478515
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.094947017729283e-17
Epoch: 42, Steps: 118 | Train Loss: 0.1547535 Vali Loss: 0.1054988 Test Loss: 0.1478515
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_SASOL_H3_iTransformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 212
test shape: (212, 3, 1) (212, 3, 1)
test shape: (212, 3, 1) (212, 3, 1)


	mse:0.002330603776499629, mae:0.035854585468769073, rmse:0.048276327550411224, r2:-0.05792093276977539, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0359, RMSE: 0.0483, RÂ²: -0.0579, MAPE: 15490299.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.463 MB of 0.463 MB uploadedwandb: \ 0.463 MB of 0.463 MB uploadedwandb: | 0.463 MB of 0.463 MB uploadedwandb: / 0.463 MB of 0.463 MB uploadedwandb: - 0.463 MB of 0.463 MB uploadedwandb: \ 0.463 MB of 0.463 MB uploadedwandb: | 0.463 MB of 0.463 MB uploadedwandb: / 0.463 MB of 0.463 MB uploadedwandb: - 0.569 MB of 0.730 MB uploaded (0.002 MB deduped)wandb: \ 0.569 MB of 0.730 MB uploaded (0.002 MB deduped)wandb: | 0.730 MB of 0.730 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ˆâ–„â–…â–†â–†â–ƒâ–…â–„â–ƒâ–…â–†â–…â–„â–ƒâ–…â–…â–…â–ˆâ–…â–‡â–‚â–‚â–‚â–ƒâ–…â–‚â–‚â–‚â–â–ƒâ–ƒâ–†â–„â–‡â–„â–ƒâ–‚â–‚â–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 41
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6338563
wandb:     model/trainable_params 6338563
wandb:                  test/loss 0.14785
wandb:                 train/loss 0.15475
wandb:   val/directional_accuracy 47.64151
wandb:                   val/loss 0.1055
wandb:                    val/mae 0.03585
wandb:                   val/mape 1549029900.0
wandb:                    val/mse 0.00233
wandb:                     val/r2 -0.05792
wandb:                   val/rmse 0.04828
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/5hr61etz
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_212951-5hr61etz/logs
Completed: SASOL H=3

Training: iTransformer on SASOL for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_213418-rzj0ppi8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/rzj0ppi8
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_SASOL_H5Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/rzj0ppi8
>>>>>>>start training : long_term_forecast_iTransformer_SASOL_H5_iTransformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 5
============================================================
train 3770
val 209
test 210
Epoch: 1, Steps: 118 | Train Loss: 0.2387393 Vali Loss: 0.1119464 Test Loss: 0.1552405
Validation loss decreased (inf --> 0.111946).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2115171 Vali Loss: 0.1076746 Test Loss: 0.1558590
Validation loss decreased (0.111946 --> 0.107675).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2387392645922758, 'val/loss': 0.11194640930209841, 'test/loss': 0.15524046548775264, '_timestamp': 1762889687.63672}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.21151706241702628, 'val/loss': 0.1076746370111193, 'test/loss': 0.15585898182221822, '_timestamp': 1762889693.4537055}).
Epoch: 3, Steps: 118 | Train Loss: 0.1913899 Vali Loss: 0.1040876 Test Loss: 0.1510100
Validation loss decreased (0.107675 --> 0.104088).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.1796160 Vali Loss: 0.1077094 Test Loss: 0.1491413
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.1722156 Vali Loss: 0.1019826 Test Loss: 0.1481557
Validation loss decreased (0.104088 --> 0.101983).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.1683377 Vali Loss: 0.1086390 Test Loss: 0.1483473
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.1673153 Vali Loss: 0.1040106 Test Loss: 0.1483781
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.1658482 Vali Loss: 0.1022358 Test Loss: 0.1485119
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.1661056 Vali Loss: 0.1050287 Test Loss: 0.1485531
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.1653409 Vali Loss: 0.1018583 Test Loss: 0.1485700
Validation loss decreased (0.101983 --> 0.101858).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.1660102 Vali Loss: 0.1029186 Test Loss: 0.1485766
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.1654687 Vali Loss: 0.1057750 Test Loss: 0.1485748
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.1653587 Vali Loss: 0.1063690 Test Loss: 0.1485764
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.1655077 Vali Loss: 0.1045272 Test Loss: 0.1485795
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.1651701 Vali Loss: 0.1031518 Test Loss: 0.1485799
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.1656298 Vali Loss: 0.1062843 Test Loss: 0.1485809
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.1653510 Vali Loss: 0.1034363 Test Loss: 0.1485808
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.1659555 Vali Loss: 0.1076962 Test Loss: 0.1485810
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.1652144 Vali Loss: 0.1024832 Test Loss: 0.1485809
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.1655004 Vali Loss: 0.1033937 Test Loss: 0.1485810
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_SASOL_H5_iTransformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 210
test shape: (210, 5, 1) (210, 5, 1)
test shape: (210, 5, 1) (210, 5, 1)


	mse:0.002325142966583371, mae:0.03578411415219307, rmse:0.04821973666548729, r2:-0.047644853591918945, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0358, RMSE: 0.0482, RÂ²: -0.0476, MAPE: 15031749.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.487 MB of 0.487 MB uploadedwandb: \ 0.487 MB of 0.487 MB uploadedwandb: | 0.487 MB of 0.487 MB uploadedwandb: / 0.487 MB of 0.487 MB uploadedwandb: - 0.487 MB of 0.645 MB uploadedwandb: \ 0.487 MB of 0.645 MB uploadedwandb: | 0.645 MB of 0.645 MB uploadedwandb: / 0.645 MB of 0.645 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–‡â–â–ˆâ–ƒâ–â–„â–â–‚â–…â–†â–„â–‚â–†â–ƒâ–‡â–‚â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6339589
wandb:     model/trainable_params 6339589
wandb:                  test/loss 0.14858
wandb:                 train/loss 0.1655
wandb:   val/directional_accuracy 47.38095
wandb:                   val/loss 0.10339
wandb:                    val/mae 0.03578
wandb:                   val/mape 1503174900.0
wandb:                    val/mse 0.00233
wandb:                     val/r2 -0.04764
wandb:                   val/rmse 0.04822
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/rzj0ppi8
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_213418-rzj0ppi8/logs
Completed: SASOL H=5

Training: iTransformer on SASOL for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_213824-0jtzfwq8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/0jtzfwq8
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_SASOL_H10Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/0jtzfwq8
>>>>>>>start training : long_term_forecast_iTransformer_SASOL_H10_iTransformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 10
============================================================
train 3765
val 204
test 205
Epoch: 1, Steps: 118 | Train Loss: 0.2385451 Vali Loss: 0.1129075 Test Loss: 0.1606857
Validation loss decreased (inf --> 0.112907).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2146439 Vali Loss: 0.1094712 Test Loss: 0.1535176
Validation loss decreased (0.112907 --> 0.109471).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 118 | Train Loss: 0.2007973 Vali Loss: 0.1077777 Test Loss: 0.1497841
Validation loss decreased (0.109471 --> 0.107778).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23854506950257187, 'val/loss': 0.11290749268872398, 'test/loss': 0.16068568612848008, '_timestamp': 1762889930.7773707}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2146439178515289, 'val/loss': 0.10947115932192121, 'test/loss': 0.15351755172014236, '_timestamp': 1762889936.3575115}).
Epoch: 4, Steps: 118 | Train Loss: 0.1908484 Vali Loss: 0.1067605 Test Loss: 0.1494005
Validation loss decreased (0.107778 --> 0.106760).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.1846386 Vali Loss: 0.1100280 Test Loss: 0.1492433
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.1820460 Vali Loss: 0.1082540 Test Loss: 0.1492437
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.1813421 Vali Loss: 0.1046067 Test Loss: 0.1492918
Validation loss decreased (0.106760 --> 0.104607).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.1806287 Vali Loss: 0.1075503 Test Loss: 0.1492226
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.1797123 Vali Loss: 0.1071650 Test Loss: 0.1492247
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.1799995 Vali Loss: 0.1064199 Test Loss: 0.1492344
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.1798486 Vali Loss: 0.1107082 Test Loss: 0.1492440
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.1792987 Vali Loss: 0.1080921 Test Loss: 0.1492464
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.1794057 Vali Loss: 0.1080131 Test Loss: 0.1492487
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.1798560 Vali Loss: 0.1073649 Test Loss: 0.1492496
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.1795094 Vali Loss: 0.1091347 Test Loss: 0.1492497
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.1793068 Vali Loss: 0.1052734 Test Loss: 0.1492495
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.1799782 Vali Loss: 0.1065786 Test Loss: 0.1492496
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_SASOL_H10_iTransformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 205
test shape: (205, 10, 1) (205, 10, 1)
test shape: (205, 10, 1) (205, 10, 1)


	mse:0.0023102709092199802, mae:0.035456154495477676, rmse:0.04806527867913246, r2:-0.040802836418151855, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0355, RMSE: 0.0481, RÂ²: -0.0408, MAPE: 12688446.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.561 MB of 0.561 MB uploadedwandb: \ 0.561 MB of 0.561 MB uploadedwandb: | 0.561 MB of 0.561 MB uploadedwandb: / 0.561 MB of 0.561 MB uploadedwandb: - 0.561 MB of 0.561 MB uploadedwandb: \ 0.561 MB of 0.719 MB uploadedwandb: | 0.561 MB of 0.719 MB uploadedwandb: / 0.719 MB of 0.719 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–ƒâ–‡â–…â–â–„â–„â–ƒâ–ˆâ–…â–…â–„â–†â–‚â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6342154
wandb:     model/trainable_params 6342154
wandb:                  test/loss 0.14925
wandb:                 train/loss 0.17998
wandb:   val/directional_accuracy 47.75068
wandb:                   val/loss 0.10658
wandb:                    val/mae 0.03546
wandb:                   val/mape 1268844600.0
wandb:                    val/mse 0.00231
wandb:                     val/r2 -0.0408
wandb:                   val/rmse 0.04807
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/0jtzfwq8
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_213824-0jtzfwq8/logs
Completed: SASOL H=10

Training: iTransformer on SASOL for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_214107-b0hrmko4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/b0hrmko4
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_SASOL_H22Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/b0hrmko4
>>>>>>>start training : long_term_forecast_iTransformer_SASOL_H22_iTransformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 22
============================================================
train 3753
val 192
test 193
Epoch: 1, Steps: 118 | Train Loss: 0.2526430 Vali Loss: 0.1141649 Test Loss: 0.1591609
Validation loss decreased (inf --> 0.114165).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2290213 Vali Loss: 0.1119657 Test Loss: 0.1606230
Validation loss decreased (0.114165 --> 0.111966).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 118 | Train Loss: 0.2166729 Vali Loss: 0.1102677 Test Loss: 0.1559536
Validation loss decreased (0.111966 --> 0.110268).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.25264298448623235, 'val/loss': 0.11416491369406383, 'test/loss': 0.15916085562535695, '_timestamp': 1762890089.8066697}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.22902125865221024, 'val/loss': 0.11196567366520564, 'test/loss': 0.16062295543295996, '_timestamp': 1762890095.2729635}).
Epoch: 4, Steps: 118 | Train Loss: 0.2075946 Vali Loss: 0.1097263 Test Loss: 0.1556478
Validation loss decreased (0.110268 --> 0.109726).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2034688 Vali Loss: 0.1096513 Test Loss: 0.1547134
Validation loss decreased (0.109726 --> 0.109651).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2016450 Vali Loss: 0.1096235 Test Loss: 0.1544498
Validation loss decreased (0.109651 --> 0.109624).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2008400 Vali Loss: 0.1094795 Test Loss: 0.1543661
Validation loss decreased (0.109624 --> 0.109480).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2008895 Vali Loss: 0.1094955 Test Loss: 0.1543250
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.1999723 Vali Loss: 0.1094751 Test Loss: 0.1542981
Validation loss decreased (0.109480 --> 0.109475).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.1998806 Vali Loss: 0.1094882 Test Loss: 0.1543026
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.1999884 Vali Loss: 0.1094876 Test Loss: 0.1542995
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.1991766 Vali Loss: 0.1094919 Test Loss: 0.1543007
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.1988949 Vali Loss: 0.1094933 Test Loss: 0.1542985
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.1997538 Vali Loss: 0.1094934 Test Loss: 0.1543000
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.1995221 Vali Loss: 0.1094938 Test Loss: 0.1543003
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.1992611 Vali Loss: 0.1094940 Test Loss: 0.1543000
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.1999857 Vali Loss: 0.1094940 Test Loss: 0.1543001
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.1994305 Vali Loss: 0.1094940 Test Loss: 0.1543001
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.1993205 Vali Loss: 0.1094940 Test Loss: 0.1543001
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_SASOL_H22_iTransformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 193
test shape: (193, 22, 1) (193, 22, 1)
test shape: (193, 22, 1) (193, 22, 1)


	mse:0.002334543038159609, mae:0.0353444442152977, rmse:0.04831710830330849, r2:-0.04028964042663574, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0353, RMSE: 0.0483, RÂ²: -0.0403, MAPE: 12399792.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.624 MB of 0.625 MB uploadedwandb: \ 0.624 MB of 0.625 MB uploadedwandb: | 0.624 MB of 0.625 MB uploadedwandb: / 0.625 MB of 0.625 MB uploadedwandb: - 0.625 MB of 0.625 MB uploadedwandb: \ 0.625 MB of 0.783 MB uploadedwandb: | 0.783 MB of 0.783 MB uploadedwandb: / 0.783 MB of 0.783 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‡â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 18
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6348310
wandb:     model/trainable_params 6348310
wandb:                  test/loss 0.1543
wandb:                 train/loss 0.19932
wandb:   val/directional_accuracy 47.59437
wandb:                   val/loss 0.10949
wandb:                    val/mae 0.03534
wandb:                   val/mape 1239979200.0
wandb:                    val/mse 0.00233
wandb:                     val/r2 -0.04029
wandb:                   val/rmse 0.04832
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/b0hrmko4
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_214107-b0hrmko4/logs
Completed: SASOL H=22

Training: iTransformer on SASOL for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_214427-c5dlva57
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/c5dlva57
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_SASOL_H50Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/c5dlva57
>>>>>>>start training : long_term_forecast_iTransformer_SASOL_H50_iTransformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 50
============================================================
train 3725
val 164
test 165
Epoch: 1, Steps: 117 | Train Loss: 0.2829805 Vali Loss: 0.1160190 Test Loss: 0.1697444
Validation loss decreased (inf --> 0.116019).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 117 | Train Loss: 0.2588377 Vali Loss: 0.1097884 Test Loss: 0.1699439
Validation loss decreased (0.116019 --> 0.109788).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 117 | Train Loss: 0.2466740 Vali Loss: 0.1063245 Test Loss: 0.1689936
Validation loss decreased (0.109788 --> 0.106324).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2829805013970432, 'val/loss': 0.11601899688442548, 'test/loss': 0.16974442700544992, '_timestamp': 1762890295.8177252}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2588377038383076, 'val/loss': 0.10978840912381808, 'test/loss': 0.1699438914656639, '_timestamp': 1762890299.76977}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2588377038383076, 'val/loss': 0.10978840912381808, 'test/loss': 0.1699438914656639, '_timestamp': 1762890299.76977}).
Epoch: 4, Steps: 117 | Train Loss: 0.2405713 Vali Loss: 0.1095735 Test Loss: 0.1694542
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 117 | Train Loss: 0.2371516 Vali Loss: 0.1097161 Test Loss: 0.1676134
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 117 | Train Loss: 0.2361689 Vali Loss: 0.1073684 Test Loss: 0.1685863
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 117 | Train Loss: 0.2350197 Vali Loss: 0.1071590 Test Loss: 0.1686429
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 117 | Train Loss: 0.2350576 Vali Loss: 0.1100740 Test Loss: 0.1686554
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 117 | Train Loss: 0.2341370 Vali Loss: 0.1068894 Test Loss: 0.1686393
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 117 | Train Loss: 0.2342540 Vali Loss: 0.1084737 Test Loss: 0.1686365
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 117 | Train Loss: 0.2342167 Vali Loss: 0.1110756 Test Loss: 0.1686481
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 117 | Train Loss: 0.2340401 Vali Loss: 0.1076363 Test Loss: 0.1686571
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 117 | Train Loss: 0.2347827 Vali Loss: 0.1092572 Test Loss: 0.1686539
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_SASOL_H50_iTransformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 165
test shape: (165, 50, 1) (165, 50, 1)
test shape: (165, 50, 1) (165, 50, 1)


	mse:0.002129900036379695, mae:0.03409022465348244, rmse:0.04615084081888199, r2:-0.03960311412811279, dtw:Not calculated


VAL - MSE: 0.0021, MAE: 0.0341, RMSE: 0.0462, RÂ²: -0.0396, MAPE: 12761763.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.646 MB of 0.648 MB uploadedwandb: \ 0.648 MB of 0.648 MB uploadedwandb: | 0.648 MB of 0.648 MB uploadedwandb: / 0.648 MB of 0.805 MB uploadedwandb: - 0.648 MB of 0.805 MB uploadedwandb: \ 0.805 MB of 0.805 MB uploadedwandb: | 0.805 MB of 0.805 MB uploadedwandb: / 0.805 MB of 0.805 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–†â–ˆâ–â–…â–…â–…â–…â–…â–…â–…â–…
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–†â–†â–ƒâ–‚â–‡â–‚â–„â–ˆâ–ƒâ–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6362674
wandb:     model/trainable_params 6362674
wandb:                  test/loss 0.16865
wandb:                 train/loss 0.23478
wandb:   val/directional_accuracy 48.55906
wandb:                   val/loss 0.10926
wandb:                    val/mae 0.03409
wandb:                   val/mape 1276176300.0
wandb:                    val/mse 0.00213
wandb:                     val/r2 -0.0396
wandb:                   val/rmse 0.04615
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/c5dlva57
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_214427-c5dlva57/logs
Completed: SASOL H=50

Training: iTransformer on SASOL for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_214731-gxjlspus
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/gxjlspus
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_SASOL_H100Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/gxjlspus
>>>>>>>start training : long_term_forecast_iTransformer_SASOL_H100_iTransformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 100
============================================================
train 3675
val 114
test 115
Epoch: 1, Steps: 115 | Train Loss: 0.3464702 Vali Loss: 0.1294734 Test Loss: 0.1770292
Validation loss decreased (inf --> 0.129473).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 115 | Train Loss: 0.3162818 Vali Loss: 0.1251334 Test Loss: 0.1764686
Validation loss decreased (0.129473 --> 0.125133).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 115 | Train Loss: 0.3053067 Vali Loss: 0.1226277 Test Loss: 0.1768563
Validation loss decreased (0.125133 --> 0.122628).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.34647021475045575, 'val/loss': 0.12947341427206993, 'test/loss': 0.17702916637063026, '_timestamp': 1762890476.863931}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3162817945946818, 'val/loss': 0.12513336539268494, 'test/loss': 0.17646856606006622, '_timestamp': 1762890482.5267324}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3162817945946818, 'val/loss': 0.12513336539268494, 'test/loss': 0.17646856606006622, '_timestamp': 1762890482.5267324}).
Epoch: 4, Steps: 115 | Train Loss: 0.3000879 Vali Loss: 0.1236607 Test Loss: 0.1757628
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 115 | Train Loss: 0.2970147 Vali Loss: 0.1240225 Test Loss: 0.1760582
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 115 | Train Loss: 0.2954656 Vali Loss: 0.1230893 Test Loss: 0.1764698
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 115 | Train Loss: 0.2945747 Vali Loss: 0.1234754 Test Loss: 0.1767054
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 115 | Train Loss: 0.2942399 Vali Loss: 0.1238371 Test Loss: 0.1766247
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 115 | Train Loss: 0.2937497 Vali Loss: 0.1229707 Test Loss: 0.1766647
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 115 | Train Loss: 0.2940803 Vali Loss: 0.1235446 Test Loss: 0.1766753
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 115 | Train Loss: 0.2939799 Vali Loss: 0.1237579 Test Loss: 0.1766943
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 115 | Train Loss: 0.2937793 Vali Loss: 0.1230756 Test Loss: 0.1766984
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 115 | Train Loss: 0.2938310 Vali Loss: 0.1220144 Test Loss: 0.1766984
Validation loss decreased (0.122628 --> 0.122014).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 115 | Train Loss: 0.2938776 Vali Loss: 0.1216415 Test Loss: 0.1766991
Validation loss decreased (0.122014 --> 0.121642).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 115 | Train Loss: 0.2937057 Vali Loss: 0.1234347 Test Loss: 0.1766998
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 115 | Train Loss: 0.2940083 Vali Loss: 0.1219867 Test Loss: 0.1766998
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 115 | Train Loss: 0.2940516 Vali Loss: 0.1220761 Test Loss: 0.1766998
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 115 | Train Loss: 0.2936666 Vali Loss: 0.1212308 Test Loss: 0.1766999
Validation loss decreased (0.121642 --> 0.121231).  Saving model ...
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 115 | Train Loss: 0.2938791 Vali Loss: 0.1239866 Test Loss: 0.1766999
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 115 | Train Loss: 0.2938384 Vali Loss: 0.1236837 Test Loss: 0.1766999
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 115 | Train Loss: 0.2938080 Vali Loss: 0.1229953 Test Loss: 0.1766999
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 115 | Train Loss: 0.2939797 Vali Loss: 0.1228293 Test Loss: 0.1766999
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 115 | Train Loss: 0.2938145 Vali Loss: 0.1216905 Test Loss: 0.1766999
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 115 | Train Loss: 0.2938819 Vali Loss: 0.1228156 Test Loss: 0.1766999
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 115 | Train Loss: 0.2938276 Vali Loss: 0.1227371 Test Loss: 0.1766999
EarlyStopping counter: 7 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 115 | Train Loss: 0.2941461 Vali Loss: 0.1229239 Test Loss: 0.1766999
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 115 | Train Loss: 0.2941114 Vali Loss: 0.1222109 Test Loss: 0.1766999
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 115 | Train Loss: 0.2938693 Vali Loss: 0.1220911 Test Loss: 0.1766999
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_SASOL_H100_iTransformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 115
test shape: (115, 100, 1) (115, 100, 1)
test shape: (115, 100, 1) (115, 100, 1)


	mse:0.0020208489149808884, mae:0.0333939827978611, rmse:0.044953852891922, r2:-0.01835775375366211, dtw:Not calculated


VAL - MSE: 0.0020, MAE: 0.0334, RMSE: 0.0450, RÂ²: -0.0184, MAPE: 11200290.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.669 MB of 0.674 MB uploadedwandb: \ 0.674 MB of 0.674 MB uploadedwandb: | 0.674 MB of 0.674 MB uploadedwandb: / 0.674 MB of 0.674 MB uploadedwandb: - 0.674 MB of 0.674 MB uploadedwandb: \ 0.674 MB of 0.833 MB uploadedwandb: | 0.676 MB of 0.833 MB uploadedwandb: / 0.833 MB of 0.833 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–ƒâ–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–‡â–ˆâ–†â–‡â–ˆâ–…â–‡â–‡â–†â–ƒâ–‚â–‡â–ƒâ–ƒâ–â–ˆâ–‡â–…â–…â–‚â–…â–…â–…â–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 27
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6388324
wandb:     model/trainable_params 6388324
wandb:                  test/loss 0.1767
wandb:                 train/loss 0.29387
wandb:   val/directional_accuracy 49.10848
wandb:                   val/loss 0.12209
wandb:                    val/mae 0.03339
wandb:                   val/mape 1120029000.0
wandb:                    val/mse 0.00202
wandb:                     val/r2 -0.01836
wandb:                   val/rmse 0.04495
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/gxjlspus
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_214731-gxjlspus/logs
Completed: SASOL H=100

Training: iTransformer on DRD_GOLD for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_215103-l6p50w4y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/l6p50w4y
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_DRD_GOLD_H3Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/l6p50w4y
>>>>>>>start training : long_term_forecast_iTransformer_DRD_GOLD_H3_iTransformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.2596387 Vali Loss: 0.1445131 Test Loss: 0.1404138
Validation loss decreased (inf --> 0.144513).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2307764 Vali Loss: 0.1368440 Test Loss: 0.1426201
Validation loss decreased (0.144513 --> 0.136844).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
Epoch: 3, Steps: 133 | Train Loss: 0.2084435 Vali Loss: 0.1320111 Test Loss: 0.1402673
Validation loss decreased (0.136844 --> 0.132011).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.25963870318312393, 'val/loss': 0.14451308641582727, 'test/loss': 0.1404138319194317, '_timestamp': 1762890687.3002896}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2307763682272201, 'val/loss': 0.13684402965009212, 'test/loss': 0.1426201043650508, '_timestamp': 1762890693.1494741}).
Epoch: 4, Steps: 133 | Train Loss: 0.1956137 Vali Loss: 0.1312074 Test Loss: 0.1386062
Validation loss decreased (0.132011 --> 0.131207).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1875072 Vali Loss: 0.1309875 Test Loss: 0.1374917
Validation loss decreased (0.131207 --> 0.130987).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1839664 Vali Loss: 0.1367498 Test Loss: 0.1373409
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1822394 Vali Loss: 0.1329364 Test Loss: 0.1374939
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1809395 Vali Loss: 0.1307206 Test Loss: 0.1374415
Validation loss decreased (0.130987 --> 0.130721).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1795237 Vali Loss: 0.1300735 Test Loss: 0.1375882
Validation loss decreased (0.130721 --> 0.130073).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1792474 Vali Loss: 0.1347490 Test Loss: 0.1375983
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1787874 Vali Loss: 0.1306816 Test Loss: 0.1376038
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1792445 Vali Loss: 0.1320240 Test Loss: 0.1376236
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1793905 Vali Loss: 0.1367349 Test Loss: 0.1376269
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1792144 Vali Loss: 0.1366936 Test Loss: 0.1376290
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1791828 Vali Loss: 0.1327276 Test Loss: 0.1376305
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1793592 Vali Loss: 0.1333916 Test Loss: 0.1376303
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1800661 Vali Loss: 0.1330764 Test Loss: 0.1376314
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1790438 Vali Loss: 0.1327269 Test Loss: 0.1376314
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1790472 Vali Loss: 0.1323421 Test Loss: 0.1376314
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_DRD_GOLD_H3_iTransformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.0010053160367533565, mae:0.022921551018953323, rmse:0.03170672059059143, r2:-0.08404624462127686, dtw:Not calculated


VAL - MSE: 0.0010, MAE: 0.0229, RMSE: 0.0317, RÂ²: -0.0840, MAPE: 5140099.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.488 MB of 0.488 MB uploadedwandb: \ 0.488 MB of 0.488 MB uploadedwandb: | 0.488 MB of 0.488 MB uploadedwandb: / 0.488 MB of 0.488 MB uploadedwandb: - 0.488 MB of 0.488 MB uploadedwandb: \ 0.488 MB of 0.488 MB uploadedwandb: | 0.488 MB of 0.488 MB uploadedwandb: / 0.488 MB of 0.488 MB uploadedwandb: - 0.594 MB of 0.751 MB uploaded (0.002 MB deduped)wandb: \ 0.751 MB of 0.751 MB uploaded (0.002 MB deduped)wandb: | 0.751 MB of 0.751 MB uploaded (0.002 MB deduped)wandb: / 0.751 MB of 0.751 MB uploaded (0.002 MB deduped)wandb: - 0.751 MB of 0.751 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–‚â–‚â–ˆâ–„â–‚â–â–†â–‚â–ƒâ–ˆâ–ˆâ–„â–„â–„â–„â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 18
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6338563
wandb:     model/trainable_params 6338563
wandb:                  test/loss 0.13763
wandb:                 train/loss 0.17905
wandb:   val/directional_accuracy 51.05485
wandb:                   val/loss 0.13234
wandb:                    val/mae 0.02292
wandb:                   val/mape 514009900.0
wandb:                    val/mse 0.00101
wandb:                     val/r2 -0.08405
wandb:                   val/rmse 0.03171
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/l6p50w4y
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_215103-l6p50w4y/logs
Completed: DRD_GOLD H=3

Training: iTransformer on DRD_GOLD for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_215420-rfntxkik
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/rfntxkik
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_DRD_GOLD_H5Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/rfntxkik
>>>>>>>start training : long_term_forecast_iTransformer_DRD_GOLD_H5_iTransformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.2628797 Vali Loss: 0.1438229 Test Loss: 0.1457741
Validation loss decreased (inf --> 0.143823).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2370124 Vali Loss: 0.1415731 Test Loss: 0.1441729
Validation loss decreased (0.143823 --> 0.141573).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2628797448443291, 'val/loss': 0.14382291212677956, 'test/loss': 0.1457741493359208, '_timestamp': 1762890884.4793952}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23701243881220208, 'val/loss': 0.14157305657863617, 'test/loss': 0.14417289663106203, '_timestamp': 1762890890.3423915}).
Epoch: 3, Steps: 133 | Train Loss: 0.2157248 Vali Loss: 0.1444150 Test Loss: 0.1416814
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2043089 Vali Loss: 0.1371379 Test Loss: 0.1395204
Validation loss decreased (0.141573 --> 0.137138).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1970303 Vali Loss: 0.1358806 Test Loss: 0.1396536
Validation loss decreased (0.137138 --> 0.135881).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1935456 Vali Loss: 0.1332811 Test Loss: 0.1390849
Validation loss decreased (0.135881 --> 0.133281).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1924337 Vali Loss: 0.1342562 Test Loss: 0.1391173
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1918561 Vali Loss: 0.1425507 Test Loss: 0.1391834
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1901408 Vali Loss: 0.1426389 Test Loss: 0.1391425
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1909194 Vali Loss: 0.1392349 Test Loss: 0.1391516
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1900593 Vali Loss: 0.1359432 Test Loss: 0.1391643
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1903131 Vali Loss: 0.1341960 Test Loss: 0.1391638
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1902286 Vali Loss: 0.1405714 Test Loss: 0.1391662
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1902920 Vali Loss: 0.1312805 Test Loss: 0.1391674
Validation loss decreased (0.133281 --> 0.131281).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1900079 Vali Loss: 0.1321943 Test Loss: 0.1391690
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1900469 Vali Loss: 0.1409169 Test Loss: 0.1391688
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1911671 Vali Loss: 0.1388125 Test Loss: 0.1391687
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1903596 Vali Loss: 0.1393953 Test Loss: 0.1391688
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1900476 Vali Loss: 0.1361979 Test Loss: 0.1391690
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1905921 Vali Loss: 0.1354378 Test Loss: 0.1391689
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1901635 Vali Loss: 0.1333383 Test Loss: 0.1391690
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1903836 Vali Loss: 0.1407535 Test Loss: 0.1391691
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1903898 Vali Loss: 0.1347618 Test Loss: 0.1391691
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.1906090 Vali Loss: 0.1389756 Test Loss: 0.1391690
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_DRD_GOLD_H5_iTransformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.0009955341229215264, mae:0.022703640162944794, rmse:0.031552087515592575, r2:-0.06611740589141846, dtw:Not calculated


VAL - MSE: 0.0010, MAE: 0.0227, RMSE: 0.0316, RÂ²: -0.0661, MAPE: 4259841.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.528 MB of 0.528 MB uploadedwandb: \ 0.528 MB of 0.528 MB uploadedwandb: | 0.528 MB of 0.528 MB uploadedwandb: / 0.528 MB of 0.528 MB uploadedwandb: - 0.528 MB of 0.686 MB uploadedwandb: \ 0.528 MB of 0.686 MB uploadedwandb: | 0.686 MB of 0.686 MB uploadedwandb: / 0.686 MB of 0.686 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–ƒâ–‚â–ƒâ–‡â–‡â–…â–ƒâ–ƒâ–†â–â–â–†â–…â–…â–„â–ƒâ–‚â–†â–ƒâ–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 23
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6339589
wandb:     model/trainable_params 6339589
wandb:                  test/loss 0.13917
wandb:                 train/loss 0.19061
wandb:   val/directional_accuracy 50.0
wandb:                   val/loss 0.13898
wandb:                    val/mae 0.0227
wandb:                   val/mape 425984150.0
wandb:                    val/mse 0.001
wandb:                     val/r2 -0.06612
wandb:                   val/rmse 0.03155
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/rfntxkik
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_215420-rfntxkik/logs
Completed: DRD_GOLD H=5

Training: iTransformer on DRD_GOLD for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_215742-qarkg80q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/qarkg80q
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_DRD_GOLD_H10Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/qarkg80q
>>>>>>>start training : long_term_forecast_iTransformer_DRD_GOLD_H10_iTransformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2704900 Vali Loss: 0.1537544 Test Loss: 0.1549872
Validation loss decreased (inf --> 0.153754).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2460412 Vali Loss: 0.1452203 Test Loss: 0.1504901
Validation loss decreased (0.153754 --> 0.145220).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2281388 Vali Loss: 0.1461109 Test Loss: 0.1499445
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2704899755859734, 'val/loss': 0.15375436376780272, 'test/loss': 0.15498719923198223, '_timestamp': 1762891084.6733263}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24604117041243648, 'val/loss': 0.14522028993815184, 'test/loss': 0.1504900697618723, '_timestamp': 1762891091.064332}).
Epoch: 4, Steps: 133 | Train Loss: 0.2194372 Vali Loss: 0.1532572 Test Loss: 0.1470653
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2130076 Vali Loss: 0.1496259 Test Loss: 0.1467843
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2114751 Vali Loss: 0.1434915 Test Loss: 0.1468375
Validation loss decreased (0.145220 --> 0.143492).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2097179 Vali Loss: 0.1460094 Test Loss: 0.1466000
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2091777 Vali Loss: 0.1450872 Test Loss: 0.1466066
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2086581 Vali Loss: 0.1451633 Test Loss: 0.1465960
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2083684 Vali Loss: 0.1453999 Test Loss: 0.1465885
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2087615 Vali Loss: 0.1466800 Test Loss: 0.1465946
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2083224 Vali Loss: 0.1431655 Test Loss: 0.1465940
Validation loss decreased (0.143492 --> 0.143166).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2080565 Vali Loss: 0.1525298 Test Loss: 0.1465931
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2083036 Vali Loss: 0.1502925 Test Loss: 0.1465925
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2086691 Vali Loss: 0.1465523 Test Loss: 0.1465925
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2083067 Vali Loss: 0.1451606 Test Loss: 0.1465926
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2083355 Vali Loss: 0.1493027 Test Loss: 0.1465923
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2080398 Vali Loss: 0.1485127 Test Loss: 0.1465926
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2083910 Vali Loss: 0.1439562 Test Loss: 0.1465924
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2088080 Vali Loss: 0.1450655 Test Loss: 0.1465925
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2084771 Vali Loss: 0.1530992 Test Loss: 0.1465925
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2085641 Vali Loss: 0.1519245 Test Loss: 0.1465925
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_DRD_GOLD_H10_iTransformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.001006429665721953, mae:0.022904368117451668, rmse:0.03172427415847778, r2:-0.06134068965911865, dtw:Not calculated


VAL - MSE: 0.0010, MAE: 0.0229, RMSE: 0.0317, RÂ²: -0.0613, MAPE: 4807365.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.559 MB of 0.560 MB uploadedwandb: \ 0.560 MB of 0.560 MB uploadedwandb: | 0.560 MB of 0.560 MB uploadedwandb: / 0.560 MB of 0.560 MB uploadedwandb: - 0.560 MB of 0.718 MB uploadedwandb: \ 0.718 MB of 0.718 MB uploadedwandb: | 0.718 MB of 0.718 MB uploadedwandb: / 0.718 MB of 0.718 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–ˆâ–…â–â–ƒâ–‚â–‚â–ƒâ–ƒâ–â–‡â–†â–ƒâ–‚â–…â–…â–‚â–‚â–ˆâ–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6342154
wandb:     model/trainable_params 6342154
wandb:                  test/loss 0.14659
wandb:                 train/loss 0.20856
wandb:   val/directional_accuracy 48.79227
wandb:                   val/loss 0.15192
wandb:                    val/mae 0.0229
wandb:                   val/mape 480736550.0
wandb:                    val/mse 0.00101
wandb:                     val/r2 -0.06134
wandb:                   val/rmse 0.03172
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/qarkg80q
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_215742-qarkg80q/logs
Completed: DRD_GOLD H=10

Training: iTransformer on DRD_GOLD for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_220102-6ylgtuik
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/6ylgtuik
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_DRD_GOLD_H22Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/6ylgtuik
>>>>>>>start training : long_term_forecast_iTransformer_DRD_GOLD_H22_iTransformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2933362 Vali Loss: 0.1738161 Test Loss: 0.1615857
Validation loss decreased (inf --> 0.173816).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2676746 Vali Loss: 0.1700414 Test Loss: 0.1588551
Validation loss decreased (0.173816 --> 0.170041).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2933361829456055, 'val/loss': 0.17381608911923, 'test/loss': 0.16158574180943625, '_timestamp': 1762891288.3158715}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2676746020037116, 'val/loss': 0.17004142488752091, 'test/loss': 0.15885506676776068, '_timestamp': 1762891294.2992811}).
Epoch: 3, Steps: 132 | Train Loss: 0.2536986 Vali Loss: 0.1682414 Test Loss: 0.1555419
Validation loss decreased (0.170041 --> 0.168241).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2464281 Vali Loss: 0.1691882 Test Loss: 0.1544636
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2421308 Vali Loss: 0.1686078 Test Loss: 0.1547738
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2398931 Vali Loss: 0.1677746 Test Loss: 0.1546545
Validation loss decreased (0.168241 --> 0.167775).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2390901 Vali Loss: 0.1676592 Test Loss: 0.1550275
Validation loss decreased (0.167775 --> 0.167659).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2388359 Vali Loss: 0.1684589 Test Loss: 0.1547779
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2383512 Vali Loss: 0.1684159 Test Loss: 0.1548226
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2380182 Vali Loss: 0.1690307 Test Loss: 0.1548114
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2379486 Vali Loss: 0.1680644 Test Loss: 0.1548341
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2378040 Vali Loss: 0.1675347 Test Loss: 0.1548351
Validation loss decreased (0.167659 --> 0.167535).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2383680 Vali Loss: 0.1684638 Test Loss: 0.1548375
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2380993 Vali Loss: 0.1683603 Test Loss: 0.1548355
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2375307 Vali Loss: 0.1678789 Test Loss: 0.1548353
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2378970 Vali Loss: 0.1678972 Test Loss: 0.1548356
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2380876 Vali Loss: 0.1678057 Test Loss: 0.1548355
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2381044 Vali Loss: 0.1687401 Test Loss: 0.1548356
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2382061 Vali Loss: 0.1685440 Test Loss: 0.1548355
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2380315 Vali Loss: 0.1676467 Test Loss: 0.1548356
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2383281 Vali Loss: 0.1683446 Test Loss: 0.1548356
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2381930 Vali Loss: 0.1682515 Test Loss: 0.1548356
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_DRD_GOLD_H22_iTransformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.0010060988133773208, mae:0.02273186296224594, rmse:0.031719062477350235, r2:-0.05651581287384033, dtw:Not calculated


VAL - MSE: 0.0010, MAE: 0.0227, RMSE: 0.0317, RÂ²: -0.0565, MAPE: 3828077.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.617 MB of 0.617 MB uploadedwandb: \ 0.617 MB of 0.617 MB uploadedwandb: | 0.617 MB of 0.617 MB uploadedwandb: / 0.617 MB of 0.617 MB uploadedwandb: - 0.617 MB of 0.776 MB uploadedwandb: \ 0.776 MB of 0.776 MB uploadedwandb: | 0.776 MB of 0.776 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–ƒâ–‚â–…â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–ˆâ–†â–‚â–‚â–…â–…â–‡â–ƒâ–â–…â–„â–‚â–ƒâ–‚â–†â–…â–â–„â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6348310
wandb:     model/trainable_params 6348310
wandb:                  test/loss 0.15484
wandb:                 train/loss 0.23819
wandb:   val/directional_accuracy 48.90782
wandb:                   val/loss 0.16825
wandb:                    val/mae 0.02273
wandb:                   val/mape 382807700.0
wandb:                    val/mse 0.00101
wandb:                     val/r2 -0.05652
wandb:                   val/rmse 0.03172
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/6ylgtuik
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_220102-6ylgtuik/logs
Completed: DRD_GOLD H=22

Training: iTransformer on DRD_GOLD for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_220424-mwq88rgv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/mwq88rgv
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_DRD_GOLD_H50Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/mwq88rgv
>>>>>>>start training : long_term_forecast_iTransformer_DRD_GOLD_H50_iTransformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.3426557 Vali Loss: 0.2093735 Test Loss: 0.1596648
Validation loss decreased (inf --> 0.209373).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.3151401 Vali Loss: 0.2056924 Test Loss: 0.1611728
Validation loss decreased (0.209373 --> 0.205692).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.3013173 Vali Loss: 0.2057393 Test Loss: 0.1570138
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3426557429360621, 'val/loss': 0.2093734567364057, 'test/loss': 0.15966479976971945, '_timestamp': 1762891489.809651}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.31514013344139763, 'val/loss': 0.20569235583146414, 'test/loss': 0.16117284695307413, '_timestamp': 1762891495.5772574}).
Epoch: 4, Steps: 132 | Train Loss: 0.2944515 Vali Loss: 0.2066420 Test Loss: 0.1589050
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2908025 Vali Loss: 0.2067022 Test Loss: 0.1598554
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2883536 Vali Loss: 0.2069130 Test Loss: 0.1605149
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2873930 Vali Loss: 0.2070347 Test Loss: 0.1603597
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2886839 Vali Loss: 0.2070707 Test Loss: 0.1606133
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2885218 Vali Loss: 0.2070835 Test Loss: 0.1605178
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2905922 Vali Loss: 0.2071154 Test Loss: 0.1604612
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2863650 Vali Loss: 0.2067371 Test Loss: 0.1604020
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2872948 Vali Loss: 0.2070704 Test Loss: 0.1604215
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_DRD_GOLD_H50_iTransformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.0009944066405296326, mae:0.022625697776675224, rmse:0.031534213572740555, r2:-0.06789624691009521, dtw:Not calculated


VAL - MSE: 0.0010, MAE: 0.0226, RMSE: 0.0315, RÂ²: -0.0679, MAPE: 2978146.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.750 MB of 0.752 MB uploadedwandb: \ 0.750 MB of 0.752 MB uploadedwandb: | 0.750 MB of 0.752 MB uploadedwandb: / 0.752 MB of 0.752 MB uploadedwandb: - 0.752 MB of 0.752 MB uploadedwandb: \ 0.752 MB of 0.909 MB uploadedwandb: | 0.909 MB of 0.909 MB uploadedwandb: / 0.909 MB of 0.909 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–…â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–‚â–‚â–ƒâ–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–†â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6362674
wandb:     model/trainable_params 6362674
wandb:                  test/loss 0.16042
wandb:                 train/loss 0.28729
wandb:   val/directional_accuracy 49.82814
wandb:                   val/loss 0.20707
wandb:                    val/mae 0.02263
wandb:                   val/mape 297814650.0
wandb:                    val/mse 0.00099
wandb:                     val/r2 -0.0679
wandb:                   val/rmse 0.03153
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/mwq88rgv
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_220424-mwq88rgv/logs
Completed: DRD_GOLD H=50

Training: iTransformer on DRD_GOLD for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_220703-5i31ddhr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/5i31ddhr
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_DRD_GOLD_H100Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/5i31ddhr
>>>>>>>start training : long_term_forecast_iTransformer_DRD_GOLD_H100_iTransformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.4252773 Vali Loss: 0.2711315 Test Loss: 0.1640635
Validation loss decreased (inf --> 0.271132).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.3894672 Vali Loss: 0.2796888 Test Loss: 0.1636090
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.3750530 Vali Loss: 0.2702967 Test Loss: 0.1636916
Validation loss decreased (0.271132 --> 0.270297).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.4252773103805689, 'val/loss': 0.2711315035820007, 'test/loss': 0.16406354904174805, '_timestamp': 1762891645.936894}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3894672159965222, 'val/loss': 0.2796888440847397, 'test/loss': 0.1636089950799942, '_timestamp': 1762891651.8525064}).
Epoch: 4, Steps: 130 | Train Loss: 0.3667574 Vali Loss: 0.2814205 Test Loss: 0.1646211
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.3619739 Vali Loss: 0.2854768 Test Loss: 0.1659749
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.3600004 Vali Loss: 0.2898006 Test Loss: 0.1655793
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.3581989 Vali Loss: 0.2918886 Test Loss: 0.1666365
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.3582263 Vali Loss: 0.2864624 Test Loss: 0.1664930
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.3573878 Vali Loss: 0.2877010 Test Loss: 0.1665402
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.3574680 Vali Loss: 0.2788535 Test Loss: 0.1666177
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.3574214 Vali Loss: 0.2817560 Test Loss: 0.1666655
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.3578052 Vali Loss: 0.2801811 Test Loss: 0.1666746
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.3581687 Vali Loss: 0.2848457 Test Loss: 0.1666628
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_DRD_GOLD_H100_iTransformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0009458996937610209, mae:0.021703805774450302, rmse:0.030755482614040375, r2:-0.046875, dtw:Not calculated


VAL - MSE: 0.0009, MAE: 0.0217, RMSE: 0.0308, RÂ²: -0.0469, MAPE: 1554637.62%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.814 MB of 0.819 MB uploadedwandb: \ 0.814 MB of 0.819 MB uploadedwandb: | 0.819 MB of 0.819 MB uploadedwandb: / 0.819 MB of 0.976 MB uploadedwandb: - 0.819 MB of 0.976 MB uploadedwandb: \ 0.976 MB of 0.976 MB uploadedwandb: | 0.976 MB of 0.976 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ƒâ–†â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–…â–†â–‡â–ˆâ–†â–‡â–„â–…â–„â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6388324
wandb:     model/trainable_params 6388324
wandb:                  test/loss 0.16666
wandb:                 train/loss 0.35817
wandb:   val/directional_accuracy 47.95094
wandb:                   val/loss 0.28485
wandb:                    val/mae 0.0217
wandb:                   val/mape 155463762.5
wandb:                    val/mse 0.00095
wandb:                     val/r2 -0.04687
wandb:                   val/rmse 0.03076
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/5i31ddhr
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_220703-5i31ddhr/logs
Completed: DRD_GOLD H=100

Training: iTransformer on ANGLO_AMERICAN for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_221009-8pemptay
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/8pemptay
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_ANGLO_AMERICAN_H3Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/8pemptay
>>>>>>>start training : long_term_forecast_iTransformer_ANGLO_AMERICAN_H3_iTransformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 3
============================================================
train 778
val 44
test 46
Epoch: 1, Steps: 25 | Train Loss: 0.3251596 Vali Loss: 0.4188872 Test Loss: 0.8458178
Validation loss decreased (inf --> 0.418887).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 25 | Train Loss: 0.2576269 Vali Loss: 0.3995993 Test Loss: 0.7997370
Validation loss decreased (0.418887 --> 0.399599).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 25 | Train Loss: 0.2301982 Vali Loss: 0.4053732 Test Loss: 0.7901570
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 25 | Train Loss: 0.2158854 Vali Loss: 0.4328853 Test Loss: 0.7654210
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 25 | Train Loss: 0.2133674 Vali Loss: 0.3808931 Test Loss: 0.7661930
Validation loss decreased (0.399599 --> 0.380893).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 25 | Train Loss: 0.2105689 Vali Loss: 0.4045551 Test Loss: 0.7615587
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.32515959441661835, 'val/loss': 0.4188871532678604, 'test/loss': 0.8458178043365479, '_timestamp': 1762891833.5062485}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25762691736221316, 'val/loss': 0.39959925413131714, 'test/loss': 0.7997369915246964, '_timestamp': 1762891837.4491794}).
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.32515959441661835, 'val/loss': 0.4188871532678604, 'test/loss': 0.8458178043365479, '_timestamp': 1762891833.5062485}).
Epoch: 7, Steps: 25 | Train Loss: 0.2080191 Vali Loss: 0.4142115 Test Loss: 0.7632621
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 25 | Train Loss: 0.2041193 Vali Loss: 0.3951266 Test Loss: 0.7634611
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 25 | Train Loss: 0.2056402 Vali Loss: 0.4241959 Test Loss: 0.7634780
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 25 | Train Loss: 0.2034598 Vali Loss: 0.4431418 Test Loss: 0.7633633
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 25 | Train Loss: 0.2034096 Vali Loss: 0.3672746 Test Loss: 0.7633097
Validation loss decreased (0.380893 --> 0.367275).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 25 | Train Loss: 0.2036617 Vali Loss: 0.4252879 Test Loss: 0.7633275
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 25 | Train Loss: 0.2053870 Vali Loss: 0.4285853 Test Loss: 0.7632956
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 25 | Train Loss: 0.2063836 Vali Loss: 0.4506362 Test Loss: 0.7632895
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 25 | Train Loss: 0.2027376 Vali Loss: 0.4334507 Test Loss: 0.7632887
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 25 | Train Loss: 0.2030722 Vali Loss: 0.3808448 Test Loss: 0.7632912
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 25 | Train Loss: 0.2047955 Vali Loss: 0.3962002 Test Loss: 0.7632934
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 25 | Train Loss: 0.2063641 Vali Loss: 0.3990559 Test Loss: 0.7632892
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 25 | Train Loss: 0.2076481 Vali Loss: 0.3960468 Test Loss: 0.7632887
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 25 | Train Loss: 0.2047081 Vali Loss: 0.4213088 Test Loss: 0.7632887
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 25 | Train Loss: 0.2052362 Vali Loss: 0.4451063 Test Loss: 0.7632888
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_ANGLO_AMERICAN_H3_iTransformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 46
test shape: (46, 3, 1) (46, 3, 1)
test shape: (46, 3, 1) (46, 3, 1)


	mse:0.00807176623493433, mae:0.06103427708148956, rmse:0.0898430123925209, r2:-0.19142413139343262, dtw:Not calculated


VAL - MSE: 0.0081, MAE: 0.0610, RMSE: 0.0898, RÂ²: -0.1914, MAPE: 164522944.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.461 MB of 0.462 MB uploadedwandb: \ 0.461 MB of 0.462 MB uploadedwandb: | 0.462 MB of 0.462 MB uploadedwandb: / 0.462 MB of 0.462 MB uploadedwandb: - 0.462 MB of 0.462 MB uploadedwandb: \ 0.462 MB of 0.462 MB uploadedwandb: | 0.567 MB of 0.724 MB uploaded (0.002 MB deduped)wandb: / 0.567 MB of 0.724 MB uploaded (0.002 MB deduped)wandb: - 0.724 MB of 0.724 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–„â–ƒâ–‚â–â–‚â–â–â–â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–‡â–‚â–„â–…â–ƒâ–†â–‡â–â–†â–†â–ˆâ–‡â–‚â–ƒâ–„â–ƒâ–†â–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6338563
wandb:     model/trainable_params 6338563
wandb:                  test/loss 0.76329
wandb:                 train/loss 0.20524
wandb:   val/directional_accuracy 35.86957
wandb:                   val/loss 0.44511
wandb:                    val/mae 0.06103
wandb:                   val/mape 16452294400.0
wandb:                    val/mse 0.00807
wandb:                     val/r2 -0.19142
wandb:                   val/rmse 0.08984
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/8pemptay
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_221009-8pemptay/logs
Completed: ANGLO_AMERICAN H=3

Training: iTransformer on ANGLO_AMERICAN for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_221319-h9xemwxj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/h9xemwxj
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_ANGLO_AMERICAN_H5Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/h9xemwxj
>>>>>>>start training : long_term_forecast_iTransformer_ANGLO_AMERICAN_H5_iTransformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 5
============================================================
train 776
val 42
test 44
Epoch: 1, Steps: 25 | Train Loss: 0.3421048 Vali Loss: 0.4641661 Test Loss: 0.8723198
Validation loss decreased (inf --> 0.464166).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 25 | Train Loss: 0.2720714 Vali Loss: 0.4391211 Test Loss: 0.8045535
Validation loss decreased (0.464166 --> 0.439121).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 25 | Train Loss: 0.2493676 Vali Loss: 0.4148480 Test Loss: 0.7867574
Validation loss decreased (0.439121 --> 0.414848).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 25 | Train Loss: 0.2326916 Vali Loss: 0.4018981 Test Loss: 0.7866437
Validation loss decreased (0.414848 --> 0.401898).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 25 | Train Loss: 0.2256988 Vali Loss: 0.4284123 Test Loss: 0.7815970
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 25 | Train Loss: 0.2244198 Vali Loss: 0.4555124 Test Loss: 0.7814658
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 25 | Train Loss: 0.2271900 Vali Loss: 0.4034537 Test Loss: 0.7808606
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.34210479736328125, 'val/loss': 0.46416614949703217, 'test/loss': 0.8723198473453522, '_timestamp': 1762892023.820243}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2720713800191879, 'val/loss': 0.4391210973262787, 'test/loss': 0.8045535087585449, '_timestamp': 1762892027.0588284}).
Epoch: 8, Steps: 25 | Train Loss: 0.2233695 Vali Loss: 0.4226983 Test Loss: 0.7807729
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 25 | Train Loss: 0.2247662 Vali Loss: 0.4571144 Test Loss: 0.7809907
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 25 | Train Loss: 0.2231831 Vali Loss: 0.4349922 Test Loss: 0.7810310
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 25 | Train Loss: 0.2201862 Vali Loss: 0.4766259 Test Loss: 0.7810204
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 25 | Train Loss: 0.2242653 Vali Loss: 0.4005014 Test Loss: 0.7810026
Validation loss decreased (0.401898 --> 0.400501).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 25 | Train Loss: 0.2213734 Vali Loss: 0.4029020 Test Loss: 0.7809989
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 25 | Train Loss: 0.2201747 Vali Loss: 0.4290526 Test Loss: 0.7809983
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 25 | Train Loss: 0.2218033 Vali Loss: 0.4183424 Test Loss: 0.7809990
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 25 | Train Loss: 0.2256130 Vali Loss: 0.4357965 Test Loss: 0.7809979
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 25 | Train Loss: 0.2249352 Vali Loss: 0.4214331 Test Loss: 0.7809979
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 25 | Train Loss: 0.2214695 Vali Loss: 0.3961271 Test Loss: 0.7810001
Validation loss decreased (0.400501 --> 0.396127).  Saving model ...
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 25 | Train Loss: 0.2213407 Vali Loss: 0.3912273 Test Loss: 0.7809998
Validation loss decreased (0.396127 --> 0.391227).  Saving model ...
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 25 | Train Loss: 0.2199477 Vali Loss: 0.4162569 Test Loss: 0.7809986
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 25 | Train Loss: 0.2212046 Vali Loss: 0.4008578 Test Loss: 0.7809987
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 25 | Train Loss: 0.2210533 Vali Loss: 0.4063575 Test Loss: 0.7809990
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 25 | Train Loss: 0.2234302 Vali Loss: 0.4185594 Test Loss: 0.7809990
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 25 | Train Loss: 0.2216728 Vali Loss: 0.4541670 Test Loss: 0.7809987
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 25 | Train Loss: 0.2219777 Vali Loss: 0.4223555 Test Loss: 0.7809985
EarlyStopping counter: 6 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 25 | Train Loss: 0.2208387 Vali Loss: 0.4062116 Test Loss: 0.7809986
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 25 | Train Loss: 0.2210272 Vali Loss: 0.4382109 Test Loss: 0.7809985
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 25 | Train Loss: 0.2227588 Vali Loss: 0.4455363 Test Loss: 0.7809985
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 25 | Train Loss: 0.2282187 Vali Loss: 0.4149788 Test Loss: 0.7809985
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_ANGLO_AMERICAN_H5_iTransformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 44
test shape: (44, 5, 1) (44, 5, 1)
test shape: (44, 5, 1) (44, 5, 1)


	mse:0.008465741761028767, mae:0.06309574842453003, rmse:0.09200946241617203, r2:-0.20048797130584717, dtw:Not calculated


VAL - MSE: 0.0085, MAE: 0.0631, RMSE: 0.0920, RÂ²: -0.2005, MAPE: 91260248.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.509 MB of 0.509 MB uploadedwandb: \ 0.509 MB of 0.509 MB uploadedwandb: | 0.509 MB of 0.509 MB uploadedwandb: / 0.509 MB of 0.509 MB uploadedwandb: - 0.509 MB of 0.668 MB uploadedwandb: \ 0.668 MB of 0.668 MB uploadedwandb: | 0.668 MB of 0.668 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–‚â–‚â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–‚â–ƒ
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–‚â–„â–†â–‚â–„â–†â–…â–ˆâ–‚â–‚â–„â–ƒâ–…â–ƒâ–â–â–ƒâ–‚â–‚â–ƒâ–†â–„â–‚â–…â–…â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 28
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6339589
wandb:     model/trainable_params 6339589
wandb:                  test/loss 0.781
wandb:                 train/loss 0.22822
wandb:   val/directional_accuracy 41.47727
wandb:                   val/loss 0.41498
wandb:                    val/mae 0.0631
wandb:                   val/mape 9126024800.0
wandb:                    val/mse 0.00847
wandb:                     val/r2 -0.20049
wandb:                   val/rmse 0.09201
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/h9xemwxj
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_221319-h9xemwxj/logs
Completed: ANGLO_AMERICAN H=5

Training: iTransformer on ANGLO_AMERICAN for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_221611-vpgcdqq2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/vpgcdqq2
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_ANGLO_AMERICAN_H10Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/vpgcdqq2
>>>>>>>start training : long_term_forecast_iTransformer_ANGLO_AMERICAN_H10_iTransformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 10
============================================================
train 771
val 37
test 39
Epoch: 1, Steps: 25 | Train Loss: 0.3518997 Vali Loss: 0.5360524 Test Loss: 0.8504982
Validation loss decreased (inf --> 0.536052).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 25 | Train Loss: 0.2918491 Vali Loss: 0.4514595 Test Loss: 0.8383203
Validation loss decreased (0.536052 --> 0.451460).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 25 | Train Loss: 0.2705796 Vali Loss: 0.4494707 Test Loss: 0.8067542
Validation loss decreased (0.451460 --> 0.449471).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 25 | Train Loss: 0.2641602 Vali Loss: 0.4803819 Test Loss: 0.8002761
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3518997210264206, 'val/loss': 0.5360523760318756, 'test/loss': 0.850498154759407, '_timestamp': 1762892195.0591702}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2918491166830063, 'val/loss': 0.45145952701568604, 'test/loss': 0.838320329785347, '_timestamp': 1762892199.3780184}).
Epoch: 5, Steps: 25 | Train Loss: 0.2597138 Vali Loss: 0.4561151 Test Loss: 0.8005039
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 25 | Train Loss: 0.2575591 Vali Loss: 0.4695309 Test Loss: 0.8025580
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 25 | Train Loss: 0.2549875 Vali Loss: 0.4473537 Test Loss: 0.8024435
Validation loss decreased (0.449471 --> 0.447354).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 25 | Train Loss: 0.2575588 Vali Loss: 0.4612904 Test Loss: 0.8014253
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 25 | Train Loss: 0.2515335 Vali Loss: 0.4903110 Test Loss: 0.8011417
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 25 | Train Loss: 0.2481820 Vali Loss: 0.4522080 Test Loss: 0.8010157
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 25 | Train Loss: 0.2500871 Vali Loss: 0.4805264 Test Loss: 0.8009614
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 25 | Train Loss: 0.2549925 Vali Loss: 0.4689831 Test Loss: 0.8009266
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 25 | Train Loss: 0.2481206 Vali Loss: 0.4758258 Test Loss: 0.8008884
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 25 | Train Loss: 0.2558775 Vali Loss: 0.4798927 Test Loss: 0.8008723
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 25 | Train Loss: 0.2723626 Vali Loss: 0.4724582 Test Loss: 0.8008724
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 25 | Train Loss: 0.2575111 Vali Loss: 0.4972111 Test Loss: 0.8008627
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 25 | Train Loss: 0.2526174 Vali Loss: 0.4575785 Test Loss: 0.8008629
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_ANGLO_AMERICAN_H10_iTransformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 39
test shape: (39, 10, 1) (39, 10, 1)
test shape: (39, 10, 1) (39, 10, 1)


	mse:0.009478350169956684, mae:0.06804206222295761, rmse:0.09735681861639023, r2:-0.22324895858764648, dtw:Not calculated


VAL - MSE: 0.0095, MAE: 0.0680, RMSE: 0.0974, RÂ²: -0.2232, MAPE: 57265640.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.531 MB of 0.532 MB uploadedwandb: \ 0.531 MB of 0.532 MB uploadedwandb: | 0.532 MB of 0.532 MB uploadedwandb: / 0.532 MB of 0.532 MB uploadedwandb: - 0.532 MB of 0.688 MB uploadedwandb: \ 0.688 MB of 0.688 MB uploadedwandb: | 0.688 MB of 0.688 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–‡â–†â–„â–„â–ƒâ–„â–‚â–â–‚â–ƒâ–â–ƒâ–ˆâ–„â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–†â–‚â–„â–â–ƒâ–‡â–‚â–†â–„â–…â–†â–…â–ˆâ–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6342154
wandb:     model/trainable_params 6342154
wandb:                  test/loss 0.80086
wandb:                 train/loss 0.25262
wandb:   val/directional_accuracy 41.31054
wandb:                   val/loss 0.45758
wandb:                    val/mae 0.06804
wandb:                   val/mape 5726564000.0
wandb:                    val/mse 0.00948
wandb:                     val/r2 -0.22325
wandb:                   val/rmse 0.09736
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/vpgcdqq2
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_221611-vpgcdqq2/logs
Completed: ANGLO_AMERICAN H=10

Training: iTransformer on ANGLO_AMERICAN for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_221847-zye9it7g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/zye9it7g
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_ANGLO_AMERICAN_H22Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/zye9it7g
>>>>>>>start training : long_term_forecast_iTransformer_ANGLO_AMERICAN_H22_iTransformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 22
============================================================
train 759
val 25
test 27
Epoch: 1, Steps: 24 | Train Loss: 0.3829079 Vali Loss: 0.6062818 Test Loss: 1.1551936
Validation loss decreased (inf --> 0.606282).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 24 | Train Loss: 0.3225503 Vali Loss: 0.5996651 Test Loss: 1.1711282
Validation loss decreased (0.606282 --> 0.599665).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 24 | Train Loss: 0.3052735 Vali Loss: 0.5803715 Test Loss: 1.1425163
Validation loss decreased (0.599665 --> 0.580371).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 24 | Train Loss: 0.2970219 Vali Loss: 0.5758525 Test Loss: 1.1380988
Validation loss decreased (0.580371 --> 0.575852).  Saving model ...
Updating learning rate to 1.25e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
Epoch: 5, Steps: 24 | Train Loss: 0.2943301 Vali Loss: 0.5707256 Test Loss: 1.1340806
Validation loss decreased (0.575852 --> 0.570726).  Saving model ...
Updating learning rate to 6.25e-06
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3829079034427802, 'val/loss': 0.6062817573547363, 'test/loss': 1.155193567276001, '_timestamp': 1762892348.1889277}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.32255033155282337, 'val/loss': 0.599665105342865, 'test/loss': 1.1711281538009644, '_timestamp': 1762892352.3390434}).
Epoch: 6, Steps: 24 | Train Loss: 0.2925423 Vali Loss: 0.5712153 Test Loss: 1.1329730
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 24 | Train Loss: 0.2909021 Vali Loss: 0.5708346 Test Loss: 1.1322947
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 24 | Train Loss: 0.2912245 Vali Loss: 0.5704566 Test Loss: 1.1321079
Validation loss decreased (0.570726 --> 0.570457).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 24 | Train Loss: 0.2901413 Vali Loss: 0.5703557 Test Loss: 1.1317337
Validation loss decreased (0.570457 --> 0.570356).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 24 | Train Loss: 0.2896511 Vali Loss: 0.5703601 Test Loss: 1.1317140
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 24 | Train Loss: 0.2911099 Vali Loss: 0.5703002 Test Loss: 1.1316638
Validation loss decreased (0.570356 --> 0.570300).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 24 | Train Loss: 0.2906520 Vali Loss: 0.5702959 Test Loss: 1.1316372
Validation loss decreased (0.570300 --> 0.570296).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 24 | Train Loss: 0.2910559 Vali Loss: 0.5702934 Test Loss: 1.1316429
Validation loss decreased (0.570296 --> 0.570293).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 24 | Train Loss: 0.2899460 Vali Loss: 0.5702857 Test Loss: 1.1316326
Validation loss decreased (0.570293 --> 0.570286).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 24 | Train Loss: 0.2893460 Vali Loss: 0.5702870 Test Loss: 1.1316322
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 24 | Train Loss: 0.2904006 Vali Loss: 0.5702859 Test Loss: 1.1316293
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 24 | Train Loss: 0.2888720 Vali Loss: 0.5702859 Test Loss: 1.1316293
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 24 | Train Loss: 0.2911479 Vali Loss: 0.5702858 Test Loss: 1.1316301
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 24 | Train Loss: 0.2895825 Vali Loss: 0.5702854 Test Loss: 1.1316293
Validation loss decreased (0.570286 --> 0.570285).  Saving model ...
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 24 | Train Loss: 0.2905191 Vali Loss: 0.5702856 Test Loss: 1.1316299
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 24 | Train Loss: 0.2897375 Vali Loss: 0.5702853 Test Loss: 1.1316297
Validation loss decreased (0.570285 --> 0.570285).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 24 | Train Loss: 0.2900916 Vali Loss: 0.5702853 Test Loss: 1.1316302
Validation loss decreased (0.570285 --> 0.570285).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 24 | Train Loss: 0.2904228 Vali Loss: 0.5702854 Test Loss: 1.1316303
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 24 | Train Loss: 0.2903096 Vali Loss: 0.5702854 Test Loss: 1.1316301
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 24 | Train Loss: 0.2898771 Vali Loss: 0.5702854 Test Loss: 1.1316301
EarlyStopping counter: 3 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 24 | Train Loss: 0.2898648 Vali Loss: 0.5702855 Test Loss: 1.1316301
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 24 | Train Loss: 0.2901409 Vali Loss: 0.5702855 Test Loss: 1.1316301
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 24 | Train Loss: 0.2912048 Vali Loss: 0.5702854 Test Loss: 1.1316301
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 24 | Train Loss: 0.2912030 Vali Loss: 0.5702855 Test Loss: 1.1316301
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 24 | Train Loss: 0.2909464 Vali Loss: 0.5702854 Test Loss: 1.1316301
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 24 | Train Loss: 0.2888801 Vali Loss: 0.5702854 Test Loss: 1.1316301
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 24 | Train Loss: 0.2911413 Vali Loss: 0.5702855 Test Loss: 1.1316301
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_iTransformer_ANGLO_AMERICAN_H22_iTransformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 27
test shape: (27, 22, 1) (27, 22, 1)
test shape: (27, 22, 1) (27, 22, 1)


	mse:0.009341294877231121, mae:0.06923278421163559, rmse:0.0966503769159317, r2:-0.18980014324188232, dtw:Not calculated


VAL - MSE: 0.0093, MAE: 0.0692, RMSE: 0.0967, RÂ²: -0.1898, MAPE: 34878144.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.506 MB of 0.507 MB uploadedwandb: \ 0.506 MB of 0.507 MB uploadedwandb: | 0.507 MB of 0.507 MB uploadedwandb: / 0.507 MB of 0.507 MB uploadedwandb: - 0.507 MB of 0.668 MB uploadedwandb: \ 0.668 MB of 0.668 MB uploadedwandb: | 0.668 MB of 0.668 MB uploadedwandb: / 0.668 MB of 0.668 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–‚â–â–‚â–â–‚â–â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 31
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 6348310
wandb:     model/trainable_params 6348310
wandb:                  test/loss 1.13163
wandb:                 train/loss 0.29114
wandb:   val/directional_accuracy 46.56085
wandb:                   val/loss 0.57029
wandb:                    val/mae 0.06923
wandb:                   val/mape 3487814400.0
wandb:                    val/mse 0.00934
wandb:                     val/r2 -0.1898
wandb:                   val/rmse 0.09665
wandb: 
wandb: ðŸš€ View run iTransformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/zye9it7g
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 5 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_221847-zye9it7g/logs
Completed: ANGLO_AMERICAN H=22

Training: iTransformer on ANGLO_AMERICAN for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_222208-jygcqef1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/jygcqef1
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_ANGLO_AMERICAN_H50Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/jygcqef1
>>>>>>>start training : long_term_forecast_iTransformer_ANGLO_AMERICAN_H50_iTransformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 50
============================================================
train 731
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.005 MB of 0.009 MB uploadedwandb: / 0.012 MB of 0.012 MB uploadedwandb: ðŸš€ View run iTransformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/jygcqef1
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_222208-jygcqef1/logs
Traceback (most recent call last):
  File "run.py", line 207, in <module>
    exp.train(setting)
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 93, in train
    vali_data, vali_loader = self._get_data(flag='val')
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 43, in _get_data
    data_set, data_loader = data_provider(self.args, flag)
  File "/home-mscluster/bkodze/research_project/forecast-research/data_provider/data_factory.py", line 79, in data_provider
    print(flag, len(data_set))
ValueError: __len__() should return >= 0
Completed: ANGLO_AMERICAN H=50

Training: iTransformer on ANGLO_AMERICAN for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251111_222441-f34yebcn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iTransformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/f34yebcn
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           iTransformer_ANGLO_AMERICAN_H100Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                iTransformer_Exp    Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: iTransformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/f34yebcn
>>>>>>>start training : long_term_forecast_iTransformer_ANGLO_AMERICAN_H100_iTransformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_iTransformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 100
============================================================
train 681
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.005 MB of 0.005 MB uploadedwandb: / 0.005 MB of 0.009 MB uploadedwandb: - 0.012 MB of 0.012 MB uploadedwandb: ðŸš€ View run iTransformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/f34yebcn
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251111_222441-f34yebcn/logs
Traceback (most recent call last):
  File "run.py", line 207, in <module>
    exp.train(setting)
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 93, in train
    vali_data, vali_loader = self._get_data(flag='val')
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 43, in _get_data
    data_set, data_loader = data_provider(self.args, flag)
  File "/home-mscluster/bkodze/research_project/forecast-research/data_provider/data_factory.py", line 79, in data_provider
    print(flag, len(data_set))
ValueError: __len__() should return >= 0
Completed: ANGLO_AMERICAN H=100

iTransformer training completed for all datasets!
