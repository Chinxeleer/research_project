##############################################################################
# Training FEDformer Model on All Datasets
##############################################################################
Training: FEDformer on NVIDIA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_030309-h4w43rh3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/h4w43rh3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_NVIDIA_H3 Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
 fourier enhanced cross attention used!
modes_q=16, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/h4w43rh3
>>>>>>>start training : long_term_forecast_FEDformer_NVIDIA_H3_FEDformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.3251230 Vali Loss: 0.1877279 Test Loss: 0.3175505
Validation loss decreased (inf --> 0.187728).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.32512302887170835, 'val/loss': 0.18772789649665356, 'test/loss': 0.3175504757091403, '_timestamp': 1762304635.382429}).
Epoch: 2, Steps: 133 | Train Loss: 0.2637455 Vali Loss: 0.1925584 Test Loss: 0.3167014
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26374552639803495, 'val/loss': 0.19255838356912136, 'test/loss': 0.3167014429345727, '_timestamp': 1762304657.2696161}).
Epoch: 3, Steps: 133 | Train Loss: 0.2507359 Vali Loss: 0.1845969 Test Loss: 0.3182096
Validation loss decreased (0.187728 --> 0.184597).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2448926 Vali Loss: 0.1782069 Test Loss: 0.3150006
Validation loss decreased (0.184597 --> 0.178207).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2423527 Vali Loss: 0.1799923 Test Loss: 0.3132941
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2400870 Vali Loss: 0.1810507 Test Loss: 0.3143522
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2386348 Vali Loss: 0.1847040 Test Loss: 0.3122104
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2384018 Vali Loss: 0.1968658 Test Loss: 0.3123389
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2384394 Vali Loss: 0.1759761 Test Loss: 0.3132521
Validation loss decreased (0.178207 --> 0.175976).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2385983 Vali Loss: 0.1869698 Test Loss: 0.3129571
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2386392 Vali Loss: 0.1763129 Test Loss: 0.3130030
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2387802 Vali Loss: 0.1745028 Test Loss: 0.3129424
Validation loss decreased (0.175976 --> 0.174503).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2392523 Vali Loss: 0.1836086 Test Loss: 0.3128949
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2390651 Vali Loss: 0.1804751 Test Loss: 0.3128904
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2386220 Vali Loss: 0.1762739 Test Loss: 0.3128973
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2384184 Vali Loss: 0.1814802 Test Loss: 0.3128981
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2388202 Vali Loss: 0.1765919 Test Loss: 0.3129014
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2386585 Vali Loss: 0.2016074 Test Loss: 0.3129011
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2375481 Vali Loss: 0.1948698 Test Loss: 0.3129015
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2374747 Vali Loss: 0.1847962 Test Loss: 0.3129016
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2381140 Vali Loss: 0.1838573 Test Loss: 0.3129016
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2388534 Vali Loss: 0.2013976 Test Loss: 0.3129017
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_NVIDIA_H3_FEDformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.0011636335402727127, mae:0.025746416300535202, rmse:0.034112073481082916, r2:-0.037303924560546875, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0257, RMSE: 0.0341, RÂ²: -0.0373, MAPE: 783253.62%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.464 MB of 0.464 MB uploadedwandb: \ 0.464 MB of 0.464 MB uploadedwandb: | 0.464 MB of 0.464 MB uploadedwandb: / 0.464 MB of 0.464 MB uploadedwandb: - 0.464 MB of 0.464 MB uploadedwandb: \ 0.464 MB of 0.464 MB uploadedwandb: | 0.464 MB of 0.464 MB uploadedwandb: / 0.592 MB of 0.805 MB uploaded (0.002 MB deduped)wandb: - 0.592 MB of 0.805 MB uploaded (0.002 MB deduped)wandb: \ 0.805 MB of 0.805 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–ƒâ–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–‚â–‚â–ƒâ–„â–‡â–â–„â–â–â–ƒâ–ƒâ–â–ƒâ–‚â–ˆâ–†â–„â–ƒâ–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 14594054
wandb:     model/trainable_params 14594054
wandb:                  test/loss 0.3129
wandb:                 train/loss 0.23885
wandb:   val/directional_accuracy 48.94515
wandb:                   val/loss 0.2014
wandb:                    val/mae 0.02575
wandb:                   val/mape 78325362.5
wandb:                    val/mse 0.00116
wandb:                     val/r2 -0.0373
wandb:                   val/rmse 0.03411
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/h4w43rh3
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_030309-h4w43rh3/logs
Completed: NVIDIA H=3

Training: FEDformer on NVIDIA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_031159-6k9o606z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/6k9o606z
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_NVIDIA_H5 Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
 fourier enhanced cross attention used!
modes_q=17, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/6k9o606z
>>>>>>>start training : long_term_forecast_FEDformer_NVIDIA_H5_FEDformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.3158834 Vali Loss: 0.1902356 Test Loss: 0.3447049
Validation loss decreased (inf --> 0.190236).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3158834320037885, 'val/loss': 0.1902355682104826, 'test/loss': 0.34470492601394653, '_timestamp': 1762305147.9471223}).
Epoch: 2, Steps: 133 | Train Loss: 0.2600418 Vali Loss: 0.1770238 Test Loss: 0.3299804
Validation loss decreased (0.190236 --> 0.177024).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2600417604347817, 'val/loss': 0.17702378053218126, 'test/loss': 0.32998035568743944, '_timestamp': 1762305171.561363}).
Epoch: 3, Steps: 133 | Train Loss: 0.2477477 Vali Loss: 0.1970205 Test Loss: 0.3339507
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2425633 Vali Loss: 0.1848360 Test Loss: 0.3267802
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2417482 Vali Loss: 0.1769424 Test Loss: 0.3247136
Validation loss decreased (0.177024 --> 0.176942).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2411582 Vali Loss: 0.1789703 Test Loss: 0.3258565
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2400797 Vali Loss: 0.1799559 Test Loss: 0.3277198
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2401492 Vali Loss: 0.1786597 Test Loss: 0.3283925
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2392889 Vali Loss: 0.1817665 Test Loss: 0.3259704
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2395992 Vali Loss: 0.1825991 Test Loss: 0.3261082
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2383116 Vali Loss: 0.1804823 Test Loss: 0.3262648
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2389981 Vali Loss: 0.1940122 Test Loss: 0.3262965
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2395006 Vali Loss: 0.1776084 Test Loss: 0.3262639
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2389624 Vali Loss: 0.1806919 Test Loss: 0.3262827
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2384085 Vali Loss: 0.1825912 Test Loss: 0.3262847
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_NVIDIA_H5_FEDformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.0011734366416931152, mae:0.025959009304642677, rmse:0.034255459904670715, r2:-0.03928542137145996, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0260, RMSE: 0.0343, RÂ²: -0.0393, MAPE: 834026.62%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.509 MB of 0.510 MB uploadedwandb: \ 0.509 MB of 0.510 MB uploadedwandb: | 0.510 MB of 0.510 MB uploadedwandb: / 0.510 MB of 0.510 MB uploadedwandb: - 0.510 MB of 0.721 MB uploadedwandb: \ 0.609 MB of 0.721 MB uploadedwandb: | 0.609 MB of 0.721 MB uploadedwandb: / 0.721 MB of 0.721 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–‚â–ƒâ–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–„â–„â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–‡â–â–‚â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 14725126
wandb:     model/trainable_params 14725126
wandb:                  test/loss 0.32628
wandb:                 train/loss 0.23841
wandb:   val/directional_accuracy 47.23404
wandb:                   val/loss 0.18259
wandb:                    val/mae 0.02596
wandb:                   val/mape 83402662.5
wandb:                    val/mse 0.00117
wandb:                     val/r2 -0.03929
wandb:                   val/rmse 0.03426
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/6k9o606z
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_031159-6k9o606z/logs
Completed: NVIDIA H=5

Training: FEDformer on NVIDIA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_031804-0k2ye3hs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/0k2ye3hs
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_NVIDIA_H10Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
 fourier enhanced cross attention used!
modes_q=20, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/0k2ye3hs
>>>>>>>start training : long_term_forecast_FEDformer_NVIDIA_H10_FEDformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.3078348 Vali Loss: 0.1867486 Test Loss: 0.3619391
Validation loss decreased (inf --> 0.186749).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.30783477616041227, 'val/loss': 0.18674862757325172, 'test/loss': 0.361939144320786, '_timestamp': 1762305514.4770062}).
Epoch: 2, Steps: 133 | Train Loss: 0.2558608 Vali Loss: 0.1877954 Test Loss: 0.3515504
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2558607594635254, 'val/loss': 0.18779538478702307, 'test/loss': 0.3515503900125623, '_timestamp': 1762305537.9301188}).
Epoch: 3, Steps: 133 | Train Loss: 0.2466403 Vali Loss: 0.1796948 Test Loss: 0.3478278
Validation loss decreased (0.186749 --> 0.179695).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2442600 Vali Loss: 0.1777817 Test Loss: 0.3534197
Validation loss decreased (0.179695 --> 0.177782).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2413843 Vali Loss: 0.2042266 Test Loss: 0.3482352
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2416168 Vali Loss: 0.1797921 Test Loss: 0.3519167
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2418981 Vali Loss: 0.1807611 Test Loss: 0.3521657
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2404746 Vali Loss: 0.1815694 Test Loss: 0.3524954
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2410971 Vali Loss: 0.1997350 Test Loss: 0.3524630
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2400475 Vali Loss: 0.1755630 Test Loss: 0.3524453
Validation loss decreased (0.177782 --> 0.175563).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2404216 Vali Loss: 0.1780931 Test Loss: 0.3525595
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2401307 Vali Loss: 0.1811490 Test Loss: 0.3527529
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2411131 Vali Loss: 0.1868863 Test Loss: 0.3527284
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2403126 Vali Loss: 0.1768594 Test Loss: 0.3527525
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2405853 Vali Loss: 0.1790587 Test Loss: 0.3527457
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2401365 Vali Loss: 0.1783496 Test Loss: 0.3527450
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2401370 Vali Loss: 0.1798025 Test Loss: 0.3527439
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2406734 Vali Loss: 0.1818836 Test Loss: 0.3527448
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2408505 Vali Loss: 0.1976162 Test Loss: 0.3527450
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2395879 Vali Loss: 0.1782906 Test Loss: 0.3527449
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_NVIDIA_H10_FEDformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.0012147747911512852, mae:0.026428034529089928, rmse:0.03485361859202385, r2:-0.05897331237792969, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0264, RMSE: 0.0349, RÂ²: -0.0590, MAPE: 882826.44%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.546 MB of 0.547 MB uploadedwandb: \ 0.546 MB of 0.547 MB uploadedwandb: | 0.546 MB of 0.547 MB uploadedwandb: / 0.547 MB of 0.547 MB uploadedwandb: - 0.547 MB of 0.547 MB uploadedwandb: \ 0.547 MB of 0.759 MB uploadedwandb: | 0.759 MB of 0.759 MB uploadedwandb: / 0.759 MB of 0.759 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–‚â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:                 train/loss â–ˆâ–†â–ƒâ–ƒâ–ƒâ–‚â–‚â–â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–‚â–ˆâ–‚â–‚â–‚â–‡â–â–‚â–‚â–„â–â–‚â–‚â–‚â–ƒâ–†â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 15118342
wandb:     model/trainable_params 15118342
wandb:                  test/loss 0.35274
wandb:                 train/loss 0.23959
wandb:   val/directional_accuracy 45.12077
wandb:                   val/loss 0.17829
wandb:                    val/mae 0.02643
wandb:                   val/mape 88282643.75
wandb:                    val/mse 0.00121
wandb:                     val/r2 -0.05897
wandb:                   val/rmse 0.03485
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/0k2ye3hs
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_031804-0k2ye3hs/logs
Completed: NVIDIA H=10

Training: FEDformer on NVIDIA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_032615-5vpp5mvg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/5vpp5mvg
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_NVIDIA_H22Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
 fourier enhanced cross attention used!
modes_q=26, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/5vpp5mvg
>>>>>>>start training : long_term_forecast_FEDformer_NVIDIA_H22_FEDformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2989069 Vali Loss: 0.1964822 Test Loss: 0.4395971
Validation loss decreased (inf --> 0.196482).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2989068552851677, 'val/loss': 0.19648224541119166, 'test/loss': 0.43959710001945496, '_timestamp': 1762306005.334863}).
Epoch: 2, Steps: 132 | Train Loss: 0.2548932 Vali Loss: 0.1953600 Test Loss: 0.4496322
Validation loss decreased (0.196482 --> 0.195360).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2548932381200068, 'val/loss': 0.19535997297082627, 'test/loss': 0.44963217207363676, '_timestamp': 1762306029.8829398}).
Epoch: 3, Steps: 132 | Train Loss: 0.2497257 Vali Loss: 0.1873674 Test Loss: 0.4356546
Validation loss decreased (0.195360 --> 0.187367).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2468237 Vali Loss: 0.1872790 Test Loss: 0.4385670
Validation loss decreased (0.187367 --> 0.187279).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2454476 Vali Loss: 0.1880856 Test Loss: 0.4397272
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2451965 Vali Loss: 0.1868730 Test Loss: 0.4354940
Validation loss decreased (0.187279 --> 0.186873).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2446418 Vali Loss: 0.1875955 Test Loss: 0.4365708
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2444327 Vali Loss: 0.1865432 Test Loss: 0.4350382
Validation loss decreased (0.186873 --> 0.186543).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2441477 Vali Loss: 0.1877889 Test Loss: 0.4352646
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2444503 Vali Loss: 0.1863253 Test Loss: 0.4355878
Validation loss decreased (0.186543 --> 0.186325).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2443324 Vali Loss: 0.1873022 Test Loss: 0.4356528
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2443338 Vali Loss: 0.1882965 Test Loss: 0.4356719
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2441705 Vali Loss: 0.1870598 Test Loss: 0.4356466
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2442762 Vali Loss: 0.1864011 Test Loss: 0.4356474
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2445214 Vali Loss: 0.1872538 Test Loss: 0.4356447
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2442659 Vali Loss: 0.1875977 Test Loss: 0.4356417
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2441077 Vali Loss: 0.1872030 Test Loss: 0.4356419
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2442435 Vali Loss: 0.1880182 Test Loss: 0.4356416
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2443671 Vali Loss: 0.1882214 Test Loss: 0.4356415
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2443243 Vali Loss: 0.1872830 Test Loss: 0.4356415
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_NVIDIA_H22_FEDformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.0012436342658475041, mae:0.02655763365328312, rmse:0.03526519984006882, r2:-0.05405628681182861, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0266, RMSE: 0.0353, RÂ²: -0.0541, MAPE: 883160.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.561 MB of 0.562 MB uploadedwandb: \ 0.561 MB of 0.562 MB uploadedwandb: | 0.562 MB of 0.562 MB uploadedwandb: / 0.562 MB of 0.775 MB uploadedwandb: - 0.564 MB of 0.775 MB uploadedwandb: \ 0.775 MB of 0.775 MB uploadedwandb: | 0.775 MB of 0.775 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‚â–†â–ˆâ–‚â–ƒâ–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–„â–‡â–ƒâ–†â–‚â–†â–â–„â–ˆâ–„â–â–„â–†â–„â–‡â–ˆâ–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 15904774
wandb:     model/trainable_params 15904774
wandb:                  test/loss 0.43564
wandb:                 train/loss 0.24432
wandb:   val/directional_accuracy 45.30363
wandb:                   val/loss 0.18728
wandb:                    val/mae 0.02656
wandb:                   val/mape 88316000.0
wandb:                    val/mse 0.00124
wandb:                     val/r2 -0.05406
wandb:                   val/rmse 0.03527
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/5vpp5mvg
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_032615-5vpp5mvg/logs
Completed: NVIDIA H=22

Training: FEDformer on NVIDIA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_033449-7wni3434
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/7wni3434
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_NVIDIA_H50Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 39]
 fourier enhanced cross attention used!
modes_q=32, index_q=[0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 21, 22, 24, 26, 28, 29, 30, 33, 34, 35, 36, 37, 38, 39]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/7wni3434
>>>>>>>start training : long_term_forecast_FEDformer_NVIDIA_H50_FEDformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.3018296 Vali Loss: 0.2051937 Test Loss: 0.5879683
Validation loss decreased (inf --> 0.205194).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3018296313556758, 'val/loss': 0.20519369840621948, 'test/loss': 0.587968277434508, '_timestamp': 1762306521.4352996}).
Epoch: 2, Steps: 132 | Train Loss: 0.2648284 Vali Loss: 0.2015679 Test Loss: 0.5694385
Validation loss decreased (0.205194 --> 0.201568).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2648283809874997, 'val/loss': 0.2015679031610489, 'test/loss': 0.5694384674231211, '_timestamp': 1762306548.101413}).
Epoch: 3, Steps: 132 | Train Loss: 0.2601876 Vali Loss: 0.2001310 Test Loss: 0.5625871
Validation loss decreased (0.201568 --> 0.200131).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2580998 Vali Loss: 0.2054998 Test Loss: 0.5711258
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2564542 Vali Loss: 0.2037030 Test Loss: 0.5686977
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2557678 Vali Loss: 0.2025276 Test Loss: 0.5705028
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2551884 Vali Loss: 0.2027274 Test Loss: 0.5682745
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2557499 Vali Loss: 0.2028681 Test Loss: 0.5686892
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2562584 Vali Loss: 0.2030885 Test Loss: 0.5694189
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2548905 Vali Loss: 0.2033818 Test Loss: 0.5696437
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2551766 Vali Loss: 0.2030167 Test Loss: 0.5699969
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2549122 Vali Loss: 0.2033821 Test Loss: 0.5701215
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2553692 Vali Loss: 0.2034689 Test Loss: 0.5700781
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_NVIDIA_H50_FEDformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.0012184251099824905, mae:0.026541905477643013, rmse:0.034905947744846344, r2:-0.02389359474182129, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0265, RMSE: 0.0349, RÂ²: -0.0239, MAPE: 617047.19%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.613 MB of 0.616 MB uploadedwandb: \ 0.613 MB of 0.616 MB uploadedwandb: | 0.616 MB of 0.827 MB uploadedwandb: / 0.824 MB of 0.827 MB uploadedwandb: - 0.827 MB of 0.827 MB uploadedwandb: \ 0.827 MB of 0.827 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–†â–‡â–†â–†â–‡â–‡â–‡â–‡â–‡
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–‚â–ƒâ–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ˆâ–†â–„â–„â–…â–…â–…â–…â–…â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 16691206
wandb:     model/trainable_params 16691206
wandb:                  test/loss 0.57008
wandb:                 train/loss 0.25537
wandb:   val/directional_accuracy 49.2159
wandb:                   val/loss 0.20347
wandb:                    val/mae 0.02654
wandb:                   val/mape 61704718.75
wandb:                    val/mse 0.00122
wandb:                     val/r2 -0.02389
wandb:                   val/rmse 0.03491
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/7wni3434
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_033449-7wni3434/logs
Completed: NVIDIA H=50

Training: FEDformer on NVIDIA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_034059-22fv9ky2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/22fv9ky2
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_NVIDIA_H100Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 8, 13, 16, 20, 22, 23, 24, 28, 29, 30, 31, 32, 34, 35, 37, 39, 43, 44, 46, 51, 52, 55, 56, 57, 62, 63, 64]
 fourier enhanced cross attention used!
modes_q=32, index_q=[0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 17, 18, 20, 21, 22, 23, 28, 30, 34, 36, 44, 45, 50, 53, 54, 55, 56, 57, 61, 63]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/22fv9ky2
>>>>>>>start training : long_term_forecast_FEDformer_NVIDIA_H100_FEDformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.3191982 Vali Loss: 0.2393869 Test Loss: 0.8359692
Validation loss decreased (inf --> 0.239387).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3191982238338544, 'val/loss': 0.2393868863582611, 'test/loss': 0.8359691619873046, '_timestamp': 1762306892.253695}).
Epoch: 2, Steps: 130 | Train Loss: 0.2820920 Vali Loss: 0.2306566 Test Loss: 0.8632101
Validation loss decreased (0.239387 --> 0.230657).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2820920107456354, 'val/loss': 0.23065660893917084, 'test/loss': 0.863210129737854, '_timestamp': 1762306918.1698618}).
Epoch: 3, Steps: 130 | Train Loss: 0.2785642 Vali Loss: 0.2386030 Test Loss: 0.8244359
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2776330 Vali Loss: 0.2352489 Test Loss: 0.8627900
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2759836 Vali Loss: 0.2410514 Test Loss: 0.8382519
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2756167 Vali Loss: 0.2458105 Test Loss: 0.8561258
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2753363 Vali Loss: 0.2414711 Test Loss: 0.8534240
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2753250 Vali Loss: 0.2382242 Test Loss: 0.8512304
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2750585 Vali Loss: 0.2369817 Test Loss: 0.8487481
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2749933 Vali Loss: 0.2385331 Test Loss: 0.8480866
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2750254 Vali Loss: 0.2404425 Test Loss: 0.8484859
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2754980 Vali Loss: 0.2389159 Test Loss: 0.8484986
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_NVIDIA_H100_FEDformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0013062600046396255, mae:0.027487825602293015, rmse:0.036142218858003616, r2:-0.014637589454650879, dtw:Not calculated


VAL - MSE: 0.0013, MAE: 0.0275, RMSE: 0.0361, RÂ²: -0.0146, MAPE: 418846.84%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.689 MB of 0.694 MB uploadedwandb: \ 0.689 MB of 0.694 MB uploadedwandb: | 0.689 MB of 0.694 MB uploadedwandb: / 0.689 MB of 0.694 MB uploadedwandb: - 0.694 MB of 0.694 MB uploadedwandb: \ 0.694 MB of 0.905 MB uploadedwandb: | 0.905 MB of 0.905 MB uploadedwandb: / 0.905 MB of 0.905 MB uploadedwandb: - 0.905 MB of 0.905 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–„â–‡â–†â–†â–…â–…â–…â–…
wandb:                 train/loss â–ˆâ–†â–ƒâ–‚â–‚â–‚â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–â–…â–ˆâ–…â–ƒâ–‚â–ƒâ–„â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 16691206
wandb:     model/trainable_params 16691206
wandb:                  test/loss 0.8485
wandb:                 train/loss 0.2755
wandb:   val/directional_accuracy 48.99711
wandb:                   val/loss 0.23892
wandb:                    val/mae 0.02749
wandb:                   val/mape 41884684.375
wandb:                    val/mse 0.00131
wandb:                     val/r2 -0.01464
wandb:                   val/rmse 0.03614
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/22fv9ky2
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_034059-22fv9ky2/logs
Completed: NVIDIA H=100

Training: FEDformer on APPLE for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_034635-ejwr8e14
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/ejwr8e14
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_APPLE_H3  Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
 fourier enhanced cross attention used!
modes_q=16, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/ejwr8e14
>>>>>>>start training : long_term_forecast_FEDformer_APPLE_H3_FEDformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.3151149 Vali Loss: 0.0968079 Test Loss: 0.1450866
Validation loss decreased (inf --> 0.096808).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.31511491929229934, 'val/loss': 0.0968078700825572, 'test/loss': 0.14508663956075907, '_timestamp': 1762307223.3792653}).
Epoch: 2, Steps: 133 | Train Loss: 0.2549902 Vali Loss: 0.0903973 Test Loss: 0.1340782
Validation loss decreased (0.096808 --> 0.090397).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2549902342988136, 'val/loss': 0.09039733558893204, 'test/loss': 0.13407821487635374, '_timestamp': 1762307245.5410652}).
Epoch: 3, Steps: 133 | Train Loss: 0.2405905 Vali Loss: 0.0878851 Test Loss: 0.1337159
Validation loss decreased (0.090397 --> 0.087885).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2355931 Vali Loss: 0.0876044 Test Loss: 0.1359655
Validation loss decreased (0.087885 --> 0.087604).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2334174 Vali Loss: 0.0896961 Test Loss: 0.1332268
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2316670 Vali Loss: 0.0855453 Test Loss: 0.1322615
Validation loss decreased (0.087604 --> 0.085545).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2305682 Vali Loss: 0.0918869 Test Loss: 0.1320614
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2303317 Vali Loss: 0.0863049 Test Loss: 0.1323502
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2301166 Vali Loss: 0.0846571 Test Loss: 0.1323984
Validation loss decreased (0.085545 --> 0.084657).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2308149 Vali Loss: 0.0874820 Test Loss: 0.1322915
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2292063 Vali Loss: 0.0847109 Test Loss: 0.1322996
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2303363 Vali Loss: 0.0893486 Test Loss: 0.1322767
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2301066 Vali Loss: 0.0842005 Test Loss: 0.1322780
Validation loss decreased (0.084657 --> 0.084201).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2302513 Vali Loss: 0.0887127 Test Loss: 0.1322906
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2295083 Vali Loss: 0.0871668 Test Loss: 0.1322893
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2301958 Vali Loss: 0.0884577 Test Loss: 0.1322874
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2311232 Vali Loss: 0.0921725 Test Loss: 0.1322874
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2306910 Vali Loss: 0.0879467 Test Loss: 0.1322871
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2293144 Vali Loss: 0.0842920 Test Loss: 0.1322872
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2293357 Vali Loss: 0.0862022 Test Loss: 0.1322872
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2300263 Vali Loss: 0.0865500 Test Loss: 0.1322874
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2305820 Vali Loss: 0.0838602 Test Loss: 0.1322873
Validation loss decreased (0.084201 --> 0.083860).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2306169 Vali Loss: 0.0849096 Test Loss: 0.1322874
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2297822 Vali Loss: 0.0850846 Test Loss: 0.1322873
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2310072 Vali Loss: 0.0847164 Test Loss: 0.1322874
EarlyStopping counter: 3 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2309458 Vali Loss: 0.0851057 Test Loss: 0.1322874
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.2298799 Vali Loss: 0.0867158 Test Loss: 0.1322874
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.2299619 Vali Loss: 0.0862504 Test Loss: 0.1322874
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.2300688 Vali Loss: 0.0868358 Test Loss: 0.1322874
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 133 | Train Loss: 0.2306308 Vali Loss: 0.0869253 Test Loss: 0.1322874
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 133 | Train Loss: 0.2302423 Vali Loss: 0.0858272 Test Loss: 0.1322874
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 133 | Train Loss: 0.2300626 Vali Loss: 0.0850264 Test Loss: 0.1322874
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_APPLE_H3_FEDformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.0002170882507925853, mae:0.010827825404703617, rmse:0.014733915217220783, r2:-0.08575201034545898, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0108, RMSE: 0.0147, RÂ²: -0.0858, MAPE: 344036.28%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.474 MB of 0.475 MB uploadedwandb: \ 0.474 MB of 0.475 MB uploadedwandb: | 0.474 MB of 0.475 MB uploadedwandb: / 0.474 MB of 0.475 MB uploadedwandb: - 0.474 MB of 0.475 MB uploadedwandb: \ 0.603 MB of 0.817 MB uploaded (0.002 MB deduped)wandb: | 0.817 MB of 0.817 MB uploaded (0.002 MB deduped)wandb: / 0.817 MB of 0.817 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–„â–ˆâ–ƒâ–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–â–‚â–‚â–â–â–‚â–‚â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–„â–†â–‚â–ˆâ–ƒâ–‚â–„â–‚â–†â–â–…â–„â–…â–ˆâ–„â–â–ƒâ–ƒâ–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–ƒâ–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 31
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 14594054
wandb:     model/trainable_params 14594054
wandb:                  test/loss 0.13229
wandb:                 train/loss 0.23006
wandb:   val/directional_accuracy 46.83544
wandb:                   val/loss 0.08503
wandb:                    val/mae 0.01083
wandb:                   val/mape 34403628.125
wandb:                    val/mse 0.00022
wandb:                     val/r2 -0.08575
wandb:                   val/rmse 0.01473
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/ejwr8e14
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_034635-ejwr8e14/logs
Completed: APPLE H=3

Training: FEDformer on APPLE for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_035852-cldsvpn3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/cldsvpn3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_APPLE_H5  Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
 fourier enhanced cross attention used!
modes_q=17, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/cldsvpn3
>>>>>>>start training : long_term_forecast_FEDformer_APPLE_H5_FEDformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.3055777 Vali Loss: 0.0921974 Test Loss: 0.1389690
Validation loss decreased (inf --> 0.092197).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.305577749932619, 'val/loss': 0.09219737630337477, 'test/loss': 0.13896899949759245, '_timestamp': 1762307960.8283095}).
Epoch: 2, Steps: 133 | Train Loss: 0.2510349 Vali Loss: 0.0903194 Test Loss: 0.1382048
Validation loss decreased (0.092197 --> 0.090319).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2510348608843366, 'val/loss': 0.09031939413398504, 'test/loss': 0.13820484094321728, '_timestamp': 1762307983.946294}).
Epoch: 3, Steps: 133 | Train Loss: 0.2389312 Vali Loss: 0.0898754 Test Loss: 0.1356751
Validation loss decreased (0.090319 --> 0.089875).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2348359 Vali Loss: 0.0863660 Test Loss: 0.1372490
Validation loss decreased (0.089875 --> 0.086366).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2335360 Vali Loss: 0.0878017 Test Loss: 0.1358463
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2325277 Vali Loss: 0.0903576 Test Loss: 0.1356838
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2315036 Vali Loss: 0.0852290 Test Loss: 0.1353253
Validation loss decreased (0.086366 --> 0.085229).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2318107 Vali Loss: 0.0852688 Test Loss: 0.1350867
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2315603 Vali Loss: 0.0836983 Test Loss: 0.1354070
Validation loss decreased (0.085229 --> 0.083698).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2311172 Vali Loss: 0.0895915 Test Loss: 0.1353303
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2301844 Vali Loss: 0.0859847 Test Loss: 0.1353286
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2301731 Vali Loss: 0.0867228 Test Loss: 0.1353157
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2317497 Vali Loss: 0.0858216 Test Loss: 0.1353246
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2306250 Vali Loss: 0.0858893 Test Loss: 0.1353299
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2301742 Vali Loss: 0.0867248 Test Loss: 0.1353309
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2303836 Vali Loss: 0.0861906 Test Loss: 0.1353325
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2300874 Vali Loss: 0.0860861 Test Loss: 0.1353319
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2305899 Vali Loss: 0.0850460 Test Loss: 0.1353319
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2301671 Vali Loss: 0.0871251 Test Loss: 0.1353319
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_APPLE_H5_FEDformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.00021913801901973784, mae:0.010877992026507854, rmse:0.014803310856223106, r2:-0.09100115299224854, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0109, RMSE: 0.0148, RÂ²: -0.0910, MAPE: 329516.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.502 MB of 0.502 MB uploadedwandb: \ 0.502 MB of 0.502 MB uploadedwandb: | 0.502 MB of 0.502 MB uploadedwandb: / 0.502 MB of 0.502 MB uploadedwandb: - 0.502 MB of 0.714 MB uploadedwandb: \ 0.714 MB of 0.714 MB uploadedwandb: | 0.714 MB of 0.714 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ƒâ–ˆâ–ƒâ–ƒâ–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–„â–…â–ˆâ–ƒâ–ƒâ–â–‡â–ƒâ–„â–ƒâ–ƒâ–„â–„â–„â–‚â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 18
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 14725126
wandb:     model/trainable_params 14725126
wandb:                  test/loss 0.13533
wandb:                 train/loss 0.23017
wandb:   val/directional_accuracy 46.06383
wandb:                   val/loss 0.08713
wandb:                    val/mae 0.01088
wandb:                   val/mape 32951600.0
wandb:                    val/mse 0.00022
wandb:                     val/r2 -0.091
wandb:                   val/rmse 0.0148
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/cldsvpn3
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_035852-cldsvpn3/logs
Completed: APPLE H=5

Training: FEDformer on APPLE for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_040631-0h2dnrn9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/0h2dnrn9
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_APPLE_H10 Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
 fourier enhanced cross attention used!
modes_q=20, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/0h2dnrn9
>>>>>>>start training : long_term_forecast_FEDformer_APPLE_H10_FEDformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2987824 Vali Loss: 0.0942567 Test Loss: 0.1401843
Validation loss decreased (inf --> 0.094257).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2987823530023259, 'val/loss': 0.09425674937665462, 'test/loss': 0.14018434192985296, '_timestamp': 1762308420.4400222}).
Epoch: 2, Steps: 133 | Train Loss: 0.2487193 Vali Loss: 0.0978768 Test Loss: 0.1434366
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2487193033435291, 'val/loss': 0.09787680394947529, 'test/loss': 0.1434366460889578, '_timestamp': 1762308444.1126897}).
Epoch: 3, Steps: 133 | Train Loss: 0.2388682 Vali Loss: 0.0957344 Test Loss: 0.1413031
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2376443 Vali Loss: 0.0920995 Test Loss: 0.1385258
Validation loss decreased (0.094257 --> 0.092100).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2340555 Vali Loss: 0.0859120 Test Loss: 0.1367637
Validation loss decreased (0.092100 --> 0.085912).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2334579 Vali Loss: 0.0873528 Test Loss: 0.1366953
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2343405 Vali Loss: 0.0883770 Test Loss: 0.1362421
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2326478 Vali Loss: 0.0879800 Test Loss: 0.1361944
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2327711 Vali Loss: 0.0887290 Test Loss: 0.1364135
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2328444 Vali Loss: 0.0864024 Test Loss: 0.1363446
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2333465 Vali Loss: 0.0873609 Test Loss: 0.1363348
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2325635 Vali Loss: 0.0881643 Test Loss: 0.1363333
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2331062 Vali Loss: 0.0896235 Test Loss: 0.1363322
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2321647 Vali Loss: 0.0854940 Test Loss: 0.1363305
Validation loss decreased (0.085912 --> 0.085494).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2323539 Vali Loss: 0.0865822 Test Loss: 0.1363302
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2319916 Vali Loss: 0.0874640 Test Loss: 0.1363309
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2323629 Vali Loss: 0.0871450 Test Loss: 0.1363312
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2326115 Vali Loss: 0.0872157 Test Loss: 0.1363313
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2341047 Vali Loss: 0.0862215 Test Loss: 0.1363312
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2322283 Vali Loss: 0.0857393 Test Loss: 0.1363313
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2336164 Vali Loss: 0.0872363 Test Loss: 0.1363313
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2327155 Vali Loss: 0.0882215 Test Loss: 0.1363313
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2325763 Vali Loss: 0.0878728 Test Loss: 0.1363314
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2327608 Vali Loss: 0.0919112 Test Loss: 0.1363313
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_APPLE_H10_FEDformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.0002199603186454624, mae:0.010807646438479424, rmse:0.014831059612333775, r2:-0.0864022970199585, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0108, RMSE: 0.0148, RÂ²: -0.0864, MAPE: 274031.22%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.574 MB of 0.574 MB uploadedwandb: \ 0.574 MB of 0.574 MB uploadedwandb: | 0.574 MB of 0.787 MB uploadedwandb: / 0.787 MB of 0.787 MB uploadedwandb: - 0.787 MB of 0.787 MB uploadedwandb: \ 0.787 MB of 0.787 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–‡â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–ƒâ–â–ƒâ–‚â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–â–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–„â–â–‚â–‚â–‚â–‚â–â–â–‚â–ƒâ–ƒâ–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 23
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 15118342
wandb:     model/trainable_params 15118342
wandb:                  test/loss 0.13633
wandb:                 train/loss 0.23276
wandb:   val/directional_accuracy 46.8599
wandb:                   val/loss 0.09191
wandb:                    val/mae 0.01081
wandb:                   val/mape 27403121.875
wandb:                    val/mse 0.00022
wandb:                     val/r2 -0.0864
wandb:                   val/rmse 0.01483
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/0h2dnrn9
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_040631-0h2dnrn9/logs
Completed: APPLE H=10

Training: FEDformer on APPLE for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_041617-re949777
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/re949777
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_APPLE_H22 Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
 fourier enhanced cross attention used!
modes_q=26, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/re949777
>>>>>>>start training : long_term_forecast_FEDformer_APPLE_H22_FEDformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2931423 Vali Loss: 0.0884190 Test Loss: 0.1376555
Validation loss decreased (inf --> 0.088419).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29314225336367433, 'val/loss': 0.08841904784951891, 'test/loss': 0.13765553597893035, '_timestamp': 1762309008.1493855}).
Epoch: 2, Steps: 132 | Train Loss: 0.2503689 Vali Loss: 0.0938768 Test Loss: 0.1404241
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25036894417170324, 'val/loss': 0.09387683655534472, 'test/loss': 0.1404240791286741, '_timestamp': 1762309032.6763253}).
Epoch: 3, Steps: 132 | Train Loss: 0.2447029 Vali Loss: 0.0888074 Test Loss: 0.1375216
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2417417 Vali Loss: 0.0885006 Test Loss: 0.1378117
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2400795 Vali Loss: 0.0881317 Test Loss: 0.1378781
Validation loss decreased (0.088419 --> 0.088132).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2396462 Vali Loss: 0.0888438 Test Loss: 0.1387521
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2391938 Vali Loss: 0.0879280 Test Loss: 0.1379406
Validation loss decreased (0.088132 --> 0.087928).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2390045 Vali Loss: 0.0883392 Test Loss: 0.1382720
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2389144 Vali Loss: 0.0882916 Test Loss: 0.1382198
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2386679 Vali Loss: 0.0881929 Test Loss: 0.1381880
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2387018 Vali Loss: 0.0884254 Test Loss: 0.1382079
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2387839 Vali Loss: 0.0881294 Test Loss: 0.1381913
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2387217 Vali Loss: 0.0880939 Test Loss: 0.1381898
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2383799 Vali Loss: 0.0881824 Test Loss: 0.1381987
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2389044 Vali Loss: 0.0884442 Test Loss: 0.1381973
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2389614 Vali Loss: 0.0882434 Test Loss: 0.1381976
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2387641 Vali Loss: 0.0881989 Test Loss: 0.1381971
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_APPLE_H22_FEDformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.00022497338068205863, mae:0.010865775868296623, rmse:0.01499911304563284, r2:-0.08418560028076172, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0109, RMSE: 0.0150, RÂ²: -0.0842, MAPE: 666272.69%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.640 MB of 0.642 MB uploadedwandb: \ 0.640 MB of 0.642 MB uploadedwandb: | 0.642 MB of 0.642 MB uploadedwandb: / 0.642 MB of 0.642 MB uploadedwandb: - 0.642 MB of 0.853 MB uploadedwandb: \ 0.853 MB of 0.853 MB uploadedwandb: | 0.853 MB of 0.853 MB uploadedwandb: / 0.853 MB of 0.853 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ƒâ–ƒâ–ˆâ–ƒâ–…â–…â–…â–…â–…â–…â–…â–…â–…â–…
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–‚â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–ƒâ–ˆâ–â–„â–„â–ƒâ–…â–ƒâ–‚â–ƒâ–…â–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 15904774
wandb:     model/trainable_params 15904774
wandb:                  test/loss 0.1382
wandb:                 train/loss 0.23876
wandb:   val/directional_accuracy 45.19441
wandb:                   val/loss 0.0882
wandb:                    val/mae 0.01087
wandb:                   val/mape 66627268.75
wandb:                    val/mse 0.00022
wandb:                     val/r2 -0.08419
wandb:                   val/rmse 0.015
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/re949777
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_041617-re949777/logs
Completed: APPLE H=22

Training: FEDformer on APPLE for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_042341-14lczhte
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/14lczhte
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_APPLE_H50 Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 39]
 fourier enhanced cross attention used!
modes_q=32, index_q=[0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 21, 22, 24, 26, 28, 29, 30, 33, 34, 35, 36, 37, 38, 39]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/14lczhte
>>>>>>>start training : long_term_forecast_FEDformer_APPLE_H50_FEDformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.2973131 Vali Loss: 0.0956706 Test Loss: 0.1558297
Validation loss decreased (inf --> 0.095671).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29731308623696817, 'val/loss': 0.09567056223750114, 'test/loss': 0.15582970902323723, '_timestamp': 1762309453.1298692}).
Epoch: 2, Steps: 132 | Train Loss: 0.2603461 Vali Loss: 0.1038160 Test Loss: 0.1630219
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26034608596202097, 'val/loss': 0.1038159504532814, 'test/loss': 0.1630219245950381, '_timestamp': 1762309479.2139537}).
Epoch: 3, Steps: 132 | Train Loss: 0.2567930 Vali Loss: 0.0924040 Test Loss: 0.1539295
Validation loss decreased (0.095671 --> 0.092404).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2528211 Vali Loss: 0.0895365 Test Loss: 0.1545683
Validation loss decreased (0.092404 --> 0.089536).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2518049 Vali Loss: 0.0916272 Test Loss: 0.1564308
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2501773 Vali Loss: 0.0912362 Test Loss: 0.1564755
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2499491 Vali Loss: 0.0910534 Test Loss: 0.1562663
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2504338 Vali Loss: 0.0905599 Test Loss: 0.1559997
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2515075 Vali Loss: 0.0904974 Test Loss: 0.1559048
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2500220 Vali Loss: 0.0905759 Test Loss: 0.1558907
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2503827 Vali Loss: 0.0906449 Test Loss: 0.1558747
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2498617 Vali Loss: 0.0906391 Test Loss: 0.1559065
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2517994 Vali Loss: 0.0904935 Test Loss: 0.1558967
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2503315 Vali Loss: 0.0905934 Test Loss: 0.1559063
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_APPLE_H50_FEDformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.00023957443772815168, mae:0.011185561306774616, rmse:0.015478191897273064, r2:-0.0797719955444336, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0112, RMSE: 0.0155, RÂ²: -0.0798, MAPE: 259363.44%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.718 MB of 0.721 MB uploadedwandb: \ 0.718 MB of 0.721 MB uploadedwandb: | 0.718 MB of 0.721 MB uploadedwandb: / 0.721 MB of 0.721 MB uploadedwandb: - 0.721 MB of 0.721 MB uploadedwandb: \ 0.721 MB of 0.932 MB uploadedwandb: | 0.932 MB of 0.932 MB uploadedwandb: / 0.932 MB of 0.932 MB uploadedwandb: - 0.932 MB of 0.932 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ƒâ–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–†â–†â–†
wandb:                 train/loss â–ˆâ–„â–ƒâ–â–â–‚â–ƒâ–â–‚â–â–ƒâ–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–†â–…â–…â–ƒâ–ƒâ–„â–„â–„â–ƒâ–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 13
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 16691206
wandb:     model/trainable_params 16691206
wandb:                  test/loss 0.15591
wandb:                 train/loss 0.25033
wandb:   val/directional_accuracy 48.31364
wandb:                   val/loss 0.09059
wandb:                    val/mae 0.01119
wandb:                   val/mape 25936343.75
wandb:                    val/mse 0.00024
wandb:                     val/r2 -0.07977
wandb:                   val/rmse 0.01548
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/14lczhte
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_042341-14lczhte/logs
Completed: APPLE H=50

Training: FEDformer on APPLE for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_043020-fw7sz8d5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/fw7sz8d5
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_APPLE_H100Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 8, 13, 16, 20, 22, 23, 24, 28, 29, 30, 31, 32, 34, 35, 37, 39, 43, 44, 46, 51, 52, 55, 56, 57, 62, 63, 64]
 fourier enhanced cross attention used!
modes_q=32, index_q=[0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 17, 18, 20, 21, 22, 23, 28, 30, 34, 36, 44, 45, 50, 53, 54, 55, 56, 57, 61, 63]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/fw7sz8d5
>>>>>>>start training : long_term_forecast_FEDformer_APPLE_H100_FEDformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.3109515 Vali Loss: 0.0977801 Test Loss: 0.1649418
Validation loss decreased (inf --> 0.097780).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3109515159175946, 'val/loss': 0.0977800577878952, 'test/loss': 0.16494175493717195, '_timestamp': 1762309853.2127125}).
Epoch: 2, Steps: 130 | Train Loss: 0.2739298 Vali Loss: 0.0933585 Test Loss: 0.1698904
Validation loss decreased (0.097780 --> 0.093358).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2739297735003325, 'val/loss': 0.09335848987102509, 'test/loss': 0.16989043354988098, '_timestamp': 1762309878.6294415}).
Epoch: 3, Steps: 130 | Train Loss: 0.2691901 Vali Loss: 0.0906686 Test Loss: 0.1681312
Validation loss decreased (0.093358 --> 0.090669).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2677164 Vali Loss: 0.0964613 Test Loss: 0.1774976
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2659343 Vali Loss: 0.0934925 Test Loss: 0.1735348
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2650054 Vali Loss: 0.0952947 Test Loss: 0.1769610
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2645807 Vali Loss: 0.0940604 Test Loss: 0.1757188
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2647895 Vali Loss: 0.0938691 Test Loss: 0.1749568
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2647789 Vali Loss: 0.0935711 Test Loss: 0.1750120
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2642452 Vali Loss: 0.0936353 Test Loss: 0.1750241
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2643605 Vali Loss: 0.0934111 Test Loss: 0.1750379
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2649602 Vali Loss: 0.0940245 Test Loss: 0.1750433
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2650263 Vali Loss: 0.0938223 Test Loss: 0.1750522
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_APPLE_H100_FEDformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0002478625683579594, mae:0.011229067109525204, rmse:0.01574365235865116, r2:-0.04976785182952881, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0112, RMSE: 0.0157, RÂ²: -0.0498, MAPE: 772742.38%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.721 MB of 0.726 MB uploadedwandb: \ 0.721 MB of 0.726 MB uploadedwandb: | 0.721 MB of 0.726 MB uploadedwandb: / 0.726 MB of 0.726 MB uploadedwandb: - 0.726 MB of 0.726 MB uploadedwandb: \ 0.726 MB of 0.938 MB uploadedwandb: | 0.938 MB of 0.938 MB uploadedwandb: / 0.938 MB of 0.938 MB uploadedwandb: - 0.938 MB of 0.938 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–…â–ˆâ–‡â–†â–†â–†â–†â–†â–†
wandb:                 train/loss â–ˆâ–†â–ƒâ–‚â–â–‚â–‚â–â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ˆâ–„â–‡â–…â–…â–…â–…â–„â–…â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 16691206
wandb:     model/trainable_params 16691206
wandb:                  test/loss 0.17505
wandb:                 train/loss 0.26503
wandb:   val/directional_accuracy 48.29726
wandb:                   val/loss 0.09382
wandb:                    val/mae 0.01123
wandb:                   val/mape 77274237.5
wandb:                    val/mse 0.00025
wandb:                     val/r2 -0.04977
wandb:                   val/rmse 0.01574
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/fw7sz8d5
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_043020-fw7sz8d5/logs
Completed: APPLE H=100

Training: FEDformer on SP500 for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_043623-tdpc069f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/tdpc069f
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_SP500_H3  Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
 fourier enhanced cross attention used!
modes_q=16, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/tdpc069f
>>>>>>>start training : long_term_forecast_FEDformer_SP500_H3_FEDformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 3
============================================================
train 4241
val 237
test 238
Epoch: 1, Steps: 133 | Train Loss: 0.2630135 Vali Loss: 0.0752003 Test Loss: 0.0832197
Validation loss decreased (inf --> 0.075200).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.26301349206526475, 'val/loss': 0.07520027598366141, 'test/loss': 0.08321969257667661, '_timestamp': 1762310213.2776194}).
Epoch: 2, Steps: 133 | Train Loss: 0.2042145 Vali Loss: 0.0748337 Test Loss: 0.0805644
Validation loss decreased (0.075200 --> 0.074834).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2042144654052598, 'val/loss': 0.07483370508998632, 'test/loss': 0.08056437876075506, '_timestamp': 1762310236.2114105}).
Epoch: 3, Steps: 133 | Train Loss: 0.1927805 Vali Loss: 0.0684580 Test Loss: 0.0766299
Validation loss decreased (0.074834 --> 0.068458).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1879296 Vali Loss: 0.0686817 Test Loss: 0.0755597
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1860149 Vali Loss: 0.0696714 Test Loss: 0.0764858
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1849351 Vali Loss: 0.0700605 Test Loss: 0.0759922
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1837759 Vali Loss: 0.0693220 Test Loss: 0.0760197
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1836804 Vali Loss: 0.0711023 Test Loss: 0.0758747
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1830204 Vali Loss: 0.0695782 Test Loss: 0.0759010
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1835239 Vali Loss: 0.0696424 Test Loss: 0.0758477
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1846376 Vali Loss: 0.0677222 Test Loss: 0.0758519
Validation loss decreased (0.068458 --> 0.067722).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1829230 Vali Loss: 0.0714592 Test Loss: 0.0758449
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1838174 Vali Loss: 0.0693773 Test Loss: 0.0758416
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1828480 Vali Loss: 0.0673785 Test Loss: 0.0758401
Validation loss decreased (0.067722 --> 0.067378).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1832031 Vali Loss: 0.0677759 Test Loss: 0.0758411
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1853230 Vali Loss: 0.0701880 Test Loss: 0.0758409
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1850592 Vali Loss: 0.0687835 Test Loss: 0.0758406
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1839628 Vali Loss: 0.0718221 Test Loss: 0.0758409
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1835325 Vali Loss: 0.0692011 Test Loss: 0.0758410
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1835433 Vali Loss: 0.0700120 Test Loss: 0.0758411
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1835922 Vali Loss: 0.0690077 Test Loss: 0.0758409
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1830881 Vali Loss: 0.0699258 Test Loss: 0.0758410
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1837876 Vali Loss: 0.0684976 Test Loss: 0.0758410
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.1835522 Vali Loss: 0.0701562 Test Loss: 0.0758410
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_SP500_H3_FEDformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 238
test shape: (238, 3, 1) (238, 3, 1)
test shape: (238, 3, 1) (238, 3, 1)


	mse:6.890169606776908e-05, mae:0.00621049152687192, rmse:0.008300704881548882, r2:-0.059842586517333984, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0062, RMSE: 0.0083, RÂ²: -0.0598, MAPE: 2.39%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.499 MB of 0.499 MB uploadedwandb: \ 0.499 MB of 0.499 MB uploadedwandb: | 0.499 MB of 0.499 MB uploadedwandb: / 0.499 MB of 0.499 MB uploadedwandb: - 0.499 MB of 0.499 MB uploadedwandb: \ 0.499 MB of 0.499 MB uploadedwandb: | 0.499 MB of 0.499 MB uploadedwandb: / 0.628 MB of 0.840 MB uploaded (0.002 MB deduped)wandb: - 0.840 MB of 0.840 MB uploaded (0.002 MB deduped)wandb: \ 0.840 MB of 0.840 MB uploaded (0.002 MB deduped)wandb: | 0.840 MB of 0.840 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–‡â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–‚â–â–‚â–â–â–ƒâ–ƒâ–‚â–â–â–‚â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–ƒâ–…â–…â–„â–‡â–„â–…â–‚â–‡â–„â–â–‚â–…â–ƒâ–ˆâ–„â–…â–„â–…â–ƒâ–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 23
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 14594054
wandb:     model/trainable_params 14594054
wandb:                  test/loss 0.07584
wandb:                 train/loss 0.18355
wandb:   val/directional_accuracy 47.26891
wandb:                   val/loss 0.07016
wandb:                    val/mae 0.00621
wandb:                   val/mape 239.24036
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.05984
wandb:                   val/rmse 0.0083
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/tdpc069f
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_043623-tdpc069f/logs
Completed: SP500 H=3

Training: FEDformer on SP500 for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_044554-x5mf6m1i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/x5mf6m1i
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_SP500_H5  Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
 fourier enhanced cross attention used!
modes_q=17, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/x5mf6m1i
>>>>>>>start training : long_term_forecast_FEDformer_SP500_H5_FEDformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 5
============================================================
train 4239
val 235
test 236
Epoch: 1, Steps: 133 | Train Loss: 0.2560796 Vali Loss: 0.0703666 Test Loss: 0.0798743
Validation loss decreased (inf --> 0.070367).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2560795585911973, 'val/loss': 0.07036656653508544, 'test/loss': 0.07987431716173887, '_timestamp': 1762310785.1247826}).
Epoch: 2, Steps: 133 | Train Loss: 0.1980718 Vali Loss: 0.0748570 Test Loss: 0.0792019
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19807181860271253, 'val/loss': 0.07485698070377111, 'test/loss': 0.07920186128467321, '_timestamp': 1762310808.2911916}).
Epoch: 3, Steps: 133 | Train Loss: 0.1897062 Vali Loss: 0.0705821 Test Loss: 0.0785351
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1848662 Vali Loss: 0.0722045 Test Loss: 0.0777099
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1839108 Vali Loss: 0.0709445 Test Loss: 0.0764575
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1826894 Vali Loss: 0.0693644 Test Loss: 0.0768168
Validation loss decreased (0.070367 --> 0.069364).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1822330 Vali Loss: 0.0687711 Test Loss: 0.0765445
Validation loss decreased (0.069364 --> 0.068771).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1827108 Vali Loss: 0.0691074 Test Loss: 0.0765628
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1818707 Vali Loss: 0.0692036 Test Loss: 0.0765111
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1825252 Vali Loss: 0.0693051 Test Loss: 0.0765435
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1822896 Vali Loss: 0.0690549 Test Loss: 0.0765890
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1818733 Vali Loss: 0.0680994 Test Loss: 0.0765809
Validation loss decreased (0.068771 --> 0.068099).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1823916 Vali Loss: 0.0708933 Test Loss: 0.0766081
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1819732 Vali Loss: 0.0690560 Test Loss: 0.0765999
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1828006 Vali Loss: 0.0697132 Test Loss: 0.0766026
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1816111 Vali Loss: 0.0703524 Test Loss: 0.0766013
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1814431 Vali Loss: 0.0693924 Test Loss: 0.0766009
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1819682 Vali Loss: 0.0690688 Test Loss: 0.0766013
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1821127 Vali Loss: 0.0683164 Test Loss: 0.0766013
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1822557 Vali Loss: 0.0723580 Test Loss: 0.0766013
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1819445 Vali Loss: 0.0690017 Test Loss: 0.0766012
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1816891 Vali Loss: 0.0706126 Test Loss: 0.0766012
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_SP500_H5_FEDformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
test shape: (236, 5, 1) (236, 5, 1)
test shape: (236, 5, 1) (236, 5, 1)


	mse:6.839821435278282e-05, mae:0.006197422742843628, rmse:0.008270321413874626, r2:-0.05290722846984863, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0062, RMSE: 0.0083, RÂ²: -0.0529, MAPE: 2.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.529 MB of 0.530 MB uploadedwandb: \ 0.529 MB of 0.530 MB uploadedwandb: | 0.530 MB of 0.530 MB uploadedwandb: / 0.530 MB of 0.530 MB uploadedwandb: - 0.530 MB of 0.742 MB uploadedwandb: \ 0.742 MB of 0.742 MB uploadedwandb: | 0.742 MB of 0.742 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–â–‚â–â–‚â–â–â–â–‚â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–ˆâ–†â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–â–†â–ƒâ–„â–…â–ƒâ–ƒâ–â–ˆâ–‚â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 14725126
wandb:     model/trainable_params 14725126
wandb:                  test/loss 0.0766
wandb:                 train/loss 0.18169
wandb:   val/directional_accuracy 48.72881
wandb:                   val/loss 0.07061
wandb:                    val/mae 0.0062
wandb:                   val/mape 249.7956
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.05291
wandb:                   val/rmse 0.00827
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/x5mf6m1i
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_044554-x5mf6m1i/logs
Completed: SP500 H=5

Training: FEDformer on SP500 for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_045446-qryzhq1n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/qryzhq1n
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_SP500_H10 Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
 fourier enhanced cross attention used!
modes_q=20, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/qryzhq1n
>>>>>>>start training : long_term_forecast_FEDformer_SP500_H10_FEDformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 10
============================================================
train 4234
val 230
test 231
Epoch: 1, Steps: 133 | Train Loss: 0.2435802 Vali Loss: 0.0743626 Test Loss: 0.0830173
Validation loss decreased (inf --> 0.074363).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.24358015770750835, 'val/loss': 0.074362569488585, 'test/loss': 0.08301732363179326, '_timestamp': 1762311318.1615975}).
Epoch: 2, Steps: 133 | Train Loss: 0.1923797 Vali Loss: 0.0729197 Test Loss: 0.0818784
Validation loss decreased (0.074363 --> 0.072920).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19237965967198065, 'val/loss': 0.07291966723278165, 'test/loss': 0.0818784050643444, '_timestamp': 1762311341.4631588}).
Epoch: 3, Steps: 133 | Train Loss: 0.1856795 Vali Loss: 0.0734278 Test Loss: 0.0804574
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1830310 Vali Loss: 0.0686911 Test Loss: 0.0810117
Validation loss decreased (0.072920 --> 0.068691).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1827509 Vali Loss: 0.0703312 Test Loss: 0.0800702
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1812886 Vali Loss: 0.0691905 Test Loss: 0.0802794
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1811476 Vali Loss: 0.0718994 Test Loss: 0.0801932
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1822214 Vali Loss: 0.0709702 Test Loss: 0.0800475
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1817141 Vali Loss: 0.0707520 Test Loss: 0.0801288
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1807077 Vali Loss: 0.0723275 Test Loss: 0.0800716
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1805846 Vali Loss: 0.0696909 Test Loss: 0.0800741
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1805185 Vali Loss: 0.0683862 Test Loss: 0.0800599
Validation loss decreased (0.068691 --> 0.068386).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1804886 Vali Loss: 0.0705020 Test Loss: 0.0800605
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1800196 Vali Loss: 0.0711607 Test Loss: 0.0800579
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1804169 Vali Loss: 0.0728684 Test Loss: 0.0800541
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1807246 Vali Loss: 0.0722349 Test Loss: 0.0800553
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1820661 Vali Loss: 0.0699858 Test Loss: 0.0800557
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1810402 Vali Loss: 0.0722189 Test Loss: 0.0800556
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1812576 Vali Loss: 0.0725454 Test Loss: 0.0800556
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1801182 Vali Loss: 0.0685732 Test Loss: 0.0800555
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1801220 Vali Loss: 0.0692394 Test Loss: 0.0800555
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1805012 Vali Loss: 0.0687864 Test Loss: 0.0800555
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_SP500_H10_FEDformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 231
test shape: (231, 10, 1) (231, 10, 1)
test shape: (231, 10, 1) (231, 10, 1)


	mse:6.881812441861257e-05, mae:0.00619858130812645, rmse:0.008295669220387936, r2:-0.05869472026824951, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0062, RMSE: 0.0083, RÂ²: -0.0587, MAPE: 2.70%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.550 MB of 0.551 MB uploadedwandb: \ 0.551 MB of 0.551 MB uploadedwandb: | 0.551 MB of 0.551 MB uploadedwandb: / 0.551 MB of 0.764 MB uploadedwandb: - 0.764 MB of 0.764 MB uploadedwandb: \ 0.764 MB of 0.764 MB uploadedwandb: | 0.764 MB of 0.764 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–„â–ˆâ–â–ƒâ–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–„â–ƒâ–‚â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–‚â–„â–‚â–ƒâ–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–„â–‚â–†â–…â–„â–†â–ƒâ–â–„â–…â–‡â–†â–ƒâ–†â–‡â–â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 15118342
wandb:     model/trainable_params 15118342
wandb:                  test/loss 0.08006
wandb:                 train/loss 0.1805
wandb:   val/directional_accuracy 48.29245
wandb:                   val/loss 0.06879
wandb:                    val/mae 0.0062
wandb:                   val/mape 270.10183
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.05869
wandb:                   val/rmse 0.0083
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/qryzhq1n
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_045446-qryzhq1n/logs
Completed: SP500 H=10

Training: FEDformer on SP500 for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_050352-oasbd4yj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/oasbd4yj
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_SP500_H22 Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
 fourier enhanced cross attention used!
modes_q=26, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/oasbd4yj
>>>>>>>start training : long_term_forecast_FEDformer_SP500_H22_FEDformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 22
============================================================
train 4222
val 218
test 219
Epoch: 1, Steps: 132 | Train Loss: 0.2314993 Vali Loss: 0.0735823 Test Loss: 0.0712485
Validation loss decreased (inf --> 0.073582).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23149933784522794, 'val/loss': 0.07358226499387197, 'test/loss': 0.0712485494358199, '_timestamp': 1762311865.0371013}).
Epoch: 2, Steps: 132 | Train Loss: 0.1904263 Vali Loss: 0.0732380 Test Loss: 0.0715576
Validation loss decreased (0.073582 --> 0.073238).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19042628230244824, 'val/loss': 0.07323801517486572, 'test/loss': 0.07155762825693403, '_timestamp': 1762311889.6189687}).
Epoch: 3, Steps: 132 | Train Loss: 0.1864322 Vali Loss: 0.0727680 Test Loss: 0.0730892
Validation loss decreased (0.073238 --> 0.072768).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.1842362 Vali Loss: 0.0726896 Test Loss: 0.0711280
Validation loss decreased (0.072768 --> 0.072690).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.1831105 Vali Loss: 0.0725782 Test Loss: 0.0718989
Validation loss decreased (0.072690 --> 0.072578).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.1826211 Vali Loss: 0.0724636 Test Loss: 0.0716807
Validation loss decreased (0.072578 --> 0.072464).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.1823854 Vali Loss: 0.0727285 Test Loss: 0.0718960
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.1822704 Vali Loss: 0.0724891 Test Loss: 0.0717591
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.1821908 Vali Loss: 0.0724505 Test Loss: 0.0716924
Validation loss decreased (0.072464 --> 0.072450).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.1822092 Vali Loss: 0.0724845 Test Loss: 0.0716180
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.1823025 Vali Loss: 0.0723089 Test Loss: 0.0716029
Validation loss decreased (0.072450 --> 0.072309).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.1820760 Vali Loss: 0.0724976 Test Loss: 0.0716104
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.1819507 Vali Loss: 0.0723337 Test Loss: 0.0716094
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.1820050 Vali Loss: 0.0725393 Test Loss: 0.0716095
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.1821368 Vali Loss: 0.0723588 Test Loss: 0.0716092
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.1819888 Vali Loss: 0.0724618 Test Loss: 0.0716083
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.1821445 Vali Loss: 0.0726145 Test Loss: 0.0716082
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.1820104 Vali Loss: 0.0726045 Test Loss: 0.0716082
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.1821597 Vali Loss: 0.0722094 Test Loss: 0.0716082
Validation loss decreased (0.072309 --> 0.072209).  Saving model ...
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.1821509 Vali Loss: 0.0724957 Test Loss: 0.0716082
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.1820062 Vali Loss: 0.0723630 Test Loss: 0.0716082
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.1821188 Vali Loss: 0.0723662 Test Loss: 0.0716082
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.1820093 Vali Loss: 0.0723022 Test Loss: 0.0716082
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 132 | Train Loss: 0.1819915 Vali Loss: 0.0724859 Test Loss: 0.0716082
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 132 | Train Loss: 0.1819516 Vali Loss: 0.0722203 Test Loss: 0.0716082
EarlyStopping counter: 6 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 132 | Train Loss: 0.1820460 Vali Loss: 0.0723170 Test Loss: 0.0716082
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 132 | Train Loss: 0.1822246 Vali Loss: 0.0722711 Test Loss: 0.0716082
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 132 | Train Loss: 0.1819774 Vali Loss: 0.0722824 Test Loss: 0.0716082
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 132 | Train Loss: 0.1821878 Vali Loss: 0.0726141 Test Loss: 0.0716082
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_SP500_H22_FEDformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 219
test shape: (219, 22, 1) (219, 22, 1)
test shape: (219, 22, 1) (219, 22, 1)


	mse:6.660429062321782e-05, mae:0.006085971370339394, rmse:0.008161145262420177, r2:-0.04321146011352539, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0082, RÂ²: -0.0432, MAPE: 2.31%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.625 MB of 0.626 MB uploadedwandb: \ 0.625 MB of 0.626 MB uploadedwandb: | 0.625 MB of 0.626 MB uploadedwandb: / 0.626 MB of 0.626 MB uploadedwandb: - 0.626 MB of 0.841 MB uploadedwandb: \ 0.841 MB of 0.841 MB uploadedwandb: | 0.841 MB of 0.841 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‡â–†â–„â–ˆâ–…â–„â–„â–‚â–…â–ƒâ–…â–ƒâ–„â–†â–†â–â–…â–ƒâ–ƒâ–‚â–„â–â–‚â–‚â–‚â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 28
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 15904774
wandb:     model/trainable_params 15904774
wandb:                  test/loss 0.07161
wandb:                 train/loss 0.18219
wandb:   val/directional_accuracy 47.70602
wandb:                   val/loss 0.07261
wandb:                    val/mae 0.00609
wandb:                   val/mape 230.92599
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.04321
wandb:                   val/rmse 0.00816
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/oasbd4yj
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_050352-oasbd4yj/logs
Completed: SP500 H=22

Training: FEDformer on SP500 for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_051612-eogppeul
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/eogppeul
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_SP500_H50 Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 39]
 fourier enhanced cross attention used!
modes_q=32, index_q=[0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 21, 22, 24, 26, 28, 29, 30, 33, 34, 35, 36, 37, 38, 39]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/eogppeul
>>>>>>>start training : long_term_forecast_FEDformer_SP500_H50_FEDformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 50
============================================================
train 4194
val 190
test 191
Epoch: 1, Steps: 132 | Train Loss: 0.2291857 Vali Loss: 0.0736835 Test Loss: 0.0770098
Validation loss decreased (inf --> 0.073683).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.229185665872964, 'val/loss': 0.0736834704875946, 'test/loss': 0.07700977039833863, '_timestamp': 1762312605.737718}).
Epoch: 2, Steps: 132 | Train Loss: 0.1931270 Vali Loss: 0.0732825 Test Loss: 0.0761469
Validation loss decreased (0.073683 --> 0.073282).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19312704343235854, 'val/loss': 0.07328249265750249, 'test/loss': 0.07614693728586037, '_timestamp': 1762312632.4768274}).
Epoch: 3, Steps: 132 | Train Loss: 0.1904423 Vali Loss: 0.0727480 Test Loss: 0.0734741
Validation loss decreased (0.073282 --> 0.072748).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.1892415 Vali Loss: 0.0726654 Test Loss: 0.0742291
Validation loss decreased (0.072748 --> 0.072665).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.1879102 Vali Loss: 0.0729718 Test Loss: 0.0739212
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.1871802 Vali Loss: 0.0733026 Test Loss: 0.0743096
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.1870206 Vali Loss: 0.0731079 Test Loss: 0.0738518
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.1867403 Vali Loss: 0.0732253 Test Loss: 0.0741498
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.1865173 Vali Loss: 0.0732540 Test Loss: 0.0740706
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.1866948 Vali Loss: 0.0732392 Test Loss: 0.0740707
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.1877877 Vali Loss: 0.0731852 Test Loss: 0.0740810
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.1868477 Vali Loss: 0.0732059 Test Loss: 0.0740863
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.1884733 Vali Loss: 0.0732382 Test Loss: 0.0740910
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.1865333 Vali Loss: 0.0731901 Test Loss: 0.0740895
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_SP500_H50_FEDformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 191
test shape: (191, 50, 1) (191, 50, 1)
test shape: (191, 50, 1) (191, 50, 1)


	mse:6.703077087877318e-05, mae:0.006096324417740107, rmse:0.008187232539057732, r2:-0.03072667121887207, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0082, RÂ²: -0.0307, MAPE: 2.56%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.704 MB of 0.707 MB uploadedwandb: \ 0.707 MB of 0.707 MB uploadedwandb: | 0.707 MB of 0.918 MB uploadedwandb: / 0.918 MB of 0.918 MB uploadedwandb: - 0.918 MB of 0.918 MB uploadedwandb: \ 0.918 MB of 0.918 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–‡â–…â–ˆâ–„â–‡â–†â–†â–†â–†â–†â–†
wandb:                 train/loss â–ˆâ–†â–ƒâ–‚â–‚â–â–â–â–ƒâ–‚â–„â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–â–„â–ˆâ–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 13
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 16691206
wandb:     model/trainable_params 16691206
wandb:                  test/loss 0.07409
wandb:                 train/loss 0.18653
wandb:   val/directional_accuracy 49.08644
wandb:                   val/loss 0.07319
wandb:                    val/mae 0.0061
wandb:                   val/mape 255.75807
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.03073
wandb:                   val/rmse 0.00819
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/eogppeul
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_051612-eogppeul/logs
Completed: SP500 H=50

Training: FEDformer on SP500 for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_052255-bwz2jm0o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/bwz2jm0o
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_SP500_H100Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 8, 13, 16, 20, 22, 23, 24, 28, 29, 30, 31, 32, 34, 35, 37, 39, 43, 44, 46, 51, 52, 55, 56, 57, 62, 63, 64]
 fourier enhanced cross attention used!
modes_q=32, index_q=[0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 17, 18, 20, 21, 22, 23, 28, 30, 34, 36, 44, 45, 50, 53, 54, 55, 56, 57, 61, 63]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/bwz2jm0o
>>>>>>>start training : long_term_forecast_FEDformer_SP500_H100_FEDformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 100
============================================================
train 4144
val 140
test 141
Epoch: 1, Steps: 130 | Train Loss: 0.2385907 Vali Loss: 0.0678773 Test Loss: 0.0816719
Validation loss decreased (inf --> 0.067877).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23859073588481317, 'val/loss': 0.06787729114294053, 'test/loss': 0.08167193979024887, '_timestamp': 1762313008.4211087}).
Epoch: 2, Steps: 130 | Train Loss: 0.2035222 Vali Loss: 0.0679918 Test Loss: 0.0825688
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.20352220827570328, 'val/loss': 0.0679918423295021, 'test/loss': 0.08256877511739731, '_timestamp': 1762313034.3056426}).
Epoch: 3, Steps: 130 | Train Loss: 0.2010469 Vali Loss: 0.0688002 Test Loss: 0.0841767
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.1998916 Vali Loss: 0.0688682 Test Loss: 0.0848592
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.1987669 Vali Loss: 0.0681986 Test Loss: 0.0832613
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.1986294 Vali Loss: 0.0681488 Test Loss: 0.0835997
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.1979081 Vali Loss: 0.0684911 Test Loss: 0.0833816
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.1981391 Vali Loss: 0.0684075 Test Loss: 0.0832271
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.1979337 Vali Loss: 0.0683184 Test Loss: 0.0834125
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.1983667 Vali Loss: 0.0683562 Test Loss: 0.0834152
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.1981333 Vali Loss: 0.0683869 Test Loss: 0.0834207
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_SP500_H100_FEDformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 141
test shape: (141, 100, 1) (141, 100, 1)
test shape: (141, 100, 1) (141, 100, 1)


	mse:6.98889052728191e-05, mae:0.00619527418166399, rmse:0.008359958417713642, r2:-0.020151853561401367, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0062, RMSE: 0.0084, RÂ²: -0.0202, MAPE: 2.74%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.808 MB of 0.813 MB uploadedwandb: \ 0.808 MB of 0.813 MB uploadedwandb: | 0.808 MB of 0.813 MB uploadedwandb: / 0.813 MB of 0.813 MB uploadedwandb: - 0.813 MB of 1.025 MB uploadedwandb: \ 0.813 MB of 1.025 MB uploadedwandb: | 1.025 MB of 1.025 MB uploadedwandb: / 1.025 MB of 1.025 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–ˆâ–â–ƒâ–‚â–â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–â–‚â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–ˆâ–â–â–„â–„â–ƒâ–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 16691206
wandb:     model/trainable_params 16691206
wandb:                  test/loss 0.08342
wandb:                 train/loss 0.19813
wandb:   val/directional_accuracy 49.49495
wandb:                   val/loss 0.06839
wandb:                    val/mae 0.0062
wandb:                   val/mape 274.25177
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.02015
wandb:                   val/rmse 0.00836
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/bwz2jm0o
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_052255-bwz2jm0o/logs
Completed: SP500 H=100

Training: FEDformer on NASDAQ for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_052812-761bxt5u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/761bxt5u
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_NASDAQ_H3 Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
 fourier enhanced cross attention used!
modes_q=16, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/761bxt5u
>>>>>>>start training : long_term_forecast_FEDformer_NASDAQ_H3_FEDformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.3329542 Vali Loss: 0.1563365 Test Loss: 0.1456417
Validation loss decreased (inf --> 0.156337).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3329542404726932, 'val/loss': 0.1563365114852786, 'test/loss': 0.14564173854887486, '_timestamp': 1762313322.3334851}).
Epoch: 2, Steps: 133 | Train Loss: 0.2710575 Vali Loss: 0.1454290 Test Loss: 0.1281158
Validation loss decreased (0.156337 --> 0.145429).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.27105748272479924, 'val/loss': 0.14542904682457447, 'test/loss': 0.12811578856781125, '_timestamp': 1762313345.0253239}).
Epoch: 3, Steps: 133 | Train Loss: 0.2558493 Vali Loss: 0.1394435 Test Loss: 0.1236017
Validation loss decreased (0.145429 --> 0.139444).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2490648 Vali Loss: 0.1432977 Test Loss: 0.1266277
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2490780 Vali Loss: 0.1377310 Test Loss: 0.1223895
Validation loss decreased (0.139444 --> 0.137731).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2451982 Vali Loss: 0.1350937 Test Loss: 0.1232205
Validation loss decreased (0.137731 --> 0.135094).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2438699 Vali Loss: 0.1349024 Test Loss: 0.1223532
Validation loss decreased (0.135094 --> 0.134902).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2439143 Vali Loss: 0.1377762 Test Loss: 0.1222587
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2447993 Vali Loss: 0.1342796 Test Loss: 0.1223262
Validation loss decreased (0.134902 --> 0.134280).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2445959 Vali Loss: 0.1376881 Test Loss: 0.1222212
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2437822 Vali Loss: 0.1471932 Test Loss: 0.1222524
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2438812 Vali Loss: 0.1467994 Test Loss: 0.1222304
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2432847 Vali Loss: 0.1384962 Test Loss: 0.1222285
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2432613 Vali Loss: 0.1343701 Test Loss: 0.1222260
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2429990 Vali Loss: 0.1378304 Test Loss: 0.1222251
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2433039 Vali Loss: 0.1356270 Test Loss: 0.1222261
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2446390 Vali Loss: 0.1345201 Test Loss: 0.1222267
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2437813 Vali Loss: 0.1348614 Test Loss: 0.1222267
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2430470 Vali Loss: 0.1376918 Test Loss: 0.1222267
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_NASDAQ_H3_FEDformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.00014299848407972604, mae:0.008767525665462017, rmse:0.011958197690546513, r2:-0.050618767738342285, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0088, RMSE: 0.0120, RÂ²: -0.0506, MAPE: 4187585.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.490 MB of 0.490 MB uploadedwandb: \ 0.490 MB of 0.490 MB uploadedwandb: | 0.490 MB of 0.490 MB uploadedwandb: / 0.490 MB of 0.490 MB uploadedwandb: - 0.490 MB of 0.490 MB uploadedwandb: \ 0.490 MB of 0.490 MB uploadedwandb: | 0.490 MB of 0.490 MB uploadedwandb: / 0.490 MB of 0.490 MB uploadedwandb: - 0.619 MB of 0.831 MB uploaded (0.002 MB deduped)wandb: \ 0.831 MB of 0.831 MB uploaded (0.002 MB deduped)wandb: | 0.831 MB of 0.831 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ƒâ–ˆâ–â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–„â–‚â–â–â–‚â–‚â–â–â–â–â–â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–†â–ƒâ–â–â–ƒâ–â–ƒâ–ˆâ–ˆâ–ƒâ–â–ƒâ–‚â–â–â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 18
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 14594054
wandb:     model/trainable_params 14594054
wandb:                  test/loss 0.12223
wandb:                 train/loss 0.24305
wandb:   val/directional_accuracy 47.67932
wandb:                   val/loss 0.13769
wandb:                    val/mae 0.00877
wandb:                   val/mape 418758500.0
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.05062
wandb:                   val/rmse 0.01196
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/761bxt5u
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_052812-761bxt5u/logs
Completed: NASDAQ H=3

Training: FEDformer on NASDAQ for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_053557-ug2le8iu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/ug2le8iu
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_NASDAQ_H5 Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
 fourier enhanced cross attention used!
modes_q=17, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/ug2le8iu
>>>>>>>start training : long_term_forecast_FEDformer_NASDAQ_H5_FEDformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.3246070 Vali Loss: 0.1551155 Test Loss: 0.1489834
Validation loss decreased (inf --> 0.155116).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3246069966178191, 'val/loss': 0.15511553175747395, 'test/loss': 0.14898337330669165, '_timestamp': 1762313786.9975812}).
Epoch: 2, Steps: 133 | Train Loss: 0.2683142 Vali Loss: 0.1429241 Test Loss: 0.1342751
Validation loss decreased (0.155116 --> 0.142924).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26831424550006266, 'val/loss': 0.14292413834482431, 'test/loss': 0.13427508156746626, '_timestamp': 1762313809.3093228}).
Epoch: 3, Steps: 133 | Train Loss: 0.2553412 Vali Loss: 0.1370623 Test Loss: 0.1338379
Validation loss decreased (0.142924 --> 0.137062).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2496357 Vali Loss: 0.1394478 Test Loss: 0.1303396
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2478513 Vali Loss: 0.1376877 Test Loss: 0.1289874
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2465018 Vali Loss: 0.1401433 Test Loss: 0.1284968
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2454788 Vali Loss: 0.1373925 Test Loss: 0.1285990
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2460144 Vali Loss: 0.1498025 Test Loss: 0.1286739
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2454854 Vali Loss: 0.1475703 Test Loss: 0.1286169
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2460108 Vali Loss: 0.1371632 Test Loss: 0.1286120
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2444230 Vali Loss: 0.1390920 Test Loss: 0.1286045
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2445359 Vali Loss: 0.1374683 Test Loss: 0.1285958
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2454555 Vali Loss: 0.1413835 Test Loss: 0.1285976
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_NASDAQ_H5_FEDformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.00014784408267587423, mae:0.008901877328753471, rmse:0.012159115634858608, r2:-0.08070862293243408, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0089, RMSE: 0.0122, RÂ²: -0.0807, MAPE: 3005534.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.513 MB of 0.514 MB uploadedwandb: \ 0.513 MB of 0.514 MB uploadedwandb: | 0.514 MB of 0.514 MB uploadedwandb: / 0.514 MB of 0.725 MB uploadedwandb: - 0.725 MB of 0.725 MB uploadedwandb: \ 0.725 MB of 0.725 MB uploadedwandb: | 0.725 MB of 0.725 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–‚â–â–ƒâ–â–ˆâ–‡â–â–‚â–â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 14725126
wandb:     model/trainable_params 14725126
wandb:                  test/loss 0.1286
wandb:                 train/loss 0.24546
wandb:   val/directional_accuracy 48.29787
wandb:                   val/loss 0.14138
wandb:                    val/mae 0.0089
wandb:                   val/mape 300553425.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.08071
wandb:                   val/rmse 0.01216
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/ug2le8iu
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_053557-ug2le8iu/logs
Completed: NASDAQ H=5

Training: FEDformer on NASDAQ for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_054125-go9l2txs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/go9l2txs
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_NASDAQ_H10Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
 fourier enhanced cross attention used!
modes_q=20, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/go9l2txs
>>>>>>>start training : long_term_forecast_FEDformer_NASDAQ_H10_FEDformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.3164092 Vali Loss: 0.1458191 Test Loss: 0.1338673
Validation loss decreased (inf --> 0.145819).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.31640916592196416, 'val/loss': 0.14581909496337175, 'test/loss': 0.13386732898652554, '_timestamp': 1762314116.1737359}).
Epoch: 2, Steps: 133 | Train Loss: 0.2624200 Vali Loss: 0.1589793 Test Loss: 0.1397348
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26241997410928397, 'val/loss': 0.15897928178310394, 'test/loss': 0.1397347692400217, '_timestamp': 1762314139.6150162}).
Epoch: 3, Steps: 133 | Train Loss: 0.2543896 Vali Loss: 0.1496755 Test Loss: 0.1383888
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2511348 Vali Loss: 0.1451289 Test Loss: 0.1324526
Validation loss decreased (0.145819 --> 0.145129).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2475819 Vali Loss: 0.1448536 Test Loss: 0.1317169
Validation loss decreased (0.145129 --> 0.144854).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2459789 Vali Loss: 0.1459005 Test Loss: 0.1332809
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2470327 Vali Loss: 0.1471066 Test Loss: 0.1322406
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2456925 Vali Loss: 0.1551304 Test Loss: 0.1321357
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2459962 Vali Loss: 0.1547408 Test Loss: 0.1323287
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2454749 Vali Loss: 0.1445667 Test Loss: 0.1321909
Validation loss decreased (0.144854 --> 0.144567).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2457665 Vali Loss: 0.1414638 Test Loss: 0.1321941
Validation loss decreased (0.144567 --> 0.141464).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2458696 Vali Loss: 0.1433473 Test Loss: 0.1322002
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2464065 Vali Loss: 0.1461015 Test Loss: 0.1322026
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2453577 Vali Loss: 0.1538790 Test Loss: 0.1321971
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2454472 Vali Loss: 0.1567675 Test Loss: 0.1321952
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2461144 Vali Loss: 0.1444457 Test Loss: 0.1321939
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2449432 Vali Loss: 0.1431943 Test Loss: 0.1321947
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2466166 Vali Loss: 0.1458827 Test Loss: 0.1321943
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2478226 Vali Loss: 0.1455887 Test Loss: 0.1321945
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2449306 Vali Loss: 0.1531855 Test Loss: 0.1321944
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2458133 Vali Loss: 0.1442372 Test Loss: 0.1321944
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_NASDAQ_H10_FEDformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.00014890587772242725, mae:0.008948043920099735, rmse:0.012202699668705463, r2:-0.07570672035217285, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0089, RMSE: 0.0122, RÂ²: -0.0757, MAPE: 4081043.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.588 MB of 0.588 MB uploadedwandb: \ 0.588 MB of 0.588 MB uploadedwandb: | 0.588 MB of 0.588 MB uploadedwandb: / 0.588 MB of 0.801 MB uploadedwandb: - 0.801 MB of 0.801 MB uploadedwandb: \ 0.801 MB of 0.801 MB uploadedwandb: | 0.801 MB of 0.801 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–â–ƒâ–‚â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–†â–ƒâ–‚â–ƒâ–‚â–‚â–â–‚â–‚â–‚â–â–â–‚â–â–‚â–ƒâ–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–ƒâ–ƒâ–ƒâ–„â–‡â–‡â–‚â–â–‚â–ƒâ–‡â–ˆâ–‚â–‚â–ƒâ–ƒâ–†â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 15118342
wandb:     model/trainable_params 15118342
wandb:                  test/loss 0.13219
wandb:                 train/loss 0.24581
wandb:   val/directional_accuracy 50.43478
wandb:                   val/loss 0.14424
wandb:                    val/mae 0.00895
wandb:                   val/mape 408104375.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.07571
wandb:                   val/rmse 0.0122
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/go9l2txs
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_054125-go9l2txs/logs
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread ChkStopThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 286, in check_stop_status
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 268, in check_network_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 848, in deliver_network_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 840, in deliver_stop_status
    return self._deliver_network_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 510, in _deliver_network_status
    return self._deliver_stop_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 494, in _deliver_stop_status
        return self._deliver_record(record)return self._deliver_record(record)

  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: NASDAQ H=10

Training: FEDformer on NASDAQ for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_055004-wxnw2efg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/wxnw2efg
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_NASDAQ_H22Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
 fourier enhanced cross attention used!
modes_q=26, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/wxnw2efg
>>>>>>>start training : long_term_forecast_FEDformer_NASDAQ_H22_FEDformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.3051374 Vali Loss: 0.1593100 Test Loss: 0.1322353
Validation loss decreased (inf --> 0.159310).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.30513740680886037, 'val/loss': 0.15931004924433573, 'test/loss': 0.13223525881767273, '_timestamp': 1762314637.2675917}).
Epoch: 2, Steps: 132 | Train Loss: 0.2625028 Vali Loss: 0.1561281 Test Loss: 0.1339916
Validation loss decreased (0.159310 --> 0.156128).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2625028165903958, 'val/loss': 0.15612814043249404, 'test/loss': 0.13399164697953633, '_timestamp': 1762314662.6501622}).
Epoch: 3, Steps: 132 | Train Loss: 0.2556751 Vali Loss: 0.1585450 Test Loss: 0.1338753
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2523283 Vali Loss: 0.1579587 Test Loss: 0.1318332
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2508343 Vali Loss: 0.1594820 Test Loss: 0.1329890
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2501232 Vali Loss: 0.1586117 Test Loss: 0.1326514
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2491578 Vali Loss: 0.1581292 Test Loss: 0.1329338
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2492017 Vali Loss: 0.1576034 Test Loss: 0.1329286
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2488841 Vali Loss: 0.1583235 Test Loss: 0.1326439
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2490452 Vali Loss: 0.1581172 Test Loss: 0.1327638
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2487727 Vali Loss: 0.1574756 Test Loss: 0.1327634
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2489988 Vali Loss: 0.1585131 Test Loss: 0.1327899
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_NASDAQ_H22_FEDformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.00014791397552471608, mae:0.008905081078410149, rmse:0.012161988765001297, r2:-0.05709099769592285, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0089, RMSE: 0.0122, RÂ²: -0.0571, MAPE: 5765250.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.661 MB of 0.663 MB uploadedwandb: \ 0.661 MB of 0.663 MB uploadedwandb: | 0.661 MB of 0.663 MB uploadedwandb: / 0.663 MB of 0.663 MB uploadedwandb: - 0.663 MB of 0.874 MB uploadedwandb: \ 0.761 MB of 0.874 MB uploadedwandb: | 0.761 MB of 0.874 MB uploadedwandb: / 0.874 MB of 0.874 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–…â–„â–…â–…â–„â–„â–„â–„
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–ƒâ–ˆâ–…â–ƒâ–â–„â–ƒâ–â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 15904774
wandb:     model/trainable_params 15904774
wandb:                  test/loss 0.13279
wandb:                 train/loss 0.249
wandb:   val/directional_accuracy 49.93447
wandb:                   val/loss 0.15851
wandb:                    val/mae 0.00891
wandb:                   val/mape 576525050.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.05709
wandb:                   val/rmse 0.01216
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/wxnw2efg
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_055004-wxnw2efg/logs
Completed: NASDAQ H=22

Training: FEDformer on NASDAQ for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_055536-n26qch53
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/n26qch53
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_NASDAQ_H50Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 39]
 fourier enhanced cross attention used!
modes_q=32, index_q=[0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 21, 22, 24, 26, 28, 29, 30, 33, 34, 35, 36, 37, 38, 39]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/n26qch53
>>>>>>>start training : long_term_forecast_FEDformer_NASDAQ_H50_FEDformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.3056335 Vali Loss: 0.1727396 Test Loss: 0.1401474
Validation loss decreased (inf --> 0.172740).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.305633525618098, 'val/loss': 0.1727395753065745, 'test/loss': 0.14014737804730734, '_timestamp': 1762314969.858523}).
Epoch: 2, Steps: 132 | Train Loss: 0.2679138 Vali Loss: 0.1643826 Test Loss: 0.1423429
Validation loss decreased (0.172740 --> 0.164383).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26791384330753126, 'val/loss': 0.1643825943271319, 'test/loss': 0.14234291513760886, '_timestamp': 1762314996.0901477}).
Epoch: 3, Steps: 132 | Train Loss: 0.2629244 Vali Loss: 0.1648635 Test Loss: 0.1394530
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2591537 Vali Loss: 0.1679640 Test Loss: 0.1376065
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2572945 Vali Loss: 0.1673826 Test Loss: 0.1380601
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2559841 Vali Loss: 0.1683597 Test Loss: 0.1371824
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2559877 Vali Loss: 0.1678148 Test Loss: 0.1376174
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2562141 Vali Loss: 0.1681272 Test Loss: 0.1375262
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2579696 Vali Loss: 0.1683998 Test Loss: 0.1373168
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2563460 Vali Loss: 0.1680393 Test Loss: 0.1373978
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2558845 Vali Loss: 0.1677340 Test Loss: 0.1374718
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2559826 Vali Loss: 0.1678919 Test Loss: 0.1374514
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_NASDAQ_H50_FEDformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.00015096517745405436, mae:0.008889425545930862, rmse:0.012286788783967495, r2:-0.04224872589111328, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0089, RMSE: 0.0123, RÂ²: -0.0422, MAPE: 4558720.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.700 MB of 0.702 MB uploadedwandb: \ 0.702 MB of 0.702 MB uploadedwandb: | 0.702 MB of 0.914 MB uploadedwandb: / 0.914 MB of 0.914 MB uploadedwandb: - 0.914 MB of 0.914 MB uploadedwandb: \ 0.914 MB of 0.914 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–„â–â–‚â–‚â–â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–„â–‚â–â–â–â–ƒâ–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–‡â–†â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 16691206
wandb:     model/trainable_params 16691206
wandb:                  test/loss 0.13745
wandb:                 train/loss 0.25598
wandb:   val/directional_accuracy 49.80666
wandb:                   val/loss 0.16789
wandb:                    val/mae 0.00889
wandb:                   val/mape 455872050.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.04225
wandb:                   val/rmse 0.01229
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/n26qch53
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_055536-n26qch53/logs
Completed: NASDAQ H=50

Training: FEDformer on NASDAQ for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_060128-p5l3thyr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/p5l3thyr
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_NASDAQ_H100Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 8, 13, 16, 20, 22, 23, 24, 28, 29, 30, 31, 32, 34, 35, 37, 39, 43, 44, 46, 51, 52, 55, 56, 57, 62, 63, 64]
 fourier enhanced cross attention used!
modes_q=32, index_q=[0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 17, 18, 20, 21, 22, 23, 28, 30, 34, 36, 44, 45, 50, 53, 54, 55, 56, 57, 61, 63]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/p5l3thyr
>>>>>>>start training : long_term_forecast_FEDformer_NASDAQ_H100_FEDformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.3121136 Vali Loss: 0.1889263 Test Loss: 0.1467724
Validation loss decreased (inf --> 0.188926).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.31211357300098125, 'val/loss': 0.18892634510993958, 'test/loss': 0.1467723548412323, '_timestamp': 1762315321.7453854}).
Epoch: 2, Steps: 130 | Train Loss: 0.2743253 Vali Loss: 0.1849535 Test Loss: 0.1491865
Validation loss decreased (0.188926 --> 0.184954).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.27432529078080103, 'val/loss': 0.18495352268218995, 'test/loss': 0.14918646216392517, '_timestamp': 1762315347.6352754}).
Epoch: 3, Steps: 130 | Train Loss: 0.2693868 Vali Loss: 0.1902956 Test Loss: 0.1447198
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2681905 Vali Loss: 0.1904088 Test Loss: 0.1438591
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2659683 Vali Loss: 0.1921387 Test Loss: 0.1436246
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2650323 Vali Loss: 0.1921304 Test Loss: 0.1427234
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2648505 Vali Loss: 0.1936029 Test Loss: 0.1426442
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2649443 Vali Loss: 0.1927712 Test Loss: 0.1429967
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2647561 Vali Loss: 0.1915335 Test Loss: 0.1424814
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2645876 Vali Loss: 0.1935426 Test Loss: 0.1425249
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2648424 Vali Loss: 0.1940350 Test Loss: 0.1425859
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2656448 Vali Loss: 0.1923473 Test Loss: 0.1426259
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_NASDAQ_H100_FEDformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0001559459196869284, mae:0.008748402819037437, rmse:0.012487830594182014, r2:-0.028165340423583984, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0087, RMSE: 0.0125, RÂ²: -0.0282, MAPE: 4859873.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.783 MB of 0.788 MB uploadedwandb: \ 0.783 MB of 0.788 MB uploadedwandb: | 0.783 MB of 0.788 MB uploadedwandb: / 0.788 MB of 0.788 MB uploadedwandb: - 0.788 MB of 0.999 MB uploadedwandb: \ 0.999 MB of 0.999 MB uploadedwandb: | 0.999 MB of 0.999 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–…â–‚â–‚â–ƒâ–â–â–â–
wandb:                 train/loss â–ˆâ–†â–ƒâ–‚â–â–‚â–â–â–â–ƒ
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–â–„â–„â–‡â–†â–ƒâ–‡â–ˆâ–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 16691206
wandb:     model/trainable_params 16691206
wandb:                  test/loss 0.14263
wandb:                 train/loss 0.26564
wandb:   val/directional_accuracy 51.65224
wandb:                   val/loss 0.19235
wandb:                    val/mae 0.00875
wandb:                   val/mape 485987350.0
wandb:                    val/mse 0.00016
wandb:                     val/r2 -0.02817
wandb:                   val/rmse 0.01249
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/p5l3thyr
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_060128-p5l3thyr/logs
Completed: NASDAQ H=100

Training: FEDformer on ABSA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_060709-xbwqt67w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/xbwqt67w
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_ABSA_H3   Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
 fourier enhanced cross attention used!
modes_q=16, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/xbwqt67w
>>>>>>>start training : long_term_forecast_FEDformer_ABSA_H3_FEDformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 238
Epoch: 1, Steps: 133 | Train Loss: 0.3911190 Vali Loss: 0.1796177 Test Loss: 0.1660653
Validation loss decreased (inf --> 0.179618).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3911190083376447, 'val/loss': 0.17961769737303257, 'test/loss': 0.16606532223522663, '_timestamp': 1762315659.1924064}).
Epoch: 2, Steps: 133 | Train Loss: 0.3281565 Vali Loss: 0.1790097 Test Loss: 0.1624401
Validation loss decreased (0.179618 --> 0.179010).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3281565241347578, 'val/loss': 0.17900971695780754, 'test/loss': 0.16244006622582674, '_timestamp': 1762315681.0446103}).
Epoch: 3, Steps: 133 | Train Loss: 0.3078469 Vali Loss: 0.1713904 Test Loss: 0.1659021
Validation loss decreased (0.179010 --> 0.171390).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.3002040 Vali Loss: 0.1713581 Test Loss: 0.1602310
Validation loss decreased (0.171390 --> 0.171358).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2957352 Vali Loss: 0.1679304 Test Loss: 0.1591149
Validation loss decreased (0.171358 --> 0.167930).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2935459 Vali Loss: 0.1635505 Test Loss: 0.1580674
Validation loss decreased (0.167930 --> 0.163550).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2929266 Vali Loss: 0.1717572 Test Loss: 0.1591127
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2921471 Vali Loss: 0.1689592 Test Loss: 0.1583180
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2916723 Vali Loss: 0.1718049 Test Loss: 0.1583359
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2905644 Vali Loss: 0.1717685 Test Loss: 0.1582178
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2926237 Vali Loss: 0.1700813 Test Loss: 0.1581652
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2901372 Vali Loss: 0.1673265 Test Loss: 0.1581523
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2915828 Vali Loss: 0.1641245 Test Loss: 0.1581463
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2923288 Vali Loss: 0.1654496 Test Loss: 0.1581435
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2908252 Vali Loss: 0.1669550 Test Loss: 0.1581482
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2920923 Vali Loss: 0.1705045 Test Loss: 0.1581454
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_ABSA_H3_FEDformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 238
test shape: (238, 3, 1) (238, 3, 1)
test shape: (238, 3, 1) (238, 3, 1)


	mse:0.0004905064706690609, mae:0.016757197678089142, rmse:0.02214737981557846, r2:-0.07718551158905029, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0168, RMSE: 0.0221, RÂ²: -0.0772, MAPE: 1.47%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.499 MB of 0.499 MB uploadedwandb: \ 0.499 MB of 0.499 MB uploadedwandb: | 0.499 MB of 0.499 MB uploadedwandb: / 0.499 MB of 0.499 MB uploadedwandb: - 0.499 MB of 0.499 MB uploadedwandb: \ 0.499 MB of 0.499 MB uploadedwandb: | 0.499 MB of 0.499 MB uploadedwandb: / 0.499 MB of 0.499 MB uploadedwandb: - 0.627 MB of 0.839 MB uploaded (0.002 MB deduped)wandb: \ 0.627 MB of 0.839 MB uploaded (0.002 MB deduped)wandb: | 0.839 MB of 0.839 MB uploaded (0.002 MB deduped)wandb: / 0.839 MB of 0.839 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–â–‚â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ˆâ–…â–â–ˆâ–†â–ˆâ–ˆâ–‡â–„â–â–ƒâ–„â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 14594054
wandb:     model/trainable_params 14594054
wandb:                  test/loss 0.15815
wandb:                 train/loss 0.29209
wandb:   val/directional_accuracy 52.52101
wandb:                   val/loss 0.1705
wandb:                    val/mae 0.01676
wandb:                   val/mape 146.76545
wandb:                    val/mse 0.00049
wandb:                     val/r2 -0.07719
wandb:                   val/rmse 0.02215
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/xbwqt67w
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_060709-xbwqt67w/logs
Completed: ABSA H=3

Training: FEDformer on ABSA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_061343-5dqljr48
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/5dqljr48
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_ABSA_H5   Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
 fourier enhanced cross attention used!
modes_q=17, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/5dqljr48
>>>>>>>start training : long_term_forecast_FEDformer_ABSA_H5_FEDformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 236
Epoch: 1, Steps: 133 | Train Loss: 0.3876514 Vali Loss: 0.1878874 Test Loss: 0.1777235
Validation loss decreased (inf --> 0.187887).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3876513811878692, 'val/loss': 0.18788743950426579, 'test/loss': 0.1777234598994255, '_timestamp': 1762316054.5169713}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.32662021822499154, 'val/loss': 0.1866315770894289, 'test/loss': 0.16873698309063911, '_timestamp': 1762316077.0158997}).
Epoch: 2, Steps: 133 | Train Loss: 0.3266202 Vali Loss: 0.1866316 Test Loss: 0.1687370
Validation loss decreased (0.187887 --> 0.186632).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.3091627 Vali Loss: 0.1683971 Test Loss: 0.1627477
Validation loss decreased (0.186632 --> 0.168397).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.3017267 Vali Loss: 0.1732022 Test Loss: 0.1617197
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2990149 Vali Loss: 0.1698350 Test Loss: 0.1631374
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2970562 Vali Loss: 0.1689138 Test Loss: 0.1637889
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2974866 Vali Loss: 0.1676110 Test Loss: 0.1637641
Validation loss decreased (0.168397 --> 0.167611).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2953050 Vali Loss: 0.1725814 Test Loss: 0.1634907
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2958541 Vali Loss: 0.1717927 Test Loss: 0.1632965
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2946027 Vali Loss: 0.1753897 Test Loss: 0.1633036
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2952819 Vali Loss: 0.1731659 Test Loss: 0.1632617
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2948781 Vali Loss: 0.1704320 Test Loss: 0.1632463
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2964086 Vali Loss: 0.1804330 Test Loss: 0.1632430
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2948271 Vali Loss: 0.1715465 Test Loss: 0.1632446
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2947266 Vali Loss: 0.1690052 Test Loss: 0.1632428
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2950579 Vali Loss: 0.1674378 Test Loss: 0.1632433
Validation loss decreased (0.167611 --> 0.167438).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2950791 Vali Loss: 0.1766176 Test Loss: 0.1632430
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2955883 Vali Loss: 0.1647821 Test Loss: 0.1632429
Validation loss decreased (0.167438 --> 0.164782).  Saving model ...
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2952706 Vali Loss: 0.1659529 Test Loss: 0.1632429
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2953737 Vali Loss: 0.1752337 Test Loss: 0.1632431
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2940289 Vali Loss: 0.1740526 Test Loss: 0.1632431
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2967172 Vali Loss: 0.1694791 Test Loss: 0.1632431
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2944859 Vali Loss: 0.1690723 Test Loss: 0.1632431
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2964850 Vali Loss: 0.1681786 Test Loss: 0.1632432
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2943245 Vali Loss: 0.1700591 Test Loss: 0.1632431
EarlyStopping counter: 7 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2949695 Vali Loss: 0.1699415 Test Loss: 0.1632431
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.2949118 Vali Loss: 0.1740737 Test Loss: 0.1632431
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.2958705 Vali Loss: 0.1666666 Test Loss: 0.1632431
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_ABSA_H5_FEDformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
test shape: (236, 5, 1) (236, 5, 1)
test shape: (236, 5, 1) (236, 5, 1)


	mse:0.0004915144527330995, mae:0.016696643084287643, rmse:0.02217012457549572, r2:-0.07312321662902832, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0167, RMSE: 0.0222, RÂ²: -0.0731, MAPE: 1.58%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.522 MB of 0.523 MB uploadedwandb: \ 0.523 MB of 0.523 MB uploadedwandb: | 0.523 MB of 0.523 MB uploadedwandb: / 0.523 MB of 0.736 MB uploadedwandb: - 0.736 MB of 0.736 MB uploadedwandb: \ 0.736 MB of 0.736 MB uploadedwandb: | 0.736 MB of 0.736 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–„â–â–†â–ˆâ–ˆâ–‡â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–ƒâ–‚â–‚â–â–‚â–â–‚â–â–â–â–â–‚â–‚â–‚â–â–‚â–â–‚â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–…â–ƒâ–ƒâ–‚â–„â–„â–†â–…â–„â–ˆâ–„â–ƒâ–‚â–†â–â–‚â–†â–…â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–…â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 27
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 14725126
wandb:     model/trainable_params 14725126
wandb:                  test/loss 0.16324
wandb:                 train/loss 0.29587
wandb:   val/directional_accuracy 52.01271
wandb:                   val/loss 0.16667
wandb:                    val/mae 0.0167
wandb:                   val/mape 157.85902
wandb:                    val/mse 0.00049
wandb:                     val/r2 -0.07312
wandb:                   val/rmse 0.02217
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/5dqljr48
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_061343-5dqljr48/logs
Completed: ABSA H=5

Training: FEDformer on ABSA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_062443-p7zjdqpq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/p7zjdqpq
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_ABSA_H10  Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
 fourier enhanced cross attention used!
modes_q=20, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/p7zjdqpq
>>>>>>>start training : long_term_forecast_FEDformer_ABSA_H10_FEDformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 231
Epoch: 1, Steps: 133 | Train Loss: 0.3799148 Vali Loss: 0.1745713 Test Loss: 0.1691296
Validation loss decreased (inf --> 0.174571).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.37991479767444436, 'val/loss': 0.1745713073760271, 'test/loss': 0.16912960354238749, '_timestamp': 1762316715.2973638}).
Epoch: 2, Steps: 133 | Train Loss: 0.3217893 Vali Loss: 0.1796960 Test Loss: 0.1680944
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.32178932921330733, 'val/loss': 0.17969603091478348, 'test/loss': 0.1680944450199604, '_timestamp': 1762316739.3495157}).
Epoch: 3, Steps: 133 | Train Loss: 0.3103232 Vali Loss: 0.1818854 Test Loss: 0.1638321
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.3053781 Vali Loss: 0.1784828 Test Loss: 0.1633017
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.3005583 Vali Loss: 0.1756598 Test Loss: 0.1625930
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2997104 Vali Loss: 0.1755492 Test Loss: 0.1627454
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.3011044 Vali Loss: 0.1802104 Test Loss: 0.1625760
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2988524 Vali Loss: 0.1790388 Test Loss: 0.1627065
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.3003349 Vali Loss: 0.1785835 Test Loss: 0.1625793
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2982941 Vali Loss: 0.1748197 Test Loss: 0.1626134
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2988713 Vali Loss: 0.1776159 Test Loss: 0.1626353
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_ABSA_H10_FEDformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 231
test shape: (231, 10, 1) (231, 10, 1)
test shape: (231, 10, 1) (231, 10, 1)


	mse:0.0005226287757977843, mae:0.017466608434915543, rmse:0.022861074656248093, r2:-0.13202500343322754, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0175, RMSE: 0.0229, RÂ²: -0.1320, MAPE: 2.07%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.559 MB of 0.560 MB uploadedwandb: \ 0.559 MB of 0.560 MB uploadedwandb: | 0.559 MB of 0.560 MB uploadedwandb: / 0.560 MB of 0.560 MB uploadedwandb: - 0.560 MB of 0.771 MB uploadedwandb: \ 0.560 MB of 0.771 MB uploadedwandb: | 0.771 MB of 0.771 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–â–‚â–â–‚â–â–â–
wandb:                 train/loss â–ˆâ–…â–‚â–‚â–ƒâ–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–‚â–‚â–†â–…â–…â–â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 15118342
wandb:     model/trainable_params 15118342
wandb:                  test/loss 0.16264
wandb:                 train/loss 0.29887
wandb:   val/directional_accuracy 51.94805
wandb:                   val/loss 0.17762
wandb:                    val/mae 0.01747
wandb:                   val/mape 207.49073
wandb:                    val/mse 0.00052
wandb:                     val/r2 -0.13203
wandb:                   val/rmse 0.02286
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/p7zjdqpq
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_062443-p7zjdqpq/logs
Completed: ABSA H=10

Training: FEDformer on ABSA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_062940-qmxbsedt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/qmxbsedt
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_ABSA_H22  Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
 fourier enhanced cross attention used!
modes_q=26, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/qmxbsedt
>>>>>>>start training : long_term_forecast_FEDformer_ABSA_H22_FEDformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 219
Epoch: 1, Steps: 132 | Train Loss: 0.3771516 Vali Loss: 0.1831838 Test Loss: 0.1601555
Validation loss decreased (inf --> 0.183184).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.37715156748890877, 'val/loss': 0.1831837977681841, 'test/loss': 0.16015546662466867, '_timestamp': 1762317012.8300478}).
Epoch: 2, Steps: 132 | Train Loss: 0.3295380 Vali Loss: 0.1861915 Test Loss: 0.1698986
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.32953802010778227, 'val/loss': 0.18619149710450852, 'test/loss': 0.16989864834717341, '_timestamp': 1762317037.2013237}).
Epoch: 3, Steps: 132 | Train Loss: 0.3209046 Vali Loss: 0.1802811 Test Loss: 0.1584140
Validation loss decreased (0.183184 --> 0.180281).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.3161559 Vali Loss: 0.1803776 Test Loss: 0.1585950
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.3139923 Vali Loss: 0.1829770 Test Loss: 0.1580549
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.3133084 Vali Loss: 0.1827610 Test Loss: 0.1582015
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.3127827 Vali Loss: 0.1827958 Test Loss: 0.1575486
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.3120157 Vali Loss: 0.1828537 Test Loss: 0.1576006
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.3119666 Vali Loss: 0.1830987 Test Loss: 0.1574674
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.3120000 Vali Loss: 0.1830859 Test Loss: 0.1575194
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.3118269 Vali Loss: 0.1828647 Test Loss: 0.1575263
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.3120021 Vali Loss: 0.1830749 Test Loss: 0.1575124
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.3118482 Vali Loss: 0.1826943 Test Loss: 0.1575118
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_ABSA_H22_FEDformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 219
test shape: (219, 22, 1) (219, 22, 1)
test shape: (219, 22, 1) (219, 22, 1)


	mse:0.0004902617656625807, mae:0.016710299998521805, rmse:0.02214185521006584, r2:-0.04640209674835205, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0167, RMSE: 0.0221, RÂ²: -0.0464, MAPE: 1.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.639 MB of 0.640 MB uploadedwandb: \ 0.639 MB of 0.640 MB uploadedwandb: | 0.639 MB of 0.640 MB uploadedwandb: / 0.639 MB of 0.640 MB uploadedwandb: - 0.640 MB of 0.640 MB uploadedwandb: \ 0.640 MB of 0.640 MB uploadedwandb: | 0.640 MB of 0.851 MB uploadedwandb: / 0.851 MB of 0.851 MB uploadedwandb: - 0.851 MB of 0.851 MB uploadedwandb: \ 0.851 MB of 0.851 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‡â–ˆâ–…â–†â–‚â–‚â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 15904774
wandb:     model/trainable_params 15904774
wandb:                  test/loss 0.15751
wandb:                 train/loss 0.31185
wandb:   val/directional_accuracy 50.29354
wandb:                   val/loss 0.18269
wandb:                    val/mae 0.01671
wandb:                   val/mape 174.63901
wandb:                    val/mse 0.00049
wandb:                     val/r2 -0.0464
wandb:                   val/rmse 0.02214
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/qmxbsedt
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_062940-qmxbsedt/logs
Completed: ABSA H=22

Training: FEDformer on ABSA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_063538-twylmha0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/twylmha0
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_ABSA_H50  Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 39]
 fourier enhanced cross attention used!
modes_q=32, index_q=[0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 21, 22, 24, 26, 28, 29, 30, 33, 34, 35, 36, 37, 38, 39]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/twylmha0
>>>>>>>start training : long_term_forecast_FEDformer_ABSA_H50_FEDformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 191
Epoch: 1, Steps: 132 | Train Loss: 0.3870797 Vali Loss: 0.1930486 Test Loss: 0.1582149
Validation loss decreased (inf --> 0.193049).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.38707969145792903, 'val/loss': 0.19304859141508737, 'test/loss': 0.15821490188439688, '_timestamp': 1762317372.6335487}).
Epoch: 2, Steps: 132 | Train Loss: 0.3490784 Vali Loss: 0.1833591 Test Loss: 0.1592521
Validation loss decreased (0.193049 --> 0.183359).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3490784082448844, 'val/loss': 0.18335913866758347, 'test/loss': 0.1592520959675312, '_timestamp': 1762317399.1738229}).
Epoch: 3, Steps: 132 | Train Loss: 0.3424041 Vali Loss: 0.1842259 Test Loss: 0.1585754
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.3395447 Vali Loss: 0.1819489 Test Loss: 0.1570932
Validation loss decreased (0.183359 --> 0.181949).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.3373496 Vali Loss: 0.1845344 Test Loss: 0.1565947
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.3351689 Vali Loss: 0.1833145 Test Loss: 0.1564816
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.3347397 Vali Loss: 0.1830154 Test Loss: 0.1569267
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.3356245 Vali Loss: 0.1828770 Test Loss: 0.1566610
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.3355017 Vali Loss: 0.1828846 Test Loss: 0.1568349
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.3349331 Vali Loss: 0.1829236 Test Loss: 0.1568412
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.3347291 Vali Loss: 0.1827761 Test Loss: 0.1568493
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.3353217 Vali Loss: 0.1828130 Test Loss: 0.1568727
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.3352991 Vali Loss: 0.1827823 Test Loss: 0.1568723
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.3399966 Vali Loss: 0.1827591 Test Loss: 0.1568679
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_ABSA_H50_FEDformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 191
test shape: (191, 50, 1) (191, 50, 1)
test shape: (191, 50, 1) (191, 50, 1)


	mse:0.0005094714579172432, mae:0.017036983743309975, rmse:0.022571474313735962, r2:-0.04723942279815674, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0170, RMSE: 0.0226, RÂ²: -0.0472, MAPE: 1.39%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.731 MB of 0.733 MB uploadedwandb: \ 0.731 MB of 0.733 MB uploadedwandb: | 0.733 MB of 0.733 MB uploadedwandb: / 0.733 MB of 0.733 MB uploadedwandb: - 0.733 MB of 0.945 MB uploadedwandb: \ 0.876 MB of 0.945 MB uploadedwandb: | 0.945 MB of 0.945 MB uploadedwandb: / 0.945 MB of 0.945 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–â–â–‚â–‚â–â–â–‚â–‚â–†
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–â–ˆâ–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 13
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 16691206
wandb:     model/trainable_params 16691206
wandb:                  test/loss 0.15687
wandb:                 train/loss 0.34
wandb:   val/directional_accuracy 48.88343
wandb:                   val/loss 0.18276
wandb:                    val/mae 0.01704
wandb:                   val/mape 139.37955
wandb:                    val/mse 0.00051
wandb:                     val/r2 -0.04724
wandb:                   val/rmse 0.02257
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/twylmha0
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_063538-twylmha0/logs
Completed: ABSA H=50

Training: FEDformer on ABSA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_064216-2jm7o5zp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/2jm7o5zp
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_ABSA_H100 Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 8, 13, 16, 20, 22, 23, 24, 28, 29, 30, 31, 32, 34, 35, 37, 39, 43, 44, 46, 51, 52, 55, 56, 57, 62, 63, 64]
 fourier enhanced cross attention used!
modes_q=32, index_q=[0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 17, 18, 20, 21, 22, 23, 28, 30, 34, 36, 44, 45, 50, 53, 54, 55, 56, 57, 61, 63]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/2jm7o5zp
>>>>>>>start training : long_term_forecast_FEDformer_ABSA_H100_FEDformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 141
Epoch: 1, Steps: 130 | Train Loss: 0.4227879 Vali Loss: 0.1935149 Test Loss: 0.1643771
Validation loss decreased (inf --> 0.193515).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.42278793224921596, 'val/loss': 0.19351494908332825, 'test/loss': 0.1643770843744278, '_timestamp': 1762317769.6452854}).
Epoch: 2, Steps: 130 | Train Loss: 0.3832995 Vali Loss: 0.1942061 Test Loss: 0.1645652
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3832995341374324, 'val/loss': 0.19420612454414368, 'test/loss': 0.16456523686647415, '_timestamp': 1762317795.141885}).
Epoch: 3, Steps: 130 | Train Loss: 0.3768354 Vali Loss: 0.1904706 Test Loss: 0.1674538
Validation loss decreased (0.193515 --> 0.190471).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.3753969 Vali Loss: 0.1910721 Test Loss: 0.1654032
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.3726393 Vali Loss: 0.1883200 Test Loss: 0.1660664
Validation loss decreased (0.190471 --> 0.188320).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.3711975 Vali Loss: 0.1890967 Test Loss: 0.1658420
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.3706369 Vali Loss: 0.1892587 Test Loss: 0.1655546
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.3709668 Vali Loss: 0.1885882 Test Loss: 0.1659853
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.3707422 Vali Loss: 0.1890375 Test Loss: 0.1657586
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.3701248 Vali Loss: 0.1886068 Test Loss: 0.1657251
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.3703855 Vali Loss: 0.1886676 Test Loss: 0.1657054
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.3711732 Vali Loss: 0.1896152 Test Loss: 0.1657073
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.3721503 Vali Loss: 0.1888369 Test Loss: 0.1657139
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.3708494 Vali Loss: 0.1891912 Test Loss: 0.1657066
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.3702873 Vali Loss: 0.1893262 Test Loss: 0.1657099
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_ABSA_H100_FEDformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 141
test shape: (141, 100, 1) (141, 100, 1)
test shape: (141, 100, 1) (141, 100, 1)


	mse:0.0005350424908101559, mae:0.017556922510266304, rmse:0.023130984976887703, r2:-0.03683614730834961, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0176, RMSE: 0.0231, RÂ²: -0.0368, MAPE: 1.24%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.846 MB of 0.851 MB uploadedwandb: \ 0.851 MB of 0.851 MB uploadedwandb: | 0.851 MB of 0.851 MB uploadedwandb: / 0.851 MB of 1.063 MB uploadedwandb: - 1.063 MB of 1.063 MB uploadedwandb: \ 1.063 MB of 1.063 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–†â–„â–‚â–‚â–‚â–‚â–â–â–‚â–ƒâ–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–ˆâ–â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–„â–‚â–ƒâ–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 16691206
wandb:     model/trainable_params 16691206
wandb:                  test/loss 0.16571
wandb:                 train/loss 0.37029
wandb:   val/directional_accuracy 47.38162
wandb:                   val/loss 0.18933
wandb:                    val/mae 0.01756
wandb:                   val/mape 123.703
wandb:                    val/mse 0.00054
wandb:                     val/r2 -0.03684
wandb:                   val/rmse 0.02313
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/2jm7o5zp
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_064216-2jm7o5zp/logs
Completed: ABSA H=100

Training: FEDformer on SASOL for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_064913-lylphaay
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/lylphaay
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_SASOL_H3  Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
 fourier enhanced cross attention used!
modes_q=16, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/lylphaay
>>>>>>>start training : long_term_forecast_FEDformer_SASOL_H3_FEDformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 3
============================================================
train 3772
val 211
test 212
Epoch: 1, Steps: 118 | Train Loss: 0.3215329 Vali Loss: 0.1118124 Test Loss: 0.1576192
Validation loss decreased (inf --> 0.111812).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3215329073748346, 'val/loss': 0.11181242444685527, 'test/loss': 0.1576191559433937, '_timestamp': 1762318180.5771275}).
Epoch: 2, Steps: 118 | Train Loss: 0.2561249 Vali Loss: 0.1128046 Test Loss: 0.1606083
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25612494829347576, 'val/loss': 0.11280461187873568, 'test/loss': 0.1606082565018109, '_timestamp': 1762318200.3623133}).
Epoch: 3, Steps: 118 | Train Loss: 0.2378032 Vali Loss: 0.1029103 Test Loss: 0.1575154
Validation loss decreased (0.111812 --> 0.102910).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2301191 Vali Loss: 0.1034200 Test Loss: 0.1516277
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2259404 Vali Loss: 0.1044866 Test Loss: 0.1504726
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2261797 Vali Loss: 0.1038706 Test Loss: 0.1507738
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2238152 Vali Loss: 0.1019806 Test Loss: 0.1501165
Validation loss decreased (0.102910 --> 0.101981).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2238209 Vali Loss: 0.1026887 Test Loss: 0.1500775
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2235964 Vali Loss: 0.1011473 Test Loss: 0.1501462
Validation loss decreased (0.101981 --> 0.101147).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2239149 Vali Loss: 0.1039742 Test Loss: 0.1500528
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2239924 Vali Loss: 0.1029608 Test Loss: 0.1500661
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2238462 Vali Loss: 0.1033824 Test Loss: 0.1500735
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2231147 Vali Loss: 0.1043712 Test Loss: 0.1500723
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2233561 Vali Loss: 0.1042275 Test Loss: 0.1500771
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.2242280 Vali Loss: 0.1047076 Test Loss: 0.1500742
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.2228710 Vali Loss: 0.1035123 Test Loss: 0.1500728
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.2230666 Vali Loss: 0.1049261 Test Loss: 0.1500729
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.2228272 Vali Loss: 0.1008679 Test Loss: 0.1500732
Validation loss decreased (0.101147 --> 0.100868).  Saving model ...
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.2244120 Vali Loss: 0.1017112 Test Loss: 0.1500733
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.2244514 Vali Loss: 0.1057122 Test Loss: 0.1500732
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 118 | Train Loss: 0.2235668 Vali Loss: 0.1012551 Test Loss: 0.1500733
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 118 | Train Loss: 0.2222731 Vali Loss: 0.1040197 Test Loss: 0.1500733
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 118 | Train Loss: 0.2227080 Vali Loss: 0.1003215 Test Loss: 0.1500732
Validation loss decreased (0.100868 --> 0.100322).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 118 | Train Loss: 0.2229378 Vali Loss: 0.1035229 Test Loss: 0.1500733
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 118 | Train Loss: 0.2236777 Vali Loss: 0.1005295 Test Loss: 0.1500732
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 118 | Train Loss: 0.2237841 Vali Loss: 0.1050399 Test Loss: 0.1500732
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 118 | Train Loss: 0.2233506 Vali Loss: 0.1054435 Test Loss: 0.1500732
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 118 | Train Loss: 0.2233582 Vali Loss: 0.1032217 Test Loss: 0.1500732
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 118 | Train Loss: 0.2226903 Vali Loss: 0.1014937 Test Loss: 0.1500732
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 118 | Train Loss: 0.2241963 Vali Loss: 0.1016964 Test Loss: 0.1500732
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 118 | Train Loss: 0.2241432 Vali Loss: 0.1022126 Test Loss: 0.1500732
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 118 | Train Loss: 0.2228610 Vali Loss: 0.1025619 Test Loss: 0.1500732
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 118 | Train Loss: 0.2237546 Vali Loss: 0.1059250 Test Loss: 0.1500732
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_SASOL_H3_FEDformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 212
test shape: (212, 3, 1) (212, 3, 1)
test shape: (212, 3, 1) (212, 3, 1)


	mse:0.0022734205704182386, mae:0.03536207973957062, rmse:0.04768040031194687, r2:-0.03196394443511963, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0354, RMSE: 0.0477, RÂ²: -0.0320, MAPE: 8682704.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.455 MB of 0.455 MB uploadedwandb: \ 0.455 MB of 0.455 MB uploadedwandb: | 0.455 MB of 0.455 MB uploadedwandb: / 0.455 MB of 0.455 MB uploadedwandb: - 0.455 MB of 0.455 MB uploadedwandb: \ 0.455 MB of 0.455 MB uploadedwandb: | 0.455 MB of 0.455 MB uploadedwandb: / 0.455 MB of 0.455 MB uploadedwandb: - 0.583 MB of 0.798 MB uploaded (0.002 MB deduped)wandb: \ 0.798 MB of 0.798 MB uploaded (0.002 MB deduped)wandb: | 0.798 MB of 0.798 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–‚â–‚â–‚â–â–â–â–‚â–‚â–â–â–â–‚â–‚â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–…â–†â–…â–ƒâ–„â–‚â–†â–„â–…â–†â–†â–†â–…â–‡â–‚â–ƒâ–ˆâ–‚â–†â–â–…â–â–‡â–‡â–…â–‚â–ƒâ–ƒâ–„â–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 32
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 14594054
wandb:     model/trainable_params 14594054
wandb:                  test/loss 0.15007
wandb:                 train/loss 0.22375
wandb:   val/directional_accuracy 49.29245
wandb:                   val/loss 0.10593
wandb:                    val/mae 0.03536
wandb:                   val/mape 868270400.0
wandb:                    val/mse 0.00227
wandb:                     val/r2 -0.03196
wandb:                   val/rmse 0.04768
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/lylphaay
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_064913-lylphaay/logs
Completed: SASOL H=3

Training: FEDformer on SASOL for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_070035-y7zuec61
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/y7zuec61
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_SASOL_H5  Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
 fourier enhanced cross attention used!
modes_q=17, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/y7zuec61
>>>>>>>start training : long_term_forecast_FEDformer_SASOL_H5_FEDformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 5
============================================================
train 3770
val 209
test 210
Epoch: 1, Steps: 118 | Train Loss: 0.3182530 Vali Loss: 0.1152206 Test Loss: 0.1743510
Validation loss decreased (inf --> 0.115221).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.31825300129288336, 'val/loss': 0.11522058823278972, 'test/loss': 0.17435102590492793, '_timestamp': 1762318863.7733545}).
Epoch: 2, Steps: 118 | Train Loss: 0.2525720 Vali Loss: 0.1054957 Test Loss: 0.1633672
Validation loss decreased (0.115221 --> 0.105496).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25257199926901674, 'val/loss': 0.10549567320517131, 'test/loss': 0.16336716072899954, '_timestamp': 1762318883.8358088}).
Epoch: 3, Steps: 118 | Train Loss: 0.2365322 Vali Loss: 0.1113553 Test Loss: 0.1554935
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2298507 Vali Loss: 0.1115301 Test Loss: 0.1556129
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2258307 Vali Loss: 0.1047389 Test Loss: 0.1540708
Validation loss decreased (0.105496 --> 0.104739).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2236132 Vali Loss: 0.1038073 Test Loss: 0.1524811
Validation loss decreased (0.104739 --> 0.103807).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2243599 Vali Loss: 0.1051448 Test Loss: 0.1523718
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2234398 Vali Loss: 0.1033151 Test Loss: 0.1519851
Validation loss decreased (0.103807 --> 0.103315).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2232180 Vali Loss: 0.1088844 Test Loss: 0.1518458
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2222888 Vali Loss: 0.1053646 Test Loss: 0.1518833
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2228261 Vali Loss: 0.1058419 Test Loss: 0.1518876
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2223683 Vali Loss: 0.1078012 Test Loss: 0.1518902
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2223119 Vali Loss: 0.1049591 Test Loss: 0.1518738
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2229470 Vali Loss: 0.1078521 Test Loss: 0.1518722
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.2223589 Vali Loss: 0.1036821 Test Loss: 0.1518694
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.2224187 Vali Loss: 0.1085708 Test Loss: 0.1518686
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.2225428 Vali Loss: 0.1054725 Test Loss: 0.1518682
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.2231599 Vali Loss: 0.1040433 Test Loss: 0.1518682
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_SASOL_H5_FEDformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 210
test shape: (210, 5, 1) (210, 5, 1)
test shape: (210, 5, 1) (210, 5, 1)


	mse:0.0022890318650752306, mae:0.035428356379270554, rmse:0.04784382879734039, r2:-0.031374216079711914, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0354, RMSE: 0.0478, RÂ²: -0.0314, MAPE: 7980252.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.495 MB of 0.496 MB uploadedwandb: \ 0.495 MB of 0.496 MB uploadedwandb: | 0.495 MB of 0.496 MB uploadedwandb: / 0.495 MB of 0.496 MB uploadedwandb: - 0.496 MB of 0.496 MB uploadedwandb: \ 0.496 MB of 0.496 MB uploadedwandb: | 0.496 MB of 0.707 MB uploadedwandb: / 0.496 MB of 0.707 MB uploadedwandb: - 0.707 MB of 0.707 MB uploadedwandb: \ 0.707 MB of 0.707 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ˆâ–…â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ˆâ–‚â–â–ƒâ–â–†â–ƒâ–ƒâ–…â–‚â–…â–â–…â–ƒâ–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 17
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 14725126
wandb:     model/trainable_params 14725126
wandb:                  test/loss 0.15187
wandb:                 train/loss 0.22316
wandb:   val/directional_accuracy 46.54762
wandb:                   val/loss 0.10404
wandb:                    val/mae 0.03543
wandb:                   val/mape 798025200.0
wandb:                    val/mse 0.00229
wandb:                     val/r2 -0.03137
wandb:                   val/rmse 0.04784
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/y7zuec61
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_070035-y7zuec61/logs
Completed: SASOL H=5

Training: FEDformer on SASOL for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_070714-84l3f8sk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/84l3f8sk
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_SASOL_H10 Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
 fourier enhanced cross attention used!
modes_q=20, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/84l3f8sk
>>>>>>>start training : long_term_forecast_FEDformer_SASOL_H10_FEDformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 10
============================================================
train 3765
val 204
test 205
Epoch: 1, Steps: 118 | Train Loss: 0.3131852 Vali Loss: 0.1070014 Test Loss: 0.1643263
Validation loss decreased (inf --> 0.107001).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3131852407576674, 'val/loss': 0.10700144193002156, 'test/loss': 0.16432633144514902, '_timestamp': 1762319264.4691298}).
Epoch: 2, Steps: 118 | Train Loss: 0.2535112 Vali Loss: 0.1138519 Test Loss: 0.1634535
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25351117248252286, 'val/loss': 0.11385189103228706, 'test/loss': 0.16345347889832088, '_timestamp': 1762319285.8287888}).
Epoch: 3, Steps: 118 | Train Loss: 0.2397142 Vali Loss: 0.1092422 Test Loss: 0.1541275
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2326198 Vali Loss: 0.1070125 Test Loss: 0.1535088
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2299749 Vali Loss: 0.1072556 Test Loss: 0.1542486
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2282094 Vali Loss: 0.1111616 Test Loss: 0.1527556
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2276704 Vali Loss: 0.1054262 Test Loss: 0.1531414
Validation loss decreased (0.107001 --> 0.105426).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2271096 Vali Loss: 0.1060847 Test Loss: 0.1530518
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2264561 Vali Loss: 0.1090269 Test Loss: 0.1528657
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2268504 Vali Loss: 0.1087801 Test Loss: 0.1529786
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2261439 Vali Loss: 0.1058264 Test Loss: 0.1529314
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2262850 Vali Loss: 0.1055760 Test Loss: 0.1529240
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2270169 Vali Loss: 0.1096791 Test Loss: 0.1529219
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2265227 Vali Loss: 0.1060368 Test Loss: 0.1529200
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.2263861 Vali Loss: 0.1093143 Test Loss: 0.1529201
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.2264761 Vali Loss: 0.1061120 Test Loss: 0.1529198
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.2262162 Vali Loss: 0.1062038 Test Loss: 0.1529198
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_SASOL_H10_FEDformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 205
test shape: (205, 10, 1) (205, 10, 1)
test shape: (205, 10, 1) (205, 10, 1)


	mse:0.002306441543623805, mae:0.03534647822380066, rmse:0.048025425523519516, r2:-0.03907763957977295, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0353, RMSE: 0.0480, RÂ²: -0.0391, MAPE: 7966569.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.542 MB of 0.543 MB uploadedwandb: \ 0.542 MB of 0.543 MB uploadedwandb: | 0.543 MB of 0.543 MB uploadedwandb: / 0.543 MB of 0.754 MB uploadedwandb: - 0.754 MB of 0.754 MB uploadedwandb: \ 0.754 MB of 0.754 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‡â–…â–ˆâ–â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–ƒâ–ƒâ–ˆâ–â–‚â–…â–…â–â–â–†â–‚â–†â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 15118342
wandb:     model/trainable_params 15118342
wandb:                  test/loss 0.15292
wandb:                 train/loss 0.22622
wandb:   val/directional_accuracy 47.91328
wandb:                   val/loss 0.1062
wandb:                    val/mae 0.03535
wandb:                   val/mape 796656950.0
wandb:                    val/mse 0.00231
wandb:                     val/r2 -0.03908
wandb:                   val/rmse 0.04803
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/84l3f8sk
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_070714-84l3f8sk/logs
Completed: SASOL H=10

Training: FEDformer on SASOL for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_071346-gm2ke2ja
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/gm2ke2ja
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_SASOL_H22 Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
 fourier enhanced cross attention used!
modes_q=26, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/gm2ke2ja
>>>>>>>start training : long_term_forecast_FEDformer_SASOL_H22_FEDformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 22
============================================================
train 3753
val 192
test 193
Epoch: 1, Steps: 118 | Train Loss: 0.3087892 Vali Loss: 0.1121079 Test Loss: 0.1629568
Validation loss decreased (inf --> 0.112108).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3087892183812998, 'val/loss': 0.11210789779822032, 'test/loss': 0.1629567550761359, '_timestamp': 1762319655.8942697}).
Epoch: 2, Steps: 118 | Train Loss: 0.2610341 Vali Loss: 0.1144920 Test Loss: 0.1610757
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26103408293703856, 'val/loss': 0.11449203516046207, 'test/loss': 0.1610757133790425, '_timestamp': 1762319678.4113}).
Epoch: 3, Steps: 118 | Train Loss: 0.2548966 Vali Loss: 0.1097647 Test Loss: 0.1602350
Validation loss decreased (0.112108 --> 0.109765).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2479702 Vali Loss: 0.1096017 Test Loss: 0.1572854
Validation loss decreased (0.109765 --> 0.109602).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2450950 Vali Loss: 0.1102307 Test Loss: 0.1559472
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2420693 Vali Loss: 0.1105278 Test Loss: 0.1567881
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2413400 Vali Loss: 0.1089693 Test Loss: 0.1557894
Validation loss decreased (0.109602 --> 0.108969).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2414749 Vali Loss: 0.1090007 Test Loss: 0.1559981
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2414797 Vali Loss: 0.1092681 Test Loss: 0.1560290
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2406387 Vali Loss: 0.1091869 Test Loss: 0.1560393
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2408403 Vali Loss: 0.1091764 Test Loss: 0.1560284
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2407059 Vali Loss: 0.1091715 Test Loss: 0.1560337
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2406075 Vali Loss: 0.1091889 Test Loss: 0.1560274
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2398343 Vali Loss: 0.1091925 Test Loss: 0.1560317
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.2409893 Vali Loss: 0.1091892 Test Loss: 0.1560305
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.2405397 Vali Loss: 0.1091908 Test Loss: 0.1560291
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.2407877 Vali Loss: 0.1091914 Test Loss: 0.1560288
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_SASOL_H22_FEDformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 193
test shape: (193, 22, 1) (193, 22, 1)
test shape: (193, 22, 1) (193, 22, 1)


	mse:0.0023182604927569628, mae:0.0351884551346302, rmse:0.048148319125175476, r2:-0.03303396701812744, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0352, RMSE: 0.0481, RÂ²: -0.0330, MAPE: 7555906.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.598 MB of 0.599 MB uploadedwandb: \ 0.598 MB of 0.599 MB uploadedwandb: | 0.598 MB of 0.599 MB uploadedwandb: / 0.598 MB of 0.599 MB uploadedwandb: - 0.599 MB of 0.599 MB uploadedwandb: \ 0.599 MB of 0.810 MB uploadedwandb: | 0.810 MB of 0.810 MB uploadedwandb: / 0.810 MB of 0.810 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–ƒâ–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–„â–‡â–ˆâ–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 15904774
wandb:     model/trainable_params 15904774
wandb:                  test/loss 0.15603
wandb:                 train/loss 0.24079
wandb:   val/directional_accuracy 47.37232
wandb:                   val/loss 0.10919
wandb:                    val/mae 0.03519
wandb:                   val/mape 755590650.0
wandb:                    val/mse 0.00232
wandb:                     val/r2 -0.03303
wandb:                   val/rmse 0.04815
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/gm2ke2ja
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_071346-gm2ke2ja/logs
Completed: SASOL H=22

Training: FEDformer on SASOL for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_072038-72uik6vk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/72uik6vk
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_SASOL_H50 Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 39]
 fourier enhanced cross attention used!
modes_q=32, index_q=[0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 21, 22, 24, 26, 28, 29, 30, 33, 34, 35, 36, 37, 38, 39]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/72uik6vk
>>>>>>>start training : long_term_forecast_FEDformer_SASOL_H50_FEDformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 50
============================================================
train 3725
val 164
test 165
Epoch: 1, Steps: 117 | Train Loss: 0.3332971 Vali Loss: 0.1074489 Test Loss: 0.1827292
Validation loss decreased (inf --> 0.107449).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3332971096293539, 'val/loss': 0.10744891812403996, 'test/loss': 0.18272919952869415, '_timestamp': 1762320068.7268555}).
Epoch: 2, Steps: 117 | Train Loss: 0.2933133 Vali Loss: 0.1049473 Test Loss: 0.1809961
Validation loss decreased (0.107449 --> 0.104947).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.29331328675278234, 'val/loss': 0.10494731490810712, 'test/loss': 0.1809961497783661, '_timestamp': 1762320092.0547302}).
Epoch: 3, Steps: 117 | Train Loss: 0.2874824 Vali Loss: 0.1078692 Test Loss: 0.1672136
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 117 | Train Loss: 0.2835893 Vali Loss: 0.1101990 Test Loss: 0.1679653
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 117 | Train Loss: 0.2813503 Vali Loss: 0.1057325 Test Loss: 0.1658520
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 117 | Train Loss: 0.2801724 Vali Loss: 0.1128210 Test Loss: 0.1647220
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 117 | Train Loss: 0.2786280 Vali Loss: 0.1137152 Test Loss: 0.1659179
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 117 | Train Loss: 0.2792325 Vali Loss: 0.1050075 Test Loss: 0.1656360
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 117 | Train Loss: 0.2780097 Vali Loss: 0.1100418 Test Loss: 0.1653566
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 117 | Train Loss: 0.2787112 Vali Loss: 0.1081504 Test Loss: 0.1653622
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 117 | Train Loss: 0.2782891 Vali Loss: 0.1042027 Test Loss: 0.1653640
Validation loss decreased (0.104947 --> 0.104203).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 117 | Train Loss: 0.2778853 Vali Loss: 0.1135035 Test Loss: 0.1653591
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 117 | Train Loss: 0.2775885 Vali Loss: 0.1058269 Test Loss: 0.1653555
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 117 | Train Loss: 0.2785891 Vali Loss: 0.1087552 Test Loss: 0.1653567
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 117 | Train Loss: 0.2779519 Vali Loss: 0.1142366 Test Loss: 0.1653557
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 117 | Train Loss: 0.2775983 Vali Loss: 0.1082959 Test Loss: 0.1653555
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 117 | Train Loss: 0.2789769 Vali Loss: 0.1114830 Test Loss: 0.1653548
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 117 | Train Loss: 0.2785998 Vali Loss: 0.1082995 Test Loss: 0.1653547
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 117 | Train Loss: 0.2782004 Vali Loss: 0.1127077 Test Loss: 0.1653545
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 117 | Train Loss: 0.2782785 Vali Loss: 0.1105376 Test Loss: 0.1653545
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 117 | Train Loss: 0.2776662 Vali Loss: 0.1100818 Test Loss: 0.1653545
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_SASOL_H50_FEDformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 165
test shape: (165, 50, 1) (165, 50, 1)
test shape: (165, 50, 1) (165, 50, 1)


	mse:0.0021086984779685736, mae:0.033745307475328445, rmse:0.045920565724372864, r2:-0.029254794120788574, dtw:Not calculated


VAL - MSE: 0.0021, MAE: 0.0337, RMSE: 0.0459, RÂ²: -0.0293, MAPE: 7387097.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.628 MB of 0.631 MB uploadedwandb: \ 0.628 MB of 0.631 MB uploadedwandb: | 0.631 MB of 0.631 MB uploadedwandb: / 0.631 MB of 0.631 MB uploadedwandb: - 0.631 MB of 0.843 MB uploadedwandb: \ 0.731 MB of 0.843 MB uploadedwandb: | 0.843 MB of 0.843 MB uploadedwandb: / 0.843 MB of 0.843 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–†â–ˆâ–ƒâ–â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–„â–ƒâ–‚â–‚â–â–‚â–â–â–â–‚â–â–â–‚â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–…â–‚â–‡â–ˆâ–‚â–…â–„â–â–‡â–‚â–„â–ˆâ–„â–†â–„â–‡â–…â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 16691206
wandb:     model/trainable_params 16691206
wandb:                  test/loss 0.16535
wandb:                 train/loss 0.27767
wandb:   val/directional_accuracy 45.60297
wandb:                   val/loss 0.11008
wandb:                    val/mae 0.03375
wandb:                   val/mape 738709750.0
wandb:                    val/mse 0.00211
wandb:                     val/r2 -0.02925
wandb:                   val/rmse 0.04592
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/72uik6vk
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_072038-72uik6vk/logs
Completed: SASOL H=50

Training: FEDformer on SASOL for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_072915-tujd442r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/tujd442r
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_SASOL_H100Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 8, 13, 16, 20, 22, 23, 24, 28, 29, 30, 31, 32, 34, 35, 37, 39, 43, 44, 46, 51, 52, 55, 56, 57, 62, 63, 64]
 fourier enhanced cross attention used!
modes_q=32, index_q=[0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 17, 18, 20, 21, 22, 23, 28, 30, 34, 36, 44, 45, 50, 53, 54, 55, 56, 57, 61, 63]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/tujd442r
>>>>>>>start training : long_term_forecast_FEDformer_SASOL_H100_FEDformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 100
============================================================
train 3675
val 114
test 115
Epoch: 1, Steps: 115 | Train Loss: 0.3919973 Vali Loss: 0.1118401 Test Loss: 0.1808261
Validation loss decreased (inf --> 0.111840).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.39199733267659725, 'val/loss': 0.11184010468423367, 'test/loss': 0.1808261163532734, '_timestamp': 1762320586.3691344}).
Epoch: 2, Steps: 115 | Train Loss: 0.3517176 Vali Loss: 0.1206509 Test Loss: 0.1682202
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.35171755046948144, 'val/loss': 0.12065094150602818, 'test/loss': 0.16822022199630737, '_timestamp': 1762320609.1578894}).
Epoch: 3, Steps: 115 | Train Loss: 0.3479139 Vali Loss: 0.1116263 Test Loss: 0.1755295
Validation loss decreased (0.111840 --> 0.111626).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 115 | Train Loss: 0.3455534 Vali Loss: 0.1170578 Test Loss: 0.1700758
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 115 | Train Loss: 0.3440852 Vali Loss: 0.1224811 Test Loss: 0.1707682
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 115 | Train Loss: 0.3435623 Vali Loss: 0.1180338 Test Loss: 0.1713780
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 115 | Train Loss: 0.3430728 Vali Loss: 0.1153611 Test Loss: 0.1717234
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 115 | Train Loss: 0.3428257 Vali Loss: 0.1156337 Test Loss: 0.1713424
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 115 | Train Loss: 0.3429289 Vali Loss: 0.1176139 Test Loss: 0.1711634
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 115 | Train Loss: 0.3425687 Vali Loss: 0.1154547 Test Loss: 0.1711039
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 115 | Train Loss: 0.3426715 Vali Loss: 0.1151498 Test Loss: 0.1710616
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 115 | Train Loss: 0.3425577 Vali Loss: 0.1162405 Test Loss: 0.1710429
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 115 | Train Loss: 0.3427461 Vali Loss: 0.1154317 Test Loss: 0.1710375
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_SASOL_H100_FEDformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 115
test shape: (115, 100, 1) (115, 100, 1)
test shape: (115, 100, 1) (115, 100, 1)


	mse:0.002012158278375864, mae:0.03304242342710495, rmse:0.04485708847641945, r2:-0.013978362083435059, dtw:Not calculated


VAL - MSE: 0.0020, MAE: 0.0330, RMSE: 0.0449, RÂ²: -0.0140, MAPE: 6335195.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.662 MB of 0.666 MB uploadedwandb: \ 0.662 MB of 0.666 MB uploadedwandb: | 0.662 MB of 0.666 MB uploadedwandb: / 0.666 MB of 0.666 MB uploadedwandb: - 0.666 MB of 0.878 MB uploadedwandb: \ 0.878 MB of 0.878 MB uploadedwandb: | 0.878 MB of 0.878 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–…â–ˆâ–…â–ƒâ–„â–…â–ƒâ–ƒâ–„â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 16691206
wandb:     model/trainable_params 16691206
wandb:                  test/loss 0.17104
wandb:                 train/loss 0.34275
wandb:   val/directional_accuracy 49.24901
wandb:                   val/loss 0.11543
wandb:                    val/mae 0.03304
wandb:                   val/mape 633519500.0
wandb:                    val/mse 0.00201
wandb:                     val/r2 -0.01398
wandb:                   val/rmse 0.04486
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/tujd442r
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_072915-tujd442r/logs
Completed: SASOL H=100

FEDformer training completed for all datasets!
