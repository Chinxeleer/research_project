##############################################################################
# Training Mamba Model on All Datasets
##############################################################################
Training: Mamba on NVIDIA for H=3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H3     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 3
============================================================
train 2201
val 123
test 125
Validation loss decreased (inf --> 0.105865).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.105865 --> 0.099161).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.099161 --> 0.093587).  Saving model ...
Updating learning rate to 2.5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
Validation loss decreased (0.093587 --> 0.091923).  Saving model ...
Updating learning rate to 6.25e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.5625e-06
Validation loss decreased (0.091923 --> 0.091456).  Saving model ...
Updating learning rate to 7.8125e-07
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.90625e-07
Validation loss decreased (0.091456 --> 0.091033).  Saving model ...
Updating learning rate to 1.953125e-07
Validation loss decreased (0.091033 --> 0.090802).  Saving model ...
Updating learning rate to 9.765625e-08
EarlyStopping counter: 1 out of 5
Updating learning rate to 4.8828125e-08
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.44140625e-08
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.220703125e-08
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.103515625e-09
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 125
test shape: (125, 3, 6) (125, 3, 6)
test shape: (125, 3, 6) (125, 3, 6)


	mse:0.18073610961437225, mae:0.27259135246276855, rmse:0.425130695104599, r2:0.8677290976047516, dtw:Not calculated


VAL - MSE: 0.1807, MAE: 0.2726, RMSE: 0.4251, RÂ²: 0.8677, MAPE: 0.57%
Completed: NVIDIA H=3

Training: Mamba on NVIDIA for H=5
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H5     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 5
============================================================
train 2199
val 121
test 123
Validation loss decreased (inf --> 0.106914).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.106914 --> 0.101012).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.101012 --> 0.096597).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.096597 --> 0.094067).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (0.094067 --> 0.092572).  Saving model ...
Updating learning rate to 6.25e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 3 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.90625e-07
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 123
test shape: (123, 5, 6) (123, 5, 6)
test shape: (123, 5, 6) (123, 5, 6)


	mse:0.2020464986562729, mae:0.2996691167354584, rmse:0.4494958221912384, r2:0.8508698642253876, dtw:Not calculated


VAL - MSE: 0.2020, MAE: 0.2997, RMSE: 0.4495, RÂ²: 0.8509, MAPE: 0.59%
Completed: NVIDIA H=5

Training: Mamba on NVIDIA for H=10
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H10    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 10
============================================================
train 2194
val 116
test 118
Validation loss decreased (inf --> 0.113091).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.113091 --> 0.107436).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.107436 --> 0.103270).  Saving model ...
Updating learning rate to 2.5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
Validation loss decreased (0.103270 --> 0.102064).  Saving model ...
Updating learning rate to 6.25e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
Validation loss decreased (0.102064 --> 0.101399).  Saving model ...
Updating learning rate to 1.5625e-06
Validation loss decreased (0.101399 --> 0.100040).  Saving model ...
Updating learning rate to 7.8125e-07
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.90625e-07
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.953125e-07
Validation loss decreased (0.100040 --> 0.098820).  Saving model ...
Updating learning rate to 9.765625e-08
EarlyStopping counter: 1 out of 5
Updating learning rate to 4.8828125e-08
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.44140625e-08
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.220703125e-08
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.103515625e-09
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 118
test shape: (118, 10, 6) (118, 10, 6)
test shape: (118, 10, 6) (118, 10, 6)


	mse:0.2258317619562149, mae:0.32685747742652893, rmse:0.4752175807952881, r2:0.8301027864217758, dtw:Not calculated


VAL - MSE: 0.2258, MAE: 0.3269, RMSE: 0.4752, RÂ²: 0.8301, MAPE: 0.66%
Completed: NVIDIA H=10

Training: Mamba on NVIDIA for H=22
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H22    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 22
============================================================
train 2182
val 104
test 106
Validation loss decreased (inf --> 0.118083).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.118083 --> 0.115245).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.115245 --> 0.112602).  Saving model ...
Updating learning rate to 2.5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
Validation loss decreased (0.112602 --> 0.112375).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (0.112375 --> 0.111746).  Saving model ...
Updating learning rate to 3.125e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
Validation loss decreased (0.111746 --> 0.110113).  Saving model ...
Updating learning rate to 3.90625e-07
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.953125e-07
EarlyStopping counter: 2 out of 5
Updating learning rate to 9.765625e-08
Validation loss decreased (0.110113 --> 0.109166).  Saving model ...
Updating learning rate to 4.8828125e-08
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.44140625e-08
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.220703125e-08
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.103515625e-09
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.0517578125e-09
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 106
test shape: (106, 22, 6) (106, 22, 6)
test shape: (106, 22, 6) (106, 22, 6)


	mse:0.3126337230205536, mae:0.40459853410720825, rmse:0.5591365694999695, r2:0.7522933781147003, dtw:Not calculated


VAL - MSE: 0.3126, MAE: 0.4046, RMSE: 0.5591, RÂ²: 0.7523, MAPE: 0.70%
Completed: NVIDIA H=22

Training: Mamba on NVIDIA for H=50
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H50    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 50
============================================================
train 2154
val 76
test 78
Validation loss decreased (inf --> 0.141974).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.141974 --> 0.140438).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.140438 --> 0.135594).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.135594 --> 0.135075).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (0.135075 --> 0.132412).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (0.132412 --> 0.131425).  Saving model ...
Updating learning rate to 3.125e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.90625e-07
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.953125e-07
Validation loss decreased (0.131425 --> 0.130715).  Saving model ...
Updating learning rate to 9.765625e-08
EarlyStopping counter: 1 out of 5
Updating learning rate to 4.8828125e-08
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.44140625e-08
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.220703125e-08
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.103515625e-09
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 78
test shape: (78, 50, 6) (78, 50, 6)
test shape: (78, 50, 6) (78, 50, 6)


	mse:0.5882484316825867, mae:0.5811243057250977, rmse:0.7669735550880432, r2:0.4835842251777649, dtw:Not calculated


VAL - MSE: 0.5882, MAE: 0.5811, RMSE: 0.7670, RÂ²: 0.4836, MAPE: 0.83%
Completed: NVIDIA H=50

Training: Mamba on NVIDIA for H=100
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NVIDIA_H100   Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_NVIDIA_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 100
============================================================
train 2104
val 26
test 28
Validation loss decreased (inf --> 0.173509).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NVIDIA_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 28
test shape: (28, 100, 6) (28, 100, 6)
test shape: (28, 100, 6) (28, 100, 6)


	mse:0.9051257967948914, mae:0.6960157155990601, rmse:0.9513809680938721, r2:0.26350462436676025, dtw:Not calculated


VAL - MSE: 0.9051, MAE: 0.6960, RMSE: 0.9514, RÂ²: 0.2635, MAPE: 0.77%
Completed: NVIDIA H=100

Training: Mamba on APPLE for H=3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H3      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 3
============================================================
train 2201
val 123
test 125
Validation loss decreased (inf --> 0.335839).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.335839 --> 0.335152).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.335152 --> 0.331642).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.331642 --> 0.330385).  Saving model ...
Updating learning rate to 1.25e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
Validation loss decreased (0.330385 --> 0.328129).  Saving model ...
Updating learning rate to 3.125e-06
Validation loss decreased (0.328129 --> 0.324612).  Saving model ...
Updating learning rate to 1.5625e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.90625e-07
Validation loss decreased (0.324612 --> 0.321818).  Saving model ...
Updating learning rate to 1.953125e-07
EarlyStopping counter: 1 out of 5
Updating learning rate to 9.765625e-08
EarlyStopping counter: 2 out of 5
Updating learning rate to 4.8828125e-08
EarlyStopping counter: 3 out of 5
Updating learning rate to 2.44140625e-08
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.220703125e-08
Validation loss decreased (0.321818 --> 0.321654).  Saving model ...
Updating learning rate to 6.103515625e-09
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.0517578125e-09
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.52587890625e-09
EarlyStopping counter: 3 out of 5
Updating learning rate to 7.62939453125e-10
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.814697265625e-10
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 125
test shape: (125, 3, 6) (125, 3, 6)
test shape: (125, 3, 6) (125, 3, 6)


	mse:0.5034381747245789, mae:0.4113079309463501, rmse:0.7095337510108948, r2:0.6295021176338196, dtw:Not calculated


VAL - MSE: 0.5034, MAE: 0.4113, RMSE: 0.7095, RÂ²: 0.6295, MAPE: 0.56%
Completed: APPLE H=3

Training: Mamba on APPLE for H=5
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H5      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 5
============================================================
train 2199
val 121
test 123
Validation loss decreased (inf --> 0.338806).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
Validation loss decreased (0.338806 --> 0.333910).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.333910 --> 0.331308).  Saving model ...
Updating learning rate to 1.25e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 123
test shape: (123, 5, 6) (123, 5, 6)
test shape: (123, 5, 6) (123, 5, 6)


	mse:0.5387709736824036, mae:0.4180929362773895, rmse:0.7340102195739746, r2:0.6026981770992279, dtw:Not calculated


VAL - MSE: 0.5388, MAE: 0.4181, RMSE: 0.7340, RÂ²: 0.6027, MAPE: 0.54%
Completed: APPLE H=5

Training: Mamba on APPLE for H=10
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H10     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 10
============================================================
train 2194
val 116
test 118
Validation loss decreased (inf --> 0.363649).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
Validation loss decreased (0.363649 --> 0.351470).  Saving model ...
Updating learning rate to 2.5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
Validation loss decreased (0.351470 --> 0.350082).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (0.350082 --> 0.343832).  Saving model ...
Updating learning rate to 3.125e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.90625e-07
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.953125e-07
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 118
test shape: (118, 10, 6) (118, 10, 6)
test shape: (118, 10, 6) (118, 10, 6)


	mse:0.5729939937591553, mae:0.4340164363384247, rmse:0.7569636702537537, r2:0.5768284797668457, dtw:Not calculated


VAL - MSE: 0.5730, MAE: 0.4340, RMSE: 0.7570, RÂ²: 0.5768, MAPE: 0.55%
Completed: APPLE H=10

Training: Mamba on APPLE for H=22
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H22     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 22
============================================================
train 2182
val 104
test 106
Validation loss decreased (inf --> 0.400417).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.400417 --> 0.381579).  Saving model ...
Updating learning rate to 5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
Validation loss decreased (0.381579 --> 0.378513).  Saving model ...
Updating learning rate to 6.25e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 3 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.90625e-07
Validation loss decreased (0.378513 --> 0.376775).  Saving model ...
Updating learning rate to 1.953125e-07
Validation loss decreased (0.376775 --> 0.375950).  Saving model ...
Updating learning rate to 9.765625e-08
EarlyStopping counter: 1 out of 5
Updating learning rate to 4.8828125e-08
Validation loss decreased (0.375950 --> 0.373719).  Saving model ...
Updating learning rate to 2.44140625e-08
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.220703125e-08
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.103515625e-09
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.0517578125e-09
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.52587890625e-09
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 106
test shape: (106, 22, 6) (106, 22, 6)
test shape: (106, 22, 6) (106, 22, 6)


	mse:0.6342467069625854, mae:0.4682047665119171, rmse:0.7963960766792297, r2:0.5292771458625793, dtw:Not calculated


VAL - MSE: 0.6342, MAE: 0.4682, RMSE: 0.7964, RÂ²: 0.5293, MAPE: 0.61%
Completed: APPLE H=22

Training: Mamba on APPLE for H=50
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H50     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 50
============================================================
train 2154
val 76
test 78
Validation loss decreased (inf --> 0.422823).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 78
test shape: (78, 50, 6) (78, 50, 6)
test shape: (78, 50, 6) (78, 50, 6)


	mse:0.6376609802246094, mae:0.4990842938423157, rmse:0.7985367774963379, r2:0.4983276128768921, dtw:Not calculated


VAL - MSE: 0.6377, MAE: 0.4991, RMSE: 0.7985, RÂ²: 0.4983, MAPE: 0.64%
Completed: APPLE H=50

Training: Mamba on APPLE for H=100
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_APPLE_H100    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_APPLE_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 100
============================================================
train 2104
val 26
test 28
Validation loss decreased (inf --> 0.496059).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_APPLE_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 28
test shape: (28, 100, 6) (28, 100, 6)
test shape: (28, 100, 6) (28, 100, 6)


	mse:0.6660568714141846, mae:0.5589410662651062, rmse:0.8161230683326721, r2:0.5012739896774292, dtw:Not calculated


VAL - MSE: 0.6661, MAE: 0.5589, RMSE: 0.8161, RÂ²: 0.5013, MAPE: 0.52%
Completed: APPLE H=100

Training: Mamba on SP500 for H=3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H3      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_SP500_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 3
============================================================
train 2201
val 123
test 125
Validation loss decreased (inf --> nan).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.125e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.5625e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.8125e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.90625e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.953125e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.765625e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.8828125e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.44140625e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.220703125e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 6.103515625e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.0517578125e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.52587890625e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.62939453125e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.814697265625e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.960464477539063e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.9802322387695314e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4901161193847657e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.450580596923828e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.725290298461914e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.862645149230957e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.313225746154786e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.656612873077393e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.3283064365386964e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1641532182693482e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.820766091346741e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.9103830456733705e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4551915228366853e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.275957614183426e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.637978807091713e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.8189894035458566e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.094947017729283e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.5474735088646414e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.2737367544323207e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1368683772161604e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.684341886080802e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.842170943040401e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4210854715202004e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.105427357601002e-19
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.552713678800501e-19
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.7763568394002505e-19
>>>>>>>testing : long_term_forecast_Mamba_SP500_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 125
test shape: (125, 3, 6) (125, 3, 6)
test shape: (125, 3, 6) (125, 3, 6)


	mse:nan, mae:nan, rmse:nan, r2:nan, dtw:Not calculated


VAL - MSE: nan, MAE: nan, RMSE: nan, RÂ²: nan, MAPE: nan%
Completed: SP500 H=3

Training: Mamba on SP500 for H=5
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H5      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_SP500_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 5
============================================================
train 2199
val 121
test 123
Validation loss decreased (inf --> nan).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.125e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.5625e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.8125e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.90625e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.953125e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.765625e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.8828125e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.44140625e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.220703125e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 6.103515625e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.0517578125e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.52587890625e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.62939453125e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.814697265625e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.960464477539063e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.9802322387695314e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4901161193847657e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.450580596923828e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.725290298461914e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.862645149230957e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.313225746154786e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.656612873077393e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.3283064365386964e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1641532182693482e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.820766091346741e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.9103830456733705e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4551915228366853e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.275957614183426e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.637978807091713e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.8189894035458566e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.094947017729283e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.5474735088646414e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.2737367544323207e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1368683772161604e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.684341886080802e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.842170943040401e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4210854715202004e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.105427357601002e-19
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.552713678800501e-19
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.7763568394002505e-19
>>>>>>>testing : long_term_forecast_Mamba_SP500_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 123
test shape: (123, 5, 6) (123, 5, 6)
test shape: (123, 5, 6) (123, 5, 6)


	mse:nan, mae:nan, rmse:nan, r2:nan, dtw:Not calculated


VAL - MSE: nan, MAE: nan, RMSE: nan, RÂ²: nan, MAPE: nan%
Completed: SP500 H=5

Training: Mamba on SP500 for H=10
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H10     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_SP500_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 10
============================================================
train 2194
val 116
test 118
Validation loss decreased (inf --> nan).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.125e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.5625e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.8125e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.90625e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.953125e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.765625e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.8828125e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.44140625e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.220703125e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 6.103515625e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.0517578125e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.52587890625e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.62939453125e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.814697265625e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.960464477539063e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.9802322387695314e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4901161193847657e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.450580596923828e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.725290298461914e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.862645149230957e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.313225746154786e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.656612873077393e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.3283064365386964e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1641532182693482e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.820766091346741e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.9103830456733705e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4551915228366853e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.275957614183426e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.637978807091713e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.8189894035458566e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.094947017729283e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.5474735088646414e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.2737367544323207e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1368683772161604e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.684341886080802e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.842170943040401e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4210854715202004e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.105427357601002e-19
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.552713678800501e-19
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.7763568394002505e-19
>>>>>>>testing : long_term_forecast_Mamba_SP500_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 118
test shape: (118, 10, 6) (118, 10, 6)
test shape: (118, 10, 6) (118, 10, 6)


	mse:nan, mae:nan, rmse:nan, r2:nan, dtw:Not calculated


VAL - MSE: nan, MAE: nan, RMSE: nan, RÂ²: nan, MAPE: nan%
Completed: SP500 H=10

Training: Mamba on SP500 for H=22
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H22     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_SP500_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 22
============================================================
train 2182
val 104
test 106
Validation loss decreased (inf --> nan).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.125e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.5625e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.8125e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.90625e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.953125e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.765625e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.8828125e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.44140625e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.220703125e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 6.103515625e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.0517578125e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.52587890625e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.62939453125e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.814697265625e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.960464477539063e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.9802322387695314e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4901161193847657e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.450580596923828e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.725290298461914e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.862645149230957e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.313225746154786e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.656612873077393e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.3283064365386964e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1641532182693482e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.820766091346741e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.9103830456733705e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4551915228366853e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.275957614183426e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.637978807091713e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.8189894035458566e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.094947017729283e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.5474735088646414e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.2737367544323207e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1368683772161604e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.684341886080802e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.842170943040401e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4210854715202004e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.105427357601002e-19
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.552713678800501e-19
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.7763568394002505e-19
>>>>>>>testing : long_term_forecast_Mamba_SP500_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 106
test shape: (106, 22, 6) (106, 22, 6)
test shape: (106, 22, 6) (106, 22, 6)


	mse:nan, mae:nan, rmse:nan, r2:nan, dtw:Not calculated


VAL - MSE: nan, MAE: nan, RMSE: nan, RÂ²: nan, MAPE: nan%
Completed: SP500 H=22

Training: Mamba on SP500 for H=50
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H50     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_SP500_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 50
============================================================
train 2154
val 76
test 78
Validation loss decreased (inf --> nan).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.125e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.5625e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.8125e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.90625e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.953125e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.765625e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.8828125e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.44140625e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.220703125e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 6.103515625e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.0517578125e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.52587890625e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.62939453125e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.814697265625e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.960464477539063e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.9802322387695314e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4901161193847657e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.450580596923828e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.725290298461914e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.862645149230957e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.313225746154786e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.656612873077393e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.3283064365386964e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1641532182693482e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.820766091346741e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.9103830456733705e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4551915228366853e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.275957614183426e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.637978807091713e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.8189894035458566e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.094947017729283e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.5474735088646414e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.2737367544323207e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1368683772161604e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.684341886080802e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.842170943040401e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4210854715202004e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.105427357601002e-19
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.552713678800501e-19
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.7763568394002505e-19
>>>>>>>testing : long_term_forecast_Mamba_SP500_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 78
test shape: (78, 50, 6) (78, 50, 6)
test shape: (78, 50, 6) (78, 50, 6)


	mse:nan, mae:nan, rmse:nan, r2:nan, dtw:Not calculated


VAL - MSE: nan, MAE: nan, RMSE: nan, RÂ²: nan, MAPE: nan%
Completed: SP500 H=50

Training: Mamba on SP500 for H=100
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SP500_H100    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_SP500_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 100
============================================================
train 2104
val 26
test 28
Validation loss decreased (inf --> nan).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.125e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.5625e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.8125e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.90625e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.953125e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.765625e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.8828125e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.44140625e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.220703125e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 6.103515625e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.0517578125e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.52587890625e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.62939453125e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.814697265625e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.960464477539063e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.9802322387695314e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4901161193847657e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.450580596923828e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.725290298461914e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.862645149230957e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.313225746154786e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.656612873077393e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.3283064365386964e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1641532182693482e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.820766091346741e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.9103830456733705e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4551915228366853e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.275957614183426e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.637978807091713e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.8189894035458566e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.094947017729283e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.5474735088646414e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.2737367544323207e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1368683772161604e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.684341886080802e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.842170943040401e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4210854715202004e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.105427357601002e-19
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.552713678800501e-19
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.7763568394002505e-19
>>>>>>>testing : long_term_forecast_Mamba_SP500_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 28
test shape: (28, 100, 6) (28, 100, 6)
test shape: (28, 100, 6) (28, 100, 6)


	mse:nan, mae:nan, rmse:nan, r2:nan, dtw:Not calculated


VAL - MSE: nan, MAE: nan, RMSE: nan, RÂ²: nan, MAPE: nan%
Completed: SP500 H=100

Training: Mamba on NASDAQ for H=3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H3     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 3
============================================================
train 2201
val 123
test 125
Validation loss decreased (inf --> 0.070009).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.070009 --> 0.055405).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.055405 --> 0.048911).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.048911 --> 0.047739).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (0.047739 --> 0.046996).  Saving model ...
Updating learning rate to 6.25e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
Validation loss decreased (0.046996 --> 0.046821).  Saving model ...
Updating learning rate to 1.5625e-06
Validation loss decreased (0.046821 --> 0.046500).  Saving model ...
Updating learning rate to 7.8125e-07
Validation loss decreased (0.046500 --> 0.046354).  Saving model ...
Updating learning rate to 3.90625e-07
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.953125e-07
EarlyStopping counter: 2 out of 5
Updating learning rate to 9.765625e-08
Validation loss decreased (0.046354 --> 0.046296).  Saving model ...
Updating learning rate to 4.8828125e-08
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.44140625e-08
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.220703125e-08
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.103515625e-09
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.0517578125e-09
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 125
test shape: (125, 3, 6) (125, 3, 6)
test shape: (125, 3, 6) (125, 3, 6)


	mse:0.10264917463064194, mae:0.20068079233169556, rmse:0.3203890919685364, r2:0.965005848556757, dtw:Not calculated


VAL - MSE: 0.1026, MAE: 0.2007, RMSE: 0.3204, RÂ²: 0.9650, MAPE: 0.32%
Completed: NASDAQ H=3

Training: Mamba on NASDAQ for H=5
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H5     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 5
============================================================
train 2199
val 121
test 123
Validation loss decreased (inf --> 0.071455).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.071455 --> 0.061065).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.061065 --> 0.052176).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.052176 --> 0.049598).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (0.049598 --> 0.048822).  Saving model ...
Updating learning rate to 6.25e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
Validation loss decreased (0.048822 --> 0.048281).  Saving model ...
Updating learning rate to 1.5625e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 7.8125e-07
Validation loss decreased (0.048281 --> 0.048232).  Saving model ...
Updating learning rate to 3.90625e-07
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.953125e-07
EarlyStopping counter: 2 out of 5
Updating learning rate to 9.765625e-08
EarlyStopping counter: 3 out of 5
Updating learning rate to 4.8828125e-08
Validation loss decreased (0.048232 --> 0.048216).  Saving model ...
Updating learning rate to 2.44140625e-08
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.220703125e-08
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.103515625e-09
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.0517578125e-09
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.52587890625e-09
Validation loss decreased (0.048216 --> 0.048065).  Saving model ...
Updating learning rate to 7.62939453125e-10
Validation loss decreased (0.048065 --> 0.047860).  Saving model ...
Updating learning rate to 3.814697265625e-10
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.9073486328125e-10
EarlyStopping counter: 2 out of 5
Updating learning rate to 9.5367431640625e-11
EarlyStopping counter: 3 out of 5
Updating learning rate to 4.76837158203125e-11
EarlyStopping counter: 4 out of 5
Updating learning rate to 2.384185791015625e-11
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 123
test shape: (123, 5, 6) (123, 5, 6)
test shape: (123, 5, 6) (123, 5, 6)


	mse:0.11016567796468735, mae:0.21161529421806335, rmse:0.33191215991973877, r2:0.9624366164207458, dtw:Not calculated


VAL - MSE: 0.1102, MAE: 0.2116, RMSE: 0.3319, RÂ²: 0.9624, MAPE: 0.33%
Completed: NASDAQ H=5

Training: Mamba on NASDAQ for H=10
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H10    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 10
============================================================
train 2194
val 116
test 118
Validation loss decreased (inf --> 0.076189).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.076189 --> 0.067140).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.067140 --> 0.059748).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.059748 --> 0.056829).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (0.056829 --> 0.056274).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (0.056274 --> 0.055365).  Saving model ...
Updating learning rate to 3.125e-06
Validation loss decreased (0.055365 --> 0.054587).  Saving model ...
Updating learning rate to 1.5625e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 7.8125e-07
Validation loss decreased (0.054587 --> 0.053796).  Saving model ...
Updating learning rate to 3.90625e-07
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.953125e-07
EarlyStopping counter: 2 out of 5
Updating learning rate to 9.765625e-08
EarlyStopping counter: 3 out of 5
Updating learning rate to 4.8828125e-08
EarlyStopping counter: 4 out of 5
Updating learning rate to 2.44140625e-08
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 118
test shape: (118, 10, 6) (118, 10, 6)
test shape: (118, 10, 6) (118, 10, 6)


	mse:0.12769153714179993, mae:0.23931223154067993, rmse:0.3573395311832428, r2:0.95655906945467, dtw:Not calculated


VAL - MSE: 0.1277, MAE: 0.2393, RMSE: 0.3573, RÂ²: 0.9566, MAPE: 0.35%
Completed: NASDAQ H=10

Training: Mamba on NASDAQ for H=22
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H22    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 22
============================================================
train 2182
val 104
test 106
Validation loss decreased (inf --> 0.085478).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.085478 --> 0.082227).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.082227 --> 0.073305).  Saving model ...
Updating learning rate to 2.5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
Validation loss decreased (0.073305 --> 0.068805).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (0.068805 --> 0.068570).  Saving model ...
Updating learning rate to 3.125e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.90625e-07
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.953125e-07
Validation loss decreased (0.068570 --> 0.067179).  Saving model ...
Updating learning rate to 9.765625e-08
EarlyStopping counter: 1 out of 5
Updating learning rate to 4.8828125e-08
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.44140625e-08
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.220703125e-08
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.103515625e-09
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 106
test shape: (106, 22, 6) (106, 22, 6)
test shape: (106, 22, 6) (106, 22, 6)


	mse:0.16402840614318848, mae:0.2853439152240753, rmse:0.40500420331954956, r2:0.9442921206355095, dtw:Not calculated


VAL - MSE: 0.1640, MAE: 0.2853, RMSE: 0.4050, RÂ²: 0.9443, MAPE: 0.37%
Completed: NASDAQ H=22

Training: Mamba on NASDAQ for H=50
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H50    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 50
============================================================
train 2154
val 76
test 78
Validation loss decreased (inf --> 0.135110).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.135110 --> 0.128212).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.128212 --> 0.116817).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.116817 --> 0.108320).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (0.108320 --> 0.105286).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (0.105286 --> 0.104389).  Saving model ...
Updating learning rate to 3.125e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
Validation loss decreased (0.104389 --> 0.104040).  Saving model ...
Updating learning rate to 3.90625e-07
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.953125e-07
Validation loss decreased (0.104040 --> 0.103653).  Saving model ...
Updating learning rate to 9.765625e-08
Validation loss decreased (0.103653 --> 0.101277).  Saving model ...
Updating learning rate to 4.8828125e-08
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.44140625e-08
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.220703125e-08
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.103515625e-09
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.0517578125e-09
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 78
test shape: (78, 50, 6) (78, 50, 6)
test shape: (78, 50, 6) (78, 50, 6)


	mse:0.25495296716690063, mae:0.3663952052593231, rmse:0.5049286484718323, r2:0.9119070693850517, dtw:Not calculated


VAL - MSE: 0.2550, MAE: 0.3664, RMSE: 0.5049, RÂ²: 0.9119, MAPE: 0.38%
Completed: NASDAQ H=50

Training: Mamba on NASDAQ for H=100
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_NASDAQ_H100   Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_NASDAQ_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 100
============================================================
train 2104
val 26
test 28
Validation loss decreased (inf --> 0.273169).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.273169 --> 0.266163).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.266163 --> 0.253814).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.253814 --> 0.244234).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (0.244234 --> 0.239252).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (0.239252 --> 0.237069).  Saving model ...
Updating learning rate to 3.125e-06
Validation loss decreased (0.237069 --> 0.236094).  Saving model ...
Updating learning rate to 1.5625e-06
Validation loss decreased (0.236094 --> 0.235562).  Saving model ...
Updating learning rate to 7.8125e-07
Validation loss decreased (0.235562 --> 0.235286).  Saving model ...
Updating learning rate to 3.90625e-07
Validation loss decreased (0.235286 --> 0.235180).  Saving model ...
Updating learning rate to 1.953125e-07
Validation loss decreased (0.235180 --> 0.235125).  Saving model ...
Updating learning rate to 9.765625e-08
Validation loss decreased (0.235125 --> 0.235099).  Saving model ...
Updating learning rate to 4.8828125e-08
Validation loss decreased (0.235099 --> 0.235086).  Saving model ...
Updating learning rate to 2.44140625e-08
Validation loss decreased (0.235086 --> 0.235079).  Saving model ...
Updating learning rate to 1.220703125e-08
Validation loss decreased (0.235079 --> 0.235076).  Saving model ...
Updating learning rate to 6.103515625e-09
Validation loss decreased (0.235076 --> 0.235074).  Saving model ...
Updating learning rate to 3.0517578125e-09
Validation loss decreased (0.235074 --> 0.235074).  Saving model ...
Updating learning rate to 1.52587890625e-09
Validation loss decreased (0.235074 --> 0.235074).  Saving model ...
Updating learning rate to 7.62939453125e-10
Validation loss decreased (0.235074 --> 0.235074).  Saving model ...
Updating learning rate to 3.814697265625e-10
Validation loss decreased (0.235074 --> 0.235074).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Validation loss decreased (0.235074 --> 0.235074).  Saving model ...
Updating learning rate to 9.5367431640625e-11
EarlyStopping counter: 1 out of 5
Updating learning rate to 4.76837158203125e-11
Validation loss decreased (0.235074 --> 0.235074).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Validation loss decreased (0.235074 --> 0.235074).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Validation loss decreased (0.235074 --> 0.235074).  Saving model ...
Updating learning rate to 5.960464477539063e-12
Validation loss decreased (0.235074 --> 0.235074).  Saving model ...
Updating learning rate to 2.9802322387695314e-12
Validation loss decreased (0.235074 --> 0.235074).  Saving model ...
Updating learning rate to 1.4901161193847657e-12
EarlyStopping counter: 1 out of 5
Updating learning rate to 7.450580596923828e-13
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.725290298461914e-13
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.862645149230957e-13
Validation loss decreased (0.235074 --> 0.235074).  Saving model ...
Updating learning rate to 9.313225746154786e-14
Validation loss decreased (0.235074 --> 0.235074).  Saving model ...
Updating learning rate to 4.656612873077393e-14
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.3283064365386964e-14
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.1641532182693482e-14
Validation loss decreased (0.235074 --> 0.235074).  Saving model ...
Updating learning rate to 5.820766091346741e-15
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.9103830456733705e-15
Validation loss decreased (0.235074 --> 0.235074).  Saving model ...
Updating learning rate to 1.4551915228366853e-15
EarlyStopping counter: 1 out of 5
Updating learning rate to 7.275957614183426e-16
Validation loss decreased (0.235074 --> 0.235074).  Saving model ...
Updating learning rate to 3.637978807091713e-16
Validation loss decreased (0.235074 --> 0.235074).  Saving model ...
Updating learning rate to 1.8189894035458566e-16
Validation loss decreased (0.235074 --> 0.235074).  Saving model ...
Updating learning rate to 9.094947017729283e-17
EarlyStopping counter: 1 out of 5
Updating learning rate to 4.5474735088646414e-17
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.2737367544323207e-17
Validation loss decreased (0.235074 --> 0.235074).  Saving model ...
Updating learning rate to 1.1368683772161604e-17
EarlyStopping counter: 1 out of 5
Updating learning rate to 5.684341886080802e-18
Validation loss decreased (0.235074 --> 0.235074).  Saving model ...
Updating learning rate to 2.842170943040401e-18
Validation loss decreased (0.235074 --> 0.235074).  Saving model ...
Updating learning rate to 1.4210854715202004e-18
Validation loss decreased (0.235074 --> 0.235074).  Saving model ...
Updating learning rate to 7.105427357601002e-19
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.552713678800501e-19
Validation loss decreased (0.235074 --> 0.235074).  Saving model ...
Updating learning rate to 1.7763568394002505e-19
>>>>>>>testing : long_term_forecast_Mamba_NASDAQ_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 28
test shape: (28, 100, 6) (28, 100, 6)
test shape: (28, 100, 6) (28, 100, 6)


	mse:0.3434217572212219, mae:0.43629226088523865, rmse:0.5860219597816467, r2:0.8830714002251625, dtw:Not calculated


VAL - MSE: 0.3434, MAE: 0.4363, RMSE: 0.5860, RÂ²: 0.8831, MAPE: 0.41%
Completed: NASDAQ H=100

Training: Mamba on ABSA for H=3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H3       Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 3
============================================================
train 2201
val 123
test 125
Validation loss decreased (inf --> 0.094460).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.094460 --> 0.091299).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.091299 --> 0.089580).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.089580 --> 0.088203).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (0.088203 --> 0.088108).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (0.088108 --> 0.087311).  Saving model ...
Updating learning rate to 3.125e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
Validation loss decreased (0.087311 --> 0.086965).  Saving model ...
Updating learning rate to 7.8125e-07
Validation loss decreased (0.086965 --> 0.086749).  Saving model ...
Updating learning rate to 3.90625e-07
Validation loss decreased (0.086749 --> 0.086615).  Saving model ...
Updating learning rate to 1.953125e-07
EarlyStopping counter: 1 out of 5
Updating learning rate to 9.765625e-08
EarlyStopping counter: 2 out of 5
Updating learning rate to 4.8828125e-08
EarlyStopping counter: 3 out of 5
Updating learning rate to 2.44140625e-08
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.220703125e-08
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 125
test shape: (125, 3, 6) (125, 3, 6)
test shape: (125, 3, 6) (125, 3, 6)


	mse:0.28478333353996277, mae:0.35689499974250793, rmse:0.5336509346961975, r2:0.8923583701252937, dtw:Not calculated


VAL - MSE: 0.2848, MAE: 0.3569, RMSE: 0.5337, RÂ²: 0.8924, MAPE: 0.85%
Completed: ABSA H=3

Training: Mamba on ABSA for H=5
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H5       Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 5
============================================================
train 2199
val 121
test 123
Validation loss decreased (inf --> 0.096193).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.096193 --> 0.093364).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.093364 --> 0.092321).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.092321 --> 0.091372).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (0.091372 --> 0.090907).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (0.090907 --> 0.089679).  Saving model ...
Updating learning rate to 3.125e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
Validation loss decreased (0.089679 --> 0.089602).  Saving model ...
Updating learning rate to 3.90625e-07
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.953125e-07
Validation loss decreased (0.089602 --> 0.089489).  Saving model ...
Updating learning rate to 9.765625e-08
EarlyStopping counter: 1 out of 5
Updating learning rate to 4.8828125e-08
Validation loss decreased (0.089489 --> 0.089245).  Saving model ...
Updating learning rate to 2.44140625e-08
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.220703125e-08
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.103515625e-09
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.0517578125e-09
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.52587890625e-09
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 123
test shape: (123, 5, 6) (123, 5, 6)
test shape: (123, 5, 6) (123, 5, 6)


	mse:0.29209810495376587, mae:0.3625437915325165, rmse:0.540461003780365, r2:0.889347642660141, dtw:Not calculated


VAL - MSE: 0.2921, MAE: 0.3625, RMSE: 0.5405, RÂ²: 0.8893, MAPE: 0.87%
Completed: ABSA H=5

Training: Mamba on ABSA for H=10
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H10      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 10
============================================================
train 2194
val 116
test 118
Validation loss decreased (inf --> 0.101819).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.101819 --> 0.100510).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.100510 --> 0.097696).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.097696 --> 0.097391).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (0.097391 --> 0.096763).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (0.096763 --> 0.096150).  Saving model ...
Updating learning rate to 3.125e-06
Validation loss decreased (0.096150 --> 0.095931).  Saving model ...
Updating learning rate to 1.5625e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.90625e-07
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.953125e-07
EarlyStopping counter: 4 out of 5
Updating learning rate to 9.765625e-08
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 118
test shape: (118, 10, 6) (118, 10, 6)
test shape: (118, 10, 6) (118, 10, 6)


	mse:0.3022223114967346, mae:0.37143492698669434, rmse:0.5497475266456604, r2:0.8852144107222557, dtw:Not calculated


VAL - MSE: 0.3022, MAE: 0.3714, RMSE: 0.5497, RÂ²: 0.8852, MAPE: 0.68%
Completed: ABSA H=10

Training: Mamba on ABSA for H=22
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H22      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 22
============================================================
train 2182
val 104
test 106
Validation loss decreased (inf --> 0.130402).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.130402 --> 0.123087).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.123087 --> 0.120115).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.120115 --> 0.116658).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (0.116658 --> 0.115124).  Saving model ...
Updating learning rate to 6.25e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.5625e-06
Validation loss decreased (0.115124 --> 0.114503).  Saving model ...
Updating learning rate to 7.8125e-07
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.90625e-07
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.953125e-07
EarlyStopping counter: 3 out of 5
Updating learning rate to 9.765625e-08
EarlyStopping counter: 4 out of 5
Updating learning rate to 4.8828125e-08
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 106
test shape: (106, 22, 6) (106, 22, 6)
test shape: (106, 22, 6) (106, 22, 6)


	mse:0.3139815330505371, mae:0.3824239671230316, rmse:0.5603405237197876, r2:0.8792895004153252, dtw:Not calculated


VAL - MSE: 0.3140, MAE: 0.3824, RMSE: 0.5603, RÂ²: 0.8793, MAPE: 0.63%
Completed: ABSA H=22

Training: Mamba on ABSA for H=50
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H50      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 50
============================================================
train 2154
val 76
test 78
Validation loss decreased (inf --> 0.174497).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.174497 --> 0.171991).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.171991 --> 0.169348).  Saving model ...
Updating learning rate to 2.5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
Validation loss decreased (0.169348 --> 0.168621).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (0.168621 --> 0.166289).  Saving model ...
Updating learning rate to 3.125e-06
Validation loss decreased (0.166289 --> 0.166234).  Saving model ...
Updating learning rate to 1.5625e-06
Validation loss decreased (0.166234 --> 0.165954).  Saving model ...
Updating learning rate to 7.8125e-07
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.90625e-07
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.953125e-07
EarlyStopping counter: 3 out of 5
Updating learning rate to 9.765625e-08
EarlyStopping counter: 4 out of 5
Updating learning rate to 4.8828125e-08
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 78
test shape: (78, 50, 6) (78, 50, 6)
test shape: (78, 50, 6) (78, 50, 6)


	mse:0.35667604207992554, mae:0.4115681052207947, rmse:0.5972236394882202, r2:0.8644575029611588, dtw:Not calculated


VAL - MSE: 0.3567, MAE: 0.4116, RMSE: 0.5972, RÂ²: 0.8645, MAPE: 0.75%
Completed: ABSA H=50

Training: Mamba on ABSA for H=100
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_ABSA_H100     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_ABSA_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 100
============================================================
train 2104
val 26
test 28
Validation loss decreased (inf --> 0.277432).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.277432 --> 0.275107).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.275107 --> 0.273222).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.273222 --> 0.271907).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (0.271907 --> 0.271067).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (0.271067 --> 0.270625).  Saving model ...
Updating learning rate to 3.125e-06
Validation loss decreased (0.270625 --> 0.270398).  Saving model ...
Updating learning rate to 1.5625e-06
Validation loss decreased (0.270398 --> 0.270286).  Saving model ...
Updating learning rate to 7.8125e-07
Validation loss decreased (0.270286 --> 0.270229).  Saving model ...
Updating learning rate to 3.90625e-07
Validation loss decreased (0.270229 --> 0.270201).  Saving model ...
Updating learning rate to 1.953125e-07
Validation loss decreased (0.270201 --> 0.270186).  Saving model ...
Updating learning rate to 9.765625e-08
Validation loss decreased (0.270186 --> 0.270180).  Saving model ...
Updating learning rate to 4.8828125e-08
Validation loss decreased (0.270180 --> 0.270176).  Saving model ...
Updating learning rate to 2.44140625e-08
Validation loss decreased (0.270176 --> 0.270174).  Saving model ...
Updating learning rate to 1.220703125e-08
Validation loss decreased (0.270174 --> 0.270173).  Saving model ...
Updating learning rate to 6.103515625e-09
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 3.0517578125e-09
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 1.52587890625e-09
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 7.62939453125e-10
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 3.814697265625e-10
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 5.960464477539063e-12
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 2.9802322387695314e-12
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 1.4901161193847657e-12
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 7.450580596923828e-13
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 3.725290298461914e-13
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 1.862645149230957e-13
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 9.313225746154786e-14
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 4.656612873077393e-14
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 2.3283064365386964e-14
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 1.1641532182693482e-14
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 5.820766091346741e-15
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 2.9103830456733705e-15
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 1.4551915228366853e-15
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 7.275957614183426e-16
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 3.637978807091713e-16
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 1.8189894035458566e-16
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 9.094947017729283e-17
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 4.5474735088646414e-17
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 2.2737367544323207e-17
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 1.1368683772161604e-17
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 5.684341886080802e-18
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 2.842170943040401e-18
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 1.4210854715202004e-18
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 7.105427357601002e-19
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 3.552713678800501e-19
Validation loss decreased (0.270173 --> 0.270173).  Saving model ...
Updating learning rate to 1.7763568394002505e-19
>>>>>>>testing : long_term_forecast_Mamba_ABSA_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 28
test shape: (28, 100, 6) (28, 100, 6)
test shape: (28, 100, 6) (28, 100, 6)


	mse:0.4350682497024536, mae:0.4802916347980499, rmse:0.6595970392227173, r2:0.8323399722576141, dtw:Not calculated


VAL - MSE: 0.4351, MAE: 0.4803, RMSE: 0.6596, RÂ²: 0.8323, MAPE: 1.33%
Completed: ABSA H=100

Training: Mamba on SASOL for H=3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H3      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 3
============================================================
train 2201
val 123
test 125
Validation loss decreased (inf --> 0.050076).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
Validation loss decreased (0.050076 --> 0.049258).  Saving model ...
Updating learning rate to 3.125e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.90625e-07
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.953125e-07
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H3_Mamba_custom_ftM_sl60_ll30_pl3_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 125
test shape: (125, 3, 6) (125, 3, 6)
test shape: (125, 3, 6) (125, 3, 6)


	mse:0.07869888097047806, mae:0.09320246428251266, rmse:0.28053319454193115, r2:0.6081277132034302, dtw:Not calculated


VAL - MSE: 0.0787, MAE: 0.0932, RMSE: 0.2805, RÂ²: 0.6081, MAPE: 0.27%
Completed: SASOL H=3

Training: Mamba on SASOL for H=5
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H5      Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 5
============================================================
train 2199
val 121
test 123
Validation loss decreased (inf --> 0.050499).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.050499 --> 0.050430).  Saving model ...
Updating learning rate to 5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H5_Mamba_custom_ftM_sl60_ll30_pl5_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 123
test shape: (123, 5, 6) (123, 5, 6)
test shape: (123, 5, 6) (123, 5, 6)


	mse:0.07824783027172089, mae:0.0925639346241951, rmse:0.27972814440727234, r2:0.6108684241771698, dtw:Not calculated


VAL - MSE: 0.0782, MAE: 0.0926, RMSE: 0.2797, RÂ²: 0.6109, MAPE: 0.23%
Completed: SASOL H=5

Training: Mamba on SASOL for H=10
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H10     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 10
============================================================
train 2194
val 116
test 118
Validation loss decreased (inf --> 0.051821).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H10_Mamba_custom_ftM_sl60_ll30_pl10_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 118
test shape: (118, 10, 6) (118, 10, 6)
test shape: (118, 10, 6) (118, 10, 6)


	mse:0.0800599455833435, mae:0.09413951635360718, rmse:0.28294867277145386, r2:0.6067371070384979, dtw:Not calculated


VAL - MSE: 0.0801, MAE: 0.0941, RMSE: 0.2829, RÂ²: 0.6067, MAPE: 0.25%
Completed: SASOL H=10

Training: Mamba on SASOL for H=22
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H22     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 22
============================================================
train 2182
val 104
test 106
Validation loss decreased (inf --> 0.061337).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.061337 --> 0.054165).  Saving model ...
Updating learning rate to 5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H22_Mamba_custom_ftM_sl60_ll30_pl22_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 106
test shape: (106, 22, 6) (106, 22, 6)
test shape: (106, 22, 6) (106, 22, 6)


	mse:0.0811978355050087, mae:0.09533527493476868, rmse:0.2849523425102234, r2:0.6040014326572418, dtw:Not calculated


VAL - MSE: 0.0812, MAE: 0.0953, RMSE: 0.2850, RÂ²: 0.6040, MAPE: 0.26%
Completed: SASOL H=22

Training: Mamba on SASOL for H=50
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H50     Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 50
============================================================
train 2154
val 76
test 78
Validation loss decreased (inf --> 0.057601).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H50_Mamba_custom_ftM_sl60_ll30_pl50_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 78
test shape: (78, 50, 6) (78, 50, 6)
test shape: (78, 50, 6) (78, 50, 6)


	mse:0.08627652376890182, mae:0.09969992935657501, rmse:0.29372864961624146, r2:0.6051149964332581, dtw:Not calculated


VAL - MSE: 0.0863, MAE: 0.0997, RMSE: 0.2937, RÂ²: 0.6051, MAPE: 0.25%
Completed: SASOL H=50

Training: Mamba on SASOL for H=100
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Mamba_SASOL_H100    Model:              Mamba               

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            64                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               16                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Mamba_Exp           Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Mamba_SASOL_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 100
============================================================
train 2104
val 26
test 28
Validation loss decreased (inf --> 0.061818).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.061818 --> 0.060916).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.060916 --> 0.059760).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.059760 --> 0.059050).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (0.059050 --> 0.058738).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (0.058738 --> 0.058603).  Saving model ...
Updating learning rate to 3.125e-06
Validation loss decreased (0.058603 --> 0.058540).  Saving model ...
Updating learning rate to 1.5625e-06
Validation loss decreased (0.058540 --> 0.058511).  Saving model ...
Updating learning rate to 7.8125e-07
Validation loss decreased (0.058511 --> 0.058497).  Saving model ...
Updating learning rate to 3.90625e-07
Validation loss decreased (0.058497 --> 0.058490).  Saving model ...
Updating learning rate to 1.953125e-07
Validation loss decreased (0.058490 --> 0.058487).  Saving model ...
Updating learning rate to 9.765625e-08
Validation loss decreased (0.058487 --> 0.058485).  Saving model ...
Updating learning rate to 4.8828125e-08
Validation loss decreased (0.058485 --> 0.058484).  Saving model ...
Updating learning rate to 2.44140625e-08
Validation loss decreased (0.058484 --> 0.058484).  Saving model ...
Updating learning rate to 1.220703125e-08
Validation loss decreased (0.058484 --> 0.058484).  Saving model ...
Updating learning rate to 6.103515625e-09
Validation loss decreased (0.058484 --> 0.058484).  Saving model ...
Updating learning rate to 3.0517578125e-09
Validation loss decreased (0.058484 --> 0.058484).  Saving model ...
Updating learning rate to 1.52587890625e-09
Validation loss decreased (0.058484 --> 0.058484).  Saving model ...
Updating learning rate to 7.62939453125e-10
Validation loss decreased (0.058484 --> 0.058484).  Saving model ...
Updating learning rate to 3.814697265625e-10
Validation loss decreased (0.058484 --> 0.058484).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Validation loss decreased (0.058484 --> 0.058484).  Saving model ...
Updating learning rate to 9.5367431640625e-11
EarlyStopping counter: 1 out of 5
Updating learning rate to 4.76837158203125e-11
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.384185791015625e-11
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.1920928955078126e-11
EarlyStopping counter: 4 out of 5
Updating learning rate to 5.960464477539063e-12
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Mamba_SASOL_H100_Mamba_custom_ftM_sl60_ll30_pl100_dm64_nh8_el2_dl1_df16_expand2_dc4_fc1_ebtimeF_dtTrue_Mamba_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 28
test shape: (28, 100, 6) (28, 100, 6)
test shape: (28, 100, 6) (28, 100, 6)


	mse:0.08175931870937347, mae:0.0966595858335495, rmse:0.2859358787536621, r2:0.6060883104801178, dtw:Not calculated


VAL - MSE: 0.0818, MAE: 0.0967, RMSE: 0.2859, RÂ²: 0.6061, MAPE: 0.22%
Completed: SASOL H=100

Mamba training completed for all datasets!
