##############################################################################
# Training Informer Model on All Datasets
##############################################################################
Training: Informer on NVIDIA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_030229-uzqxinxe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/uzqxinxe
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NVIDIA_H3  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/uzqxinxe
>>>>>>>start training : long_term_forecast_Informer_NVIDIA_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
Overriding target from 'OT' to 'close' for stock data
val 237
Overriding target from 'OT' to 'close' for stock data
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.3092060 Vali Loss: 0.2380560 Test Loss: 1.0246976
Validation loss decreased (inf --> 0.238056).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.30920603949772685, 'val/loss': 0.23805595375597477, 'test/loss': 1.0246976017951965, '_timestamp': 1762304569.4766722}).
Epoch: 2, Steps: 133 | Train Loss: 0.2474007 Vali Loss: 0.2714066 Test Loss: 1.3219431
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2337844 Vali Loss: 0.1910184 Test Loss: 1.0579908
Validation loss decreased (0.238056 --> 0.191018).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2474006937634676, 'val/loss': 0.27140662260353565, 'test/loss': 1.3219430968165398, '_timestamp': 1762304577.426071}).
Epoch: 4, Steps: 133 | Train Loss: 0.2292633 Vali Loss: 0.1885632 Test Loss: 1.0014596
Validation loss decreased (0.191018 --> 0.188563).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2264398 Vali Loss: 0.1912394 Test Loss: 0.9892681
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2244266 Vali Loss: 0.1869651 Test Loss: 0.9844257
Validation loss decreased (0.188563 --> 0.186965).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2243065 Vali Loss: 0.1859608 Test Loss: 1.0051426
Validation loss decreased (0.186965 --> 0.185961).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2230077 Vali Loss: 0.1906001 Test Loss: 1.0054739
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2231390 Vali Loss: 0.1867145 Test Loss: 0.9870500
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2229142 Vali Loss: 0.1867931 Test Loss: 1.0027333
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2234532 Vali Loss: 0.1893399 Test Loss: 0.9929677
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2232678 Vali Loss: 0.1884977 Test Loss: 1.0085600
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2228132 Vali Loss: 0.1903714 Test Loss: 1.0033847
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2227190 Vali Loss: 0.1849438 Test Loss: 0.9868511
Validation loss decreased (0.185961 --> 0.184944).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2236027 Vali Loss: 0.2043948 Test Loss: 0.9889193
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2229244 Vali Loss: 0.1830884 Test Loss: 1.0019176
Validation loss decreased (0.184944 --> 0.183088).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2230219 Vali Loss: 0.1968854 Test Loss: 0.9982335
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2232155 Vali Loss: 0.1901583 Test Loss: 1.0046927
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2226090 Vali Loss: 0.1877813 Test Loss: 0.9857749
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2224534 Vali Loss: 0.1861462 Test Loss: 1.0015678
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2224628 Vali Loss: 0.1883312 Test Loss: 0.9814591
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2225315 Vali Loss: 0.1831153 Test Loss: 0.9934078
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2227184 Vali Loss: 0.2055530 Test Loss: 0.9996711
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2232706 Vali Loss: 0.1871542 Test Loss: 1.0001120
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2232926 Vali Loss: 0.2071897 Test Loss: 0.9964806
EarlyStopping counter: 9 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2230159 Vali Loss: 0.1883433 Test Loss: 1.0109430
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NVIDIA_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.0011428359430283308, mae:0.025298219174146652, rmse:0.033805858343839645, r2:-0.018764138221740723, dtw:Not calculated


VAL - MSE: 0.0011, MAE: 0.0253, RMSE: 0.0338, RÂ²: -0.0188, MAPE: 81628.88%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.472 MB of 0.473 MB uploadedwandb: \ 0.472 MB of 0.473 MB uploadedwandb: | 0.472 MB of 0.473 MB uploadedwandb: / 0.473 MB of 0.473 MB uploadedwandb: - 0.473 MB of 0.473 MB uploadedwandb: \ 0.473 MB of 0.473 MB uploadedwandb: | 0.473 MB of 0.473 MB uploadedwandb: / 0.473 MB of 0.473 MB uploadedwandb: - 0.654 MB of 0.955 MB uploaded (0.002 MB deduped)wandb: \ 0.877 MB of 0.955 MB uploaded (0.002 MB deduped)wandb: | 0.955 MB of 0.955 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–â–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–ƒâ–â–‚â–ƒâ–ƒâ–ƒâ–â–ƒâ–â–‚â–ƒâ–ƒâ–‚â–„
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–‚â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–‡â–â–…â–ƒâ–‚â–‚â–ƒâ–â–ˆâ–‚â–ˆâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 25
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 1.01094
wandb:                 train/loss 0.22302
wandb:   val/directional_accuracy 50.42194
wandb:                   val/loss 0.18834
wandb:                    val/mae 0.0253
wandb:                   val/mape 8162887.5
wandb:                    val/mse 0.00114
wandb:                     val/r2 -0.01876
wandb:                   val/rmse 0.03381
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/uzqxinxe
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_030229-uzqxinxe/logs
Completed: NVIDIA H=3

Training: Informer on NVIDIA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_030600-zw4dxq7k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/zw4dxq7k
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NVIDIA_H5  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/zw4dxq7k
>>>>>>>start training : long_term_forecast_Informer_NVIDIA_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
Overriding target from 'OT' to 'close' for stock data
val 235
Overriding target from 'OT' to 'close' for stock data
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.3088799 Vali Loss: 0.2126735 Test Loss: 1.1749312
Validation loss decreased (inf --> 0.212673).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2494907 Vali Loss: 0.2267873 Test Loss: 1.2209602
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3088798822092831, 'val/loss': 0.21267345175147057, 'test/loss': 1.1749311871826649, '_timestamp': 1762304773.0263283}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2494907086729107, 'val/loss': 0.22678725607693195, 'test/loss': 1.2209601551294327, '_timestamp': 1762304779.8625498}).
Epoch: 3, Steps: 133 | Train Loss: 0.2376209 Vali Loss: 0.2196487 Test Loss: 1.0592566
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2323134 Vali Loss: 0.2309817 Test Loss: 1.1309684
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2293372 Vali Loss: 0.2040017 Test Loss: 1.0344384
Validation loss decreased (0.212673 --> 0.204002).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2276636 Vali Loss: 0.1995232 Test Loss: 1.0389182
Validation loss decreased (0.204002 --> 0.199523).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2274310 Vali Loss: 0.2024686 Test Loss: 0.9899098
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2268553 Vali Loss: 0.1959058 Test Loss: 1.0096707
Validation loss decreased (0.199523 --> 0.195906).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2263836 Vali Loss: 0.1939921 Test Loss: 1.0091940
Validation loss decreased (0.195906 --> 0.193992).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2276894 Vali Loss: 0.1958617 Test Loss: 1.0071834
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2264030 Vali Loss: 0.1907791 Test Loss: 1.0152418
Validation loss decreased (0.193992 --> 0.190779).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2264520 Vali Loss: 0.2108485 Test Loss: 1.0014269
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2262622 Vali Loss: 0.2250220 Test Loss: 1.0097733
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2268748 Vali Loss: 0.2127704 Test Loss: 1.0067561
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2262875 Vali Loss: 0.1919699 Test Loss: 1.0130792
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2264795 Vali Loss: 0.1923428 Test Loss: 0.9949224
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2265844 Vali Loss: 0.1911963 Test Loss: 0.9981940
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2273657 Vali Loss: 0.2014279 Test Loss: 1.0072907
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2260666 Vali Loss: 0.1890701 Test Loss: 0.9984894
Validation loss decreased (0.190779 --> 0.189070).  Saving model ...
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2267845 Vali Loss: 0.1945339 Test Loss: 1.0050386
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2268446 Vali Loss: 0.1962798 Test Loss: 1.0068675
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2261500 Vali Loss: 0.1902037 Test Loss: 0.9996372
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2263050 Vali Loss: 0.2100888 Test Loss: 0.9889526
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2258584 Vali Loss: 0.1899909 Test Loss: 0.9999033
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2267188 Vali Loss: 0.1910689 Test Loss: 1.0008470
EarlyStopping counter: 6 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2262137 Vali Loss: 0.2009339 Test Loss: 1.0070256
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.2259994 Vali Loss: 0.1922315 Test Loss: 1.0062711
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.2258825 Vali Loss: 0.1913151 Test Loss: 0.9954429
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.2267779 Vali Loss: 0.1998525 Test Loss: 1.0022645
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NVIDIA_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.0011443115072324872, mae:0.02535969205200672, rmse:0.03382767364382744, r2:-0.013489961624145508, dtw:Not calculated


VAL - MSE: 0.0011, MAE: 0.0254, RMSE: 0.0338, RÂ²: -0.0135, MAPE: 56186.22%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.528 MB of 0.528 MB uploadedwandb: \ 0.528 MB of 0.528 MB uploadedwandb: | 0.528 MB of 0.528 MB uploadedwandb: / 0.528 MB of 0.829 MB uploadedwandb: - 0.630 MB of 0.829 MB uploadedwandb: \ 0.829 MB of 0.829 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–„â–ˆâ–ƒâ–ƒâ–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–‚â–â–â–â–‚â–â–‚â–‚â–â–â–â–‚â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–ˆâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–…â–‡â–…â–â–‚â–â–ƒâ–â–‚â–‚â–â–…â–â–â–ƒâ–‚â–â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 28
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 1.00226
wandb:                 train/loss 0.22678
wandb:   val/directional_accuracy 52.44681
wandb:                   val/loss 0.19985
wandb:                    val/mae 0.02536
wandb:                   val/mape 5618621.875
wandb:                    val/mse 0.00114
wandb:                     val/r2 -0.01349
wandb:                   val/rmse 0.03383
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/zw4dxq7k
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_030600-zw4dxq7k/logs
Completed: NVIDIA H=5

Training: Informer on NVIDIA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_030936-aq9rb35m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/aq9rb35m
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NVIDIA_H10 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/aq9rb35m
>>>>>>>start training : long_term_forecast_Informer_NVIDIA_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
Overriding target from 'OT' to 'close' for stock data
val 230
Overriding target from 'OT' to 'close' for stock data
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.3095088 Vali Loss: 0.2174982 Test Loss: 1.3119170
Validation loss decreased (inf --> 0.217498).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2517564 Vali Loss: 0.2116467 Test Loss: 1.2202968
Validation loss decreased (0.217498 --> 0.211647).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3095087679032993, 'val/loss': 0.21749824658036232, 'test/loss': 1.3119170181453228, '_timestamp': 1762304990.0546}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25175636480177255, 'val/loss': 0.2116466760635376, 'test/loss': 1.2202968262135983, '_timestamp': 1762304997.0269845}).
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
Epoch: 3, Steps: 133 | Train Loss: 0.2414633 Vali Loss: 0.2128289 Test Loss: 1.1611637
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2348503 Vali Loss: 0.2205543 Test Loss: 1.0703576
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2329889 Vali Loss: 0.2107502 Test Loss: 1.0236551
Validation loss decreased (0.211647 --> 0.210750).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2323991 Vali Loss: 0.2111027 Test Loss: 1.1336171
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2299460 Vali Loss: 0.2037433 Test Loss: 1.1088780
Validation loss decreased (0.210750 --> 0.203743).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2302424 Vali Loss: 0.2273870 Test Loss: 1.1082921
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2295933 Vali Loss: 0.2094301 Test Loss: 1.0986019
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2288913 Vali Loss: 0.2216856 Test Loss: 1.1133822
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2294751 Vali Loss: 0.2161747 Test Loss: 1.1000018
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2289385 Vali Loss: 0.2083348 Test Loss: 1.0934741
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2291877 Vali Loss: 0.2027972 Test Loss: 1.1118052
Validation loss decreased (0.203743 --> 0.202797).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2284000 Vali Loss: 0.2085745 Test Loss: 1.1088400
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2297049 Vali Loss: 0.2060354 Test Loss: 1.1085798
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2291961 Vali Loss: 0.2210229 Test Loss: 1.1119634
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2287463 Vali Loss: 0.2041144 Test Loss: 1.1061804
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2288550 Vali Loss: 0.2078540 Test Loss: 1.0947288
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2293080 Vali Loss: 0.2066014 Test Loss: 1.1068137
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2292679 Vali Loss: 0.2081053 Test Loss: 1.1141386
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2284530 Vali Loss: 0.2038944 Test Loss: 1.1066880
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2283131 Vali Loss: 0.2036877 Test Loss: 1.1104023
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2291374 Vali Loss: 0.2056676 Test Loss: 1.1068853
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NVIDIA_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.0011649494990706444, mae:0.025623556226491928, rmse:0.034131355583667755, r2:-0.015538334846496582, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0256, RMSE: 0.0341, RÂ²: -0.0155, MAPE: 68076.03%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.552 MB of 0.552 MB uploadedwandb: \ 0.552 MB of 0.552 MB uploadedwandb: | 0.552 MB of 0.552 MB uploadedwandb: / 0.552 MB of 0.552 MB uploadedwandb: - 0.552 MB of 0.552 MB uploadedwandb: \ 0.552 MB of 0.852 MB uploadedwandb: | 0.552 MB of 0.852 MB uploadedwandb: / 0.852 MB of 0.852 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–‡â–…â–…â–…â–†â–…â–…â–…â–…â–…â–…â–…â–…â–…â–†â–…â–…â–…
wandb:                 train/loss â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–‚â–â–â–â–‚â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–†â–ƒâ–ƒâ–â–ˆâ–ƒâ–†â–…â–ƒâ–â–ƒâ–‚â–†â–â–‚â–‚â–ƒâ–â–â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 1.10689
wandb:                 train/loss 0.22914
wandb:   val/directional_accuracy 51.01449
wandb:                   val/loss 0.20567
wandb:                    val/mae 0.02562
wandb:                   val/mape 6807603.125
wandb:                    val/mse 0.00116
wandb:                     val/r2 -0.01554
wandb:                   val/rmse 0.03413
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/aq9rb35m
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_030936-aq9rb35m/logs
Completed: NVIDIA H=10

Training: Informer on NVIDIA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_031244-zmol2uvy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/zmol2uvy
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NVIDIA_H22 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/zmol2uvy
>>>>>>>start training : long_term_forecast_Informer_NVIDIA_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
Overriding target from 'OT' to 'close' for stock data
val 218
Overriding target from 'OT' to 'close' for stock data
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.3124645 Vali Loss: 0.2080685 Test Loss: 1.3491376
Validation loss decreased (inf --> 0.208068).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2560030 Vali Loss: 0.2631267 Test Loss: 1.3937419
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3124644568923748, 'val/loss': 0.2080684666122709, 'test/loss': 1.3491375616618566, '_timestamp': 1762305176.6718762}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25600296247637633, 'val/loss': 0.26312669473034994, 'test/loss': 1.393741888659341, '_timestamp': 1762305184.1872962}).
Epoch: 3, Steps: 132 | Train Loss: 0.2445472 Vali Loss: 0.2584841 Test Loss: 1.4219230
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2394854 Vali Loss: 0.2260484 Test Loss: 1.2294880
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2362609 Vali Loss: 0.2314446 Test Loss: 1.2302748
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2349389 Vali Loss: 0.2291546 Test Loss: 1.2632315
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2337507 Vali Loss: 0.2278294 Test Loss: 1.2560026
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2335231 Vali Loss: 0.2297235 Test Loss: 1.2424120
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2332621 Vali Loss: 0.2287347 Test Loss: 1.2431713
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2333210 Vali Loss: 0.2285325 Test Loss: 1.2363816
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2330805 Vali Loss: 0.2257034 Test Loss: 1.2396910
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NVIDIA_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.0012054102262482047, mae:0.026016805320978165, rmse:0.034719016402959824, r2:-0.021659135818481445, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0260, RMSE: 0.0347, RÂ²: -0.0217, MAPE: 183402.66%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.587 MB of 0.588 MB uploadedwandb: \ 0.587 MB of 0.588 MB uploadedwandb: | 0.587 MB of 0.588 MB uploadedwandb: / 0.588 MB of 0.588 MB uploadedwandb: - 0.588 MB of 0.887 MB uploadedwandb: \ 0.812 MB of 0.887 MB uploadedwandb: | 0.887 MB of 0.887 MB uploadedwandb: / 0.887 MB of 0.887 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–â–‚â–‚â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–‚â–‚â–â–‚â–‚â–‚â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 1.23969
wandb:                 train/loss 0.23308
wandb:   val/directional_accuracy 50.41503
wandb:                   val/loss 0.2257
wandb:                    val/mae 0.02602
wandb:                   val/mape 18340265.625
wandb:                    val/mse 0.00121
wandb:                     val/r2 -0.02166
wandb:                   val/rmse 0.03472
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/zmol2uvy
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_031244-zmol2uvy/logs
Completed: NVIDIA H=22

Training: Informer on NVIDIA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_031424-xmbsc3vh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/xmbsc3vh
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NVIDIA_H50 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/xmbsc3vh
>>>>>>>start training : long_term_forecast_Informer_NVIDIA_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
Overriding target from 'OT' to 'close' for stock data
val 190
Overriding target from 'OT' to 'close' for stock data
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.3227505 Vali Loss: 0.2323637 Test Loss: 1.4034649
Validation loss decreased (inf --> 0.232364).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2645811 Vali Loss: 0.2669290 Test Loss: 1.4302206
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3227505406195467, 'val/loss': 0.23236369093259177, 'test/loss': 1.403464913368225, '_timestamp': 1762305276.4091413}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2645810672053785, 'val/loss': 0.2669290279348691, 'test/loss': 1.4302205840746562, '_timestamp': 1762305283.3198574}).
Epoch: 3, Steps: 132 | Train Loss: 0.2527827 Vali Loss: 0.2723293 Test Loss: 1.5488678
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2468794 Vali Loss: 0.2785339 Test Loss: 1.4696587
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2424194 Vali Loss: 0.2730148 Test Loss: 1.4608701
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2429743 Vali Loss: 0.2732433 Test Loss: 1.4534732
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2440783 Vali Loss: 0.2617324 Test Loss: 1.4540254
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2409047 Vali Loss: 0.2876254 Test Loss: 1.4588776
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2459267 Vali Loss: 0.2682693 Test Loss: 1.4607386
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2392665 Vali Loss: 0.2808965 Test Loss: 1.4448952
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2438209 Vali Loss: 0.2821435 Test Loss: 1.4505248
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NVIDIA_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.0012216826435178518, mae:0.026568999513983727, rmse:0.03495257720351219, r2:-0.02663099765777588, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0266, RMSE: 0.0350, RÂ²: -0.0266, MAPE: 240489.83%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.625 MB of 0.627 MB uploadedwandb: \ 0.625 MB of 0.627 MB uploadedwandb: | 0.625 MB of 0.627 MB uploadedwandb: / 0.627 MB of 0.627 MB uploadedwandb: - 0.627 MB of 0.926 MB uploadedwandb: \ 0.851 MB of 0.926 MB uploadedwandb: | 0.926 MB of 0.926 MB uploadedwandb: / 0.926 MB of 0.926 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–ƒâ–‚â–„â–â–ƒ
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–†â–„â–„â–â–ˆâ–ƒâ–†â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 1.45052
wandb:                 train/loss 0.24382
wandb:   val/directional_accuracy 50.12889
wandb:                   val/loss 0.28214
wandb:                    val/mae 0.02657
wandb:                   val/mape 24048982.8125
wandb:                    val/mse 0.00122
wandb:                     val/r2 -0.02663
wandb:                   val/rmse 0.03495
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/xmbsc3vh
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_031424-xmbsc3vh/logs
Completed: NVIDIA H=50

Training: Informer on NVIDIA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_031608-w423lzun
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/w423lzun
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NVIDIA_H100Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/w423lzun
>>>>>>>start training : long_term_forecast_Informer_NVIDIA_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
Overriding target from 'OT' to 'close' for stock data
val 140
Overriding target from 'OT' to 'close' for stock data
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.3292045 Vali Loss: 0.3003460 Test Loss: 1.4684297
Validation loss decreased (inf --> 0.300346).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2681503 Vali Loss: 0.3287699 Test Loss: 1.7734007
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3292044738164315, 'val/loss': 0.30034598112106325, 'test/loss': 1.468429684638977, '_timestamp': 1762305380.8404167}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2681502579496457, 'val/loss': 0.32876994609832766, 'test/loss': 1.7734006881713866, '_timestamp': 1762305387.7194517}).
Epoch: 3, Steps: 130 | Train Loss: 0.2529098 Vali Loss: 0.2481800 Test Loss: 1.6256946
Validation loss decreased (0.300346 --> 0.248180).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2475293 Vali Loss: 0.2737892 Test Loss: 1.6143546
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2441911 Vali Loss: 0.2637871 Test Loss: 1.5919721
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2428251 Vali Loss: 0.2610474 Test Loss: 1.6208941
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2420136 Vali Loss: 0.2620569 Test Loss: 1.5812330
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2416571 Vali Loss: 0.2623961 Test Loss: 1.5932130
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2410617 Vali Loss: 0.2633692 Test Loss: 1.5860764
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2413663 Vali Loss: 0.2571011 Test Loss: 1.5819991
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2416060 Vali Loss: 0.2616259 Test Loss: 1.5889952
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2408569 Vali Loss: 0.2602394 Test Loss: 1.5882866
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2411081 Vali Loss: 0.2559140 Test Loss: 1.5861159
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NVIDIA_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0012924664188176394, mae:0.027468366548419, rmse:0.03595088794827461, r2:-0.0039234161376953125, dtw:Not calculated


VAL - MSE: 0.0013, MAE: 0.0275, RMSE: 0.0360, RÂ²: -0.0039, MAPE: 82435.88%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.645 MB of 0.650 MB uploadedwandb: \ 0.645 MB of 0.650 MB uploadedwandb: | 0.650 MB of 0.650 MB uploadedwandb: / 0.650 MB of 0.650 MB uploadedwandb: - 0.650 MB of 0.948 MB uploadedwandb: \ 0.874 MB of 0.948 MB uploadedwandb: | 0.948 MB of 0.948 MB uploadedwandb: / 0.948 MB of 0.948 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–†â–ƒâ–‡â–â–ƒâ–‚â–â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ˆâ–…â–…â–…â–…â–…â–ƒâ–…â–„â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 1.58612
wandb:                 train/loss 0.24111
wandb:   val/directional_accuracy 49.84127
wandb:                   val/loss 0.25591
wandb:                    val/mae 0.02747
wandb:                   val/mape 8243588.28125
wandb:                    val/mse 0.00129
wandb:                     val/r2 -0.00392
wandb:                   val/rmse 0.03595
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/w423lzun
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_031608-w423lzun/logs
Completed: NVIDIA H=100

Training: Informer on APPLE for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_031803-akgviuyg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/akgviuyg
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_APPLE_H3   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/akgviuyg
>>>>>>>start training : long_term_forecast_Informer_APPLE_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
Overriding target from 'OT' to 'close' for stock data
val 237
Overriding target from 'OT' to 'close' for stock data
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.2922156 Vali Loss: 0.1147579 Test Loss: 0.1703242
Validation loss decreased (inf --> 0.114758).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2378728 Vali Loss: 0.1050312 Test Loss: 0.1633980
Validation loss decreased (0.114758 --> 0.105031).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2922155868290062, 'val/loss': 0.11475793737918139, 'test/loss': 0.1703242240473628, '_timestamp': 1762305496.110952}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23787277163867665, 'val/loss': 0.10503115225583315, 'test/loss': 0.16339801903814077, '_timestamp': 1762305503.0424814}).
Epoch: 3, Steps: 133 | Train Loss: 0.2225949 Vali Loss: 0.0889896 Test Loss: 0.1399977
Validation loss decreased (0.105031 --> 0.088990).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2169182 Vali Loss: 0.0879375 Test Loss: 0.1436581
Validation loss decreased (0.088990 --> 0.087937).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2161013 Vali Loss: 0.0882302 Test Loss: 0.1436318
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2134960 Vali Loss: 0.0865499 Test Loss: 0.1431155
Validation loss decreased (0.087937 --> 0.086550).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2124676 Vali Loss: 0.0921083 Test Loss: 0.1470269
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2122261 Vali Loss: 0.0939272 Test Loss: 0.1460572
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2119317 Vali Loss: 0.0899646 Test Loss: 0.1445557
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2112378 Vali Loss: 0.0880914 Test Loss: 0.1448592
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2118892 Vali Loss: 0.0896899 Test Loss: 0.1450607
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2117585 Vali Loss: 0.0876455 Test Loss: 0.1434217
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2114312 Vali Loss: 0.0881035 Test Loss: 0.1451220
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2108938 Vali Loss: 0.0911144 Test Loss: 0.1483470
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2117369 Vali Loss: 0.0878061 Test Loss: 0.1444484
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2117422 Vali Loss: 0.0891685 Test Loss: 0.1460922
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_APPLE_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.00020294837304390967, mae:0.010403035208582878, rmse:0.014245995320379734, r2:-0.015032410621643066, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0104, RMSE: 0.0142, RÂ²: -0.0150, MAPE: 447545.56%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.466 MB of 0.466 MB uploadedwandb: \ 0.466 MB of 0.466 MB uploadedwandb: | 0.466 MB of 0.466 MB uploadedwandb: / 0.466 MB of 0.466 MB uploadedwandb: - 0.466 MB of 0.466 MB uploadedwandb: \ 0.466 MB of 0.466 MB uploadedwandb: | 0.466 MB of 0.466 MB uploadedwandb: / 0.466 MB of 0.466 MB uploadedwandb: - 0.466 MB of 0.466 MB uploadedwandb: \ 0.648 MB of 0.946 MB uploaded (0.002 MB deduped)wandb: | 0.946 MB of 0.946 MB uploaded (0.002 MB deduped)wandb: / 0.946 MB of 0.946 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–„â–„â–„â–‡â–†â–…â–…â–…â–„â–…â–ˆâ–…â–†
wandb:                 train/loss â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–‚â–ƒâ–â–†â–ˆâ–„â–‚â–„â–‚â–‚â–…â–‚â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.14609
wandb:                 train/loss 0.21174
wandb:   val/directional_accuracy 48.94515
wandb:                   val/loss 0.08917
wandb:                    val/mae 0.0104
wandb:                   val/mape 44754556.25
wandb:                    val/mse 0.0002
wandb:                     val/r2 -0.01503
wandb:                   val/rmse 0.01425
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/akgviuyg
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_031803-akgviuyg/logs
Completed: APPLE H=3

Training: Informer on APPLE for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_032028-8kocvh06
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/8kocvh06
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_APPLE_H5   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/8kocvh06
>>>>>>>start training : long_term_forecast_Informer_APPLE_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
Overriding target from 'OT' to 'close' for stock data
val 235
Overriding target from 'OT' to 'close' for stock data
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.2905987 Vali Loss: 0.1105205 Test Loss: 0.1697174
Validation loss decreased (inf --> 0.110521).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2386013 Vali Loss: 0.0952029 Test Loss: 0.1578874
Validation loss decreased (0.110521 --> 0.095203).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29059874742550956, 'val/loss': 0.11052053235471249, 'test/loss': 0.16971737518906593, '_timestamp': 1762305641.4534452}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23860133243234535, 'val/loss': 0.09520288370549679, 'test/loss': 0.15788735076785088, '_timestamp': 1762305648.4217858}).
Epoch: 3, Steps: 133 | Train Loss: 0.2252904 Vali Loss: 0.0923672 Test Loss: 0.1549956
Validation loss decreased (0.095203 --> 0.092367).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2190220 Vali Loss: 0.0923205 Test Loss: 0.1632948
Validation loss decreased (0.092367 --> 0.092321).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2163612 Vali Loss: 0.0993910 Test Loss: 0.1664035
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2148466 Vali Loss: 0.0876486 Test Loss: 0.1515087
Validation loss decreased (0.092321 --> 0.087649).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2145117 Vali Loss: 0.0954575 Test Loss: 0.1599948
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2137641 Vali Loss: 0.0939172 Test Loss: 0.1579608
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2131654 Vali Loss: 0.0956938 Test Loss: 0.1588526
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2139829 Vali Loss: 0.0929720 Test Loss: 0.1586075
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2131324 Vali Loss: 0.0943425 Test Loss: 0.1606341
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2126471 Vali Loss: 0.0925928 Test Loss: 0.1579569
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2137359 Vali Loss: 0.0913902 Test Loss: 0.1571965
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2135216 Vali Loss: 0.0908108 Test Loss: 0.1553530
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2133246 Vali Loss: 0.0951709 Test Loss: 0.1588648
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2137727 Vali Loss: 0.0951981 Test Loss: 0.1582341
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_APPLE_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.00020267305080778897, mae:0.01030613575130701, rmse:0.014236328192055225, r2:-0.00902855396270752, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0103, RMSE: 0.0142, RÂ²: -0.0090, MAPE: 595272.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.497 MB of 0.498 MB uploadedwandb: \ 0.497 MB of 0.498 MB uploadedwandb: | 0.497 MB of 0.498 MB uploadedwandb: / 0.497 MB of 0.498 MB uploadedwandb: - 0.498 MB of 0.498 MB uploadedwandb: \ 0.498 MB of 0.498 MB uploadedwandb: | 0.498 MB of 0.797 MB uploadedwandb: / 0.797 MB of 0.797 MB uploadedwandb: - 0.797 MB of 0.797 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ƒâ–‡â–ˆâ–â–…â–„â–„â–„â–…â–„â–„â–ƒâ–„â–„
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–‚â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–„â–ˆâ–â–†â–…â–†â–„â–…â–„â–ƒâ–ƒâ–…â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.15823
wandb:                 train/loss 0.21377
wandb:   val/directional_accuracy 48.61702
wandb:                   val/loss 0.0952
wandb:                    val/mae 0.01031
wandb:                   val/mape 59527200.0
wandb:                    val/mse 0.0002
wandb:                     val/r2 -0.00903
wandb:                   val/rmse 0.01424
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/8kocvh06
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_032028-8kocvh06/logs
Completed: APPLE H=5

Training: Informer on APPLE for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_032250-bvif3gg6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/bvif3gg6
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_APPLE_H10  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/bvif3gg6
>>>>>>>start training : long_term_forecast_Informer_APPLE_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
Overriding target from 'OT' to 'close' for stock data
val 230
Overriding target from 'OT' to 'close' for stock data
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2938028 Vali Loss: 0.1565434 Test Loss: 0.2019399
Validation loss decreased (inf --> 0.156543).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2391534 Vali Loss: 0.0970681 Test Loss: 0.1630490
Validation loss decreased (0.156543 --> 0.097068).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29380276407066147, 'val/loss': 0.1565434318035841, 'test/loss': 0.20193985477089882, '_timestamp': 1762305783.6345725}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23915335379148783, 'val/loss': 0.0970681281760335, 'test/loss': 0.16304897982627153, '_timestamp': 1762305790.5745957}).
Epoch: 3, Steps: 133 | Train Loss: 0.2272293 Vali Loss: 0.1019239 Test Loss: 0.1717372
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2206655 Vali Loss: 0.1256890 Test Loss: 0.1978164
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2179055 Vali Loss: 0.1093152 Test Loss: 0.1764414
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2187782 Vali Loss: 0.1164904 Test Loss: 0.1884973
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2160711 Vali Loss: 0.1140403 Test Loss: 0.1835495
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2154185 Vali Loss: 0.1125082 Test Loss: 0.1830731
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2153626 Vali Loss: 0.1104016 Test Loss: 0.1818498
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2155271 Vali Loss: 0.1082092 Test Loss: 0.1802437
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2151648 Vali Loss: 0.1128537 Test Loss: 0.1812024
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2150210 Vali Loss: 0.1113462 Test Loss: 0.1793818
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_APPLE_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.00021162768825888634, mae:0.010701249353587627, rmse:0.014547429047524929, r2:-0.04524672031402588, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0107, RMSE: 0.0145, RÂ²: -0.0452, MAPE: 549163.44%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.576 MB of 0.577 MB uploadedwandb: \ 0.576 MB of 0.577 MB uploadedwandb: | 0.577 MB of 0.577 MB uploadedwandb: / 0.577 MB of 0.577 MB uploadedwandb: - 0.577 MB of 0.875 MB uploadedwandb: \ 0.875 MB of 0.875 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–‚â–…â–„â–„â–„â–ƒâ–„â–ƒ
wandb:                 train/loss â–ˆâ–„â–ƒâ–ƒâ–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ˆâ–ƒâ–…â–…â–„â–ƒâ–ƒâ–„â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.17938
wandb:                 train/loss 0.21502
wandb:   val/directional_accuracy 49.13043
wandb:                   val/loss 0.11135
wandb:                    val/mae 0.0107
wandb:                   val/mape 54916343.75
wandb:                    val/mse 0.00021
wandb:                     val/r2 -0.04525
wandb:                   val/rmse 0.01455
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/bvif3gg6
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_032250-bvif3gg6/logs
Completed: APPLE H=10

Training: Informer on APPLE for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_032439-tk15quey
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/tk15quey
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_APPLE_H22  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/tk15quey
>>>>>>>start training : long_term_forecast_Informer_APPLE_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
Overriding target from 'OT' to 'close' for stock data
val 218
Overriding target from 'OT' to 'close' for stock data
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2957530 Vali Loss: 0.1368075 Test Loss: 0.2058260
Validation loss decreased (inf --> 0.136808).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2418173 Vali Loss: 0.1432907 Test Loss: 0.2234380
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2957529964094812, 'val/loss': 0.13680750557354518, 'test/loss': 0.20582603131021773, '_timestamp': 1762305892.1547596}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2418173352877299, 'val/loss': 0.1432907176869256, 'test/loss': 0.2234379713024412, '_timestamp': 1762305899.055002}).
Epoch: 3, Steps: 132 | Train Loss: 0.2302290 Vali Loss: 0.1132934 Test Loss: 0.2000080
Validation loss decreased (0.136808 --> 0.113293).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2242487 Vali Loss: 0.1056552 Test Loss: 0.1887745
Validation loss decreased (0.113293 --> 0.105655).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2208817 Vali Loss: 0.1137131 Test Loss: 0.1959163
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2196386 Vali Loss: 0.1184971 Test Loss: 0.2051382
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2184147 Vali Loss: 0.1152359 Test Loss: 0.2016837
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2179762 Vali Loss: 0.1196039 Test Loss: 0.2059022
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2179131 Vali Loss: 0.1189053 Test Loss: 0.2049301
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2176473 Vali Loss: 0.1174720 Test Loss: 0.2031711
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2180359 Vali Loss: 0.1175962 Test Loss: 0.2036629
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2176194 Vali Loss: 0.1194625 Test Loss: 0.2050541
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2178806 Vali Loss: 0.1174906 Test Loss: 0.2038715
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2173981 Vali Loss: 0.1180824 Test Loss: 0.2033362
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_APPLE_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.0002095503150485456, mae:0.010385771282017231, rmse:0.014475853182375431, r2:-0.009859204292297363, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0104, RMSE: 0.0145, RÂ²: -0.0099, MAPE: 702445.94%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.615 MB of 0.616 MB uploadedwandb: \ 0.615 MB of 0.616 MB uploadedwandb: | 0.616 MB of 0.616 MB uploadedwandb: / 0.616 MB of 0.616 MB uploadedwandb: - 0.616 MB of 0.915 MB uploadedwandb: \ 0.915 MB of 0.915 MB uploadedwandb: | 0.915 MB of 0.915 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–†â–â–„â–ˆâ–†â–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–‡
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–â–…â–‡â–†â–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 13
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.20334
wandb:                 train/loss 0.2174
wandb:   val/directional_accuracy 48.88598
wandb:                   val/loss 0.11808
wandb:                    val/mae 0.01039
wandb:                   val/mape 70244593.75
wandb:                    val/mse 0.00021
wandb:                     val/r2 -0.00986
wandb:                   val/rmse 0.01448
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/tk15quey
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_032439-tk15quey/logs
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread IntMsgThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 268, in check_network_status
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 300, in check_internal_messages
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 856, in deliver_internal_messages
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 848, in deliver_network_status
    return self._deliver_internal_messages(internal_message)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 516, in _deliver_internal_messages
    return self._deliver_network_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 510, in _deliver_network_status
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: APPLE H=22

Training: Informer on APPLE for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_032642-amuymode
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/amuymode
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_APPLE_H50  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/amuymode
>>>>>>>start training : long_term_forecast_Informer_APPLE_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
Overriding target from 'OT' to 'close' for stock data
val 190
Overriding target from 'OT' to 'close' for stock data
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.3044765 Vali Loss: 0.1637878 Test Loss: 0.2484590
Validation loss decreased (inf --> 0.163788).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2526245 Vali Loss: 0.1601486 Test Loss: 0.2563867
Validation loss decreased (0.163788 --> 0.160149).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3044765108462536, 'val/loss': 0.16378780951102576, 'test/loss': 0.24845898896455765, '_timestamp': 1762306016.3972366}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2526245007686543, 'val/loss': 0.1601485734184583, 'test/loss': 0.2563866724570592, '_timestamp': 1762306023.2909188}).
Epoch: 3, Steps: 132 | Train Loss: 0.2389021 Vali Loss: 0.2102720 Test Loss: 0.3305549
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2315994 Vali Loss: 0.1448494 Test Loss: 0.2599009
Validation loss decreased (0.160149 --> 0.144849).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2266322 Vali Loss: 0.1590491 Test Loss: 0.2755797
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2255001 Vali Loss: 0.1658842 Test Loss: 0.2886767
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2246250 Vali Loss: 0.1613273 Test Loss: 0.2801130
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2256419 Vali Loss: 0.1563252 Test Loss: 0.2788573
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2248913 Vali Loss: 0.1562273 Test Loss: 0.2790637
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2255531 Vali Loss: 0.1678455 Test Loss: 0.2896984
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2233221 Vali Loss: 0.1561967 Test Loss: 0.2764835
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2252084 Vali Loss: 0.1438804 Test Loss: 0.2607545
Validation loss decreased (0.144849 --> 0.143880).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2246078 Vali Loss: 0.1514975 Test Loss: 0.2718000
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2259612 Vali Loss: 0.1475021 Test Loss: 0.2650389
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2240118 Vali Loss: 0.1548398 Test Loss: 0.2730062
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2233496 Vali Loss: 0.1585662 Test Loss: 0.2778989
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2265753 Vali Loss: 0.1488764 Test Loss: 0.2674011
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2242210 Vali Loss: 0.1517245 Test Loss: 0.2732539
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2241057 Vali Loss: 0.1622620 Test Loss: 0.2835132
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2258201 Vali Loss: 0.1512603 Test Loss: 0.2735732
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2241747 Vali Loss: 0.1560215 Test Loss: 0.2755167
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2234974 Vali Loss: 0.1612889 Test Loss: 0.2821690
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_APPLE_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.00022344326134771109, mae:0.010749193839728832, rmse:0.014948018826544285, r2:-0.007068157196044922, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0107, RMSE: 0.0149, RÂ²: -0.0071, MAPE: 186315.95%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.678 MB of 0.681 MB uploadedwandb: \ 0.678 MB of 0.681 MB uploadedwandb: | 0.678 MB of 0.681 MB uploadedwandb: / 0.681 MB of 0.681 MB uploadedwandb: - 0.681 MB of 0.681 MB uploadedwandb: \ 0.681 MB of 0.981 MB uploadedwandb: | 0.981 MB of 0.981 MB uploadedwandb: / 0.981 MB of 0.981 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–ƒâ–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–â–‚â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–…â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–‚â–â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–ƒâ–ƒâ–ƒâ–‚â–‚â–„â–‚â–â–‚â–â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.28217
wandb:                 train/loss 0.2235
wandb:   val/directional_accuracy 49.41998
wandb:                   val/loss 0.16129
wandb:                    val/mae 0.01075
wandb:                   val/mape 18631595.3125
wandb:                    val/mse 0.00022
wandb:                     val/r2 -0.00707
wandb:                   val/rmse 0.01495
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/amuymode
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_032642-amuymode/logs
Completed: APPLE H=50

Training: Informer on APPLE for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_032941-ul7ayxss
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/ul7ayxss
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_APPLE_H100 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/ul7ayxss
>>>>>>>start training : long_term_forecast_Informer_APPLE_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
Overriding target from 'OT' to 'close' for stock data
val 140
Overriding target from 'OT' to 'close' for stock data
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.3123793 Vali Loss: 0.1471374 Test Loss: 0.2406861
Validation loss decreased (inf --> 0.147137).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2573049 Vali Loss: 0.1587906 Test Loss: 0.2828564
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.31237930139670006, 'val/loss': 0.1471373587846756, 'test/loss': 0.24068609476089478, '_timestamp': 1762306194.400166}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2573048817423674, 'val/loss': 0.15879055261611938, 'test/loss': 0.28285637497901917, '_timestamp': 1762306201.2796772}).
Epoch: 3, Steps: 130 | Train Loss: 0.2420651 Vali Loss: 0.1685897 Test Loss: 0.3275291
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2342587 Vali Loss: 0.1845242 Test Loss: 0.3379420
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2303795 Vali Loss: 0.1719026 Test Loss: 0.3202917
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2285940 Vali Loss: 0.1589687 Test Loss: 0.3107028
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2282051 Vali Loss: 0.1670168 Test Loss: 0.3203242
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2271586 Vali Loss: 0.1644156 Test Loss: 0.3146081
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2268982 Vali Loss: 0.1667482 Test Loss: 0.3175278
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2273876 Vali Loss: 0.1644212 Test Loss: 0.3147588
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2269666 Vali Loss: 0.1643736 Test Loss: 0.3203591
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_APPLE_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.00024024413141887635, mae:0.01111440360546112, rmse:0.015499810688197613, r2:-0.0175015926361084, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0111, RMSE: 0.0155, RÂ²: -0.0175, MAPE: 191756.59%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.701 MB of 0.706 MB uploadedwandb: \ 0.701 MB of 0.706 MB uploadedwandb: | 0.701 MB of 0.706 MB uploadedwandb: / 0.706 MB of 0.706 MB uploadedwandb: - 0.706 MB of 1.005 MB uploadedwandb: \ 1.005 MB of 1.005 MB uploadedwandb: | 1.005 MB of 1.005 MB uploadedwandb: / 1.005 MB of 1.005 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–ˆâ–ƒâ–â–ƒâ–‚â–ƒâ–‚â–ƒ
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–ˆâ–…â–â–ƒâ–‚â–ƒâ–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.32036
wandb:                 train/loss 0.22697
wandb:   val/directional_accuracy 50.51948
wandb:                   val/loss 0.16437
wandb:                    val/mae 0.01111
wandb:                   val/mape 19175659.375
wandb:                    val/mse 0.00024
wandb:                     val/r2 -0.0175
wandb:                   val/rmse 0.0155
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/ul7ayxss
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_032941-ul7ayxss/logs
Completed: APPLE H=100

Training: Informer on SP500 for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_033125-9gu2qfgj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/9gu2qfgj
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SP500_H3   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/9gu2qfgj
>>>>>>>start training : long_term_forecast_Informer_SP500_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 3
============================================================
train 4241
Overriding target from 'OT' to 'close' for stock data
val 237
Overriding target from 'OT' to 'close' for stock data
test 238
Epoch: 1, Steps: 133 | Train Loss: 0.2328217 Vali Loss: 0.0771658 Test Loss: 0.1065681
Validation loss decreased (inf --> 0.077166).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.1930351 Vali Loss: 0.0890776 Test Loss: 0.0899140
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23282170318123094, 'val/loss': 0.07716578524559736, 'test/loss': 0.10656814649701118, '_timestamp': 1762306297.0950553}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19303514298639798, 'val/loss': 0.0890776002779603, 'test/loss': 0.0899139535613358, '_timestamp': 1762306304.1232655}).
Epoch: 3, Steps: 133 | Train Loss: 0.1805724 Vali Loss: 0.0774933 Test Loss: 0.0830101
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1781451 Vali Loss: 0.0678367 Test Loss: 0.0947292
Validation loss decreased (0.077166 --> 0.067837).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1748422 Vali Loss: 0.0662410 Test Loss: 0.0918911
Validation loss decreased (0.067837 --> 0.066241).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1726583 Vali Loss: 0.0672496 Test Loss: 0.1015660
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1725486 Vali Loss: 0.0675485 Test Loss: 0.1007954
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1726433 Vali Loss: 0.0681199 Test Loss: 0.0931702
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1722620 Vali Loss: 0.0658914 Test Loss: 0.0975978
Validation loss decreased (0.066241 --> 0.065891).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1714426 Vali Loss: 0.0658940 Test Loss: 0.0978049
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1715177 Vali Loss: 0.0663239 Test Loss: 0.0969116
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1712462 Vali Loss: 0.0667784 Test Loss: 0.0939665
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1722387 Vali Loss: 0.0674278 Test Loss: 0.0917517
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1722479 Vali Loss: 0.0654183 Test Loss: 0.0953059
Validation loss decreased (0.065891 --> 0.065418).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1719046 Vali Loss: 0.0695579 Test Loss: 0.0897716
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1715537 Vali Loss: 0.0650503 Test Loss: 0.0940280
Validation loss decreased (0.065418 --> 0.065050).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1710894 Vali Loss: 0.0671257 Test Loss: 0.0940635
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1721112 Vali Loss: 0.0669999 Test Loss: 0.0906439
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1717837 Vali Loss: 0.0662723 Test Loss: 0.0973469
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1712260 Vali Loss: 0.0654243 Test Loss: 0.0964270
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1711499 Vali Loss: 0.0660003 Test Loss: 0.0953470
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1733448 Vali Loss: 0.0673440 Test Loss: 0.0946967
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1712804 Vali Loss: 0.0681719 Test Loss: 0.0946613
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.1703343 Vali Loss: 0.0662738 Test Loss: 0.0924123
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.1711989 Vali Loss: 0.0665471 Test Loss: 0.0921020
EarlyStopping counter: 9 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.1718027 Vali Loss: 0.0652404 Test Loss: 0.0921195
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SP500_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 238
test shape: (238, 3, 1) (238, 3, 1)
test shape: (238, 3, 1) (238, 3, 1)


	mse:6.829459744039923e-05, mae:0.0062713162042200565, rmse:0.008264054544270039, r2:-0.050504207611083984, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0063, RMSE: 0.0083, RÂ²: -0.0505, MAPE: 3.02%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.487 MB of 0.487 MB uploadedwandb: \ 0.487 MB of 0.487 MB uploadedwandb: | 0.487 MB of 0.487 MB uploadedwandb: / 0.487 MB of 0.487 MB uploadedwandb: - 0.487 MB of 0.487 MB uploadedwandb: \ 0.487 MB of 0.487 MB uploadedwandb: | 0.487 MB of 0.487 MB uploadedwandb: / 0.669 MB of 0.969 MB uploaded (0.002 MB deduped)wandb: - 0.895 MB of 0.969 MB uploaded (0.002 MB deduped)wandb: \ 0.969 MB of 0.969 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–…â–„â–ˆâ–ˆâ–…â–‡â–‡â–†â–…â–„â–†â–„â–…â–…â–„â–†â–†â–†â–…â–…â–…â–„â–„
wandb:                 train/loss â–ˆâ–†â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–‚â–‚â–‚â–ƒâ–â–â–‚â–‚â–‚â–â–„â–â–‚â–‚â–‚â–â–‚â–‚â–ƒâ–‚â–‚â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 25
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.09212
wandb:                 train/loss 0.1718
wandb:   val/directional_accuracy 49.57983
wandb:                   val/loss 0.06524
wandb:                    val/mae 0.00627
wandb:                   val/mape 301.56078
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.0505
wandb:                   val/rmse 0.00826
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/9gu2qfgj
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_033125-9gu2qfgj/logs
Completed: SP500 H=3

Training: Informer on SP500 for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_033447-tzxnf9b8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/tzxnf9b8
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SP500_H5   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/tzxnf9b8
>>>>>>>start training : long_term_forecast_Informer_SP500_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 5
============================================================
train 4239
Overriding target from 'OT' to 'close' for stock data
val 235
Overriding target from 'OT' to 'close' for stock data
test 236
Epoch: 1, Steps: 133 | Train Loss: 0.2365279 Vali Loss: 0.0943376 Test Loss: 0.1295982
Validation loss decreased (inf --> 0.094338).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.1898448 Vali Loss: 0.1098489 Test Loss: 0.0842293
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2365279180326856, 'val/loss': 0.09433760493993759, 'test/loss': 0.1295982301235199, '_timestamp': 1762306500.8999636}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.18984482958352655, 'val/loss': 0.10984888393431902, 'test/loss': 0.08422930911183357, '_timestamp': 1762306507.9718552}).
Epoch: 3, Steps: 133 | Train Loss: 0.1831729 Vali Loss: 0.0684575 Test Loss: 0.0988840
Validation loss decreased (0.094338 --> 0.068458).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1810830 Vali Loss: 0.0717599 Test Loss: 0.1116560
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1782939 Vali Loss: 0.0689982 Test Loss: 0.0845443
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1768299 Vali Loss: 0.0689010 Test Loss: 0.0856104
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1762901 Vali Loss: 0.0709778 Test Loss: 0.0886883
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1760923 Vali Loss: 0.0683220 Test Loss: 0.0879841
Validation loss decreased (0.068458 --> 0.068322).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1763813 Vali Loss: 0.0692946 Test Loss: 0.0899652
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1768175 Vali Loss: 0.0670791 Test Loss: 0.0890634
Validation loss decreased (0.068322 --> 0.067079).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1763470 Vali Loss: 0.0699048 Test Loss: 0.0902494
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1759005 Vali Loss: 0.0675063 Test Loss: 0.0877585
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1760880 Vali Loss: 0.0685364 Test Loss: 0.0897389
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1770151 Vali Loss: 0.0676028 Test Loss: 0.0872789
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1767198 Vali Loss: 0.0686325 Test Loss: 0.0885417
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1762857 Vali Loss: 0.0683638 Test Loss: 0.0882731
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1766454 Vali Loss: 0.0669123 Test Loss: 0.0870440
Validation loss decreased (0.067079 --> 0.066912).  Saving model ...
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1789222 Vali Loss: 0.0692762 Test Loss: 0.0891840
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1760516 Vali Loss: 0.0686437 Test Loss: 0.0877613
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1763745 Vali Loss: 0.0673953 Test Loss: 0.0883400
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1764611 Vali Loss: 0.0675478 Test Loss: 0.0881973
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1766654 Vali Loss: 0.0694940 Test Loss: 0.0871310
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1760855 Vali Loss: 0.0678388 Test Loss: 0.0870893
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.1770150 Vali Loss: 0.0667081 Test Loss: 0.0913341
Validation loss decreased (0.066912 --> 0.066708).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.1762162 Vali Loss: 0.0690101 Test Loss: 0.0901061
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.1762604 Vali Loss: 0.0672326 Test Loss: 0.0896411
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.1764273 Vali Loss: 0.0669349 Test Loss: 0.0897989
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.1763145 Vali Loss: 0.0692686 Test Loss: 0.0859028
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.1761173 Vali Loss: 0.0670466 Test Loss: 0.0901653
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 133 | Train Loss: 0.1765502 Vali Loss: 0.0686066 Test Loss: 0.0868983
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 133 | Train Loss: 0.1761567 Vali Loss: 0.0674570 Test Loss: 0.0889636
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 133 | Train Loss: 0.1763257 Vali Loss: 0.0684533 Test Loss: 0.0878286
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 133 | Train Loss: 0.1758545 Vali Loss: 0.0685648 Test Loss: 0.0882295
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 133 | Train Loss: 0.1779508 Vali Loss: 0.0678063 Test Loss: 0.0911043
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SP500_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 236
test shape: (236, 5, 1) (236, 5, 1)
test shape: (236, 5, 1) (236, 5, 1)


	mse:6.702540122205392e-05, mae:0.006119162309914827, rmse:0.008186904713511467, r2:-0.03177452087402344, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0082, RÂ²: -0.0318, MAPE: 1.83%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.517 MB of 0.517 MB uploadedwandb: \ 0.517 MB of 0.517 MB uploadedwandb: | 0.517 MB of 0.517 MB uploadedwandb: / 0.517 MB of 0.517 MB uploadedwandb: - 0.517 MB of 0.819 MB uploadedwandb: \ 0.819 MB of 0.819 MB uploadedwandb: | 0.819 MB of 0.819 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–ˆâ–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–ƒ
wandb:                 train/loss â–ˆâ–†â–ƒâ–‚â–â–â–‚â–‚â–â–â–â–‚â–‚â–â–‚â–„â–â–â–‚â–‚â–â–‚â–â–â–‚â–â–â–‚â–â–â–â–ƒ
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–ˆâ–„â–„â–‡â–ƒâ–…â–‚â–…â–‚â–„â–‚â–„â–ƒâ–â–…â–„â–‚â–‚â–…â–ƒâ–â–„â–‚â–â–…â–â–„â–‚â–ƒâ–„â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 33
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.0911
wandb:                 train/loss 0.17795
wandb:   val/directional_accuracy 48.72881
wandb:                   val/loss 0.06781
wandb:                    val/mae 0.00612
wandb:                   val/mape 182.94628
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.03177
wandb:                   val/rmse 0.00819
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/tzxnf9b8
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_033447-tzxnf9b8/logs
Completed: SP500 H=5

Training: Informer on SP500 for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_033858-6n2dmjpx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/6n2dmjpx
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SP500_H10  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/6n2dmjpx
>>>>>>>start training : long_term_forecast_Informer_SP500_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 10
============================================================
train 4234
Overriding target from 'OT' to 'close' for stock data
val 230
Overriding target from 'OT' to 'close' for stock data
test 231
Epoch: 1, Steps: 133 | Train Loss: 0.2346169 Vali Loss: 0.0881243 Test Loss: 0.0995021
Validation loss decreased (inf --> 0.088124).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.1891585 Vali Loss: 0.0726465 Test Loss: 0.0975058
Validation loss decreased (0.088124 --> 0.072647).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23461694399217017, 'val/loss': 0.08812429290264845, 'test/loss': 0.0995020866394043, '_timestamp': 1762306751.0421395}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.1891584533049648, 'val/loss': 0.07264653639867902, 'test/loss': 0.09750578366219997, '_timestamp': 1762306758.0928867}).
Epoch: 3, Steps: 133 | Train Loss: 0.1823437 Vali Loss: 0.0696842 Test Loss: 0.1043855
Validation loss decreased (0.072647 --> 0.069684).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1792274 Vali Loss: 0.0676339 Test Loss: 0.1103628
Validation loss decreased (0.069684 --> 0.067634).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1775942 Vali Loss: 0.0668036 Test Loss: 0.1000461
Validation loss decreased (0.067634 --> 0.066804).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1768905 Vali Loss: 0.0669972 Test Loss: 0.1036744
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1767973 Vali Loss: 0.0648548 Test Loss: 0.1072657
Validation loss decreased (0.066804 --> 0.064855).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1763200 Vali Loss: 0.0683059 Test Loss: 0.0995315
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1767149 Vali Loss: 0.0681924 Test Loss: 0.0995304
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1763546 Vali Loss: 0.0672600 Test Loss: 0.0997495
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1765370 Vali Loss: 0.0666711 Test Loss: 0.1024448
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1763020 Vali Loss: 0.0664269 Test Loss: 0.0979390
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1766377 Vali Loss: 0.0681303 Test Loss: 0.0998655
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1754461 Vali Loss: 0.0680470 Test Loss: 0.1025724
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1780506 Vali Loss: 0.0664955 Test Loss: 0.0988112
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1765376 Vali Loss: 0.0690032 Test Loss: 0.0988901
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1768474 Vali Loss: 0.0686855 Test Loss: 0.1030931
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SP500_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 231
test shape: (231, 10, 1) (231, 10, 1)
test shape: (231, 10, 1) (231, 10, 1)


	mse:6.660536746494472e-05, mae:0.006078736856579781, rmse:0.008161211386322975, r2:-0.02465379238128662, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0082, RÂ²: -0.0247, MAPE: 1.55%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.538 MB of 0.539 MB uploadedwandb: \ 0.538 MB of 0.539 MB uploadedwandb: | 0.538 MB of 0.539 MB uploadedwandb: / 0.539 MB of 0.539 MB uploadedwandb: - 0.539 MB of 0.838 MB uploadedwandb: \ 0.539 MB of 0.838 MB uploadedwandb: | 0.838 MB of 0.838 MB uploadedwandb: / 0.838 MB of 0.838 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–ˆâ–‚â–„â–†â–‚â–‚â–‚â–„â–â–‚â–„â–â–‚â–„
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–„â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–„â–„â–â–†â–†â–„â–„â–ƒâ–†â–†â–ƒâ–‡â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.10309
wandb:                 train/loss 0.17685
wandb:   val/directional_accuracy 49.63925
wandb:                   val/loss 0.06869
wandb:                    val/mae 0.00608
wandb:                   val/mape 155.42626
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.02465
wandb:                   val/rmse 0.00816
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/6n2dmjpx
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_033858-6n2dmjpx/logs
Completed: SP500 H=10

Training: Informer on SP500 for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_034126-hdenda9e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/hdenda9e
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SP500_H22  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/hdenda9e
>>>>>>>start training : long_term_forecast_Informer_SP500_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 22
============================================================
train 4222
Overriding target from 'OT' to 'close' for stock data
val 218
Overriding target from 'OT' to 'close' for stock data
test 219
Epoch: 1, Steps: 132 | Train Loss: 0.2366191 Vali Loss: 0.0753853 Test Loss: 0.1383908
Validation loss decreased (inf --> 0.075385).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.1903309 Vali Loss: 0.0772711 Test Loss: 0.0825496
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23661909495113473, 'val/loss': 0.0753852863396917, 'test/loss': 0.13839078907455718, '_timestamp': 1762306904.236333}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19033092333737647, 'val/loss': 0.07727106767041343, 'test/loss': 0.08254957518407277, '_timestamp': 1762306911.223733}).
Epoch: 3, Steps: 132 | Train Loss: 0.1837297 Vali Loss: 0.0734882 Test Loss: 0.0864838
Validation loss decreased (0.075385 --> 0.073488).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.1812362 Vali Loss: 0.0725828 Test Loss: 0.0902218
Validation loss decreased (0.073488 --> 0.072583).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.1798521 Vali Loss: 0.0713492 Test Loss: 0.0880756
Validation loss decreased (0.072583 --> 0.071349).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.1790848 Vali Loss: 0.0697872 Test Loss: 0.0906386
Validation loss decreased (0.071349 --> 0.069787).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.1788965 Vali Loss: 0.0708632 Test Loss: 0.0889407
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.1787386 Vali Loss: 0.0692973 Test Loss: 0.0955641
Validation loss decreased (0.069787 --> 0.069297).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.1786346 Vali Loss: 0.0687749 Test Loss: 0.0967847
Validation loss decreased (0.069297 --> 0.068775).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.1784901 Vali Loss: 0.0689875 Test Loss: 0.0946712
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.1782853 Vali Loss: 0.0688888 Test Loss: 0.0968651
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.1785145 Vali Loss: 0.0686996 Test Loss: 0.0962589
Validation loss decreased (0.068775 --> 0.068700).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.1785186 Vali Loss: 0.0689096 Test Loss: 0.0966403
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.1785320 Vali Loss: 0.0687735 Test Loss: 0.0969242
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.1783723 Vali Loss: 0.0688245 Test Loss: 0.0978939
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.1781874 Vali Loss: 0.0690146 Test Loss: 0.0965523
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.1784747 Vali Loss: 0.0694383 Test Loss: 0.0940174
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.1785236 Vali Loss: 0.0690686 Test Loss: 0.0961766
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.1785299 Vali Loss: 0.0689626 Test Loss: 0.0965804
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.1785880 Vali Loss: 0.0689070 Test Loss: 0.0962464
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.1784907 Vali Loss: 0.0688621 Test Loss: 0.0968388
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.1787039 Vali Loss: 0.0687801 Test Loss: 0.0985802
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SP500_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 219
test shape: (219, 22, 1) (219, 22, 1)
test shape: (219, 22, 1) (219, 22, 1)


	mse:6.567513628397137e-05, mae:0.006043195724487305, rmse:0.008104019798338413, r2:-0.028658270835876465, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0060, RMSE: 0.0081, RÂ²: -0.0287, MAPE: 1.54%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.609 MB of 0.610 MB uploadedwandb: \ 0.609 MB of 0.610 MB uploadedwandb: | 0.610 MB of 0.610 MB uploadedwandb: / 0.610 MB of 0.610 MB uploadedwandb: - 0.610 MB of 0.911 MB uploadedwandb: \ 0.911 MB of 0.911 MB uploadedwandb: | 0.911 MB of 0.911 MB uploadedwandb: / 0.911 MB of 0.911 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ƒâ–‚â–ƒâ–‚â–†â–‡â–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–…â–‡â–‡â–‡â–‡â–ˆ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–‚â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‡â–…â–ƒâ–„â–‚â–â–â–â–â–â–â–â–â–‚â–‚â–â–â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.09858
wandb:                 train/loss 0.1787
wandb:   val/directional_accuracy 50.46749
wandb:                   val/loss 0.06878
wandb:                    val/mae 0.00604
wandb:                   val/mape 153.56624
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.02866
wandb:                   val/rmse 0.0081
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/hdenda9e
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_034126-hdenda9e/logs
Completed: SP500 H=22

Training: Informer on SP500 for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_034428-1fgti0dh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/1fgti0dh
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SP500_H50  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/1fgti0dh
>>>>>>>start training : long_term_forecast_Informer_SP500_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 50
============================================================
train 4194
Overriding target from 'OT' to 'close' for stock data
val 190
Overriding target from 'OT' to 'close' for stock data
test 191
Epoch: 1, Steps: 132 | Train Loss: 0.2439899 Vali Loss: 0.0849456 Test Loss: 0.1058601
Validation loss decreased (inf --> 0.084946).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.1944811 Vali Loss: 0.0828510 Test Loss: 0.1833055
Validation loss decreased (0.084946 --> 0.082851).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2439899335079121, 'val/loss': 0.08494557067751884, 'test/loss': 0.1058600867787997, '_timestamp': 1762307080.9334364}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19448105274050526, 'val/loss': 0.08285102248191833, 'test/loss': 0.18330546716849008, '_timestamp': 1762307087.8486726}).
Epoch: 3, Steps: 132 | Train Loss: 0.1889813 Vali Loss: 0.0717892 Test Loss: 0.1091993
Validation loss decreased (0.082851 --> 0.071789).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.1842718 Vali Loss: 0.0701453 Test Loss: 0.1132261
Validation loss decreased (0.071789 --> 0.070145).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.1819419 Vali Loss: 0.0709701 Test Loss: 0.1135428
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.1827408 Vali Loss: 0.0695797 Test Loss: 0.1120202
Validation loss decreased (0.070145 --> 0.069580).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.1837788 Vali Loss: 0.0696758 Test Loss: 0.1138549
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.1816383 Vali Loss: 0.0694720 Test Loss: 0.1222087
Validation loss decreased (0.069580 --> 0.069472).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.1824565 Vali Loss: 0.0698226 Test Loss: 0.1183152
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.1816552 Vali Loss: 0.0697402 Test Loss: 0.1148798
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.1843698 Vali Loss: 0.0692683 Test Loss: 0.1173716
Validation loss decreased (0.069472 --> 0.069268).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.1808184 Vali Loss: 0.0698590 Test Loss: 0.1143081
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.1815830 Vali Loss: 0.0697951 Test Loss: 0.1148526
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.1806106 Vali Loss: 0.0697447 Test Loss: 0.1145199
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.1810675 Vali Loss: 0.0694658 Test Loss: 0.1170213
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.1811163 Vali Loss: 0.0696791 Test Loss: 0.1145812
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.1808828 Vali Loss: 0.0700331 Test Loss: 0.1137895
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.1817298 Vali Loss: 0.0698701 Test Loss: 0.1171125
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.1877243 Vali Loss: 0.0696192 Test Loss: 0.1146075
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.1894410 Vali Loss: 0.0693258 Test Loss: 0.1162335
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.1814574 Vali Loss: 0.0696059 Test Loss: 0.1155218
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SP500_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 191
test shape: (191, 50, 1) (191, 50, 1)
test shape: (191, 50, 1) (191, 50, 1)


	mse:6.618913903366774e-05, mae:0.006063091568648815, rmse:0.00813567079603672, r2:-0.017785072326660156, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0081, RÂ²: -0.0178, MAPE: 1.35%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.661 MB of 0.664 MB uploadedwandb: \ 0.661 MB of 0.664 MB uploadedwandb: | 0.664 MB of 0.664 MB uploadedwandb: / 0.664 MB of 0.664 MB uploadedwandb: - 0.664 MB of 0.964 MB uploadedwandb: \ 0.886 MB of 0.964 MB uploadedwandb: | 0.964 MB of 0.964 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ƒâ–ƒâ–ƒâ–„â–ˆâ–†â–„â–…â–„â–„â–„â–…â–„â–ƒâ–…â–„â–…â–„
wandb:                 train/loss â–ˆâ–„â–‚â–ƒâ–„â–‚â–‚â–‚â–„â–â–‚â–â–â–â–â–‚â–‡â–ˆâ–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–†â–‚â–‚â–‚â–ƒâ–‚â–â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.11552
wandb:                 train/loss 0.18146
wandb:   val/directional_accuracy 49.48178
wandb:                   val/loss 0.06961
wandb:                    val/mae 0.00606
wandb:                   val/mape 135.3173
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.01779
wandb:                   val/rmse 0.00814
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/1fgti0dh
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_034428-1fgti0dh/logs
Completed: SP500 H=50

Training: Informer on SP500 for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_034719-capo9jd8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/capo9jd8
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SP500_H100 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/capo9jd8
>>>>>>>start training : long_term_forecast_Informer_SP500_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 100
============================================================
train 4144
Overriding target from 'OT' to 'close' for stock data
val 140
Overriding target from 'OT' to 'close' for stock data
test 141
Epoch: 1, Steps: 130 | Train Loss: 0.2515255 Vali Loss: 0.0759842 Test Loss: 0.2088991
Validation loss decreased (inf --> 0.075984).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2018531 Vali Loss: 0.0712602 Test Loss: 0.1803094
Validation loss decreased (0.075984 --> 0.071260).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.25152545135754806, 'val/loss': 0.07598421424627304, 'test/loss': 0.20889911949634551, '_timestamp': 1762307251.3782773}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.20185308112547948, 'val/loss': 0.07126024216413498, 'test/loss': 0.18030937910079955, '_timestamp': 1762307258.2822773}).
Epoch: 3, Steps: 130 | Train Loss: 0.1899833 Vali Loss: 0.0667426 Test Loss: 0.1688216
Validation loss decreased (0.071260 --> 0.066743).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.1858276 Vali Loss: 0.0704076 Test Loss: 0.1972204
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.1840366 Vali Loss: 0.0651766 Test Loss: 0.1802495
Validation loss decreased (0.066743 --> 0.065177).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.1831301 Vali Loss: 0.0662267 Test Loss: 0.1707160
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.1830565 Vali Loss: 0.0655363 Test Loss: 0.1686107
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.1832880 Vali Loss: 0.0652266 Test Loss: 0.1701169
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.1823680 Vali Loss: 0.0655776 Test Loss: 0.1749099
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.1824805 Vali Loss: 0.0659339 Test Loss: 0.1698884
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.1827588 Vali Loss: 0.0657932 Test Loss: 0.1670874
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.1823308 Vali Loss: 0.0652771 Test Loss: 0.1694897
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.1824803 Vali Loss: 0.0658995 Test Loss: 0.1703289
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.1821496 Vali Loss: 0.0659085 Test Loss: 0.1719449
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.1828506 Vali Loss: 0.0652186 Test Loss: 0.1678029
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SP500_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 141
test shape: (141, 100, 1) (141, 100, 1)
test shape: (141, 100, 1) (141, 100, 1)


	mse:7.001699123065919e-05, mae:0.006227573379874229, rmse:0.00836761575192213, r2:-0.02202141284942627, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0062, RMSE: 0.0084, RÂ²: -0.0220, MAPE: 1.55%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.762 MB of 0.767 MB uploadedwandb: \ 0.762 MB of 0.767 MB uploadedwandb: | 0.762 MB of 0.767 MB uploadedwandb: / 0.767 MB of 0.767 MB uploadedwandb: - 0.767 MB of 1.066 MB uploadedwandb: \ 1.066 MB of 1.066 MB uploadedwandb: | 1.066 MB of 1.066 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–„â–‚â–â–‚â–ƒâ–‚â–â–‚â–‚â–‚â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–‚â–â–â–‚â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–ˆâ–â–‚â–â–â–‚â–‚â–‚â–â–‚â–‚â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.1678
wandb:                 train/loss 0.18285
wandb:   val/directional_accuracy 49.29436
wandb:                   val/loss 0.06522
wandb:                    val/mae 0.00623
wandb:                   val/mape 155.12413
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.02202
wandb:                   val/rmse 0.00837
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/capo9jd8
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_034719-capo9jd8/logs
Completed: SP500 H=100

Training: Informer on NASDAQ for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_034929-cyqdjrmz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/cyqdjrmz
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NASDAQ_H3  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/cyqdjrmz
>>>>>>>start training : long_term_forecast_Informer_NASDAQ_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
Overriding target from 'OT' to 'close' for stock data
val 237
Overriding target from 'OT' to 'close' for stock data
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.3078312 Vali Loss: 0.1891015 Test Loss: 0.1592144
Validation loss decreased (inf --> 0.189102).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2519163 Vali Loss: 0.1643347 Test Loss: 0.1576522
Validation loss decreased (0.189102 --> 0.164335).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3078312232978362, 'val/loss': 0.1891015488654375, 'test/loss': 0.15921439696103334, '_timestamp': 1762307382.1475594}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2519163094965139, 'val/loss': 0.16433465667068958, 'test/loss': 0.15765220019966364, '_timestamp': 1762307389.1579537}).
Epoch: 3, Steps: 133 | Train Loss: 0.2365047 Vali Loss: 0.1373179 Test Loss: 0.1292740
Validation loss decreased (0.164335 --> 0.137318).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2325005 Vali Loss: 0.1380880 Test Loss: 0.1260573
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2290992 Vali Loss: 0.1454025 Test Loss: 0.1375357
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2283215 Vali Loss: 0.1447785 Test Loss: 0.1240415
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2268146 Vali Loss: 0.1385576 Test Loss: 0.1254539
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2255004 Vali Loss: 0.1382037 Test Loss: 0.1267026
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2260120 Vali Loss: 0.1366067 Test Loss: 0.1261286
Validation loss decreased (0.137318 --> 0.136607).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2260027 Vali Loss: 0.1391267 Test Loss: 0.1240063
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2262949 Vali Loss: 0.1379493 Test Loss: 0.1226478
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2255143 Vali Loss: 0.1347486 Test Loss: 0.1234134
Validation loss decreased (0.136607 --> 0.134749).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2260634 Vali Loss: 0.1392424 Test Loss: 0.1239827
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2254724 Vali Loss: 0.1377663 Test Loss: 0.1246624
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2257579 Vali Loss: 0.1322869 Test Loss: 0.1226340
Validation loss decreased (0.134749 --> 0.132287).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2257877 Vali Loss: 0.1373079 Test Loss: 0.1241476
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2256787 Vali Loss: 0.1377295 Test Loss: 0.1259418
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2257809 Vali Loss: 0.1342115 Test Loss: 0.1225111
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2250184 Vali Loss: 0.1368872 Test Loss: 0.1251645
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2256522 Vali Loss: 0.1345252 Test Loss: 0.1222237
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2248743 Vali Loss: 0.1385426 Test Loss: 0.1257627
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2254783 Vali Loss: 0.1313626 Test Loss: 0.1218002
Validation loss decreased (0.132287 --> 0.131363).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2252139 Vali Loss: 0.1348714 Test Loss: 0.1227653
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2255384 Vali Loss: 0.1335998 Test Loss: 0.1236766
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2255722 Vali Loss: 0.1371264 Test Loss: 0.1244670
EarlyStopping counter: 3 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2254540 Vali Loss: 0.1331952 Test Loss: 0.1226693
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.2253743 Vali Loss: 0.1369480 Test Loss: 0.1241352
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.2247630 Vali Loss: 0.1392161 Test Loss: 0.1257781
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.2270291 Vali Loss: 0.1338852 Test Loss: 0.1218071
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 133 | Train Loss: 0.2251516 Vali Loss: 0.1330963 Test Loss: 0.1200163
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 133 | Train Loss: 0.2253631 Vali Loss: 0.1376993 Test Loss: 0.1247749
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 133 | Train Loss: 0.2255561 Vali Loss: 0.1349932 Test Loss: 0.1231585
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NASDAQ_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.00013966878759674728, mae:0.008639060892164707, rmse:0.011818154715001583, r2:-0.02615535259246826, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0086, RMSE: 0.0118, RÂ²: -0.0262, MAPE: 2123377.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.489 MB of 0.489 MB uploadedwandb: \ 0.489 MB of 0.489 MB uploadedwandb: | 0.489 MB of 0.489 MB uploadedwandb: / 0.489 MB of 0.489 MB uploadedwandb: - 0.489 MB of 0.489 MB uploadedwandb: \ 0.489 MB of 0.489 MB uploadedwandb: | 0.489 MB of 0.489 MB uploadedwandb: / 0.671 MB of 0.972 MB uploaded (0.002 MB deduped)wandb: - 0.898 MB of 0.972 MB uploaded (0.002 MB deduped)wandb: \ 0.972 MB of 0.972 MB uploaded (0.002 MB deduped)wandb: | 0.972 MB of 0.972 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–ƒâ–ˆâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–â–ƒâ–‚
wandb:                 train/loss â–ˆâ–†â–„â–ƒâ–‚â–â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–„â–ˆâ–ˆâ–…â–„â–„â–…â–„â–ƒâ–…â–„â–â–„â–„â–‚â–„â–ƒâ–…â–â–ƒâ–‚â–„â–‚â–„â–…â–‚â–‚â–„â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 31
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.12316
wandb:                 train/loss 0.22556
wandb:   val/directional_accuracy 48.94515
wandb:                   val/loss 0.13499
wandb:                    val/mae 0.00864
wandb:                   val/mape 212337700.0
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.02616
wandb:                   val/rmse 0.01182
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/cyqdjrmz
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_034929-cyqdjrmz/logs
Completed: NASDAQ H=3

Training: Informer on NASDAQ for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_035334-eokf2iik
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/eokf2iik
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NASDAQ_H5  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/eokf2iik
>>>>>>>start training : long_term_forecast_Informer_NASDAQ_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
Overriding target from 'OT' to 'close' for stock data
val 235
Overriding target from 'OT' to 'close' for stock data
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.3106749 Vali Loss: 0.1685200 Test Loss: 0.1476468
Validation loss decreased (inf --> 0.168520).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.31067487194125815, 'val/loss': 0.1685199961066246, 'test/loss': 0.14764679968357086, '_timestamp': 1762307626.6512845}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.255487867315909, 'val/loss': 0.15956678427755833, 'test/loss': 0.15850248746573925, '_timestamp': 1762307633.716116}).
Epoch: 2, Steps: 133 | Train Loss: 0.2554879 Vali Loss: 0.1595668 Test Loss: 0.1585025
Validation loss decreased (0.168520 --> 0.159567).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2429533 Vali Loss: 0.1588464 Test Loss: 0.1381931
Validation loss decreased (0.159567 --> 0.158846).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2365386 Vali Loss: 0.1517079 Test Loss: 0.1617237
Validation loss decreased (0.158846 --> 0.151708).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2338463 Vali Loss: 0.1396702 Test Loss: 0.1379220
Validation loss decreased (0.151708 --> 0.139670).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2321118 Vali Loss: 0.1419160 Test Loss: 0.1304261
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2313334 Vali Loss: 0.1406432 Test Loss: 0.1315899
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2309568 Vali Loss: 0.1423118 Test Loss: 0.1352905
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2303826 Vali Loss: 0.1448848 Test Loss: 0.1353831
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2319419 Vali Loss: 0.1483240 Test Loss: 0.1322552
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2315791 Vali Loss: 0.1387936 Test Loss: 0.1313060
Validation loss decreased (0.139670 --> 0.138794).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2305738 Vali Loss: 0.1430043 Test Loss: 0.1358936
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2309091 Vali Loss: 0.1426538 Test Loss: 0.1286094
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2308363 Vali Loss: 0.1535224 Test Loss: 0.1394699
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2308146 Vali Loss: 0.1394583 Test Loss: 0.1306889
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2297003 Vali Loss: 0.1414209 Test Loss: 0.1356074
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2302334 Vali Loss: 0.1425814 Test Loss: 0.1348632
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2303438 Vali Loss: 0.1437140 Test Loss: 0.1393335
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2302285 Vali Loss: 0.1399125 Test Loss: 0.1339461
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2315672 Vali Loss: 0.1381593 Test Loss: 0.1309631
Validation loss decreased (0.138794 --> 0.138159).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2314362 Vali Loss: 0.1392705 Test Loss: 0.1347447
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2311981 Vali Loss: 0.1490653 Test Loss: 0.1313654
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2305077 Vali Loss: 0.1530671 Test Loss: 0.1411538
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2301260 Vali Loss: 0.1386998 Test Loss: 0.1314181
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2310828 Vali Loss: 0.1404121 Test Loss: 0.1329452
EarlyStopping counter: 5 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2308076 Vali Loss: 0.1467183 Test Loss: 0.1309264
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.2301686 Vali Loss: 0.1506593 Test Loss: 0.1350050
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.2304878 Vali Loss: 0.1490411 Test Loss: 0.1341570
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.2310880 Vali Loss: 0.1405255 Test Loss: 0.1353340
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 133 | Train Loss: 0.2299942 Vali Loss: 0.1387162 Test Loss: 0.1322367
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NASDAQ_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.00014361132343765348, mae:0.008763544261455536, rmse:0.011983794160187244, r2:-0.04976809024810791, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0088, RMSE: 0.0120, RÂ²: -0.0498, MAPE: 2115545.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.523 MB of 0.524 MB uploadedwandb: \ 0.523 MB of 0.524 MB uploadedwandb: | 0.523 MB of 0.524 MB uploadedwandb: / 0.524 MB of 0.524 MB uploadedwandb: - 0.524 MB of 0.524 MB uploadedwandb: \ 0.524 MB of 0.825 MB uploadedwandb: | 0.751 MB of 0.825 MB uploadedwandb: / 0.825 MB of 0.825 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ƒâ–ˆâ–ƒâ–â–‚â–‚â–‚â–‚â–‚â–ƒâ–â–ƒâ–â–‚â–‚â–ƒâ–‚â–â–‚â–‚â–„â–‚â–‚â–â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–â–â–â–â–‚â–‚â–‚â–â–â–‚â–‚â–â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–‚â–‚â–‚â–‚â–ƒâ–„â–â–ƒâ–ƒâ–†â–â–‚â–‚â–ƒâ–‚â–â–â–…â–†â–â–‚â–„â–…â–…â–‚â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 29
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.13224
wandb:                 train/loss 0.22999
wandb:   val/directional_accuracy 51.2766
wandb:                   val/loss 0.13872
wandb:                    val/mae 0.00876
wandb:                   val/mape 211554575.0
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.04977
wandb:                   val/rmse 0.01198
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/eokf2iik
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_035334-eokf2iik/logs
Completed: NASDAQ H=5

Training: Informer on NASDAQ for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_035727-4k8y13cd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/4k8y13cd
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NASDAQ_H10 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/4k8y13cd
>>>>>>>start training : long_term_forecast_Informer_NASDAQ_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
Overriding target from 'OT' to 'close' for stock data
val 230
Overriding target from 'OT' to 'close' for stock data
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.3079918 Vali Loss: 0.1752533 Test Loss: 0.1616070
Validation loss decreased (inf --> 0.175253).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2573471 Vali Loss: 0.1536410 Test Loss: 0.1453011
Validation loss decreased (0.175253 --> 0.153641).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3079918209547387, 'val/loss': 0.17525333166122437, 'test/loss': 0.1616069721058011, '_timestamp': 1762307859.8738592}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25734714480271015, 'val/loss': 0.1536410003900528, 'test/loss': 0.1453011091798544, '_timestamp': 1762307866.923534}).
Epoch: 3, Steps: 133 | Train Loss: 0.2461889 Vali Loss: 0.1634089 Test Loss: 0.1656881
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2384068 Vali Loss: 0.1693861 Test Loss: 0.1770948
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2379711 Vali Loss: 0.1561311 Test Loss: 0.1602258
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2373689 Vali Loss: 0.1499207 Test Loss: 0.1506875
Validation loss decreased (0.153641 --> 0.149921).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2346277 Vali Loss: 0.1499160 Test Loss: 0.1525479
Validation loss decreased (0.149921 --> 0.149916).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2342519 Vali Loss: 0.1603776 Test Loss: 0.1648870
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2348951 Vali Loss: 0.1586318 Test Loss: 0.1593947
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2339200 Vali Loss: 0.1537361 Test Loss: 0.1516401
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2339111 Vali Loss: 0.1523504 Test Loss: 0.1537882
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2342930 Vali Loss: 0.1590624 Test Loss: 0.1660545
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2340141 Vali Loss: 0.1509407 Test Loss: 0.1468524
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2337230 Vali Loss: 0.1559168 Test Loss: 0.1587562
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2341575 Vali Loss: 0.1516536 Test Loss: 0.1515199
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2344491 Vali Loss: 0.1507680 Test Loss: 0.1488938
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2335237 Vali Loss: 0.1588817 Test Loss: 0.1574212
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NASDAQ_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.00014534429647028446, mae:0.008809071034193039, rmse:0.012055882252752781, r2:-0.049977660179138184, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0088, RMSE: 0.0121, RÂ²: -0.0500, MAPE: 1938627.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.591 MB of 0.591 MB uploadedwandb: \ 0.591 MB of 0.591 MB uploadedwandb: | 0.591 MB of 0.591 MB uploadedwandb: / 0.591 MB of 0.591 MB uploadedwandb: - 0.591 MB of 0.890 MB uploadedwandb: \ 0.890 MB of 0.890 MB uploadedwandb: | 0.890 MB of 0.890 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–ˆâ–„â–‚â–‚â–…â–„â–‚â–ƒâ–…â–â–„â–‚â–â–ƒ
wandb:                 train/loss â–ˆâ–„â–ƒâ–ƒâ–‚â–â–‚â–â–â–â–â–â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–ˆâ–ƒâ–â–â–…â–„â–‚â–‚â–„â–â–ƒâ–‚â–â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.15742
wandb:                 train/loss 0.23352
wandb:   val/directional_accuracy 49.51691
wandb:                   val/loss 0.15888
wandb:                    val/mae 0.00881
wandb:                   val/mape 193862750.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.04998
wandb:                   val/rmse 0.01206
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/4k8y13cd
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_035727-4k8y13cd/logs
Completed: NASDAQ H=10

Training: Informer on NASDAQ for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_035951-p76lhbie
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/p76lhbie
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NASDAQ_H22 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/p76lhbie
>>>>>>>start training : long_term_forecast_Informer_NASDAQ_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
Overriding target from 'OT' to 'close' for stock data
val 218
Overriding target from 'OT' to 'close' for stock data
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.3129754 Vali Loss: 0.2219999 Test Loss: 0.1792164
Validation loss decreased (inf --> 0.222000).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2618598 Vali Loss: 0.1762946 Test Loss: 0.1587539
Validation loss decreased (0.222000 --> 0.176295).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3129754015667872, 'val/loss': 0.22199991984026773, 'test/loss': 0.17921643917049682, '_timestamp': 1762308004.013007}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2618597778632785, 'val/loss': 0.17629464609282358, 'test/loss': 0.15875394003731863, '_timestamp': 1762308011.010258}).
Epoch: 3, Steps: 132 | Train Loss: 0.2489030 Vali Loss: 0.1608431 Test Loss: 0.1480014
Validation loss decreased (0.176295 --> 0.160843).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2451466 Vali Loss: 0.1603091 Test Loss: 0.1389658
Validation loss decreased (0.160843 --> 0.160309).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2419856 Vali Loss: 0.1605222 Test Loss: 0.1485043
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2402988 Vali Loss: 0.1593245 Test Loss: 0.1465928
Validation loss decreased (0.160309 --> 0.159325).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2394202 Vali Loss: 0.1588294 Test Loss: 0.1413352
Validation loss decreased (0.159325 --> 0.158829).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2390972 Vali Loss: 0.1596082 Test Loss: 0.1445069
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2387332 Vali Loss: 0.1587194 Test Loss: 0.1471125
Validation loss decreased (0.158829 --> 0.158719).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2385916 Vali Loss: 0.1619216 Test Loss: 0.1452796
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2388135 Vali Loss: 0.1578785 Test Loss: 0.1432684
Validation loss decreased (0.158719 --> 0.157879).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2385898 Vali Loss: 0.1599410 Test Loss: 0.1445244
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2385687 Vali Loss: 0.1594505 Test Loss: 0.1449899
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2387509 Vali Loss: 0.1600580 Test Loss: 0.1437808
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2390526 Vali Loss: 0.1585324 Test Loss: 0.1466201
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2385354 Vali Loss: 0.1601083 Test Loss: 0.1417399
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2383663 Vali Loss: 0.1607628 Test Loss: 0.1402458
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2381220 Vali Loss: 0.1592248 Test Loss: 0.1446515
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2385394 Vali Loss: 0.1592288 Test Loss: 0.1463123
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2386194 Vali Loss: 0.1596029 Test Loss: 0.1456423
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2389646 Vali Loss: 0.1586658 Test Loss: 0.1469736
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NASDAQ_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.0001538928336231038, mae:0.009148122742772102, rmse:0.012405355460941792, r2:-0.09981989860534668, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0091, RMSE: 0.0124, RÂ²: -0.0998, MAPE: 2448010.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.643 MB of 0.644 MB uploadedwandb: \ 0.643 MB of 0.644 MB uploadedwandb: | 0.643 MB of 0.644 MB uploadedwandb: / 0.644 MB of 0.644 MB uploadedwandb: - 0.644 MB of 0.944 MB uploadedwandb: \ 0.644 MB of 0.944 MB uploadedwandb: | 0.944 MB of 0.944 MB uploadedwandb: / 0.944 MB of 0.944 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–ˆâ–‡â–ƒâ–…â–‡â–†â–„â–…â–…â–…â–‡â–ƒâ–‚â–…â–†â–†â–‡
wandb:                 train/loss â–ˆâ–†â–„â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–…â–†â–„â–ƒâ–„â–‚â–ˆâ–â–…â–„â–…â–‚â–…â–†â–ƒâ–ƒâ–„â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.14697
wandb:                 train/loss 0.23896
wandb:   val/directional_accuracy 51.15771
wandb:                   val/loss 0.15867
wandb:                    val/mae 0.00915
wandb:                   val/mape 244801075.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.09982
wandb:                   val/rmse 0.01241
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/p76lhbie
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_035951-p76lhbie/logs
Completed: NASDAQ H=22

Training: Informer on NASDAQ for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_040244-k9wnli1d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/k9wnli1d
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NASDAQ_H50 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/k9wnli1d
>>>>>>>start training : long_term_forecast_Informer_NASDAQ_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
Overriding target from 'OT' to 'close' for stock data
val 190
Overriding target from 'OT' to 'close' for stock data
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.3185289 Vali Loss: 0.2275209 Test Loss: 0.2155781
Validation loss decreased (inf --> 0.227521).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2661726 Vali Loss: 0.1855627 Test Loss: 0.1673154
Validation loss decreased (0.227521 --> 0.185563).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.31852888293338544, 'val/loss': 0.22752087066570917, 'test/loss': 0.21557807425657907, '_timestamp': 1762308176.9752085}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26617258245294745, 'val/loss': 0.18556274473667145, 'test/loss': 0.1673154172797998, '_timestamp': 1762308183.944838}).
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
Epoch: 3, Steps: 132 | Train Loss: 0.2548024 Vali Loss: 0.2110337 Test Loss: 0.1531407
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2493779 Vali Loss: 0.2016147 Test Loss: 0.1605472
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2441317 Vali Loss: 0.1860543 Test Loss: 0.1604096
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2439162 Vali Loss: 0.1847623 Test Loss: 0.1622057
Validation loss decreased (0.185563 --> 0.184762).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2429498 Vali Loss: 0.1904869 Test Loss: 0.1597336
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2420279 Vali Loss: 0.1778779 Test Loss: 0.1723614
Validation loss decreased (0.184762 --> 0.177878).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2437164 Vali Loss: 0.1791908 Test Loss: 0.1693199
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2407519 Vali Loss: 0.1803733 Test Loss: 0.1677027
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2412255 Vali Loss: 0.1773012 Test Loss: 0.1689433
Validation loss decreased (0.177878 --> 0.177301).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2429964 Vali Loss: 0.1986306 Test Loss: 0.1601412
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2415313 Vali Loss: 0.1859960 Test Loss: 0.1629825
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2430164 Vali Loss: 0.1938360 Test Loss: 0.1606176
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2433889 Vali Loss: 0.1922596 Test Loss: 0.1597393
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2407544 Vali Loss: 0.1798265 Test Loss: 0.1664027
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2429420 Vali Loss: 0.1891051 Test Loss: 0.1626989
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2451428 Vali Loss: 0.1878047 Test Loss: 0.1646743
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2412185 Vali Loss: 0.1798484 Test Loss: 0.1670321
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2416048 Vali Loss: 0.1783006 Test Loss: 0.1706692
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2409573 Vali Loss: 0.1790465 Test Loss: 0.1670822
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NASDAQ_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.00016048077668529004, mae:0.009276168420910835, rmse:0.012668101117014885, r2:-0.10794341564178467, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0093, RMSE: 0.0127, RÂ²: -0.1079, MAPE: 3175836.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.705 MB of 0.707 MB uploadedwandb: \ 0.705 MB of 0.707 MB uploadedwandb: | 0.705 MB of 0.707 MB uploadedwandb: / 0.705 MB of 0.707 MB uploadedwandb: - 0.707 MB of 0.707 MB uploadedwandb: \ 0.707 MB of 1.007 MB uploadedwandb: | 0.886 MB of 1.007 MB uploadedwandb: / 1.007 MB of 1.007 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–„â–„â–„â–ƒâ–ˆâ–‡â–†â–‡â–„â–…â–„â–ƒâ–†â–„â–…â–†â–‡â–†
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–‚â–â–‚â–‚â–â–‚â–ƒâ–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–ƒâ–ƒâ–„â–â–â–‚â–â–…â–ƒâ–„â–„â–‚â–ƒâ–ƒâ–‚â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.16708
wandb:                 train/loss 0.24096
wandb:   val/directional_accuracy 51.03115
wandb:                   val/loss 0.17905
wandb:                    val/mae 0.00928
wandb:                   val/mape 317583650.0
wandb:                    val/mse 0.00016
wandb:                     val/r2 -0.10794
wandb:                   val/rmse 0.01267
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/k9wnli1d
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_040244-k9wnli1d/logs
Completed: NASDAQ H=50

Training: Informer on NASDAQ for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_040541-sgfp48b6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/sgfp48b6
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NASDAQ_H100Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/sgfp48b6
>>>>>>>start training : long_term_forecast_Informer_NASDAQ_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
Overriding target from 'OT' to 'close' for stock data
val 140
Overriding target from 'OT' to 'close' for stock data
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.3241449 Vali Loss: 0.3724255 Test Loss: 0.2883412
Validation loss decreased (inf --> 0.372425).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2673089 Vali Loss: 0.3458441 Test Loss: 0.3165812
Validation loss decreased (0.372425 --> 0.345844).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.32414492735495937, 'val/loss': 0.37242549657821655, 'test/loss': 0.2883411943912506, '_timestamp': 1762308355.995426}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2673088937997818, 'val/loss': 0.3458441078662872, 'test/loss': 0.3165811777114868, '_timestamp': 1762308362.9561162}).
Epoch: 3, Steps: 130 | Train Loss: 0.2541378 Vali Loss: 0.3646283 Test Loss: 0.3432292
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2486343 Vali Loss: 0.3040226 Test Loss: 0.3422628
Validation loss decreased (0.345844 --> 0.304023).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2454613 Vali Loss: 0.3081929 Test Loss: 0.4189372
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2442267 Vali Loss: 0.2995825 Test Loss: 0.3756014
Validation loss decreased (0.304023 --> 0.299582).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2439304 Vali Loss: 0.3007983 Test Loss: 0.3907340
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2433462 Vali Loss: 0.3012174 Test Loss: 0.3925715
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2431183 Vali Loss: 0.3010869 Test Loss: 0.3934559
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2434362 Vali Loss: 0.2988704 Test Loss: 0.3882587
Validation loss decreased (0.299582 --> 0.298870).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2433133 Vali Loss: 0.3054761 Test Loss: 0.3997470
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2425282 Vali Loss: 0.3007040 Test Loss: 0.4076349
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2428655 Vali Loss: 0.2986350 Test Loss: 0.3893467
Validation loss decreased (0.298870 --> 0.298635).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.2432278 Vali Loss: 0.3018454 Test Loss: 0.3892799
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.2426751 Vali Loss: 0.3006088 Test Loss: 0.3910625
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 130 | Train Loss: 0.2427965 Vali Loss: 0.3047671 Test Loss: 0.3898256
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 130 | Train Loss: 0.2429115 Vali Loss: 0.3057865 Test Loss: 0.3948081
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 130 | Train Loss: 0.2430291 Vali Loss: 0.3029698 Test Loss: 0.3937224
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 130 | Train Loss: 0.2427540 Vali Loss: 0.3063562 Test Loss: 0.4152484
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 130 | Train Loss: 0.2434115 Vali Loss: 0.3008199 Test Loss: 0.3975287
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 130 | Train Loss: 0.2422683 Vali Loss: 0.2975206 Test Loss: 0.3990666
Validation loss decreased (0.298635 --> 0.297521).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 130 | Train Loss: 0.2427448 Vali Loss: 0.3033203 Test Loss: 0.3983770
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 130 | Train Loss: 0.2437376 Vali Loss: 0.3014045 Test Loss: 0.3915701
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 130 | Train Loss: 0.2429274 Vali Loss: 0.3003467 Test Loss: 0.3910505
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 130 | Train Loss: 0.2428485 Vali Loss: 0.3017719 Test Loss: 0.4025988
EarlyStopping counter: 4 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 130 | Train Loss: 0.2429623 Vali Loss: 0.3032339 Test Loss: 0.3902273
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 130 | Train Loss: 0.2431805 Vali Loss: 0.3001841 Test Loss: 0.3962394
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 130 | Train Loss: 0.2430901 Vali Loss: 0.3037989 Test Loss: 0.3951246
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 130 | Train Loss: 0.2426945 Vali Loss: 0.3000837 Test Loss: 0.3909124
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 130 | Train Loss: 0.2426021 Vali Loss: 0.3064732 Test Loss: 0.4306033
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 130 | Train Loss: 0.2434207 Vali Loss: 0.3034159 Test Loss: 0.3947838
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NASDAQ_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.00016089780547190458, mae:0.008944346569478512, rmse:0.012684550136327744, r2:-0.0608135461807251, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0089, RMSE: 0.0127, RÂ²: -0.0608, MAPE: 2013365.62%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.750 MB of 0.755 MB uploadedwandb: \ 0.750 MB of 0.755 MB uploadedwandb: | 0.755 MB of 0.755 MB uploadedwandb: / 0.755 MB of 1.057 MB uploadedwandb: - 0.755 MB of 1.057 MB uploadedwandb: \ 1.057 MB of 1.057 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–â–‡â–„â–…â–…â–…â–…â–†â–†â–…â–…â–…â–…â–…â–…â–‡â–…â–†â–…â–…â–…â–†â–…â–…â–…â–…â–ˆâ–…
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–‚â–â–â–‚â–â–â–â–‚â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–‚â–â–‚â–â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 30
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.39478
wandb:                 train/loss 0.24342
wandb:   val/directional_accuracy 50.15873
wandb:                   val/loss 0.30342
wandb:                    val/mae 0.00894
wandb:                   val/mape 201336562.5
wandb:                    val/mse 0.00016
wandb:                     val/r2 -0.06081
wandb:                   val/rmse 0.01268
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/sgfp48b6
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_040541-sgfp48b6/logs
Completed: NASDAQ H=100

Training: Informer on ABSA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_040935-92qdqxfh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/92qdqxfh
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ABSA_H3    Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/92qdqxfh
>>>>>>>start training : long_term_forecast_Informer_ABSA_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 3
============================================================
train 4240
Overriding target from 'OT' to 'close' for stock data
val 237
Overriding target from 'OT' to 'close' for stock data
test 238
Epoch: 1, Steps: 133 | Train Loss: 0.3541361 Vali Loss: 0.1893594 Test Loss: 0.1808309
Validation loss decreased (inf --> 0.189359).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2905836 Vali Loss: 0.2057634 Test Loss: 0.1970572
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.35413608896104914, 'val/loss': 0.18935942091047764, 'test/loss': 0.18083085678517818, '_timestamp': 1762308588.4716449}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2905835892472948, 'val/loss': 0.20576341077685356, 'test/loss': 0.19705719873309135, '_timestamp': 1762308595.3286731}).
Epoch: 3, Steps: 133 | Train Loss: 0.2772658 Vali Loss: 0.1744586 Test Loss: 0.1633984
Validation loss decreased (0.189359 --> 0.174459).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2702580 Vali Loss: 0.1629985 Test Loss: 0.1537775
Validation loss decreased (0.174459 --> 0.162999).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2654495 Vali Loss: 0.1633909 Test Loss: 0.1563907
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2650584 Vali Loss: 0.1616420 Test Loss: 0.1535212
Validation loss decreased (0.162999 --> 0.161642).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2617814 Vali Loss: 0.1648969 Test Loss: 0.1570168
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2610043 Vali Loss: 0.1589625 Test Loss: 0.1560422
Validation loss decreased (0.161642 --> 0.158963).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2622017 Vali Loss: 0.1655838 Test Loss: 0.1563470
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2631374 Vali Loss: 0.1614225 Test Loss: 0.1548970
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2613911 Vali Loss: 0.1611518 Test Loss: 0.1551599
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2601647 Vali Loss: 0.1595972 Test Loss: 0.1546794
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2626712 Vali Loss: 0.1630169 Test Loss: 0.1559049
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2599297 Vali Loss: 0.1632602 Test Loss: 0.1550902
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2624273 Vali Loss: 0.1690326 Test Loss: 0.1553585
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2611697 Vali Loss: 0.1669324 Test Loss: 0.1537671
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2613933 Vali Loss: 0.1617254 Test Loss: 0.1570844
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2616276 Vali Loss: 0.1606638 Test Loss: 0.1557472
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ABSA_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 238
test shape: (238, 3, 1) (238, 3, 1)
test shape: (238, 3, 1) (238, 3, 1)


	mse:0.00046334604849107563, mae:0.01642153225839138, rmse:0.02152547426521778, r2:-0.017539381980895996, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0164, RMSE: 0.0215, RÂ²: -0.0175, MAPE: 1.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.496 MB of 0.496 MB uploadedwandb: \ 0.496 MB of 0.496 MB uploadedwandb: | 0.496 MB of 0.496 MB uploadedwandb: / 0.496 MB of 0.496 MB uploadedwandb: - 0.496 MB of 0.496 MB uploadedwandb: \ 0.496 MB of 0.496 MB uploadedwandb: | 0.496 MB of 0.496 MB uploadedwandb: / 0.678 MB of 0.976 MB uploaded (0.002 MB deduped)wandb: - 0.976 MB of 0.976 MB uploaded (0.002 MB deduped)wandb: \ 0.976 MB of 0.976 MB uploaded (0.002 MB deduped)wandb: | 0.976 MB of 0.976 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–ƒâ–â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–â–„â–ƒ
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–ƒâ–‚â–„â–â–„â–‚â–‚â–â–ƒâ–ƒâ–†â–…â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 17
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.15575
wandb:                 train/loss 0.26163
wandb:   val/directional_accuracy 47.47899
wandb:                   val/loss 0.16066
wandb:                    val/mae 0.01642
wandb:                   val/mape 124.55996
wandb:                    val/mse 0.00046
wandb:                     val/r2 -0.01754
wandb:                   val/rmse 0.02153
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/92qdqxfh
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_040935-92qdqxfh/logs
Completed: ABSA H=3

Training: Informer on ABSA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_041215-a7hxr797
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/a7hxr797
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ABSA_H5    Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/a7hxr797
>>>>>>>start training : long_term_forecast_Informer_ABSA_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 5
============================================================
train 4238
Overriding target from 'OT' to 'close' for stock data
val 235
Overriding target from 'OT' to 'close' for stock data
test 236
Epoch: 1, Steps: 133 | Train Loss: 0.3629580 Vali Loss: 0.1786464 Test Loss: 0.1682002
Validation loss decreased (inf --> 0.178646).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2964949 Vali Loss: 0.1774233 Test Loss: 0.1660740
Validation loss decreased (0.178646 --> 0.177423).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.36295804605448156, 'val/loss': 0.17864643782377243, 'test/loss': 0.1682002181187272, '_timestamp': 1762308751.3982272}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2964949437550136, 'val/loss': 0.17742330767214298, 'test/loss': 0.166073989123106, '_timestamp': 1762308758.405285}).
Epoch: 3, Steps: 133 | Train Loss: 0.2835455 Vali Loss: 0.1696374 Test Loss: 0.1607690
Validation loss decreased (0.177423 --> 0.169637).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2765022 Vali Loss: 0.1704210 Test Loss: 0.1681427
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2723541 Vali Loss: 0.1675352 Test Loss: 0.1610960
Validation loss decreased (0.169637 --> 0.167535).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2703290 Vali Loss: 0.1655993 Test Loss: 0.1605558
Validation loss decreased (0.167535 --> 0.165599).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2693697 Vali Loss: 0.1667506 Test Loss: 0.1599130
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2690826 Vali Loss: 0.1727645 Test Loss: 0.1602538
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2682800 Vali Loss: 0.1715563 Test Loss: 0.1605020
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2686761 Vali Loss: 0.1659986 Test Loss: 0.1601576
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2680326 Vali Loss: 0.1651810 Test Loss: 0.1603218
Validation loss decreased (0.165599 --> 0.165181).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2677254 Vali Loss: 0.1672542 Test Loss: 0.1608311
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2701309 Vali Loss: 0.1596130 Test Loss: 0.1592737
Validation loss decreased (0.165181 --> 0.159613).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2688523 Vali Loss: 0.1680871 Test Loss: 0.1610177
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2673806 Vali Loss: 0.1657753 Test Loss: 0.1599806
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2675145 Vali Loss: 0.1642400 Test Loss: 0.1605861
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2678503 Vali Loss: 0.1645018 Test Loss: 0.1601906
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2673469 Vali Loss: 0.1645757 Test Loss: 0.1601660
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2681459 Vali Loss: 0.1642555 Test Loss: 0.1604155
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2692494 Vali Loss: 0.1642665 Test Loss: 0.1601572
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2697256 Vali Loss: 0.1635341 Test Loss: 0.1598526
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2684958 Vali Loss: 0.1656726 Test Loss: 0.1604889
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2679100 Vali Loss: 0.1634095 Test Loss: 0.1609248
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ABSA_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 236
test shape: (236, 5, 1) (236, 5, 1)
test shape: (236, 5, 1) (236, 5, 1)


	mse:0.00046175133320502937, mae:0.016350381076335907, rmse:0.021488400176167488, r2:-0.008141398429870605, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0164, RMSE: 0.0215, RÂ²: -0.0081, MAPE: 1.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.528 MB of 0.528 MB uploadedwandb: \ 0.528 MB of 0.528 MB uploadedwandb: | 0.528 MB of 0.528 MB uploadedwandb: / 0.528 MB of 0.828 MB uploadedwandb: - 0.528 MB of 0.828 MB uploadedwandb: \ 0.828 MB of 0.828 MB uploadedwandb: | 0.828 MB of 0.828 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‚â–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–‚â–‚â–â–â–â–â–â–‚â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–‡â–…â–„â–…â–ˆâ–‡â–„â–„â–…â–â–†â–„â–ƒâ–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.16092
wandb:                 train/loss 0.26791
wandb:   val/directional_accuracy 51.80085
wandb:                   val/loss 0.16341
wandb:                    val/mae 0.01635
wandb:                   val/mape 125.16112
wandb:                    val/mse 0.00046
wandb:                     val/r2 -0.00814
wandb:                   val/rmse 0.02149
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/a7hxr797
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_041215-a7hxr797/logs
Completed: ABSA H=5

Training: Informer on ABSA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_041530-oiiktoge
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/oiiktoge
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ABSA_H10   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/oiiktoge
>>>>>>>start training : long_term_forecast_Informer_ABSA_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 10
============================================================
train 4233
Overriding target from 'OT' to 'close' for stock data
val 230
Overriding target from 'OT' to 'close' for stock data
test 231
Epoch: 1, Steps: 133 | Train Loss: 0.3640344 Vali Loss: 0.1832191 Test Loss: 0.1720336
Validation loss decreased (inf --> 0.183219).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.3004082 Vali Loss: 0.1843588 Test Loss: 0.1832148
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.36403443300186245, 'val/loss': 0.18321910873055458, 'test/loss': 0.17203356977552176, '_timestamp': 1762308943.3367596}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3004082457015389, 'val/loss': 0.18435883708298206, 'test/loss': 0.18321483209729195, '_timestamp': 1762308950.295185}).
Epoch: 3, Steps: 133 | Train Loss: 0.2875129 Vali Loss: 0.1693891 Test Loss: 0.1650364
Validation loss decreased (0.183219 --> 0.169389).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2788673 Vali Loss: 0.1783692 Test Loss: 0.1688824
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2754961 Vali Loss: 0.1724457 Test Loss: 0.1693386
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2739571 Vali Loss: 0.1709622 Test Loss: 0.1692390
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2714246 Vali Loss: 0.1701507 Test Loss: 0.1679005
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2702188 Vali Loss: 0.1747401 Test Loss: 0.1681981
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2707610 Vali Loss: 0.1772165 Test Loss: 0.1699876
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2701694 Vali Loss: 0.1742082 Test Loss: 0.1679605
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2693496 Vali Loss: 0.1680632 Test Loss: 0.1668974
Validation loss decreased (0.169389 --> 0.168063).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2705479 Vali Loss: 0.1699743 Test Loss: 0.1690816
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2707703 Vali Loss: 0.1687744 Test Loss: 0.1677260
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2696971 Vali Loss: 0.1755578 Test Loss: 0.1681275
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2700650 Vali Loss: 0.1664522 Test Loss: 0.1673619
Validation loss decreased (0.168063 --> 0.166452).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2700560 Vali Loss: 0.1706630 Test Loss: 0.1674804
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2693850 Vali Loss: 0.1732004 Test Loss: 0.1673616
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2692630 Vali Loss: 0.1734491 Test Loss: 0.1674723
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2694944 Vali Loss: 0.1748928 Test Loss: 0.1686181
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2703534 Vali Loss: 0.1687388 Test Loss: 0.1675567
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2694075 Vali Loss: 0.1719891 Test Loss: 0.1687079
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2691571 Vali Loss: 0.1754964 Test Loss: 0.1682688
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2691832 Vali Loss: 0.1709182 Test Loss: 0.1682944
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2703832 Vali Loss: 0.1689800 Test Loss: 0.1676213
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2688266 Vali Loss: 0.1730404 Test Loss: 0.1678384
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ABSA_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 231
test shape: (231, 10, 1) (231, 10, 1)
test shape: (231, 10, 1) (231, 10, 1)


	mse:0.00046814943198114634, mae:0.016432128846645355, rmse:0.021636761724948883, r2:-0.014021635055541992, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0164, RMSE: 0.0216, RÂ²: -0.0140, MAPE: 1.16%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.557 MB of 0.558 MB uploadedwandb: \ 0.557 MB of 0.558 MB uploadedwandb: | 0.557 MB of 0.558 MB uploadedwandb: / 0.558 MB of 0.558 MB uploadedwandb: - 0.558 MB of 0.558 MB uploadedwandb: \ 0.558 MB of 0.858 MB uploadedwandb: | 0.558 MB of 0.858 MB uploadedwandb: / 0.858 MB of 0.858 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–†â–‡â–‡â–…â–…â–ˆâ–…â–„â–‡â–…â–…â–„â–„â–„â–„â–†â–…â–†â–†â–†â–…â–…
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–ˆâ–…â–„â–ƒâ–†â–‡â–†â–‚â–ƒâ–‚â–†â–â–ƒâ–…â–…â–†â–‚â–„â–†â–„â–‚â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 24
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.16784
wandb:                 train/loss 0.26883
wandb:   val/directional_accuracy 49.15825
wandb:                   val/loss 0.17304
wandb:                    val/mae 0.01643
wandb:                   val/mape 115.85345
wandb:                    val/mse 0.00047
wandb:                     val/r2 -0.01402
wandb:                   val/rmse 0.02164
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/oiiktoge
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_041530-oiiktoge/logs
Completed: ABSA H=10

Training: Informer on ABSA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_041851-vsivj5wy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/vsivj5wy
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ABSA_H22   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/vsivj5wy
>>>>>>>start training : long_term_forecast_Informer_ABSA_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 22
============================================================
train 4221
Overriding target from 'OT' to 'close' for stock data
val 218
Overriding target from 'OT' to 'close' for stock data
test 219
Epoch: 1, Steps: 132 | Train Loss: 0.3721897 Vali Loss: 0.2063901 Test Loss: 0.1806227
Validation loss decreased (inf --> 0.206390).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.3104061 Vali Loss: 0.2012352 Test Loss: 0.1795692
Validation loss decreased (0.206390 --> 0.201235).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3721897109891429, 'val/loss': 0.20639005729130336, 'test/loss': 0.18062265855925425, '_timestamp': 1762309146.3101356}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.31040610722971684, 'val/loss': 0.20123517513275146, 'test/loss': 0.17956922948360443, '_timestamp': 1762309153.302756}).
Epoch: 3, Steps: 132 | Train Loss: 0.2934003 Vali Loss: 0.1889047 Test Loss: 0.1712112
Validation loss decreased (0.201235 --> 0.188905).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2845176 Vali Loss: 0.1824306 Test Loss: 0.1675092
Validation loss decreased (0.188905 --> 0.182431).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2796592 Vali Loss: 0.1796630 Test Loss: 0.1644503
Validation loss decreased (0.182431 --> 0.179663).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2763549 Vali Loss: 0.1792774 Test Loss: 0.1632704
Validation loss decreased (0.179663 --> 0.179277).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2743005 Vali Loss: 0.1770894 Test Loss: 0.1640796
Validation loss decreased (0.179277 --> 0.177089).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2738947 Vali Loss: 0.1787031 Test Loss: 0.1631448
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2734090 Vali Loss: 0.1779760 Test Loss: 0.1638348
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2732651 Vali Loss: 0.1779679 Test Loss: 0.1638737
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2731147 Vali Loss: 0.1782389 Test Loss: 0.1646969
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2730692 Vali Loss: 0.1780355 Test Loss: 0.1641711
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2733724 Vali Loss: 0.1778231 Test Loss: 0.1650721
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2727465 Vali Loss: 0.1770163 Test Loss: 0.1637227
Validation loss decreased (0.177089 --> 0.177016).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2731297 Vali Loss: 0.1777459 Test Loss: 0.1646323
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2728055 Vali Loss: 0.1775998 Test Loss: 0.1634203
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2731967 Vali Loss: 0.1777143 Test Loss: 0.1637422
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2728121 Vali Loss: 0.1771857 Test Loss: 0.1645659
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2731369 Vali Loss: 0.1783221 Test Loss: 0.1639428
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2727627 Vali Loss: 0.1780654 Test Loss: 0.1642230
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2734389 Vali Loss: 0.1775352 Test Loss: 0.1647348
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2730389 Vali Loss: 0.1778169 Test Loss: 0.1638174
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.2731900 Vali Loss: 0.1778996 Test Loss: 0.1645544
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 132 | Train Loss: 0.2727516 Vali Loss: 0.1772830 Test Loss: 0.1638012
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ABSA_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 219
test shape: (219, 22, 1) (219, 22, 1)
test shape: (219, 22, 1) (219, 22, 1)


	mse:0.000476379762403667, mae:0.016588376834988594, rmse:0.0218261256814003, r2:-0.01677262783050537, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0166, RMSE: 0.0218, RÂ²: -0.0168, MAPE: 1.35%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.623 MB of 0.624 MB uploadedwandb: \ 0.623 MB of 0.624 MB uploadedwandb: | 0.624 MB of 0.624 MB uploadedwandb: / 0.624 MB of 0.925 MB uploadedwandb: - 0.726 MB of 0.925 MB uploadedwandb: \ 0.925 MB of 0.925 MB uploadedwandb: | 0.925 MB of 0.925 MB uploadedwandb: / 0.925 MB of 0.925 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–ƒâ–‚â–â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–‚â–â–â–‚â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 23
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.1638
wandb:                 train/loss 0.27275
wandb:   val/directional_accuracy 49.94564
wandb:                   val/loss 0.17728
wandb:                    val/mae 0.01659
wandb:                   val/mape 135.12424
wandb:                    val/mse 0.00048
wandb:                     val/r2 -0.01677
wandb:                   val/rmse 0.02183
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/vsivj5wy
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_041851-vsivj5wy/logs
Completed: ABSA H=22

Training: Informer on ABSA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_042209-v2e12sw4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/v2e12sw4
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ABSA_H50   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/v2e12sw4
>>>>>>>start training : long_term_forecast_Informer_ABSA_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 50
============================================================
train 4193
Overriding target from 'OT' to 'close' for stock data
val 190
Overriding target from 'OT' to 'close' for stock data
test 191
Epoch: 1, Steps: 132 | Train Loss: 0.3892776 Vali Loss: 0.2174077 Test Loss: 0.1806711
Validation loss decreased (inf --> 0.217408).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.3192922 Vali Loss: 0.1867849 Test Loss: 0.1652050
Validation loss decreased (0.217408 --> 0.186785).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.38927755358092714, 'val/loss': 0.21740769098202387, 'test/loss': 0.18067113310098648, '_timestamp': 1762309343.9890735}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3192922363452839, 'val/loss': 0.18678490072488785, 'test/loss': 0.16520496581991514, '_timestamp': 1762309350.961133}).
Epoch: 3, Steps: 132 | Train Loss: 0.3064489 Vali Loss: 0.2031398 Test Loss: 0.1643010
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2978797 Vali Loss: 0.1887677 Test Loss: 0.1594307
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2904739 Vali Loss: 0.1839937 Test Loss: 0.1592938
Validation loss decreased (0.186785 --> 0.183994).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2885078 Vali Loss: 0.1884675 Test Loss: 0.1609815
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2869003 Vali Loss: 0.1908223 Test Loss: 0.1617748
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2855010 Vali Loss: 0.1892386 Test Loss: 0.1637944
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2866198 Vali Loss: 0.1939613 Test Loss: 0.1674938
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2842249 Vali Loss: 0.1925582 Test Loss: 0.1630218
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2845715 Vali Loss: 0.1875368 Test Loss: 0.1601721
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2844019 Vali Loss: 0.1868289 Test Loss: 0.1609236
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2865054 Vali Loss: 0.1825243 Test Loss: 0.1605616
Validation loss decreased (0.183994 --> 0.182524).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2859276 Vali Loss: 0.1855669 Test Loss: 0.1614906
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2860692 Vali Loss: 0.1860715 Test Loss: 0.1613085
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2843593 Vali Loss: 0.1893810 Test Loss: 0.1618720
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2855991 Vali Loss: 0.1846537 Test Loss: 0.1606335
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2862535 Vali Loss: 0.1896189 Test Loss: 0.1636508
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2854483 Vali Loss: 0.1918701 Test Loss: 0.1639238
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2869816 Vali Loss: 0.1880846 Test Loss: 0.1651182
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2845173 Vali Loss: 0.1871200 Test Loss: 0.1600124
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2840327 Vali Loss: 0.1899818 Test Loss: 0.1627402
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.2880006 Vali Loss: 0.1910493 Test Loss: 0.1627436
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ABSA_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 191
test shape: (191, 50, 1) (191, 50, 1)
test shape: (191, 50, 1) (191, 50, 1)


	mse:0.0004895735182799399, mae:0.01692684181034565, rmse:0.02212630771100521, r2:-0.006338357925415039, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0169, RMSE: 0.0221, RÂ²: -0.0063, MAPE: 1.10%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.706 MB of 0.709 MB uploadedwandb: \ 0.706 MB of 0.709 MB uploadedwandb: | 0.706 MB of 0.709 MB uploadedwandb: / 0.709 MB of 0.709 MB uploadedwandb: - 0.709 MB of 1.009 MB uploadedwandb: \ 0.994 MB of 1.009 MB uploadedwandb: | 1.009 MB of 1.009 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–â–â–‚â–ƒâ–…â–ˆâ–„â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–…â–…â–†â–‚â–„â–„
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–‚â–â–â–â–‚â–‚â–‚â–â–â–‚â–â–‚â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–â–ƒâ–„â–ƒâ–…â–„â–ƒâ–‚â–â–‚â–‚â–ƒâ–‚â–ƒâ–„â–ƒâ–ƒâ–„â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.16274
wandb:                 train/loss 0.288
wandb:   val/directional_accuracy 50.27247
wandb:                   val/loss 0.19105
wandb:                    val/mae 0.01693
wandb:                   val/mape 110.3863
wandb:                    val/mse 0.00049
wandb:                     val/r2 -0.00634
wandb:                   val/rmse 0.02213
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/v2e12sw4
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_042209-v2e12sw4/logs
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 268, in check_network_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 848, in deliver_network_status
    return self._deliver_network_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 510, in _deliver_network_status
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
Exception in thread IntMsgThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 300, in check_internal_messages
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 856, in deliver_internal_messages
    return self._deliver_internal_messages(internal_message)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 516, in _deliver_internal_messages
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
        self.send_server_request(server_req)self.send_server_request(server_req)

  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: ABSA H=50

Training: Informer on ABSA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_042516-9azg1kod
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/9azg1kod
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ABSA_H100  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/9azg1kod
>>>>>>>start training : long_term_forecast_Informer_ABSA_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 100
============================================================
train 4143
Overriding target from 'OT' to 'close' for stock data
val 140
Overriding target from 'OT' to 'close' for stock data
test 141
Epoch: 1, Steps: 130 | Train Loss: 0.4023363 Vali Loss: 0.2550584 Test Loss: 0.2268198
Validation loss decreased (inf --> 0.255058).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.3283304 Vali Loss: 0.2233989 Test Loss: 0.1802833
Validation loss decreased (0.255058 --> 0.223399).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.40233633598456015, 'val/loss': 0.2550583630800247, 'test/loss': 0.22681983113288878, '_timestamp': 1762309530.869808}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3283304385267771, 'val/loss': 0.22339890897274017, 'test/loss': 0.1802832543849945, '_timestamp': 1762309537.753982}).
Epoch: 3, Steps: 130 | Train Loss: 0.3105112 Vali Loss: 0.2488715 Test Loss: 0.2054182
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.3037378 Vali Loss: 0.2222438 Test Loss: 0.1797473
Validation loss decreased (0.223399 --> 0.222244).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2995734 Vali Loss: 0.2186320 Test Loss: 0.1788204
Validation loss decreased (0.222244 --> 0.218632).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2972975 Vali Loss: 0.2221958 Test Loss: 0.1805333
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2963143 Vali Loss: 0.2418055 Test Loss: 0.1923881
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2962051 Vali Loss: 0.2416023 Test Loss: 0.1939111
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2953009 Vali Loss: 0.2362769 Test Loss: 0.1910675
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2957455 Vali Loss: 0.2356674 Test Loss: 0.1879763
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2953241 Vali Loss: 0.2343419 Test Loss: 0.1899276
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2946085 Vali Loss: 0.2454645 Test Loss: 0.1926105
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2946202 Vali Loss: 0.2394918 Test Loss: 0.1896566
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.2949556 Vali Loss: 0.2336500 Test Loss: 0.1882957
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.2943499 Vali Loss: 0.2381230 Test Loss: 0.1886589
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ABSA_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 141
test shape: (141, 100, 1) (141, 100, 1)
test shape: (141, 100, 1) (141, 100, 1)


	mse:0.0005185437039472163, mae:0.0173517893999815, rmse:0.022771554067730904, r2:-0.004863858222961426, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0174, RMSE: 0.0228, RÂ²: -0.0049, MAPE: 1.04%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.800 MB of 0.805 MB uploadedwandb: \ 0.800 MB of 0.805 MB uploadedwandb: | 0.805 MB of 0.805 MB uploadedwandb: / 0.805 MB of 0.805 MB uploadedwandb: - 0.805 MB of 1.104 MB uploadedwandb: \ 1.104 MB of 1.104 MB uploadedwandb: | 1.104 MB of 1.104 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–â–â–…â–…â–„â–ƒâ–„â–…â–„â–ƒâ–„
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‚â–â–‚â–†â–†â–…â–…â–…â–‡â–†â–„â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.18866
wandb:                 train/loss 0.29435
wandb:   val/directional_accuracy 49.75285
wandb:                   val/loss 0.23812
wandb:                    val/mae 0.01735
wandb:                   val/mape 104.267
wandb:                    val/mse 0.00052
wandb:                     val/r2 -0.00486
wandb:                   val/rmse 0.02277
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/9azg1kod
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_042516-9azg1kod/logs
Completed: ABSA H=100

Training: Informer on SASOL for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_042733-qguvz3eu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/qguvz3eu
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SASOL_H3   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/qguvz3eu
>>>>>>>start training : long_term_forecast_Informer_SASOL_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 3
============================================================
train 3772
Overriding target from 'OT' to 'close' for stock data
val 211
Overriding target from 'OT' to 'close' for stock data
test 212
Epoch: 1, Steps: 118 | Train Loss: 0.2686465 Vali Loss: 0.1162216 Test Loss: 0.1569043
Validation loss decreased (inf --> 0.116222).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2166230 Vali Loss: 0.1104895 Test Loss: 0.1504581
Validation loss decreased (0.116222 --> 0.110489).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.26864649986816663, 'val/loss': 0.11622161524636405, 'test/loss': 0.15690431956733977, '_timestamp': 1762309668.1219838}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.21662297188225438, 'val/loss': 0.11048945039510727, 'test/loss': 0.15045806446245738, '_timestamp': 1762309674.5361035}).
Epoch: 3, Steps: 118 | Train Loss: 0.2048354 Vali Loss: 0.1158337 Test Loss: 0.1722933
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.1998870 Vali Loss: 0.1112413 Test Loss: 0.1537750
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.1974811 Vali Loss: 0.1020794 Test Loss: 0.1483592
Validation loss decreased (0.110489 --> 0.102079).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.1959657 Vali Loss: 0.1032940 Test Loss: 0.1494861
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.1952985 Vali Loss: 0.1034788 Test Loss: 0.1488763
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.1943524 Vali Loss: 0.1059170 Test Loss: 0.1495673
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.1944403 Vali Loss: 0.1020879 Test Loss: 0.1474850
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.1949776 Vali Loss: 0.1030538 Test Loss: 0.1473183
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.1942709 Vali Loss: 0.1029465 Test Loss: 0.1477650
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.1944851 Vali Loss: 0.1020180 Test Loss: 0.1477566
Validation loss decreased (0.102079 --> 0.102018).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.1946646 Vali Loss: 0.1014699 Test Loss: 0.1455706
Validation loss decreased (0.102018 --> 0.101470).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.1936457 Vali Loss: 0.1050655 Test Loss: 0.1475763
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.1945670 Vali Loss: 0.1009672 Test Loss: 0.1481659
Validation loss decreased (0.101470 --> 0.100967).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.1946577 Vali Loss: 0.1065357 Test Loss: 0.1496690
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.1936968 Vali Loss: 0.1030233 Test Loss: 0.1491301
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.1948881 Vali Loss: 0.1025089 Test Loss: 0.1472432
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.1943061 Vali Loss: 0.1037373 Test Loss: 0.1484409
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.1943864 Vali Loss: 0.1028989 Test Loss: 0.1478454
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 118 | Train Loss: 0.1947460 Vali Loss: 0.1019981 Test Loss: 0.1470677
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 118 | Train Loss: 0.1940543 Vali Loss: 0.1042884 Test Loss: 0.1478238
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 118 | Train Loss: 0.1944264 Vali Loss: 0.1027644 Test Loss: 0.1477801
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 118 | Train Loss: 0.1953137 Vali Loss: 0.1047346 Test Loss: 0.1484025
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 118 | Train Loss: 0.1944369 Vali Loss: 0.1002771 Test Loss: 0.1478320
Validation loss decreased (0.100967 --> 0.100277).  Saving model ...
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 118 | Train Loss: 0.1944261 Vali Loss: 0.1040226 Test Loss: 0.1483865
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 118 | Train Loss: 0.1943881 Vali Loss: 0.1016735 Test Loss: 0.1462640
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 118 | Train Loss: 0.1937780 Vali Loss: 0.1019053 Test Loss: 0.1471068
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 118 | Train Loss: 0.1946023 Vali Loss: 0.1034693 Test Loss: 0.1474694
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 118 | Train Loss: 0.1943863 Vali Loss: 0.1029769 Test Loss: 0.1480577
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 118 | Train Loss: 0.1938351 Vali Loss: 0.1016740 Test Loss: 0.1475649
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 118 | Train Loss: 0.1949659 Vali Loss: 0.1047557 Test Loss: 0.1479267
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 118 | Train Loss: 0.1945807 Vali Loss: 0.1052003 Test Loss: 0.1484382
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 118 | Train Loss: 0.1945910 Vali Loss: 0.1015701 Test Loss: 0.1478521
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1641532182693482e-14
Epoch: 35, Steps: 118 | Train Loss: 0.1943685 Vali Loss: 0.1048151 Test Loss: 0.1471936
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SASOL_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 212
test shape: (212, 3, 1) (212, 3, 1)
test shape: (212, 3, 1) (212, 3, 1)


	mse:0.00233169156126678, mae:0.036571308970451355, rmse:0.04828759282827377, r2:-0.05841469764709473, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0366, RMSE: 0.0483, RÂ²: -0.0584, MAPE: 22825444.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.469 MB of 0.469 MB uploadedwandb: \ 0.469 MB of 0.469 MB uploadedwandb: | 0.469 MB of 0.469 MB uploadedwandb: / 0.469 MB of 0.469 MB uploadedwandb: - 0.469 MB of 0.469 MB uploadedwandb: \ 0.469 MB of 0.469 MB uploadedwandb: | 0.469 MB of 0.469 MB uploadedwandb: / 0.651 MB of 0.952 MB uploaded (0.002 MB deduped)wandb: - 0.651 MB of 0.952 MB uploaded (0.002 MB deduped)wandb: \ 0.952 MB of 0.952 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–‚â–‚â–‚â–‚â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–‚â–â–‚â–‚â–â–‚â–‚â–â–‚â–â–â–‚â–â–â–‚â–â–â–â–â–‚â–â–â–‚â–‚â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–‚â–‚â–‚â–„â–‚â–‚â–‚â–‚â–‚â–ƒâ–â–„â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–â–ƒâ–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 34
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.14719
wandb:                 train/loss 0.19437
wandb:   val/directional_accuracy 48.82075
wandb:                   val/loss 0.10482
wandb:                    val/mae 0.03657
wandb:                   val/mape 2282544400.0
wandb:                    val/mse 0.00233
wandb:                     val/r2 -0.05841
wandb:                   val/rmse 0.04829
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/qguvz3eu
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_042733-qguvz3eu/logs
Completed: SASOL H=3

Training: Informer on SASOL for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_043137-z9p2i8kg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/z9p2i8kg
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SASOL_H5   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/z9p2i8kg
>>>>>>>start training : long_term_forecast_Informer_SASOL_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 5
============================================================
train 3770
Overriding target from 'OT' to 'close' for stock data
val 209
Overriding target from 'OT' to 'close' for stock data
test 210
Epoch: 1, Steps: 118 | Train Loss: 0.2841650 Vali Loss: 0.1150363 Test Loss: 0.1602637
Validation loss decreased (inf --> 0.115036).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2202090 Vali Loss: 0.1182399 Test Loss: 0.1857060
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2841649924294423, 'val/loss': 0.11503630131483078, 'test/loss': 0.16026373101132257, '_timestamp': 1762309909.4387395}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.22020899043497394, 'val/loss': 0.1182398572564125, 'test/loss': 0.18570601407970702, '_timestamp': 1762309915.9287126}).
Epoch: 3, Steps: 118 | Train Loss: 0.2085972 Vali Loss: 0.1069357 Test Loss: 0.1574172
Validation loss decreased (0.115036 --> 0.106936).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2034934 Vali Loss: 0.1103915 Test Loss: 0.1525973
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2012018 Vali Loss: 0.1061710 Test Loss: 0.1516588
Validation loss decreased (0.106936 --> 0.106171).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.1999449 Vali Loss: 0.1099623 Test Loss: 0.1525160
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.1989181 Vali Loss: 0.1100918 Test Loss: 0.1533330
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.1986659 Vali Loss: 0.1063227 Test Loss: 0.1521312
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.1987306 Vali Loss: 0.1065406 Test Loss: 0.1504061
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.1986521 Vali Loss: 0.1049902 Test Loss: 0.1509133
Validation loss decreased (0.106171 --> 0.104990).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.1979540 Vali Loss: 0.1054092 Test Loss: 0.1502756
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.1985061 Vali Loss: 0.1058933 Test Loss: 0.1523955
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.1981212 Vali Loss: 0.1091810 Test Loss: 0.1513297
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.1981849 Vali Loss: 0.1079151 Test Loss: 0.1516642
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.1986439 Vali Loss: 0.1079912 Test Loss: 0.1510032
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.1984871 Vali Loss: 0.1085666 Test Loss: 0.1512262
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.1987049 Vali Loss: 0.1067956 Test Loss: 0.1529880
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.1981520 Vali Loss: 0.1056535 Test Loss: 0.1520119
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.1980737 Vali Loss: 0.1058722 Test Loss: 0.1523976
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.1980224 Vali Loss: 0.1050348 Test Loss: 0.1517661
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SASOL_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 210
test shape: (210, 5, 1) (210, 5, 1)
test shape: (210, 5, 1) (210, 5, 1)


	mse:0.002312717493623495, mae:0.03630606457591057, rmse:0.048090722411870956, r2:-0.042046308517456055, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0363, RMSE: 0.0481, RÂ²: -0.0420, MAPE: 19552824.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.491 MB of 0.491 MB uploadedwandb: \ 0.491 MB of 0.491 MB uploadedwandb: | 0.491 MB of 0.491 MB uploadedwandb: / 0.491 MB of 0.491 MB uploadedwandb: - 0.491 MB of 0.791 MB uploadedwandb: \ 0.624 MB of 0.791 MB uploadedwandb: | 0.791 MB of 0.791 MB uploadedwandb: / 0.791 MB of 0.791 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–ƒâ–„â–ƒâ–â–‚â–â–ƒâ–‚â–‚â–‚â–‚â–„â–ƒâ–ƒâ–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–ˆâ–ƒâ–‡â–ˆâ–ƒâ–ƒâ–â–‚â–‚â–†â–…â–…â–†â–ƒâ–‚â–‚â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.15177
wandb:                 train/loss 0.19802
wandb:   val/directional_accuracy 50.71429
wandb:                   val/loss 0.10503
wandb:                    val/mae 0.03631
wandb:                   val/mape 1955282400.0
wandb:                    val/mse 0.00231
wandb:                     val/r2 -0.04205
wandb:                   val/rmse 0.04809
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/z9p2i8kg
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_043137-z9p2i8kg/logs
Exception in thread IntMsgThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 300, in check_internal_messages
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 268, in check_network_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 856, in deliver_internal_messages
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 848, in deliver_network_status
    return self._deliver_internal_messages(internal_message)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 516, in _deliver_internal_messages
    return self._deliver_network_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 510, in _deliver_network_status
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: SASOL H=5

Training: Informer on SASOL for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_043409-m5ye6osh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/m5ye6osh
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SASOL_H10  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/m5ye6osh
>>>>>>>start training : long_term_forecast_Informer_SASOL_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 10
============================================================
train 3765
Overriding target from 'OT' to 'close' for stock data
val 204
Overriding target from 'OT' to 'close' for stock data
test 205
Epoch: 1, Steps: 118 | Train Loss: 0.2796320 Vali Loss: 0.1155563 Test Loss: 0.1558954
Validation loss decreased (inf --> 0.115556).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2236168 Vali Loss: 0.1113864 Test Loss: 0.1562673
Validation loss decreased (0.115556 --> 0.111386).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2796319911288003, 'val/loss': 0.11555625178984233, 'test/loss': 0.15589538110154016, '_timestamp': 1762310061.7722673}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.22361677960824158, 'val/loss': 0.11138642792190824, 'test/loss': 0.15626725873776845, '_timestamp': 1762310068.206201}).
Epoch: 3, Steps: 118 | Train Loss: 0.2111849 Vali Loss: 0.1148007 Test Loss: 0.1636811
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2068516 Vali Loss: 0.1135427 Test Loss: 0.1545957
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2045513 Vali Loss: 0.1099626 Test Loss: 0.1544418
Validation loss decreased (0.111386 --> 0.109963).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2028320 Vali Loss: 0.1109362 Test Loss: 0.1551610
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2020378 Vali Loss: 0.1102963 Test Loss: 0.1525239
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2026011 Vali Loss: 0.1129778 Test Loss: 0.1533557
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2018228 Vali Loss: 0.1099272 Test Loss: 0.1540119
Validation loss decreased (0.109963 --> 0.109927).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2013080 Vali Loss: 0.1123563 Test Loss: 0.1535310
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2013297 Vali Loss: 0.1096938 Test Loss: 0.1538741
Validation loss decreased (0.109927 --> 0.109694).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2011834 Vali Loss: 0.1098592 Test Loss: 0.1535664
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2020473 Vali Loss: 0.1122130 Test Loss: 0.1542103
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2019857 Vali Loss: 0.1113403 Test Loss: 0.1538951
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.2020688 Vali Loss: 0.1086895 Test Loss: 0.1546048
Validation loss decreased (0.109694 --> 0.108689).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.2014898 Vali Loss: 0.1115621 Test Loss: 0.1533941
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.2015195 Vali Loss: 0.1126179 Test Loss: 0.1539587
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.2011549 Vali Loss: 0.1113890 Test Loss: 0.1544943
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.2015025 Vali Loss: 0.1100485 Test Loss: 0.1537009
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.2019040 Vali Loss: 0.1100334 Test Loss: 0.1537369
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 118 | Train Loss: 0.2016229 Vali Loss: 0.1105034 Test Loss: 0.1538293
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 118 | Train Loss: 0.2016071 Vali Loss: 0.1125550 Test Loss: 0.1526738
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 118 | Train Loss: 0.2011258 Vali Loss: 0.1118921 Test Loss: 0.1543311
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 118 | Train Loss: 0.2014930 Vali Loss: 0.1112823 Test Loss: 0.1537763
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 118 | Train Loss: 0.2013881 Vali Loss: 0.1093382 Test Loss: 0.1550798
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SASOL_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 205
test shape: (205, 10, 1) (205, 10, 1)
test shape: (205, 10, 1) (205, 10, 1)


	mse:0.002319801365956664, mae:0.03638214245438576, rmse:0.04816431552171707, r2:-0.045096397399902344, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0364, RMSE: 0.0482, RÂ²: -0.0451, MAPE: 18753206.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.540 MB of 0.541 MB uploadedwandb: \ 0.540 MB of 0.541 MB uploadedwandb: | 0.541 MB of 0.541 MB uploadedwandb: / 0.541 MB of 0.541 MB uploadedwandb: - 0.541 MB of 0.841 MB uploadedwandb: \ 0.767 MB of 0.841 MB uploadedwandb: | 0.841 MB of 0.841 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–‚â–ƒâ–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–ƒ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–‚â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‡â–‚â–„â–ƒâ–†â–‚â–…â–‚â–‚â–…â–„â–â–„â–…â–„â–ƒâ–ƒâ–ƒâ–…â–…â–„â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 24
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.15508
wandb:                 train/loss 0.20139
wandb:   val/directional_accuracy 49.6477
wandb:                   val/loss 0.10934
wandb:                    val/mae 0.03638
wandb:                   val/mape 1875320600.0
wandb:                    val/mse 0.00232
wandb:                     val/r2 -0.0451
wandb:                   val/rmse 0.04816
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/m5ye6osh
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_043409-m5ye6osh/logs
Completed: SASOL H=10

Training: Informer on SASOL for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_043709-noxhqun4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/noxhqun4
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SASOL_H22  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/noxhqun4
>>>>>>>start training : long_term_forecast_Informer_SASOL_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 22
============================================================
train 3753
Overriding target from 'OT' to 'close' for stock data
val 192
Overriding target from 'OT' to 'close' for stock data
test 193
Epoch: 1, Steps: 118 | Train Loss: 0.2894976 Vali Loss: 0.1352815 Test Loss: 0.1780046
Validation loss decreased (inf --> 0.135282).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2328073 Vali Loss: 0.1223822 Test Loss: 0.1702779
Validation loss decreased (0.135282 --> 0.122382).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.28949755303940533, 'val/loss': 0.13528152431050935, 'test/loss': 0.17800457030534744, '_timestamp': 1762310241.5740273}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23280733710123322, 'val/loss': 0.12238221615552902, 'test/loss': 0.170277866934027, '_timestamp': 1762310247.9507458}).
Epoch: 3, Steps: 118 | Train Loss: 0.2191858 Vali Loss: 0.1263575 Test Loss: 0.1658777
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2146910 Vali Loss: 0.1207092 Test Loss: 0.1749569
Validation loss decreased (0.122382 --> 0.120709).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2121874 Vali Loss: 0.1197876 Test Loss: 0.1668680
Validation loss decreased (0.120709 --> 0.119788).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2096060 Vali Loss: 0.1234752 Test Loss: 0.1671285
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2095531 Vali Loss: 0.1189712 Test Loss: 0.1673030
Validation loss decreased (0.119788 --> 0.118971).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2087024 Vali Loss: 0.1203104 Test Loss: 0.1691586
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2087970 Vali Loss: 0.1204312 Test Loss: 0.1687236
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2077891 Vali Loss: 0.1189146 Test Loss: 0.1664940
Validation loss decreased (0.118971 --> 0.118915).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2086437 Vali Loss: 0.1195118 Test Loss: 0.1686387
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2101971 Vali Loss: 0.1189771 Test Loss: 0.1653286
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2088676 Vali Loss: 0.1183825 Test Loss: 0.1677992
Validation loss decreased (0.118915 --> 0.118382).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2086687 Vali Loss: 0.1207618 Test Loss: 0.1700860
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.2101426 Vali Loss: 0.1202597 Test Loss: 0.1680299
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.2081272 Vali Loss: 0.1190181 Test Loss: 0.1675534
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.2083427 Vali Loss: 0.1198675 Test Loss: 0.1669078
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.2079098 Vali Loss: 0.1210939 Test Loss: 0.1687264
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.2083696 Vali Loss: 0.1199535 Test Loss: 0.1696663
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.2091471 Vali Loss: 0.1195057 Test Loss: 0.1685821
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 118 | Train Loss: 0.2085146 Vali Loss: 0.1200354 Test Loss: 0.1663984
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 118 | Train Loss: 0.2085047 Vali Loss: 0.1216559 Test Loss: 0.1687023
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 118 | Train Loss: 0.2083558 Vali Loss: 0.1197847 Test Loss: 0.1701336
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SASOL_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 193
test shape: (193, 22, 1) (193, 22, 1)
test shape: (193, 22, 1) (193, 22, 1)


	mse:0.002362179337069392, mae:0.03662016615271568, rmse:0.048602256923913956, r2:-0.0526045560836792, dtw:Not calculated


VAL - MSE: 0.0024, MAE: 0.0366, RMSE: 0.0486, RÂ²: -0.0526, MAPE: 19849734.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.595 MB of 0.596 MB uploadedwandb: \ 0.595 MB of 0.596 MB uploadedwandb: | 0.595 MB of 0.596 MB uploadedwandb: / 0.596 MB of 0.596 MB uploadedwandb: - 0.596 MB of 0.596 MB uploadedwandb: \ 0.596 MB of 0.897 MB uploadedwandb: | 0.822 MB of 0.897 MB uploadedwandb: / 0.897 MB of 0.897 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–‚â–‚â–‚â–„â–ƒâ–‚â–ƒâ–â–ƒâ–„â–ƒâ–ƒâ–‚â–ƒâ–„â–ƒâ–‚â–ƒâ–„
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–‚â–…â–‚â–ƒâ–ƒâ–â–‚â–‚â–â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–„â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.17013
wandb:                 train/loss 0.20836
wandb:   val/directional_accuracy 49.76561
wandb:                   val/loss 0.11978
wandb:                    val/mae 0.03662
wandb:                   val/mape 1984973400.0
wandb:                    val/mse 0.00236
wandb:                     val/r2 -0.0526
wandb:                   val/rmse 0.0486
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/noxhqun4
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_043709-noxhqun4/logs
Completed: SASOL H=22

Training: Informer on SASOL for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_044004-7b1wnulo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/7b1wnulo
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SASOL_H50  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/7b1wnulo
>>>>>>>start training : long_term_forecast_Informer_SASOL_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 50
============================================================
train 3725
Overriding target from 'OT' to 'close' for stock data
val 164
Overriding target from 'OT' to 'close' for stock data
test 165
Epoch: 1, Steps: 117 | Train Loss: 0.3136495 Vali Loss: 0.1480757 Test Loss: 0.1891380
Validation loss decreased (inf --> 0.148076).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 117 | Train Loss: 0.2449416 Vali Loss: 0.1503029 Test Loss: 0.3453365
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.31364948398027664, 'val/loss': 0.14807567248741785, 'test/loss': 0.1891380473971367, '_timestamp': 1762310418.775583}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2449416262217057, 'val/loss': 0.15030288447936377, 'test/loss': 0.345336452126503, '_timestamp': 1762310425.2420819}).
Epoch: 3, Steps: 117 | Train Loss: 0.2304545 Vali Loss: 0.1207342 Test Loss: 0.4143807
Validation loss decreased (0.148076 --> 0.120734).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 117 | Train Loss: 0.2221425 Vali Loss: 0.1456291 Test Loss: 0.3348730
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 117 | Train Loss: 0.2180005 Vali Loss: 0.1304275 Test Loss: 0.4553593
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 117 | Train Loss: 0.2160783 Vali Loss: 0.1297494 Test Loss: 0.4032653
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 117 | Train Loss: 0.2150170 Vali Loss: 0.1340782 Test Loss: 0.4203600
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 117 | Train Loss: 0.2151600 Vali Loss: 0.1325849 Test Loss: 0.4508836
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 117 | Train Loss: 0.2142943 Vali Loss: 0.1376905 Test Loss: 0.4472351
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 117 | Train Loss: 0.2137478 Vali Loss: 0.1289204 Test Loss: 0.4368731
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 117 | Train Loss: 0.2136777 Vali Loss: 0.1346862 Test Loss: 0.4372476
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 117 | Train Loss: 0.2147403 Vali Loss: 0.1354125 Test Loss: 0.4255301
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 117 | Train Loss: 0.2138913 Vali Loss: 0.1330292 Test Loss: 0.4359769
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SASOL_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 165
test shape: (165, 50, 1) (165, 50, 1)
test shape: (165, 50, 1) (165, 50, 1)


	mse:0.00240594451315701, mae:0.03813705965876579, rmse:0.049050427973270416, r2:-0.17434048652648926, dtw:Not calculated


VAL - MSE: 0.0024, MAE: 0.0381, RMSE: 0.0491, RÂ²: -0.1743, MAPE: 34237156.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.646 MB of 0.648 MB uploadedwandb: \ 0.646 MB of 0.648 MB uploadedwandb: | 0.646 MB of 0.648 MB uploadedwandb: / 0.648 MB of 0.648 MB uploadedwandb: - 0.648 MB of 0.947 MB uploadedwandb: \ 0.648 MB of 0.947 MB uploadedwandb: | 0.947 MB of 0.947 MB uploadedwandb: / 0.947 MB of 0.947 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–†â–â–ˆâ–…â–†â–ˆâ–ˆâ–‡â–‡â–†â–‡
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ˆâ–„â–„â–…â–„â–†â–ƒâ–…â–…â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.43598
wandb:                 train/loss 0.21389
wandb:   val/directional_accuracy 49.53618
wandb:                   val/loss 0.13303
wandb:                    val/mae 0.03814
wandb:                   val/mape 3423715600.0
wandb:                    val/mse 0.00241
wandb:                     val/r2 -0.17434
wandb:                   val/rmse 0.04905
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/7b1wnulo
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_044004-7b1wnulo/logs
Completed: SASOL H=50

Training: Informer on SASOL for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_044201-5jgnd0fn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/5jgnd0fn
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SASOL_H100 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/5jgnd0fn
>>>>>>>start training : long_term_forecast_Informer_SASOL_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 100
============================================================
train 3675
Overriding target from 'OT' to 'close' for stock data
val 114
Overriding target from 'OT' to 'close' for stock data
test 115
Epoch: 1, Steps: 115 | Train Loss: 0.3474777 Vali Loss: 0.1605277 Test Loss: 0.4713182
Validation loss decreased (inf --> 0.160528).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 115 | Train Loss: 0.2769174 Vali Loss: 0.1772950 Test Loss: 0.4738343
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3474776769461839, 'val/loss': 0.16052765026688576, 'test/loss': 0.47131823748350143, '_timestamp': 1762310535.9227202}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.27691739357036094, 'val/loss': 0.1772949919104576, 'test/loss': 0.473834328353405, '_timestamp': 1762310542.2423975}).
Epoch: 3, Steps: 115 | Train Loss: 0.2488266 Vali Loss: 0.1423577 Test Loss: 1.0646261
Validation loss decreased (0.160528 --> 0.142358).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 115 | Train Loss: 0.2341562 Vali Loss: 0.1464622 Test Loss: 1.2828955
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 115 | Train Loss: 0.2277505 Vali Loss: 0.1533036 Test Loss: 1.2853722
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 115 | Train Loss: 0.2260714 Vali Loss: 0.1541646 Test Loss: 1.2375377
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 115 | Train Loss: 0.2251764 Vali Loss: 0.1533619 Test Loss: 1.2902931
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 115 | Train Loss: 0.2248919 Vali Loss: 0.1540862 Test Loss: 1.3454058
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 115 | Train Loss: 0.2245448 Vali Loss: 0.1553484 Test Loss: 1.2171616
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 115 | Train Loss: 0.2235792 Vali Loss: 0.1544973 Test Loss: 1.2494323
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 115 | Train Loss: 0.2242392 Vali Loss: 0.1578403 Test Loss: 1.2301993
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 115 | Train Loss: 0.2232737 Vali Loss: 0.1561221 Test Loss: 1.2716282
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 115 | Train Loss: 0.2236469 Vali Loss: 0.1590322 Test Loss: 1.2628369
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SASOL_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 115
test shape: (115, 100, 1) (115, 100, 1)
test shape: (115, 100, 1) (115, 100, 1)


	mse:0.0023245590273290873, mae:0.03787972405552864, rmse:0.04821367934346199, r2:-0.17140507698059082, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0379, RMSE: 0.0482, RÂ²: -0.1714, MAPE: 35778504.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.691 MB of 0.695 MB uploadedwandb: \ 0.695 MB of 0.695 MB uploadedwandb: | 0.695 MB of 0.695 MB uploadedwandb: / 0.695 MB of 0.994 MB uploadedwandb: - 0.994 MB of 0.994 MB uploadedwandb: \ 0.994 MB of 0.994 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–†â–‡â–…â–‡â–ˆâ–…â–†â–…â–†â–†
wandb:                 train/loss â–ˆâ–„â–‚â–‚â–‚â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ƒâ–†â–†â–†â–†â–†â–†â–‡â–‡â–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 1.26284
wandb:                 train/loss 0.22365
wandb:   val/directional_accuracy 48.44093
wandb:                   val/loss 0.15903
wandb:                    val/mae 0.03788
wandb:                   val/mape 3577850400.0
wandb:                    val/mse 0.00232
wandb:                     val/r2 -0.17141
wandb:                   val/rmse 0.04821
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/5jgnd0fn
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_044201-5jgnd0fn/logs
Completed: SASOL H=100

Informer training completed for all datasets!
