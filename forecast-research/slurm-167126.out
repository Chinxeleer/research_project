##############################################################################
# Training FEDformer Model on All Datasets
##############################################################################
Training: FEDformer on NVIDIA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_143541-15gkou38
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/15gkou38
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_NVIDIA_H3 Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
 fourier enhanced cross attention used!
modes_q=16, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/15gkou38
>>>>>>>start training : long_term_forecast_FEDformer_NVIDIA_H3_FEDformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.3251237 Vali Loss: 0.1877287 Test Loss: 0.3175514
Validation loss decreased (inf --> 0.187729).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3251236609946516, 'val/loss': 0.18772874772548676, 'test/loss': 0.31755139864981174, '_timestamp': 1762778169.7087548}).
Epoch: 2, Steps: 133 | Train Loss: 0.2637453 Vali Loss: 0.1925579 Test Loss: 0.3167030
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26374531811789464, 'val/loss': 0.1925578834488988, 'test/loss': 0.3167029730975628, '_timestamp': 1762778192.0972219}).
Epoch: 3, Steps: 133 | Train Loss: 0.2507357 Vali Loss: 0.1845967 Test Loss: 0.3182118
Validation loss decreased (0.187729 --> 0.184597).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2448930 Vali Loss: 0.1782073 Test Loss: 0.3150024
Validation loss decreased (0.184597 --> 0.178207).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2423529 Vali Loss: 0.1799927 Test Loss: 0.3132913
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2400870 Vali Loss: 0.1810503 Test Loss: 0.3143538
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2386350 Vali Loss: 0.1847051 Test Loss: 0.3122144
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2384016 Vali Loss: 0.1968674 Test Loss: 0.3123392
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2384389 Vali Loss: 0.1759749 Test Loss: 0.3132515
Validation loss decreased (0.178207 --> 0.175975).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2385980 Vali Loss: 0.1869685 Test Loss: 0.3129544
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2386396 Vali Loss: 0.1763129 Test Loss: 0.3129985
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2387804 Vali Loss: 0.1745049 Test Loss: 0.3129438
Validation loss decreased (0.175975 --> 0.174505).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2392524 Vali Loss: 0.1836078 Test Loss: 0.3128933
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2390654 Vali Loss: 0.1804749 Test Loss: 0.3128917
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2386218 Vali Loss: 0.1762743 Test Loss: 0.3129008
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2384181 Vali Loss: 0.1814803 Test Loss: 0.3129005
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2388201 Vali Loss: 0.1765922 Test Loss: 0.3129021
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2386587 Vali Loss: 0.2016075 Test Loss: 0.3129013
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2375482 Vali Loss: 0.1948698 Test Loss: 0.3129019
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2374748 Vali Loss: 0.1847965 Test Loss: 0.3129021
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2381136 Vali Loss: 0.1838577 Test Loss: 0.3129021
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2388533 Vali Loss: 0.2013981 Test Loss: 0.3129021
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_NVIDIA_H3_FEDformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.0011636391282081604, mae:0.025746535509824753, rmse:0.03411215543746948, r2:-0.03730881214141846, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0257, RMSE: 0.0341, RÂ²: -0.0373, MAPE: 783258.31%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.463 MB of 0.463 MB uploadedwandb: \ 0.463 MB of 0.463 MB uploadedwandb: | 0.463 MB of 0.463 MB uploadedwandb: / 0.463 MB of 0.463 MB uploadedwandb: - 0.463 MB of 0.463 MB uploadedwandb: \ 0.463 MB of 0.463 MB uploadedwandb: | 0.463 MB of 0.463 MB uploadedwandb: / 0.463 MB of 0.463 MB uploadedwandb: - 0.592 MB of 0.804 MB uploaded (0.002 MB deduped)wandb: \ 0.804 MB of 0.804 MB uploaded (0.002 MB deduped)wandb: | 0.804 MB of 0.804 MB uploaded (0.002 MB deduped)wandb: / 0.804 MB of 0.804 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–ƒâ–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–‚â–‚â–ƒâ–„â–‡â–â–„â–â–â–ƒâ–ƒâ–â–ƒâ–‚â–ˆâ–†â–„â–ƒâ–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 14594054
wandb:     model/trainable_params 14594054
wandb:                  test/loss 0.3129
wandb:                 train/loss 0.23885
wandb:   val/directional_accuracy 48.94515
wandb:                   val/loss 0.2014
wandb:                    val/mae 0.02575
wandb:                   val/mape 78325831.25
wandb:                    val/mse 0.00116
wandb:                     val/r2 -0.03731
wandb:                   val/rmse 0.03411
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/15gkou38
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_143541-15gkou38/logs
Completed: NVIDIA H=3

Training: FEDformer on NVIDIA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_144421-ctqubg7s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/ctqubg7s
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_NVIDIA_H5 Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
 fourier enhanced cross attention used!
modes_q=17, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/ctqubg7s
>>>>>>>start training : long_term_forecast_FEDformer_NVIDIA_H5_FEDformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.3158832 Vali Loss: 0.1902368 Test Loss: 0.3447066
Validation loss decreased (inf --> 0.190237).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.31588324164985715, 'val/loss': 0.1902368012815714, 'test/loss': 0.3447066079825163, '_timestamp': 1762778690.1124868}).
Epoch: 2, Steps: 133 | Train Loss: 0.2600416 Vali Loss: 0.1770237 Test Loss: 0.3299772
Validation loss decreased (0.190237 --> 0.177024).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2600415520426026, 'val/loss': 0.177023746073246, 'test/loss': 0.3299771938472986, '_timestamp': 1762778713.496449}).
Epoch: 3, Steps: 133 | Train Loss: 0.2477478 Vali Loss: 0.1970199 Test Loss: 0.3339467
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2425631 Vali Loss: 0.1848350 Test Loss: 0.3267894
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2417481 Vali Loss: 0.1769413 Test Loss: 0.3247144
Validation loss decreased (0.177024 --> 0.176941).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2411584 Vali Loss: 0.1789684 Test Loss: 0.3258556
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2400798 Vali Loss: 0.1799550 Test Loss: 0.3277174
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2401494 Vali Loss: 0.1786609 Test Loss: 0.3283967
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2392889 Vali Loss: 0.1817649 Test Loss: 0.3259727
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2395992 Vali Loss: 0.1825995 Test Loss: 0.3261037
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2383115 Vali Loss: 0.1804847 Test Loss: 0.3262690
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2389981 Vali Loss: 0.1940124 Test Loss: 0.3262950
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2395005 Vali Loss: 0.1776084 Test Loss: 0.3262659
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2389622 Vali Loss: 0.1806924 Test Loss: 0.3262859
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2384085 Vali Loss: 0.1825905 Test Loss: 0.3262833
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_NVIDIA_H5_FEDformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.001173434779047966, mae:0.025958940386772156, rmse:0.034255433827638626, r2:-0.0392838716506958, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0260, RMSE: 0.0343, RÂ²: -0.0393, MAPE: 833912.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.510 MB of 0.510 MB uploadedwandb: \ 0.510 MB of 0.510 MB uploadedwandb: | 0.510 MB of 0.510 MB uploadedwandb: / 0.510 MB of 0.510 MB uploadedwandb: - 0.510 MB of 0.721 MB uploadedwandb: \ 0.721 MB of 0.721 MB uploadedwandb: | 0.721 MB of 0.721 MB uploadedwandb: / 0.721 MB of 0.721 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–‚â–ƒâ–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–„â–„â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–‡â–â–‚â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 14725126
wandb:     model/trainable_params 14725126
wandb:                  test/loss 0.32628
wandb:                 train/loss 0.23841
wandb:   val/directional_accuracy 47.23404
wandb:                   val/loss 0.18259
wandb:                    val/mae 0.02596
wandb:                   val/mape 83391225.0
wandb:                    val/mse 0.00117
wandb:                     val/r2 -0.03928
wandb:                   val/rmse 0.03426
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/ctqubg7s
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_144421-ctqubg7s/logs
Completed: NVIDIA H=5

Training: FEDformer on NVIDIA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_145030-xwi5ji5c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/xwi5ji5c
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_NVIDIA_H10Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
 fourier enhanced cross attention used!
modes_q=20, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/xwi5ji5c
>>>>>>>start training : long_term_forecast_FEDformer_NVIDIA_H10_FEDformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.3078348 Vali Loss: 0.1867487 Test Loss: 0.3619372
Validation loss decreased (inf --> 0.186749).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3078347825466242, 'val/loss': 0.1867487020790577, 'test/loss': 0.36193715408444405, '_timestamp': 1762779061.2207928}).
Epoch: 2, Steps: 133 | Train Loss: 0.2558605 Vali Loss: 0.1877957 Test Loss: 0.3515521
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25586050222242684, 'val/loss': 0.18779570795595646, 'test/loss': 0.3515521455556154, '_timestamp': 1762779084.826852}).
Epoch: 3, Steps: 133 | Train Loss: 0.2466405 Vali Loss: 0.1796943 Test Loss: 0.3478270
Validation loss decreased (0.186749 --> 0.179694).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2442601 Vali Loss: 0.1777817 Test Loss: 0.3534191
Validation loss decreased (0.179694 --> 0.177782).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2413843 Vali Loss: 0.2042273 Test Loss: 0.3482324
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2416169 Vali Loss: 0.1797917 Test Loss: 0.3519198
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2418981 Vali Loss: 0.1807617 Test Loss: 0.3521649
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2404748 Vali Loss: 0.1815701 Test Loss: 0.3524972
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2410971 Vali Loss: 0.1997367 Test Loss: 0.3524642
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2400478 Vali Loss: 0.1755626 Test Loss: 0.3524453
Validation loss decreased (0.177782 --> 0.175563).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2404216 Vali Loss: 0.1780935 Test Loss: 0.3525618
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2401307 Vali Loss: 0.1811487 Test Loss: 0.3527517
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2411131 Vali Loss: 0.1868866 Test Loss: 0.3527268
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2403125 Vali Loss: 0.1768582 Test Loss: 0.3527505
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2405855 Vali Loss: 0.1790593 Test Loss: 0.3527455
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2401365 Vali Loss: 0.1783512 Test Loss: 0.3527447
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2401368 Vali Loss: 0.1798033 Test Loss: 0.3527438
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2406735 Vali Loss: 0.1818849 Test Loss: 0.3527447
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2408504 Vali Loss: 0.1976174 Test Loss: 0.3527448
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2395881 Vali Loss: 0.1782918 Test Loss: 0.3527447
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_NVIDIA_H10_FEDformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.0012147781671956182, mae:0.02642807736992836, rmse:0.03485366702079773, r2:-0.058976173400878906, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0264, RMSE: 0.0349, RÂ²: -0.0590, MAPE: 882804.94%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.546 MB of 0.547 MB uploadedwandb: \ 0.546 MB of 0.547 MB uploadedwandb: | 0.546 MB of 0.547 MB uploadedwandb: / 0.547 MB of 0.547 MB uploadedwandb: - 0.547 MB of 0.547 MB uploadedwandb: \ 0.547 MB of 0.759 MB uploadedwandb: | 0.759 MB of 0.759 MB uploadedwandb: / 0.759 MB of 0.759 MB uploadedwandb: - 0.759 MB of 0.759 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–‚â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:                 train/loss â–ˆâ–†â–ƒâ–ƒâ–ƒâ–‚â–‚â–â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–‚â–ˆâ–‚â–‚â–‚â–‡â–â–‚â–‚â–„â–â–‚â–‚â–‚â–ƒâ–†â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 15118342
wandb:     model/trainable_params 15118342
wandb:                  test/loss 0.35274
wandb:                 train/loss 0.23959
wandb:   val/directional_accuracy 45.12077
wandb:                   val/loss 0.17829
wandb:                    val/mae 0.02643
wandb:                   val/mape 88280493.75
wandb:                    val/mse 0.00121
wandb:                     val/r2 -0.05898
wandb:                   val/rmse 0.03485
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/xwi5ji5c
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_145030-xwi5ji5c/logs
Completed: NVIDIA H=10

Training: FEDformer on NVIDIA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_145851-v7n0t6l2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/v7n0t6l2
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_NVIDIA_H22Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
 fourier enhanced cross attention used!
modes_q=26, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/v7n0t6l2
>>>>>>>start training : long_term_forecast_FEDformer_NVIDIA_H22_FEDformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2989068 Vali Loss: 0.1964817 Test Loss: 0.4395965
Validation loss decreased (inf --> 0.196482).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2989068254828453, 'val/loss': 0.1964817132268633, 'test/loss': 0.43959646352699827, '_timestamp': 1762779561.4381528}).
Epoch: 2, Steps: 132 | Train Loss: 0.2548933 Vali Loss: 0.1953597 Test Loss: 0.4496318
Validation loss decreased (0.196482 --> 0.195360).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2548932927575978, 'val/loss': 0.19535969197750092, 'test/loss': 0.4496317718710218, '_timestamp': 1762779586.0064435}).
Epoch: 3, Steps: 132 | Train Loss: 0.2497257 Vali Loss: 0.1873673 Test Loss: 0.4356543
Validation loss decreased (0.195360 --> 0.187367).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2468237 Vali Loss: 0.1872787 Test Loss: 0.4385664
Validation loss decreased (0.187367 --> 0.187279).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2454476 Vali Loss: 0.1880856 Test Loss: 0.4397273
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2451965 Vali Loss: 0.1868730 Test Loss: 0.4354942
Validation loss decreased (0.187279 --> 0.186873).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2446418 Vali Loss: 0.1875954 Test Loss: 0.4365705
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2444328 Vali Loss: 0.1865430 Test Loss: 0.4350382
Validation loss decreased (0.186873 --> 0.186543).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2441477 Vali Loss: 0.1877886 Test Loss: 0.4352645
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2444503 Vali Loss: 0.1863255 Test Loss: 0.4355873
Validation loss decreased (0.186543 --> 0.186325).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2443325 Vali Loss: 0.1873022 Test Loss: 0.4356527
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2443338 Vali Loss: 0.1882965 Test Loss: 0.4356722
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2441705 Vali Loss: 0.1870599 Test Loss: 0.4356475
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2442763 Vali Loss: 0.1864010 Test Loss: 0.4356481
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2445215 Vali Loss: 0.1872540 Test Loss: 0.4356447
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2442660 Vali Loss: 0.1875979 Test Loss: 0.4356419
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2441077 Vali Loss: 0.1872030 Test Loss: 0.4356420
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2442436 Vali Loss: 0.1880182 Test Loss: 0.4356418
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2443671 Vali Loss: 0.1882215 Test Loss: 0.4356416
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2443243 Vali Loss: 0.1872830 Test Loss: 0.4356416
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_NVIDIA_H22_FEDformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.0012436346150934696, mae:0.026557644829154015, rmse:0.035265203565359116, r2:-0.054056525230407715, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0266, RMSE: 0.0353, RÂ²: -0.0541, MAPE: 883169.62%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.562 MB of 0.563 MB uploadedwandb: \ 0.562 MB of 0.563 MB uploadedwandb: | 0.562 MB of 0.563 MB uploadedwandb: / 0.563 MB of 0.563 MB uploadedwandb: - 0.563 MB of 0.563 MB uploadedwandb: \ 0.563 MB of 0.775 MB uploadedwandb: | 0.678 MB of 0.775 MB uploadedwandb: / 0.775 MB of 0.775 MB uploadedwandb: - 0.775 MB of 0.775 MB uploadedwandb: \ 0.775 MB of 0.775 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‚â–†â–ˆâ–‚â–ƒâ–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–„â–‡â–ƒâ–†â–‚â–†â–â–„â–ˆâ–„â–â–„â–†â–„â–‡â–ˆâ–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 15904774
wandb:     model/trainable_params 15904774
wandb:                  test/loss 0.43564
wandb:                 train/loss 0.24432
wandb:   val/directional_accuracy 45.25994
wandb:                   val/loss 0.18728
wandb:                    val/mae 0.02656
wandb:                   val/mape 88316962.5
wandb:                    val/mse 0.00124
wandb:                     val/r2 -0.05406
wandb:                   val/rmse 0.03527
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/v7n0t6l2
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_145851-v7n0t6l2/logs
Completed: NVIDIA H=22

Training: FEDformer on NVIDIA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_150733-epuwseka
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/epuwseka
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_NVIDIA_H50Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 39]
 fourier enhanced cross attention used!
modes_q=32, index_q=[0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 21, 22, 24, 26, 28, 29, 30, 33, 34, 35, 36, 37, 38, 39]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/epuwseka
>>>>>>>start training : long_term_forecast_FEDformer_NVIDIA_H50_FEDformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.3018296 Vali Loss: 0.2051935 Test Loss: 0.5879675
Validation loss decreased (inf --> 0.205193).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.30182963327476475, 'val/loss': 0.20519345253705978, 'test/loss': 0.5879674653212229, '_timestamp': 1762780085.7225423}).
Epoch: 2, Steps: 132 | Train Loss: 0.2648283 Vali Loss: 0.2015682 Test Loss: 0.5694353
Validation loss decreased (0.205193 --> 0.201568).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2648283201410915, 'val/loss': 0.20156821608543396, 'test/loss': 0.5694353009263674, '_timestamp': 1762780112.2282276}).
Epoch: 3, Steps: 132 | Train Loss: 0.2601876 Vali Loss: 0.2001299 Test Loss: 0.5625857
Validation loss decreased (0.201568 --> 0.200130).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2580998 Vali Loss: 0.2054997 Test Loss: 0.5711238
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2564543 Vali Loss: 0.2037043 Test Loss: 0.5686991
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2557677 Vali Loss: 0.2025274 Test Loss: 0.5705058
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2551884 Vali Loss: 0.2027279 Test Loss: 0.5682762
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2557498 Vali Loss: 0.2028672 Test Loss: 0.5686880
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2562583 Vali Loss: 0.2030897 Test Loss: 0.5694191
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2548906 Vali Loss: 0.2033810 Test Loss: 0.5696452
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2551766 Vali Loss: 0.2030164 Test Loss: 0.5700000
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2549121 Vali Loss: 0.2033808 Test Loss: 0.5701251
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2553692 Vali Loss: 0.2034685 Test Loss: 0.5700763
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_NVIDIA_H50_FEDformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.0012184270890429616, mae:0.0265419352799654, rmse:0.03490597382187843, r2:-0.02389514446258545, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0265, RMSE: 0.0349, RÂ²: -0.0239, MAPE: 617064.44%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.613 MB of 0.616 MB uploadedwandb: \ 0.616 MB of 0.616 MB uploadedwandb: | 0.616 MB of 0.616 MB uploadedwandb: / 0.616 MB of 0.827 MB uploadedwandb: - 0.827 MB of 0.827 MB uploadedwandb: \ 0.827 MB of 0.827 MB uploadedwandb: | 0.827 MB of 0.827 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–†â–‡â–†â–†â–‡â–‡â–‡â–‡â–‡
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–‚â–ƒâ–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ˆâ–†â–„â–„â–…â–…â–…â–…â–…â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 16691206
wandb:     model/trainable_params 16691206
wandb:                  test/loss 0.57008
wandb:                 train/loss 0.25537
wandb:   val/directional_accuracy 49.20516
wandb:                   val/loss 0.20347
wandb:                    val/mae 0.02654
wandb:                   val/mape 61706443.75
wandb:                    val/mse 0.00122
wandb:                     val/r2 -0.0239
wandb:                   val/rmse 0.03491
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/epuwseka
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_150733-epuwseka/logs
Completed: NVIDIA H=50

Training: FEDformer on NVIDIA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_151345-tryzjwoj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/tryzjwoj
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_NVIDIA_H100Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 8, 13, 16, 20, 22, 23, 24, 28, 29, 30, 31, 32, 34, 35, 37, 39, 43, 44, 46, 51, 52, 55, 56, 57, 62, 63, 64]
 fourier enhanced cross attention used!
modes_q=32, index_q=[0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 17, 18, 20, 21, 22, 23, 28, 30, 34, 36, 44, 45, 50, 53, 54, 55, 56, 57, 61, 63]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/tryzjwoj
>>>>>>>start training : long_term_forecast_FEDformer_NVIDIA_H100_FEDformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.3191982 Vali Loss: 0.2393869 Test Loss: 0.8359692
Validation loss decreased (inf --> 0.239387).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3191982200512519, 'val/loss': 0.2393869489431381, 'test/loss': 0.8359692215919494, '_timestamp': 1762780457.8772824}).
Epoch: 2, Steps: 130 | Train Loss: 0.2820920 Vali Loss: 0.2306563 Test Loss: 0.8632097
Validation loss decreased (0.239387 --> 0.230656).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.28209199068637997, 'val/loss': 0.2306563138961792, 'test/loss': 0.8632096767425537, '_timestamp': 1762780484.2518833}).
Epoch: 3, Steps: 130 | Train Loss: 0.2785642 Vali Loss: 0.2386036 Test Loss: 0.8244371
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2776330 Vali Loss: 0.2352490 Test Loss: 0.8627902
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2759836 Vali Loss: 0.2410516 Test Loss: 0.8382514
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2756167 Vali Loss: 0.2458110 Test Loss: 0.8561252
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2753363 Vali Loss: 0.2414714 Test Loss: 0.8534252
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2753251 Vali Loss: 0.2382247 Test Loss: 0.8512311
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2750585 Vali Loss: 0.2369818 Test Loss: 0.8487479
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2749934 Vali Loss: 0.2385333 Test Loss: 0.8480861
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2750254 Vali Loss: 0.2404424 Test Loss: 0.8484869
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2754980 Vali Loss: 0.2389156 Test Loss: 0.8484993
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_NVIDIA_H100_FEDformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0013062598882243037, mae:0.027487827464938164, rmse:0.036142218858003616, r2:-0.014637470245361328, dtw:Not calculated


VAL - MSE: 0.0013, MAE: 0.0275, RMSE: 0.0361, RÂ²: -0.0146, MAPE: 418859.59%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.689 MB of 0.694 MB uploadedwandb: \ 0.689 MB of 0.694 MB uploadedwandb: | 0.689 MB of 0.694 MB uploadedwandb: / 0.694 MB of 0.694 MB uploadedwandb: - 0.694 MB of 0.694 MB uploadedwandb: \ 0.694 MB of 0.905 MB uploadedwandb: | 0.905 MB of 0.905 MB uploadedwandb: / 0.905 MB of 0.905 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–„â–‡â–†â–†â–…â–…â–…â–…
wandb:                 train/loss â–ˆâ–†â–ƒâ–‚â–‚â–‚â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–â–…â–ˆâ–…â–ƒâ–‚â–ƒâ–„â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 16691206
wandb:     model/trainable_params 16691206
wandb:                  test/loss 0.8485
wandb:                 train/loss 0.2755
wandb:   val/directional_accuracy 48.99711
wandb:                   val/loss 0.23892
wandb:                    val/mae 0.02749
wandb:                   val/mape 41885959.375
wandb:                    val/mse 0.00131
wandb:                     val/r2 -0.01464
wandb:                   val/rmse 0.03614
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/tryzjwoj
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_151345-tryzjwoj/logs
Completed: NVIDIA H=100

Training: FEDformer on APPLE for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_151927-wlh82xes
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/wlh82xes
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_APPLE_H3  Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
 fourier enhanced cross attention used!
modes_q=16, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/wlh82xes
>>>>>>>start training : long_term_forecast_FEDformer_APPLE_H3_FEDformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.3151149 Vali Loss: 0.0968072 Test Loss: 0.1450857
Validation loss decreased (inf --> 0.096807).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3151148546459083, 'val/loss': 0.09680721256881952, 'test/loss': 0.1450856775045395, '_timestamp': 1762780796.0527263}).
Epoch: 2, Steps: 133 | Train Loss: 0.2549899 Vali Loss: 0.0903981 Test Loss: 0.1340790
Validation loss decreased (0.096807 --> 0.090398).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2549899184614196, 'val/loss': 0.09039814304560423, 'test/loss': 0.13407902838662267, '_timestamp': 1762780818.06568}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2549899184614196, 'val/loss': 0.09039814304560423, 'test/loss': 0.13407902838662267, '_timestamp': 1762780818.06568}).
Epoch: 3, Steps: 133 | Train Loss: 0.2405900 Vali Loss: 0.0878843 Test Loss: 0.1337158
Validation loss decreased (0.090398 --> 0.087884).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2355934 Vali Loss: 0.0876055 Test Loss: 0.1359662
Validation loss decreased (0.087884 --> 0.087605).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2334173 Vali Loss: 0.0896958 Test Loss: 0.1332270
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2316667 Vali Loss: 0.0855442 Test Loss: 0.1322616
Validation loss decreased (0.087605 --> 0.085544).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2305681 Vali Loss: 0.0918867 Test Loss: 0.1320619
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2303314 Vali Loss: 0.0863056 Test Loss: 0.1323498
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2301163 Vali Loss: 0.0846567 Test Loss: 0.1323990
Validation loss decreased (0.085544 --> 0.084657).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2308150 Vali Loss: 0.0874812 Test Loss: 0.1322910
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2292065 Vali Loss: 0.0847106 Test Loss: 0.1322982
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2303367 Vali Loss: 0.0893489 Test Loss: 0.1322754
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2301068 Vali Loss: 0.0842001 Test Loss: 0.1322763
Validation loss decreased (0.084657 --> 0.084200).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2302516 Vali Loss: 0.0887126 Test Loss: 0.1322901
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2295085 Vali Loss: 0.0871672 Test Loss: 0.1322888
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2301956 Vali Loss: 0.0884575 Test Loss: 0.1322858
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2311234 Vali Loss: 0.0921723 Test Loss: 0.1322859
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2306910 Vali Loss: 0.0879463 Test Loss: 0.1322863
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2293144 Vali Loss: 0.0842920 Test Loss: 0.1322865
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2293356 Vali Loss: 0.0862025 Test Loss: 0.1322864
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2300261 Vali Loss: 0.0865505 Test Loss: 0.1322864
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2305820 Vali Loss: 0.0838603 Test Loss: 0.1322863
Validation loss decreased (0.084200 --> 0.083860).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2306165 Vali Loss: 0.0849098 Test Loss: 0.1322864
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2297819 Vali Loss: 0.0850847 Test Loss: 0.1322864
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2310072 Vali Loss: 0.0847167 Test Loss: 0.1322864
EarlyStopping counter: 3 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2309458 Vali Loss: 0.0851056 Test Loss: 0.1322864
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.2298795 Vali Loss: 0.0867161 Test Loss: 0.1322863
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.2299619 Vali Loss: 0.0862506 Test Loss: 0.1322863
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.2300688 Vali Loss: 0.0868361 Test Loss: 0.1322863
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 133 | Train Loss: 0.2306303 Vali Loss: 0.0869256 Test Loss: 0.1322863
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 133 | Train Loss: 0.2302426 Vali Loss: 0.0858275 Test Loss: 0.1322863
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 133 | Train Loss: 0.2300625 Vali Loss: 0.0850268 Test Loss: 0.1322863
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_APPLE_H3_FEDformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.0002170858788304031, mae:0.010827734135091305, rmse:0.014733834192156792, r2:-0.08574020862579346, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0108, RMSE: 0.0147, RÂ²: -0.0857, MAPE: 344043.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.475 MB of 0.475 MB uploadedwandb: \ 0.475 MB of 0.475 MB uploadedwandb: | 0.475 MB of 0.475 MB uploadedwandb: / 0.475 MB of 0.475 MB uploadedwandb: - 0.475 MB of 0.475 MB uploadedwandb: \ 0.475 MB of 0.475 MB uploadedwandb: | 0.604 MB of 0.818 MB uploaded (0.002 MB deduped)wandb: / 0.818 MB of 0.818 MB uploaded (0.002 MB deduped)wandb: - 0.818 MB of 0.818 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–„â–ˆâ–ƒâ–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–â–‚â–‚â–â–â–‚â–‚â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–„â–†â–‚â–ˆâ–ƒâ–‚â–„â–‚â–†â–â–…â–„â–…â–ˆâ–„â–â–ƒâ–ƒâ–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–ƒâ–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 31
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 14594054
wandb:     model/trainable_params 14594054
wandb:                  test/loss 0.13229
wandb:                 train/loss 0.23006
wandb:   val/directional_accuracy 46.83544
wandb:                   val/loss 0.08503
wandb:                    val/mae 0.01083
wandb:                   val/mape 34404300.0
wandb:                    val/mse 0.00022
wandb:                     val/r2 -0.08574
wandb:                   val/rmse 0.01473
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/wlh82xes
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_151927-wlh82xes/logs
Completed: APPLE H=3

Training: FEDformer on APPLE for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_153145-dthwsbr8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/dthwsbr8
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_APPLE_H5  Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
 fourier enhanced cross attention used!
modes_q=17, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/dthwsbr8
>>>>>>>start training : long_term_forecast_FEDformer_APPLE_H5_FEDformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.3055780 Vali Loss: 0.0921956 Test Loss: 0.1389674
Validation loss decreased (inf --> 0.092196).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.30557802857312943, 'val/loss': 0.09219556953758001, 'test/loss': 0.13896740227937698, '_timestamp': 1762781534.1585631}).
Epoch: 2, Steps: 133 | Train Loss: 0.2510346 Vali Loss: 0.0903202 Test Loss: 0.1382065
Validation loss decreased (0.092196 --> 0.090320).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25103458235586495, 'val/loss': 0.09032017644494772, 'test/loss': 0.1382064837962389, '_timestamp': 1762781557.182987}).
Epoch: 3, Steps: 133 | Train Loss: 0.2389312 Vali Loss: 0.0898750 Test Loss: 0.1356744
Validation loss decreased (0.090320 --> 0.089875).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2348362 Vali Loss: 0.0863674 Test Loss: 0.1372501
Validation loss decreased (0.089875 --> 0.086367).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2335361 Vali Loss: 0.0878013 Test Loss: 0.1358459
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2325280 Vali Loss: 0.0903572 Test Loss: 0.1356831
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2315037 Vali Loss: 0.0852293 Test Loss: 0.1353250
Validation loss decreased (0.086367 --> 0.085229).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2318107 Vali Loss: 0.0852688 Test Loss: 0.1350861
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2315603 Vali Loss: 0.0836983 Test Loss: 0.1354081
Validation loss decreased (0.085229 --> 0.083698).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2311175 Vali Loss: 0.0895912 Test Loss: 0.1353299
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2301842 Vali Loss: 0.0859853 Test Loss: 0.1353304
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2301732 Vali Loss: 0.0867226 Test Loss: 0.1353152
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2317500 Vali Loss: 0.0858213 Test Loss: 0.1353238
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2306250 Vali Loss: 0.0858894 Test Loss: 0.1353293
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2301740 Vali Loss: 0.0867249 Test Loss: 0.1353314
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2303835 Vali Loss: 0.0861906 Test Loss: 0.1353329
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2300874 Vali Loss: 0.0860865 Test Loss: 0.1353329
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2305900 Vali Loss: 0.0850462 Test Loss: 0.1353324
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2301671 Vali Loss: 0.0871252 Test Loss: 0.1353323
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_APPLE_H5_FEDformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.0002191392850363627, mae:0.010878017172217369, rmse:0.014803353697061539, r2:-0.09100747108459473, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0109, RMSE: 0.0148, RÂ²: -0.0910, MAPE: 329467.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.502 MB of 0.502 MB uploadedwandb: \ 0.502 MB of 0.502 MB uploadedwandb: | 0.502 MB of 0.502 MB uploadedwandb: / 0.502 MB of 0.714 MB uploadedwandb: - 0.714 MB of 0.714 MB uploadedwandb: \ 0.714 MB of 0.714 MB uploadedwandb: | 0.714 MB of 0.714 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ƒâ–ˆâ–ƒâ–ƒâ–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–„â–…â–ˆâ–ƒâ–ƒâ–â–‡â–ƒâ–„â–ƒâ–ƒâ–„â–„â–„â–‚â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 18
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 14725126
wandb:     model/trainable_params 14725126
wandb:                  test/loss 0.13533
wandb:                 train/loss 0.23017
wandb:   val/directional_accuracy 46.06383
wandb:                   val/loss 0.08713
wandb:                    val/mae 0.01088
wandb:                   val/mape 32946700.0
wandb:                    val/mse 0.00022
wandb:                     val/r2 -0.09101
wandb:                   val/rmse 0.0148
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/dthwsbr8
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_153145-dthwsbr8/logs
Completed: APPLE H=5

Training: FEDformer on APPLE for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_153927-htt02t5c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/htt02t5c
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_APPLE_H10 Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
 fourier enhanced cross attention used!
modes_q=20, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/htt02t5c
>>>>>>>start training : long_term_forecast_FEDformer_APPLE_H10_FEDformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2987824 Vali Loss: 0.0942567 Test Loss: 0.1401849
Validation loss decreased (inf --> 0.094257).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2987824043160991, 'val/loss': 0.09425674751400948, 'test/loss': 0.1401849128305912, '_timestamp': 1762781996.473875}).
Epoch: 2, Steps: 133 | Train Loss: 0.2487196 Vali Loss: 0.0978762 Test Loss: 0.1434352
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24871956204113208, 'val/loss': 0.09787623025476933, 'test/loss': 0.1434351596981287, '_timestamp': 1762782020.1724982}).
Epoch: 3, Steps: 133 | Train Loss: 0.2388683 Vali Loss: 0.0957350 Test Loss: 0.1413034
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2376441 Vali Loss: 0.0921001 Test Loss: 0.1385261
Validation loss decreased (0.094257 --> 0.092100).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2340553 Vali Loss: 0.0859120 Test Loss: 0.1367634
Validation loss decreased (0.092100 --> 0.085912).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2334578 Vali Loss: 0.0873532 Test Loss: 0.1366947
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2343406 Vali Loss: 0.0883767 Test Loss: 0.1362418
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2326478 Vali Loss: 0.0879798 Test Loss: 0.1361939
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2327713 Vali Loss: 0.0887292 Test Loss: 0.1364135
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2328445 Vali Loss: 0.0864019 Test Loss: 0.1363446
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2333465 Vali Loss: 0.0873604 Test Loss: 0.1363344
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2325636 Vali Loss: 0.0881635 Test Loss: 0.1363334
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2331062 Vali Loss: 0.0896233 Test Loss: 0.1363320
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2321648 Vali Loss: 0.0854946 Test Loss: 0.1363300
Validation loss decreased (0.085912 --> 0.085495).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2323539 Vali Loss: 0.0865821 Test Loss: 0.1363300
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2319917 Vali Loss: 0.0874639 Test Loss: 0.1363304
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2323629 Vali Loss: 0.0871451 Test Loss: 0.1363310
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2326115 Vali Loss: 0.0872159 Test Loss: 0.1363312
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2341047 Vali Loss: 0.0862215 Test Loss: 0.1363310
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2322285 Vali Loss: 0.0857393 Test Loss: 0.1363311
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2336165 Vali Loss: 0.0872364 Test Loss: 0.1363311
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2327154 Vali Loss: 0.0882218 Test Loss: 0.1363310
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2325764 Vali Loss: 0.0878728 Test Loss: 0.1363311
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2327608 Vali Loss: 0.0919113 Test Loss: 0.1363310
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_APPLE_H10_FEDformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.00021995966380927712, mae:0.010807598941028118, rmse:0.014831037260591984, r2:-0.08639907836914062, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0108, RMSE: 0.0148, RÂ²: -0.0864, MAPE: 274021.72%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.574 MB of 0.574 MB uploadedwandb: \ 0.574 MB of 0.574 MB uploadedwandb: | 0.574 MB of 0.574 MB uploadedwandb: / 0.574 MB of 0.787 MB uploadedwandb: - 0.753 MB of 0.787 MB uploadedwandb: \ 0.787 MB of 0.787 MB uploadedwandb: | 0.787 MB of 0.787 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–‡â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–ƒâ–â–ƒâ–‚â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–â–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–„â–â–‚â–‚â–‚â–‚â–â–â–‚â–ƒâ–ƒâ–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 23
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 15118342
wandb:     model/trainable_params 15118342
wandb:                  test/loss 0.13633
wandb:                 train/loss 0.23276
wandb:   val/directional_accuracy 46.8599
wandb:                   val/loss 0.09191
wandb:                    val/mae 0.01081
wandb:                   val/mape 27402171.875
wandb:                    val/mse 0.00022
wandb:                     val/r2 -0.0864
wandb:                   val/rmse 0.01483
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/htt02t5c
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_153927-htt02t5c/logs
Completed: APPLE H=10

Training: FEDformer on APPLE for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_154911-ws5rhy3m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/ws5rhy3m
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_APPLE_H22 Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
 fourier enhanced cross attention used!
modes_q=26, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/ws5rhy3m
>>>>>>>start training : long_term_forecast_FEDformer_APPLE_H22_FEDformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2931423 Vali Loss: 0.0884189 Test Loss: 0.1376556
Validation loss decreased (inf --> 0.088419).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2931422932129918, 'val/loss': 0.08841889351606369, 'test/loss': 0.13765560196978704, '_timestamp': 1762782583.1195683}).
Epoch: 2, Steps: 132 | Train Loss: 0.2503689 Vali Loss: 0.0938773 Test Loss: 0.1404244
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25036894315571495, 'val/loss': 0.09387727720396859, 'test/loss': 0.14042437183005468, '_timestamp': 1762782608.0619411}).
Epoch: 3, Steps: 132 | Train Loss: 0.2447029 Vali Loss: 0.0888076 Test Loss: 0.1375214
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2417417 Vali Loss: 0.0885006 Test Loss: 0.1378116
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2400796 Vali Loss: 0.0881320 Test Loss: 0.1378781
Validation loss decreased (0.088419 --> 0.088132).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2396462 Vali Loss: 0.0888437 Test Loss: 0.1387521
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2391937 Vali Loss: 0.0879278 Test Loss: 0.1379406
Validation loss decreased (0.088132 --> 0.087928).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2390045 Vali Loss: 0.0883393 Test Loss: 0.1382723
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2389144 Vali Loss: 0.0882919 Test Loss: 0.1382196
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2386679 Vali Loss: 0.0881927 Test Loss: 0.1381882
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2387018 Vali Loss: 0.0884254 Test Loss: 0.1382079
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2387839 Vali Loss: 0.0881297 Test Loss: 0.1381914
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2387217 Vali Loss: 0.0880938 Test Loss: 0.1381898
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2383799 Vali Loss: 0.0881824 Test Loss: 0.1381986
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2389043 Vali Loss: 0.0884443 Test Loss: 0.1381972
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2389614 Vali Loss: 0.0882435 Test Loss: 0.1381972
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2387641 Vali Loss: 0.0881988 Test Loss: 0.1381968
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_APPLE_H22_FEDformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.0002249732642667368, mae:0.010865772143006325, rmse:0.014999108389019966, r2:-0.08418500423431396, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0109, RMSE: 0.0150, RÂ²: -0.0842, MAPE: 666167.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.641 MB of 0.642 MB uploadedwandb: \ 0.641 MB of 0.642 MB uploadedwandb: | 0.642 MB of 0.642 MB uploadedwandb: / 0.642 MB of 0.642 MB uploadedwandb: - 0.642 MB of 0.854 MB uploadedwandb: \ 0.642 MB of 0.854 MB uploadedwandb: | 0.854 MB of 0.854 MB uploadedwandb: / 0.854 MB of 0.854 MB uploadedwandb: - 0.854 MB of 0.854 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ƒâ–ƒâ–ˆâ–ƒâ–…â–…â–…â–…â–…â–…â–…â–…â–…â–…
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–‚â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–ƒâ–ˆâ–â–„â–„â–ƒâ–…â–ƒâ–‚â–ƒâ–…â–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 15904774
wandb:     model/trainable_params 15904774
wandb:                  test/loss 0.1382
wandb:                 train/loss 0.23876
wandb:   val/directional_accuracy 45.21625
wandb:                   val/loss 0.0882
wandb:                    val/mae 0.01087
wandb:                   val/mape 66616775.0
wandb:                    val/mse 0.00022
wandb:                     val/r2 -0.08419
wandb:                   val/rmse 0.015
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/ws5rhy3m
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_154911-ws5rhy3m/logs
Completed: APPLE H=22

Training: FEDformer on APPLE for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_155641-wjbn9wc0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/wjbn9wc0
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_APPLE_H50 Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 39]
 fourier enhanced cross attention used!
modes_q=32, index_q=[0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 21, 22, 24, 26, 28, 29, 30, 33, 34, 35, 36, 37, 38, 39]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/wjbn9wc0
>>>>>>>start training : long_term_forecast_FEDformer_APPLE_H50_FEDformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.2973131 Vali Loss: 0.0956708 Test Loss: 0.1558298
Validation loss decreased (inf --> 0.095671).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29731311592640297, 'val/loss': 0.09567079196373622, 'test/loss': 0.15582978477080664, '_timestamp': 1762783033.9473686}).
Epoch: 2, Steps: 132 | Train Loss: 0.2603461 Vali Loss: 0.1038176 Test Loss: 0.1630236
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26034614861463057, 'val/loss': 0.10381762559215228, 'test/loss': 0.16302357986569405, '_timestamp': 1762783061.0446026}).
Epoch: 3, Steps: 132 | Train Loss: 0.2567931 Vali Loss: 0.0924040 Test Loss: 0.1539295
Validation loss decreased (0.095671 --> 0.092404).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2528211 Vali Loss: 0.0895361 Test Loss: 0.1545679
Validation loss decreased (0.092404 --> 0.089536).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2518048 Vali Loss: 0.0916264 Test Loss: 0.1564299
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2501773 Vali Loss: 0.0912361 Test Loss: 0.1564752
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2499490 Vali Loss: 0.0910538 Test Loss: 0.1562668
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2504338 Vali Loss: 0.0905593 Test Loss: 0.1559995
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2515075 Vali Loss: 0.0904978 Test Loss: 0.1559050
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2500221 Vali Loss: 0.0905765 Test Loss: 0.1558914
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2503827 Vali Loss: 0.0906450 Test Loss: 0.1558748
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2498617 Vali Loss: 0.0906391 Test Loss: 0.1559064
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2517994 Vali Loss: 0.0904940 Test Loss: 0.1558971
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2503315 Vali Loss: 0.0905937 Test Loss: 0.1559063
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_APPLE_H50_FEDformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.0002395748597336933, mae:0.011185561306774616, rmse:0.015478205867111683, r2:-0.0797739028930664, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0112, RMSE: 0.0155, RÂ²: -0.0798, MAPE: 259351.92%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.718 MB of 0.721 MB uploadedwandb: \ 0.721 MB of 0.721 MB uploadedwandb: | 0.721 MB of 0.721 MB uploadedwandb: / 0.721 MB of 0.932 MB uploadedwandb: - 0.932 MB of 0.932 MB uploadedwandb: \ 0.932 MB of 0.932 MB uploadedwandb: | 0.932 MB of 0.932 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ƒâ–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–†â–†â–†
wandb:                 train/loss â–ˆâ–„â–ƒâ–â–â–‚â–ƒâ–â–‚â–â–ƒâ–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–†â–…â–…â–ƒâ–ƒâ–„â–„â–„â–ƒâ–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 13
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 16691206
wandb:     model/trainable_params 16691206
wandb:                  test/loss 0.15591
wandb:                 train/loss 0.25033
wandb:   val/directional_accuracy 48.3029
wandb:                   val/loss 0.09059
wandb:                    val/mae 0.01119
wandb:                   val/mape 25935192.1875
wandb:                    val/mse 0.00024
wandb:                     val/r2 -0.07977
wandb:                   val/rmse 0.01548
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/wjbn9wc0
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_155641-wjbn9wc0/logs
Completed: APPLE H=50

Training: FEDformer on APPLE for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_160319-b3mr03v7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/b3mr03v7
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_APPLE_H100Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 8, 13, 16, 20, 22, 23, 24, 28, 29, 30, 31, 32, 34, 35, 37, 39, 43, 44, 46, 51, 52, 55, 56, 57, 62, 63, 64]
 fourier enhanced cross attention used!
modes_q=32, index_q=[0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 17, 18, 20, 21, 22, 23, 28, 30, 34, 36, 44, 45, 50, 53, 54, 55, 56, 57, 61, 63]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/b3mr03v7
>>>>>>>start training : long_term_forecast_FEDformer_APPLE_H100_FEDformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.3109515 Vali Loss: 0.0977799 Test Loss: 0.1649416
Validation loss decreased (inf --> 0.097780).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.31095150617452766, 'val/loss': 0.0977798968553543, 'test/loss': 0.16494157910346985, '_timestamp': 1762783430.7045596}).
Epoch: 2, Steps: 130 | Train Loss: 0.2739298 Vali Loss: 0.0933590 Test Loss: 0.1698909
Validation loss decreased (0.097780 --> 0.093359).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2739297705201002, 'val/loss': 0.09335898458957673, 'test/loss': 0.16989092230796815, '_timestamp': 1762783456.785107}).
Epoch: 3, Steps: 130 | Train Loss: 0.2691900 Vali Loss: 0.0906689 Test Loss: 0.1681315
Validation loss decreased (0.093359 --> 0.090669).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2677164 Vali Loss: 0.0964613 Test Loss: 0.1774975
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2659343 Vali Loss: 0.0934922 Test Loss: 0.1735343
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2650054 Vali Loss: 0.0952944 Test Loss: 0.1769598
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2645806 Vali Loss: 0.0940603 Test Loss: 0.1757191
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2647895 Vali Loss: 0.0938687 Test Loss: 0.1749564
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2647789 Vali Loss: 0.0935708 Test Loss: 0.1750115
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2642452 Vali Loss: 0.0936353 Test Loss: 0.1750241
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2643605 Vali Loss: 0.0934112 Test Loss: 0.1750381
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2649602 Vali Loss: 0.0940246 Test Loss: 0.1750435
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2650262 Vali Loss: 0.0938223 Test Loss: 0.1750523
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_APPLE_H100_FEDformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0002478626847732812, mae:0.011229078285396099, rmse:0.01574365608394146, r2:-0.04976832866668701, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0112, RMSE: 0.0157, RÂ²: -0.0498, MAPE: 772755.88%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.721 MB of 0.726 MB uploadedwandb: \ 0.721 MB of 0.726 MB uploadedwandb: | 0.726 MB of 0.726 MB uploadedwandb: / 0.726 MB of 0.726 MB uploadedwandb: - 0.726 MB of 0.938 MB uploadedwandb: \ 0.938 MB of 0.938 MB uploadedwandb: | 0.938 MB of 0.938 MB uploadedwandb: / 0.938 MB of 0.938 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–…â–ˆâ–‡â–†â–†â–†â–†â–†â–†
wandb:                 train/loss â–ˆâ–†â–ƒâ–‚â–â–‚â–‚â–â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ˆâ–„â–‡â–…â–…â–…â–…â–„â–…â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 16691206
wandb:     model/trainable_params 16691206
wandb:                  test/loss 0.17505
wandb:                 train/loss 0.26503
wandb:   val/directional_accuracy 48.29004
wandb:                   val/loss 0.09382
wandb:                    val/mae 0.01123
wandb:                   val/mape 77275587.5
wandb:                    val/mse 0.00025
wandb:                     val/r2 -0.04977
wandb:                   val/rmse 0.01574
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/b3mr03v7
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_160319-b3mr03v7/logs
Exception in thread IntMsgThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 300, in check_internal_messages
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 268, in check_network_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 848, in deliver_network_status
    return self._deliver_network_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 510, in _deliver_network_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 856, in deliver_internal_messages
    return self._deliver_internal_messages(internal_message)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 516, in _deliver_internal_messages
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: APPLE H=100

Training: FEDformer on SP500 for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_160922-krh8ix20
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/krh8ix20
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_SP500_H3  Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
 fourier enhanced cross attention used!
modes_q=16, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/krh8ix20
>>>>>>>start training : long_term_forecast_FEDformer_SP500_H3_FEDformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 3
============================================================
train 4241
val 237
test 238
Epoch: 1, Steps: 133 | Train Loss: 0.2630136 Vali Loss: 0.0751999 Test Loss: 0.0832186
Validation loss decreased (inf --> 0.075200).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.26301362875260803, 'val/loss': 0.07519990019500256, 'test/loss': 0.083218555431813, '_timestamp': 1762783790.1615353}).
Epoch: 2, Steps: 133 | Train Loss: 0.2042141 Vali Loss: 0.0748341 Test Loss: 0.0805647
Validation loss decreased (0.075200 --> 0.074834).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.20421407158885682, 'val/loss': 0.07483409158885479, 'test/loss': 0.08056470984593034, '_timestamp': 1762783812.8421936}).
Epoch: 3, Steps: 133 | Train Loss: 0.1927808 Vali Loss: 0.0684577 Test Loss: 0.0766307
Validation loss decreased (0.074834 --> 0.068458).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1879295 Vali Loss: 0.0686817 Test Loss: 0.0755595
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1860153 Vali Loss: 0.0696709 Test Loss: 0.0764862
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1849350 Vali Loss: 0.0700607 Test Loss: 0.0759923
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1837756 Vali Loss: 0.0693218 Test Loss: 0.0760196
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1836801 Vali Loss: 0.0711016 Test Loss: 0.0758739
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1830201 Vali Loss: 0.0695782 Test Loss: 0.0759007
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1835243 Vali Loss: 0.0696430 Test Loss: 0.0758487
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1846374 Vali Loss: 0.0677221 Test Loss: 0.0758515
Validation loss decreased (0.068458 --> 0.067722).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1829232 Vali Loss: 0.0714586 Test Loss: 0.0758447
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1838172 Vali Loss: 0.0693769 Test Loss: 0.0758421
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1828481 Vali Loss: 0.0673783 Test Loss: 0.0758403
Validation loss decreased (0.067722 --> 0.067378).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1832031 Vali Loss: 0.0677758 Test Loss: 0.0758417
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1853230 Vali Loss: 0.0701874 Test Loss: 0.0758414
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1850592 Vali Loss: 0.0687829 Test Loss: 0.0758411
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1839629 Vali Loss: 0.0718221 Test Loss: 0.0758409
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1835325 Vali Loss: 0.0692008 Test Loss: 0.0758412
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1835434 Vali Loss: 0.0700116 Test Loss: 0.0758411
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1835918 Vali Loss: 0.0690075 Test Loss: 0.0758411
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1830884 Vali Loss: 0.0699257 Test Loss: 0.0758411
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1837875 Vali Loss: 0.0684975 Test Loss: 0.0758410
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.1835521 Vali Loss: 0.0701559 Test Loss: 0.0758410
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_SP500_H3_FEDformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 238
test shape: (238, 3, 1) (238, 3, 1)
test shape: (238, 3, 1) (238, 3, 1)


	mse:6.890163058415055e-05, mae:0.0062104640528559685, rmse:0.008300700224936008, r2:-0.05984163284301758, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0062, RMSE: 0.0083, RÂ²: -0.0598, MAPE: 2.39%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.499 MB of 0.499 MB uploadedwandb: \ 0.499 MB of 0.499 MB uploadedwandb: | 0.499 MB of 0.499 MB uploadedwandb: / 0.499 MB of 0.499 MB uploadedwandb: - 0.499 MB of 0.499 MB uploadedwandb: \ 0.499 MB of 0.499 MB uploadedwandb: | 0.499 MB of 0.499 MB uploadedwandb: / 0.499 MB of 0.499 MB uploadedwandb: - 0.628 MB of 0.840 MB uploaded (0.002 MB deduped)wandb: \ 0.840 MB of 0.840 MB uploaded (0.002 MB deduped)wandb: | 0.840 MB of 0.840 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–‡â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–‚â–â–‚â–â–â–ƒâ–ƒâ–‚â–â–â–‚â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–ƒâ–…â–…â–„â–‡â–„â–…â–‚â–‡â–„â–â–‚â–…â–ƒâ–ˆâ–„â–…â–„â–…â–ƒâ–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 23
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 14594054
wandb:     model/trainable_params 14594054
wandb:                  test/loss 0.07584
wandb:                 train/loss 0.18355
wandb:   val/directional_accuracy 47.26891
wandb:                   val/loss 0.07016
wandb:                    val/mae 0.00621
wandb:                   val/mape 239.22954
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.05984
wandb:                   val/rmse 0.0083
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/krh8ix20
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_160922-krh8ix20/logs
Completed: SP500 H=3

Training: FEDformer on SP500 for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_161854-ymkqsvdj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/ymkqsvdj
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_SP500_H5  Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
 fourier enhanced cross attention used!
modes_q=17, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/ymkqsvdj
>>>>>>>start training : long_term_forecast_FEDformer_SP500_H5_FEDformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 5
============================================================
train 4239
val 235
test 236
Epoch: 1, Steps: 133 | Train Loss: 0.2560796 Vali Loss: 0.0703669 Test Loss: 0.0798756
Validation loss decreased (inf --> 0.070367).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.25607960788827194, 'val/loss': 0.07036690134555101, 'test/loss': 0.07987563638016582, '_timestamp': 1762784362.8715847}).
Epoch: 2, Steps: 133 | Train Loss: 0.1980718 Vali Loss: 0.0748580 Test Loss: 0.0792021
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.1980717639837946, 'val/loss': 0.07485801493749022, 'test/loss': 0.07920211181044579, '_timestamp': 1762784385.8710427}).
Epoch: 3, Steps: 133 | Train Loss: 0.1897061 Vali Loss: 0.0705813 Test Loss: 0.0785343
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1848664 Vali Loss: 0.0722038 Test Loss: 0.0777090
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1839111 Vali Loss: 0.0709449 Test Loss: 0.0764578
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1826895 Vali Loss: 0.0693649 Test Loss: 0.0768168
Validation loss decreased (0.070367 --> 0.069365).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1822329 Vali Loss: 0.0687713 Test Loss: 0.0765443
Validation loss decreased (0.069365 --> 0.068771).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1827110 Vali Loss: 0.0691069 Test Loss: 0.0765628
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1818705 Vali Loss: 0.0692043 Test Loss: 0.0765103
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1825251 Vali Loss: 0.0693046 Test Loss: 0.0765436
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1822897 Vali Loss: 0.0690550 Test Loss: 0.0765895
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1818734 Vali Loss: 0.0680994 Test Loss: 0.0765818
Validation loss decreased (0.068771 --> 0.068099).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1823914 Vali Loss: 0.0708925 Test Loss: 0.0766077
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1819733 Vali Loss: 0.0690556 Test Loss: 0.0765999
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1828008 Vali Loss: 0.0697133 Test Loss: 0.0766022
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1816108 Vali Loss: 0.0703526 Test Loss: 0.0766015
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1814430 Vali Loss: 0.0693926 Test Loss: 0.0766013
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1819681 Vali Loss: 0.0690690 Test Loss: 0.0766015
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1821128 Vali Loss: 0.0683165 Test Loss: 0.0766015
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1822557 Vali Loss: 0.0723578 Test Loss: 0.0766014
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1819446 Vali Loss: 0.0690016 Test Loss: 0.0766014
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1816890 Vali Loss: 0.0706126 Test Loss: 0.0766015
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_SP500_H5_FEDformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
test shape: (236, 5, 1) (236, 5, 1)
test shape: (236, 5, 1) (236, 5, 1)


	mse:6.839839625172317e-05, mae:0.006197414360940456, rmse:0.008270332589745522, r2:-0.05291008949279785, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0062, RMSE: 0.0083, RÂ²: -0.0529, MAPE: 2.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.529 MB of 0.529 MB uploadedwandb: \ 0.529 MB of 0.529 MB uploadedwandb: | 0.529 MB of 0.529 MB uploadedwandb: / 0.529 MB of 0.529 MB uploadedwandb: - 0.529 MB of 0.529 MB uploadedwandb: \ 0.529 MB of 0.529 MB uploadedwandb: | 0.529 MB of 0.742 MB uploadedwandb: / 0.742 MB of 0.742 MB uploadedwandb: - 0.742 MB of 0.742 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–â–‚â–â–‚â–â–â–â–‚â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–ˆâ–†â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–â–†â–ƒâ–„â–…â–ƒâ–ƒâ–â–ˆâ–‚â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 14725126
wandb:     model/trainable_params 14725126
wandb:                  test/loss 0.0766
wandb:                 train/loss 0.18169
wandb:   val/directional_accuracy 48.72881
wandb:                   val/loss 0.07061
wandb:                    val/mae 0.0062
wandb:                   val/mape 249.80929
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.05291
wandb:                   val/rmse 0.00827
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/ymkqsvdj
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_161854-ymkqsvdj/logs
Completed: SP500 H=5

Training: FEDformer on SP500 for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_162741-7xok0u5g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/7xok0u5g
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_SP500_H10 Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
 fourier enhanced cross attention used!
modes_q=20, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/7xok0u5g
>>>>>>>start training : long_term_forecast_FEDformer_SP500_H10_FEDformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 10
============================================================
train 4234
val 230
test 231
Epoch: 1, Steps: 133 | Train Loss: 0.2435803 Vali Loss: 0.0743626 Test Loss: 0.0830177
Validation loss decreased (inf --> 0.074363).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.24358031204096356, 'val/loss': 0.07436257414519787, 'test/loss': 0.08301765285432339, '_timestamp': 1762784891.3416135}).
Epoch: 2, Steps: 133 | Train Loss: 0.1923796 Vali Loss: 0.0729197 Test Loss: 0.0818790
Validation loss decreased (0.074363 --> 0.072920).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19237958331753438, 'val/loss': 0.07291973708197474, 'test/loss': 0.0818789629265666, '_timestamp': 1762784914.9330623}).
Epoch: 3, Steps: 133 | Train Loss: 0.1856796 Vali Loss: 0.0734274 Test Loss: 0.0804586
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1830311 Vali Loss: 0.0686914 Test Loss: 0.0810110
Validation loss decreased (0.072920 --> 0.068691).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1827508 Vali Loss: 0.0703312 Test Loss: 0.0800699
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1812886 Vali Loss: 0.0691908 Test Loss: 0.0802792
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1811475 Vali Loss: 0.0719000 Test Loss: 0.0801931
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1822214 Vali Loss: 0.0709695 Test Loss: 0.0800476
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1817143 Vali Loss: 0.0707520 Test Loss: 0.0801287
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1807077 Vali Loss: 0.0723267 Test Loss: 0.0800719
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1805847 Vali Loss: 0.0696905 Test Loss: 0.0800743
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1805185 Vali Loss: 0.0683862 Test Loss: 0.0800598
Validation loss decreased (0.068691 --> 0.068386).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1804886 Vali Loss: 0.0705020 Test Loss: 0.0800600
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1800195 Vali Loss: 0.0711605 Test Loss: 0.0800575
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1804167 Vali Loss: 0.0728682 Test Loss: 0.0800539
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1807245 Vali Loss: 0.0722352 Test Loss: 0.0800552
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1820661 Vali Loss: 0.0699858 Test Loss: 0.0800556
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1810401 Vali Loss: 0.0722189 Test Loss: 0.0800554
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1812576 Vali Loss: 0.0725454 Test Loss: 0.0800554
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1801181 Vali Loss: 0.0685732 Test Loss: 0.0800552
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1801220 Vali Loss: 0.0692392 Test Loss: 0.0800553
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1805014 Vali Loss: 0.0687861 Test Loss: 0.0800553
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_SP500_H10_FEDformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 231
test shape: (231, 10, 1) (231, 10, 1)
test shape: (231, 10, 1) (231, 10, 1)


	mse:6.881802983116359e-05, mae:0.006198592949658632, rmse:0.008295663632452488, r2:-0.05869340896606445, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0062, RMSE: 0.0083, RÂ²: -0.0587, MAPE: 2.70%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.551 MB of 0.551 MB uploadedwandb: \ 0.551 MB of 0.551 MB uploadedwandb: | 0.551 MB of 0.551 MB uploadedwandb: / 0.551 MB of 0.551 MB uploadedwandb: - 0.551 MB of 0.764 MB uploadedwandb: \ 0.764 MB of 0.764 MB uploadedwandb: | 0.764 MB of 0.764 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–„â–ˆâ–â–ƒâ–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–„â–ƒâ–‚â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–‚â–„â–‚â–ƒâ–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–„â–‚â–†â–…â–„â–†â–ƒâ–â–„â–…â–‡â–†â–ƒâ–†â–‡â–â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 15118342
wandb:     model/trainable_params 15118342
wandb:                  test/loss 0.08006
wandb:                 train/loss 0.1805
wandb:   val/directional_accuracy 48.29245
wandb:                   val/loss 0.06879
wandb:                    val/mae 0.0062
wandb:                   val/mape 270.06924
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.05869
wandb:                   val/rmse 0.0083
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/7xok0u5g
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_162741-7xok0u5g/logs
Completed: SP500 H=10

Training: FEDformer on SP500 for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_163648-7mzq57la
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/7mzq57la
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_SP500_H22 Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
 fourier enhanced cross attention used!
modes_q=26, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/7mzq57la
>>>>>>>start training : long_term_forecast_FEDformer_SP500_H22_FEDformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 22
============================================================
train 4222
val 218
test 219
Epoch: 1, Steps: 132 | Train Loss: 0.2314993 Vali Loss: 0.0735826 Test Loss: 0.0712488
Validation loss decreased (inf --> 0.073583).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23149933570036382, 'val/loss': 0.07358260984931674, 'test/loss': 0.07124883947627884, '_timestamp': 1762785438.8465955}).
Epoch: 2, Steps: 132 | Train Loss: 0.1904264 Vali Loss: 0.0732384 Test Loss: 0.0715579
Validation loss decreased (0.073583 --> 0.073238).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.1904263635815093, 'val/loss': 0.07323836748089109, 'test/loss': 0.07155788210885865, '_timestamp': 1762785463.9160464}).
Epoch: 3, Steps: 132 | Train Loss: 0.1864323 Vali Loss: 0.0727678 Test Loss: 0.0730890
Validation loss decreased (0.073238 --> 0.072768).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.1842362 Vali Loss: 0.0726895 Test Loss: 0.0711279
Validation loss decreased (0.072768 --> 0.072689).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.1831105 Vali Loss: 0.0725783 Test Loss: 0.0718987
Validation loss decreased (0.072689 --> 0.072578).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.1826211 Vali Loss: 0.0724638 Test Loss: 0.0716808
Validation loss decreased (0.072578 --> 0.072464).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.1823855 Vali Loss: 0.0727287 Test Loss: 0.0718959
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.1822704 Vali Loss: 0.0724890 Test Loss: 0.0717594
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.1821908 Vali Loss: 0.0724505 Test Loss: 0.0716923
Validation loss decreased (0.072464 --> 0.072451).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.1822091 Vali Loss: 0.0724845 Test Loss: 0.0716178
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.1823025 Vali Loss: 0.0723086 Test Loss: 0.0716028
Validation loss decreased (0.072451 --> 0.072309).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.1820760 Vali Loss: 0.0724978 Test Loss: 0.0716106
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.1819508 Vali Loss: 0.0723336 Test Loss: 0.0716093
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.1820050 Vali Loss: 0.0725391 Test Loss: 0.0716095
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.1821369 Vali Loss: 0.0723588 Test Loss: 0.0716093
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.1819887 Vali Loss: 0.0724619 Test Loss: 0.0716082
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.1821444 Vali Loss: 0.0726146 Test Loss: 0.0716082
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.1820104 Vali Loss: 0.0726047 Test Loss: 0.0716083
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.1821597 Vali Loss: 0.0722096 Test Loss: 0.0716083
Validation loss decreased (0.072309 --> 0.072210).  Saving model ...
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.1821510 Vali Loss: 0.0724959 Test Loss: 0.0716083
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.1820062 Vali Loss: 0.0723632 Test Loss: 0.0716083
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.1821188 Vali Loss: 0.0723663 Test Loss: 0.0716083
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.1820093 Vali Loss: 0.0723023 Test Loss: 0.0716083
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 132 | Train Loss: 0.1819915 Vali Loss: 0.0724862 Test Loss: 0.0716082
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 132 | Train Loss: 0.1819516 Vali Loss: 0.0722205 Test Loss: 0.0716083
EarlyStopping counter: 6 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 132 | Train Loss: 0.1820460 Vali Loss: 0.0723172 Test Loss: 0.0716083
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 132 | Train Loss: 0.1822246 Vali Loss: 0.0722713 Test Loss: 0.0716083
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 132 | Train Loss: 0.1819775 Vali Loss: 0.0722828 Test Loss: 0.0716083
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 132 | Train Loss: 0.1821877 Vali Loss: 0.0726143 Test Loss: 0.0716083
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_SP500_H22_FEDformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 219
test shape: (219, 22, 1) (219, 22, 1)
test shape: (219, 22, 1) (219, 22, 1)


	mse:6.660435610683635e-05, mae:0.0060859727673232555, rmse:0.008161148987710476, r2:-0.0432124137878418, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0082, RÂ²: -0.0432, MAPE: 2.31%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.625 MB of 0.626 MB uploadedwandb: \ 0.625 MB of 0.626 MB uploadedwandb: | 0.625 MB of 0.626 MB uploadedwandb: / 0.626 MB of 0.626 MB uploadedwandb: - 0.626 MB of 0.841 MB uploadedwandb: \ 0.841 MB of 0.841 MB uploadedwandb: | 0.841 MB of 0.841 MB uploadedwandb: / 0.841 MB of 0.841 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‡â–†â–„â–ˆâ–…â–„â–„â–‚â–…â–ƒâ–…â–ƒâ–„â–†â–†â–â–…â–ƒâ–ƒâ–‚â–„â–â–‚â–‚â–‚â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 28
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 15904774
wandb:     model/trainable_params 15904774
wandb:                  test/loss 0.07161
wandb:                 train/loss 0.18219
wandb:   val/directional_accuracy 47.70602
wandb:                   val/loss 0.07261
wandb:                    val/mae 0.00609
wandb:                   val/mape 230.92475
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.04321
wandb:                   val/rmse 0.00816
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/7mzq57la
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_163648-7mzq57la/logs
Completed: SP500 H=22

Training: FEDformer on SP500 for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_164920-42onyvea
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/42onyvea
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_SP500_H50 Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 39]
 fourier enhanced cross attention used!
modes_q=32, index_q=[0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 21, 22, 24, 26, 28, 29, 30, 33, 34, 35, 36, 37, 38, 39]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/42onyvea
>>>>>>>start training : long_term_forecast_FEDformer_SP500_H50_FEDformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 50
============================================================
train 4194
val 190
test 191
Epoch: 1, Steps: 132 | Train Loss: 0.2291856 Vali Loss: 0.0736833 Test Loss: 0.0770098
Validation loss decreased (inf --> 0.073683).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.22918563042626236, 'val/loss': 0.07368330160776775, 'test/loss': 0.0770098486294349, '_timestamp': 1762786194.6251616}).
Epoch: 2, Steps: 132 | Train Loss: 0.1931270 Vali Loss: 0.0732825 Test Loss: 0.0761481
Validation loss decreased (0.073683 --> 0.073282).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19312701583134406, 'val/loss': 0.07328247278928757, 'test/loss': 0.07614810268084209, '_timestamp': 1762786220.693742}).
Epoch: 3, Steps: 132 | Train Loss: 0.1904422 Vali Loss: 0.0727475 Test Loss: 0.0734739
Validation loss decreased (0.073282 --> 0.072747).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.1892415 Vali Loss: 0.0726655 Test Loss: 0.0742292
Validation loss decreased (0.072747 --> 0.072665).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.1879101 Vali Loss: 0.0729714 Test Loss: 0.0739212
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.1871802 Vali Loss: 0.0733022 Test Loss: 0.0743095
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.1870206 Vali Loss: 0.0731079 Test Loss: 0.0738515
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.1867403 Vali Loss: 0.0732249 Test Loss: 0.0741492
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.1865173 Vali Loss: 0.0732540 Test Loss: 0.0740707
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.1866948 Vali Loss: 0.0732390 Test Loss: 0.0740705
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.1877877 Vali Loss: 0.0731852 Test Loss: 0.0740810
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.1868477 Vali Loss: 0.0732060 Test Loss: 0.0740866
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.1884733 Vali Loss: 0.0732384 Test Loss: 0.0740905
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.1865333 Vali Loss: 0.0731906 Test Loss: 0.0740893
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_SP500_H50_FEDformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 191
test shape: (191, 50, 1) (191, 50, 1)
test shape: (191, 50, 1) (191, 50, 1)


	mse:6.703085091430694e-05, mae:0.006096333730965853, rmse:0.008187237195670605, r2:-0.03072798252105713, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0082, RÂ²: -0.0307, MAPE: 2.56%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.704 MB of 0.707 MB uploadedwandb: \ 0.707 MB of 0.707 MB uploadedwandb: | 0.707 MB of 0.919 MB uploadedwandb: / 0.919 MB of 0.919 MB uploadedwandb: - 0.919 MB of 0.919 MB uploadedwandb: \ 0.919 MB of 0.919 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–‡â–…â–ˆâ–„â–‡â–†â–†â–†â–†â–†â–†
wandb:                 train/loss â–ˆâ–†â–ƒâ–‚â–‚â–â–â–â–ƒâ–‚â–„â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–â–„â–ˆâ–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 13
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 16691206
wandb:     model/trainable_params 16691206
wandb:                  test/loss 0.07409
wandb:                 train/loss 0.18653
wandb:   val/directional_accuracy 49.08644
wandb:                   val/loss 0.07319
wandb:                    val/mae 0.0061
wandb:                   val/mape 255.76227
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.03073
wandb:                   val/rmse 0.00819
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/42onyvea
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_164920-42onyvea/logs
Completed: SP500 H=50

Training: FEDformer on SP500 for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_165602-5ay5crp5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/5ay5crp5
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_SP500_H100Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 8, 13, 16, 20, 22, 23, 24, 28, 29, 30, 31, 32, 34, 35, 37, 39, 43, 44, 46, 51, 52, 55, 56, 57, 62, 63, 64]
 fourier enhanced cross attention used!
modes_q=32, index_q=[0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 17, 18, 20, 21, 22, 23, 28, 30, 34, 36, 44, 45, 50, 53, 54, 55, 56, 57, 61, 63]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/5ay5crp5
>>>>>>>start training : long_term_forecast_FEDformer_SP500_H100_FEDformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 100
============================================================
train 4144
val 140
test 141
Epoch: 1, Steps: 130 | Train Loss: 0.2385907 Vali Loss: 0.0678772 Test Loss: 0.0816718
Validation loss decreased (inf --> 0.067877).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23859073840654813, 'val/loss': 0.06787715256214141, 'test/loss': 0.08167178332805633, '_timestamp': 1762786596.2671812}).
Epoch: 2, Steps: 130 | Train Loss: 0.2035222 Vali Loss: 0.0679917 Test Loss: 0.0825687
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.20352218609589798, 'val/loss': 0.06799165457487107, 'test/loss': 0.08256870061159134, '_timestamp': 1762786621.9823275}).
Epoch: 3, Steps: 130 | Train Loss: 0.2010469 Vali Loss: 0.0688002 Test Loss: 0.0841769
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.1998916 Vali Loss: 0.0688681 Test Loss: 0.0848593
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.1987669 Vali Loss: 0.0681986 Test Loss: 0.0832613
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.1986294 Vali Loss: 0.0681489 Test Loss: 0.0835999
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.1979081 Vali Loss: 0.0684910 Test Loss: 0.0833816
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.1981391 Vali Loss: 0.0684075 Test Loss: 0.0832270
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.1979337 Vali Loss: 0.0683185 Test Loss: 0.0834125
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.1983668 Vali Loss: 0.0683562 Test Loss: 0.0834152
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.1981333 Vali Loss: 0.0683869 Test Loss: 0.0834204
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_SP500_H100_FEDformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 141
test shape: (141, 100, 1) (141, 100, 1)
test shape: (141, 100, 1) (141, 100, 1)


	mse:6.988888344494626e-05, mae:0.006195275578647852, rmse:0.008359957486391068, r2:-0.020151495933532715, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0062, RMSE: 0.0084, RÂ²: -0.0202, MAPE: 2.74%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.808 MB of 0.813 MB uploadedwandb: \ 0.808 MB of 0.813 MB uploadedwandb: | 0.813 MB of 0.813 MB uploadedwandb: / 0.813 MB of 1.025 MB uploadedwandb: - 0.813 MB of 1.025 MB uploadedwandb: \ 1.025 MB of 1.025 MB uploadedwandb: | 1.025 MB of 1.025 MB uploadedwandb: / 1.025 MB of 1.025 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–ˆâ–â–ƒâ–‚â–â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–â–‚â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–ˆâ–â–â–„â–„â–ƒâ–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 16691206
wandb:     model/trainable_params 16691206
wandb:                  test/loss 0.08342
wandb:                 train/loss 0.19813
wandb:   val/directional_accuracy 49.48779
wandb:                   val/loss 0.06839
wandb:                    val/mae 0.0062
wandb:                   val/mape 274.25411
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.02015
wandb:                   val/rmse 0.00836
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/5ay5crp5
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_165602-5ay5crp5/logs
Completed: SP500 H=100

Training: FEDformer on NASDAQ for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_170120-w4zzlhbc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/w4zzlhbc
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_NASDAQ_H3 Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
 fourier enhanced cross attention used!
modes_q=16, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/w4zzlhbc
>>>>>>>start training : long_term_forecast_FEDformer_NASDAQ_H3_FEDformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.3329547 Vali Loss: 0.1563373 Test Loss: 0.1456411
Validation loss decreased (inf --> 0.156337).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3329546971428663, 'val/loss': 0.15633726585656404, 'test/loss': 0.14564109779894352, '_timestamp': 1762786910.5347404}).
Epoch: 2, Steps: 133 | Train Loss: 0.2710570 Vali Loss: 0.1454289 Test Loss: 0.1281158
Validation loss decreased (0.156337 --> 0.145429).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.27105697832609477, 'val/loss': 0.14542889315634966, 'test/loss': 0.12811578810214996, '_timestamp': 1762786933.8416026}).
Epoch: 3, Steps: 133 | Train Loss: 0.2558496 Vali Loss: 0.1394446 Test Loss: 0.1236018
Validation loss decreased (0.145429 --> 0.139445).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2490645 Vali Loss: 0.1432981 Test Loss: 0.1266277
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2490783 Vali Loss: 0.1377304 Test Loss: 0.1223899
Validation loss decreased (0.139445 --> 0.137730).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2451982 Vali Loss: 0.1350916 Test Loss: 0.1232215
Validation loss decreased (0.137730 --> 0.135092).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2438697 Vali Loss: 0.1349027 Test Loss: 0.1223517
Validation loss decreased (0.135092 --> 0.134903).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2439147 Vali Loss: 0.1377757 Test Loss: 0.1222583
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2447991 Vali Loss: 0.1342783 Test Loss: 0.1223255
Validation loss decreased (0.134903 --> 0.134278).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2445956 Vali Loss: 0.1376883 Test Loss: 0.1222210
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2437825 Vali Loss: 0.1471945 Test Loss: 0.1222525
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2438811 Vali Loss: 0.1467998 Test Loss: 0.1222294
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2432848 Vali Loss: 0.1384971 Test Loss: 0.1222279
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2432615 Vali Loss: 0.1343688 Test Loss: 0.1222264
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2429988 Vali Loss: 0.1378307 Test Loss: 0.1222248
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2433042 Vali Loss: 0.1356281 Test Loss: 0.1222256
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2446391 Vali Loss: 0.1345212 Test Loss: 0.1222263
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2437812 Vali Loss: 0.1348631 Test Loss: 0.1222261
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2430474 Vali Loss: 0.1376929 Test Loss: 0.1222262
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_NASDAQ_H3_FEDformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.00014299691247288138, mae:0.008767474442720413, rmse:0.011958131566643715, r2:-0.05060732364654541, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0088, RMSE: 0.0120, RÂ²: -0.0506, MAPE: 4186813.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.491 MB of 0.491 MB uploadedwandb: \ 0.491 MB of 0.491 MB uploadedwandb: | 0.491 MB of 0.491 MB uploadedwandb: / 0.491 MB of 0.491 MB uploadedwandb: - 0.491 MB of 0.491 MB uploadedwandb: \ 0.491 MB of 0.491 MB uploadedwandb: | 0.491 MB of 0.491 MB uploadedwandb: / 0.619 MB of 0.831 MB uploaded (0.002 MB deduped)wandb: - 0.831 MB of 0.831 MB uploaded (0.002 MB deduped)wandb: \ 0.831 MB of 0.831 MB uploaded (0.002 MB deduped)wandb: | 0.831 MB of 0.831 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ƒâ–ˆâ–â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–„â–‚â–â–â–‚â–‚â–â–â–â–â–â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–†â–ƒâ–â–â–ƒâ–â–ƒâ–ˆâ–ˆâ–ƒâ–â–ƒâ–‚â–â–â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 18
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 14594054
wandb:     model/trainable_params 14594054
wandb:                  test/loss 0.12223
wandb:                 train/loss 0.24305
wandb:   val/directional_accuracy 47.67932
wandb:                   val/loss 0.13769
wandb:                    val/mae 0.00877
wandb:                   val/mape 418681375.0
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.05061
wandb:                   val/rmse 0.01196
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/w4zzlhbc
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_170120-w4zzlhbc/logs
Completed: NASDAQ H=3

Training: FEDformer on NASDAQ for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_170906-ixjs716n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/ixjs716n
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_NASDAQ_H5 Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
 fourier enhanced cross attention used!
modes_q=17, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/ixjs716n
>>>>>>>start training : long_term_forecast_FEDformer_NASDAQ_H5_FEDformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.3246067 Vali Loss: 0.1551170 Test Loss: 0.1489829
Validation loss decreased (inf --> 0.155117).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.32460667797945497, 'val/loss': 0.15511699952185154, 'test/loss': 0.14898288995027542, '_timestamp': 1762787378.5924137}).
Epoch: 2, Steps: 133 | Train Loss: 0.2683142 Vali Loss: 0.1429231 Test Loss: 0.1342729
Validation loss decreased (0.155117 --> 0.142923).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2683141770443522, 'val/loss': 0.1429231083020568, 'test/loss': 0.13427292136475444, '_timestamp': 1762787402.143796}).
Epoch: 3, Steps: 133 | Train Loss: 0.2553410 Vali Loss: 0.1370609 Test Loss: 0.1338352
Validation loss decreased (0.142923 --> 0.137061).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2496356 Vali Loss: 0.1394470 Test Loss: 0.1303388
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2478512 Vali Loss: 0.1376864 Test Loss: 0.1289884
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2465020 Vali Loss: 0.1401440 Test Loss: 0.1284968
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2454787 Vali Loss: 0.1373921 Test Loss: 0.1285986
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2460149 Vali Loss: 0.1498031 Test Loss: 0.1286740
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2454849 Vali Loss: 0.1475694 Test Loss: 0.1286161
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2460108 Vali Loss: 0.1371622 Test Loss: 0.1286115
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2444234 Vali Loss: 0.1390913 Test Loss: 0.1286046
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2445358 Vali Loss: 0.1374670 Test Loss: 0.1285951
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2454553 Vali Loss: 0.1413846 Test Loss: 0.1285968
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_NASDAQ_H5_FEDformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.00014784099766984582, mae:0.00890175811946392, rmse:0.012158988043665886, r2:-0.08068609237670898, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0089, RMSE: 0.0122, RÂ²: -0.0807, MAPE: 3006953.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.512 MB of 0.512 MB uploadedwandb: \ 0.512 MB of 0.512 MB uploadedwandb: | 0.512 MB of 0.512 MB uploadedwandb: / 0.512 MB of 0.723 MB uploadedwandb: - 0.723 MB of 0.723 MB uploadedwandb: \ 0.723 MB of 0.723 MB uploadedwandb: | 0.723 MB of 0.723 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–‚â–â–ƒâ–â–ˆâ–‡â–â–‚â–â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 14725126
wandb:     model/trainable_params 14725126
wandb:                  test/loss 0.1286
wandb:                 train/loss 0.24546
wandb:   val/directional_accuracy 48.29787
wandb:                   val/loss 0.14138
wandb:                    val/mae 0.0089
wandb:                   val/mape 300695300.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.08069
wandb:                   val/rmse 0.01216
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/ixjs716n
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_170906-ixjs716n/logs
Completed: NASDAQ H=5

Training: FEDformer on NASDAQ for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_171443-f7jrb7br
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/f7jrb7br
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_NASDAQ_H10Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
 fourier enhanced cross attention used!
modes_q=20, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/f7jrb7br
>>>>>>>start training : long_term_forecast_FEDformer_NASDAQ_H10_FEDformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.3164091 Vali Loss: 0.1458195 Test Loss: 0.1338671
Validation loss decreased (inf --> 0.145819).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.31640909903479697, 'val/loss': 0.14581946469843388, 'test/loss': 0.1338670952245593, '_timestamp': 1762787713.6605248}).
Epoch: 2, Steps: 133 | Train Loss: 0.2624199 Vali Loss: 0.1589811 Test Loss: 0.1397352
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2624199225714332, 'val/loss': 0.1589811109006405, 'test/loss': 0.13973517529666424, '_timestamp': 1762787736.7358987}).
Epoch: 3, Steps: 133 | Train Loss: 0.2543896 Vali Loss: 0.1496751 Test Loss: 0.1383882
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2511350 Vali Loss: 0.1451289 Test Loss: 0.1324518
Validation loss decreased (0.145819 --> 0.145129).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2475817 Vali Loss: 0.1448534 Test Loss: 0.1317170
Validation loss decreased (0.145129 --> 0.144853).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2459788 Vali Loss: 0.1459001 Test Loss: 0.1332810
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2470326 Vali Loss: 0.1471067 Test Loss: 0.1322405
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2456924 Vali Loss: 0.1551307 Test Loss: 0.1321362
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2459962 Vali Loss: 0.1547412 Test Loss: 0.1323291
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2454748 Vali Loss: 0.1445652 Test Loss: 0.1321919
Validation loss decreased (0.144853 --> 0.144565).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2457664 Vali Loss: 0.1414642 Test Loss: 0.1321936
Validation loss decreased (0.144565 --> 0.141464).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2458696 Vali Loss: 0.1433473 Test Loss: 0.1322000
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2464066 Vali Loss: 0.1461013 Test Loss: 0.1322023
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2453576 Vali Loss: 0.1538783 Test Loss: 0.1321976
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2454471 Vali Loss: 0.1567677 Test Loss: 0.1321946
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2461144 Vali Loss: 0.1444456 Test Loss: 0.1321940
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2449433 Vali Loss: 0.1431940 Test Loss: 0.1321943
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2466167 Vali Loss: 0.1458828 Test Loss: 0.1321942
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2478225 Vali Loss: 0.1455889 Test Loss: 0.1321944
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2449307 Vali Loss: 0.1531855 Test Loss: 0.1321943
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2458135 Vali Loss: 0.1442372 Test Loss: 0.1321943
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_NASDAQ_H10_FEDformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.000148905543028377, mae:0.008948059752583504, rmse:0.012202685698866844, r2:-0.07570433616638184, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0089, RMSE: 0.0122, RÂ²: -0.0757, MAPE: 4079824.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.588 MB of 0.588 MB uploadedwandb: \ 0.588 MB of 0.588 MB uploadedwandb: | 0.588 MB of 0.588 MB uploadedwandb: / 0.588 MB of 0.801 MB uploadedwandb: - 0.801 MB of 0.801 MB uploadedwandb: \ 0.801 MB of 0.801 MB uploadedwandb: | 0.801 MB of 0.801 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–â–ƒâ–‚â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–†â–ƒâ–‚â–ƒâ–‚â–‚â–â–‚â–‚â–‚â–â–â–‚â–â–‚â–ƒâ–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–ƒâ–ƒâ–ƒâ–„â–‡â–‡â–‚â–â–‚â–ƒâ–‡â–ˆâ–‚â–‚â–ƒâ–ƒâ–†â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 15118342
wandb:     model/trainable_params 15118342
wandb:                  test/loss 0.13219
wandb:                 train/loss 0.24581
wandb:   val/directional_accuracy 50.43478
wandb:                   val/loss 0.14424
wandb:                    val/mae 0.00895
wandb:                   val/mape 407982475.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.0757
wandb:                   val/rmse 0.0122
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/f7jrb7br
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_171443-f7jrb7br/logs
Completed: NASDAQ H=10

Training: FEDformer on NASDAQ for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_172325-j2o4vnkl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/j2o4vnkl
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_NASDAQ_H22Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
 fourier enhanced cross attention used!
modes_q=26, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/j2o4vnkl
>>>>>>>start training : long_term_forecast_FEDformer_NASDAQ_H22_FEDformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.3051374 Vali Loss: 0.1593100 Test Loss: 0.1322352
Validation loss decreased (inf --> 0.159310).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3051374053413218, 'val/loss': 0.15930999389716557, 'test/loss': 0.1322352492383548, '_timestamp': 1762788237.2122488}).
Epoch: 2, Steps: 132 | Train Loss: 0.2625028 Vali Loss: 0.1561277 Test Loss: 0.1339919
Validation loss decreased (0.159310 --> 0.156128).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2625027866751859, 'val/loss': 0.15612773384366715, 'test/loss': 0.13399193435907364, '_timestamp': 1762788262.1958542}).
Epoch: 3, Steps: 132 | Train Loss: 0.2556750 Vali Loss: 0.1585453 Test Loss: 0.1338756
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2523284 Vali Loss: 0.1579585 Test Loss: 0.1318329
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2508342 Vali Loss: 0.1594822 Test Loss: 0.1329890
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2501232 Vali Loss: 0.1586116 Test Loss: 0.1326517
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2491578 Vali Loss: 0.1581294 Test Loss: 0.1329338
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2492017 Vali Loss: 0.1576036 Test Loss: 0.1329287
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2488841 Vali Loss: 0.1583235 Test Loss: 0.1326438
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2490452 Vali Loss: 0.1581174 Test Loss: 0.1327635
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2487727 Vali Loss: 0.1574757 Test Loss: 0.1327633
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2489988 Vali Loss: 0.1585130 Test Loss: 0.1327901
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_NASDAQ_H22_FEDformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.00014791340800002217, mae:0.008905048482120037, rmse:0.012161965481936932, r2:-0.057087063789367676, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0089, RMSE: 0.0122, RÂ²: -0.0571, MAPE: 5765637.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.661 MB of 0.663 MB uploadedwandb: \ 0.661 MB of 0.663 MB uploadedwandb: | 0.663 MB of 0.663 MB uploadedwandb: / 0.663 MB of 0.663 MB uploadedwandb: - 0.663 MB of 0.874 MB uploadedwandb: \ 0.874 MB of 0.874 MB uploadedwandb: | 0.874 MB of 0.874 MB uploadedwandb: / 0.874 MB of 0.874 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–…â–„â–…â–…â–„â–„â–„â–„
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–ƒâ–ˆâ–…â–ƒâ–â–„â–ƒâ–â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 15904774
wandb:     model/trainable_params 15904774
wandb:                  test/loss 0.13279
wandb:                 train/loss 0.249
wandb:   val/directional_accuracy 49.93447
wandb:                   val/loss 0.15851
wandb:                    val/mae 0.00891
wandb:                   val/mape 576563750.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.05709
wandb:                   val/rmse 0.01216
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/j2o4vnkl
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_172325-j2o4vnkl/logs
Completed: NASDAQ H=22

Training: FEDformer on NASDAQ for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_172859-c2michom
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/c2michom
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_NASDAQ_H50Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 39]
 fourier enhanced cross attention used!
modes_q=32, index_q=[0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 21, 22, 24, 26, 28, 29, 30, 33, 34, 35, 36, 37, 38, 39]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/c2michom
>>>>>>>start training : long_term_forecast_FEDformer_NASDAQ_H50_FEDformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.3056336 Vali Loss: 0.1727398 Test Loss: 0.1401477
Validation loss decreased (inf --> 0.172740).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3056335524853432, 'val/loss': 0.1727397764722506, 'test/loss': 0.14014770090579987, '_timestamp': 1762788571.9299235}).
Epoch: 2, Steps: 132 | Train Loss: 0.2679139 Vali Loss: 0.1643823 Test Loss: 0.1423441
Validation loss decreased (0.172740 --> 0.164382).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26791389557448303, 'val/loss': 0.1643823410073916, 'test/loss': 0.14234409853816032, '_timestamp': 1762788598.1656966}).
Epoch: 3, Steps: 132 | Train Loss: 0.2629245 Vali Loss: 0.1648631 Test Loss: 0.1394528
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2591539 Vali Loss: 0.1679639 Test Loss: 0.1376065
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2572946 Vali Loss: 0.1673832 Test Loss: 0.1380607
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2559840 Vali Loss: 0.1683600 Test Loss: 0.1371825
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2559877 Vali Loss: 0.1678139 Test Loss: 0.1376178
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2562141 Vali Loss: 0.1681266 Test Loss: 0.1375262
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2579696 Vali Loss: 0.1684010 Test Loss: 0.1373161
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2563460 Vali Loss: 0.1680400 Test Loss: 0.1373978
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2558845 Vali Loss: 0.1677337 Test Loss: 0.1374720
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2559826 Vali Loss: 0.1678915 Test Loss: 0.1374519
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_NASDAQ_H50_FEDformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.00015096536662895232, mae:0.00888944510370493, rmse:0.012286796234548092, r2:-0.04225003719329834, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0089, RMSE: 0.0123, RÂ²: -0.0423, MAPE: 4558829.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.700 MB of 0.703 MB uploadedwandb: \ 0.700 MB of 0.703 MB uploadedwandb: | 0.703 MB of 0.703 MB uploadedwandb: / 0.703 MB of 0.703 MB uploadedwandb: - 0.703 MB of 0.914 MB uploadedwandb: \ 0.914 MB of 0.914 MB uploadedwandb: | 0.914 MB of 0.914 MB uploadedwandb: / 0.914 MB of 0.914 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–„â–â–‚â–‚â–â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–„â–‚â–â–â–â–ƒâ–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–‡â–†â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 16691206
wandb:     model/trainable_params 16691206
wandb:                  test/loss 0.13745
wandb:                 train/loss 0.25598
wandb:   val/directional_accuracy 49.80666
wandb:                   val/loss 0.16789
wandb:                    val/mae 0.00889
wandb:                   val/mape 455882900.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.04225
wandb:                   val/rmse 0.01229
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/c2michom
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_172859-c2michom/logs
Completed: NASDAQ H=50

Training: FEDformer on NASDAQ for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_173445-o97jfmf6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/o97jfmf6
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_NASDAQ_H100Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 8, 13, 16, 20, 22, 23, 24, 28, 29, 30, 31, 32, 34, 35, 37, 39, 43, 44, 46, 51, 52, 55, 56, 57, 62, 63, 64]
 fourier enhanced cross attention used!
modes_q=32, index_q=[0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 17, 18, 20, 21, 22, 23, 28, 30, 34, 36, 44, 45, 50, 53, 54, 55, 56, 57, 61, 63]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/o97jfmf6
>>>>>>>start training : long_term_forecast_FEDformer_NASDAQ_H100_FEDformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.3121136 Vali Loss: 0.1889262 Test Loss: 0.1467725
Validation loss decreased (inf --> 0.188926).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.312113581597805, 'val/loss': 0.18892619013786316, 'test/loss': 0.1467725157737732, '_timestamp': 1762788917.8241138}).
Epoch: 2, Steps: 130 | Train Loss: 0.2743253 Vali Loss: 0.1849531 Test Loss: 0.1491869
Validation loss decreased (0.188926 --> 0.184953).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2743252890614363, 'val/loss': 0.1849530667066574, 'test/loss': 0.14918688237667083, '_timestamp': 1762788943.599637}).
Epoch: 3, Steps: 130 | Train Loss: 0.2693867 Vali Loss: 0.1902953 Test Loss: 0.1447197
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2681905 Vali Loss: 0.1904089 Test Loss: 0.1438593
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2659683 Vali Loss: 0.1921386 Test Loss: 0.1436248
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2650323 Vali Loss: 0.1921306 Test Loss: 0.1427233
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2648505 Vali Loss: 0.1936036 Test Loss: 0.1426441
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2649444 Vali Loss: 0.1927712 Test Loss: 0.1429966
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2647561 Vali Loss: 0.1915336 Test Loss: 0.1424817
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2645876 Vali Loss: 0.1935423 Test Loss: 0.1425252
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2648424 Vali Loss: 0.1940349 Test Loss: 0.1425858
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2656448 Vali Loss: 0.1923473 Test Loss: 0.1426256
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_NASDAQ_H100_FEDformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0001559458178235218, mae:0.00874839536845684, rmse:0.012487826868891716, r2:-0.02816462516784668, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0087, RMSE: 0.0125, RÂ²: -0.0282, MAPE: 4859875.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.783 MB of 0.788 MB uploadedwandb: \ 0.783 MB of 0.788 MB uploadedwandb: | 0.783 MB of 0.788 MB uploadedwandb: / 0.783 MB of 0.788 MB uploadedwandb: - 0.788 MB of 0.788 MB uploadedwandb: \ 0.788 MB of 0.788 MB uploadedwandb: | 0.788 MB of 0.999 MB uploadedwandb: / 0.999 MB of 0.999 MB uploadedwandb: - 0.999 MB of 0.999 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–…â–‚â–‚â–ƒâ–â–â–â–
wandb:                 train/loss â–ˆâ–†â–ƒâ–‚â–â–‚â–â–â–â–ƒ
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–â–„â–„â–‡â–†â–ƒâ–‡â–ˆâ–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 16691206
wandb:     model/trainable_params 16691206
wandb:                  test/loss 0.14263
wandb:                 train/loss 0.26564
wandb:   val/directional_accuracy 51.65224
wandb:                   val/loss 0.19235
wandb:                    val/mae 0.00875
wandb:                   val/mape 485987550.0
wandb:                    val/mse 0.00016
wandb:                     val/r2 -0.02816
wandb:                   val/rmse 0.01249
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/o97jfmf6
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_173445-o97jfmf6/logs
Completed: NASDAQ H=100

Training: FEDformer on ABSA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_174027-6g54aexd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/6g54aexd
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_ABSA_H3   Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
 fourier enhanced cross attention used!
modes_q=16, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/6g54aexd
>>>>>>>start training : long_term_forecast_FEDformer_ABSA_H3_FEDformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 238
Epoch: 1, Steps: 133 | Train Loss: 0.3911191 Vali Loss: 0.1796188 Test Loss: 0.1660669
Validation loss decreased (inf --> 0.179619).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3911190718636477, 'val/loss': 0.17961879633367062, 'test/loss': 0.16606687754392624, '_timestamp': 1762789256.9219894}).
Epoch: 2, Steps: 133 | Train Loss: 0.3281563 Vali Loss: 0.1790094 Test Loss: 0.1624381
Validation loss decreased (0.179619 --> 0.179009).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.328156279778122, 'val/loss': 0.17900941520929337, 'test/loss': 0.16243809275329113, '_timestamp': 1762789279.928499}).
Epoch: 3, Steps: 133 | Train Loss: 0.3078470 Vali Loss: 0.1713914 Test Loss: 0.1659025
Validation loss decreased (0.179009 --> 0.171391).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.3002043 Vali Loss: 0.1713564 Test Loss: 0.1602306
Validation loss decreased (0.171391 --> 0.171356).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2957353 Vali Loss: 0.1679304 Test Loss: 0.1591180
Validation loss decreased (0.171356 --> 0.167930).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2935465 Vali Loss: 0.1635508 Test Loss: 0.1580681
Validation loss decreased (0.167930 --> 0.163551).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2929260 Vali Loss: 0.1717572 Test Loss: 0.1591145
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2921472 Vali Loss: 0.1689597 Test Loss: 0.1583183
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2916721 Vali Loss: 0.1718039 Test Loss: 0.1583354
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2905646 Vali Loss: 0.1717675 Test Loss: 0.1582176
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2926238 Vali Loss: 0.1700813 Test Loss: 0.1581652
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2901367 Vali Loss: 0.1673257 Test Loss: 0.1581518
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2915827 Vali Loss: 0.1641236 Test Loss: 0.1581453
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2923288 Vali Loss: 0.1654487 Test Loss: 0.1581442
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2908251 Vali Loss: 0.1669560 Test Loss: 0.1581473
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2920920 Vali Loss: 0.1705056 Test Loss: 0.1581451
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_ABSA_H3_FEDformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 238
test shape: (238, 3, 1) (238, 3, 1)
test shape: (238, 3, 1) (238, 3, 1)


	mse:0.0004905034438706934, mae:0.016757162287831306, rmse:0.02214731276035309, r2:-0.07717883586883545, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0168, RMSE: 0.0221, RÂ²: -0.0772, MAPE: 1.47%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.498 MB of 0.498 MB uploadedwandb: \ 0.498 MB of 0.498 MB uploadedwandb: | 0.498 MB of 0.498 MB uploadedwandb: / 0.498 MB of 0.498 MB uploadedwandb: - 0.498 MB of 0.498 MB uploadedwandb: \ 0.498 MB of 0.498 MB uploadedwandb: | 0.498 MB of 0.498 MB uploadedwandb: / 0.627 MB of 0.838 MB uploaded (0.002 MB deduped)wandb: - 0.726 MB of 0.838 MB uploaded (0.002 MB deduped)wandb: \ 0.838 MB of 0.838 MB uploaded (0.002 MB deduped)wandb: | 0.838 MB of 0.838 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–â–‚â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ˆâ–…â–â–ˆâ–†â–ˆâ–ˆâ–‡â–„â–â–ƒâ–„â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 14594054
wandb:     model/trainable_params 14594054
wandb:                  test/loss 0.15815
wandb:                 train/loss 0.29209
wandb:   val/directional_accuracy 52.73109
wandb:                   val/loss 0.17051
wandb:                    val/mae 0.01676
wandb:                   val/mape 146.74225
wandb:                    val/mse 0.00049
wandb:                     val/r2 -0.07718
wandb:                   val/rmse 0.02215
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/6g54aexd
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_174027-6g54aexd/logs
Completed: ABSA H=3

Training: FEDformer on ABSA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_174712-zwu3oyaq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/zwu3oyaq
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_ABSA_H5   Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
 fourier enhanced cross attention used!
modes_q=17, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/zwu3oyaq
>>>>>>>start training : long_term_forecast_FEDformer_ABSA_H5_FEDformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 236
Epoch: 1, Steps: 133 | Train Loss: 0.3876515 Vali Loss: 0.1878850 Test Loss: 0.1777209
Validation loss decreased (inf --> 0.187885).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.38765145177231697, 'val/loss': 0.1878849659115076, 'test/loss': 0.17772091552615166, '_timestamp': 1762789661.7053895}).
Epoch: 2, Steps: 133 | Train Loss: 0.3266201 Vali Loss: 0.1866260 Test Loss: 0.1687327
Validation loss decreased (0.187885 --> 0.186626).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3266201319551109, 'val/loss': 0.186626011505723, 'test/loss': 0.16873274371027946, '_timestamp': 1762789684.7327187}).
Epoch: 3, Steps: 133 | Train Loss: 0.3091628 Vali Loss: 0.1683979 Test Loss: 0.1627467
Validation loss decreased (0.186626 --> 0.168398).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.3017268 Vali Loss: 0.1732012 Test Loss: 0.1617192
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2990151 Vali Loss: 0.1698357 Test Loss: 0.1631373
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2970557 Vali Loss: 0.1689135 Test Loss: 0.1637884
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2974866 Vali Loss: 0.1676106 Test Loss: 0.1637636
Validation loss decreased (0.168398 --> 0.167611).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2953052 Vali Loss: 0.1725820 Test Loss: 0.1634908
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2958540 Vali Loss: 0.1717932 Test Loss: 0.1632962
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2946027 Vali Loss: 0.1753894 Test Loss: 0.1633036
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2952820 Vali Loss: 0.1731666 Test Loss: 0.1632616
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2948778 Vali Loss: 0.1704314 Test Loss: 0.1632474
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2964088 Vali Loss: 0.1804329 Test Loss: 0.1632437
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2948269 Vali Loss: 0.1715464 Test Loss: 0.1632449
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2947268 Vali Loss: 0.1690049 Test Loss: 0.1632432
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2950581 Vali Loss: 0.1674381 Test Loss: 0.1632425
Validation loss decreased (0.167611 --> 0.167438).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2950791 Vali Loss: 0.1766182 Test Loss: 0.1632427
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2955883 Vali Loss: 0.1647823 Test Loss: 0.1632426
Validation loss decreased (0.167438 --> 0.164782).  Saving model ...
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2952707 Vali Loss: 0.1659528 Test Loss: 0.1632426
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2953734 Vali Loss: 0.1752339 Test Loss: 0.1632426
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2940291 Vali Loss: 0.1740530 Test Loss: 0.1632426
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2967175 Vali Loss: 0.1694794 Test Loss: 0.1632425
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2944858 Vali Loss: 0.1690721 Test Loss: 0.1632425
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2964850 Vali Loss: 0.1681789 Test Loss: 0.1632425
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2943244 Vali Loss: 0.1700594 Test Loss: 0.1632425
EarlyStopping counter: 7 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2949697 Vali Loss: 0.1699422 Test Loss: 0.1632425
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.2949117 Vali Loss: 0.1740738 Test Loss: 0.1632425
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.2958704 Vali Loss: 0.1666668 Test Loss: 0.1632425
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_ABSA_H5_FEDformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
test shape: (236, 5, 1) (236, 5, 1)
test shape: (236, 5, 1) (236, 5, 1)


	mse:0.0004915104364044964, mae:0.016696568578481674, rmse:0.022170035168528557, r2:-0.07311451435089111, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0167, RMSE: 0.0222, RÂ²: -0.0731, MAPE: 1.58%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.522 MB of 0.522 MB uploadedwandb: \ 0.522 MB of 0.522 MB uploadedwandb: | 0.522 MB of 0.522 MB uploadedwandb: / 0.522 MB of 0.736 MB uploadedwandb: - 0.713 MB of 0.736 MB uploadedwandb: \ 0.736 MB of 0.736 MB uploadedwandb: | 0.736 MB of 0.736 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–„â–â–†â–ˆâ–ˆâ–‡â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–ƒâ–‚â–‚â–â–‚â–â–‚â–â–â–â–â–‚â–‚â–‚â–â–‚â–â–‚â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–…â–ƒâ–ƒâ–‚â–„â–„â–†â–…â–„â–ˆâ–„â–ƒâ–‚â–†â–â–‚â–†â–…â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–…â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 27
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 14725126
wandb:     model/trainable_params 14725126
wandb:                  test/loss 0.16324
wandb:                 train/loss 0.29587
wandb:   val/directional_accuracy 52.01271
wandb:                   val/loss 0.16667
wandb:                    val/mae 0.0167
wandb:                   val/mape 157.84329
wandb:                    val/mse 0.00049
wandb:                     val/r2 -0.07311
wandb:                   val/rmse 0.02217
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/zwu3oyaq
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_174712-zwu3oyaq/logs
Completed: ABSA H=5

Training: FEDformer on ABSA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_175823-pypsd3k8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/pypsd3k8
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_ABSA_H10  Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
 fourier enhanced cross attention used!
modes_q=20, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/pypsd3k8
>>>>>>>start training : long_term_forecast_FEDformer_ABSA_H10_FEDformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 231
Epoch: 1, Steps: 133 | Train Loss: 0.3799149 Vali Loss: 0.1745719 Test Loss: 0.1691299
Validation loss decreased (inf --> 0.174572).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.379914915091113, 'val/loss': 0.1745719388127327, 'test/loss': 0.16912985127419233, '_timestamp': 1762790334.7163944}).
Epoch: 2, Steps: 133 | Train Loss: 0.3217893 Vali Loss: 0.1796965 Test Loss: 0.1680942
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.32178934445058494, 'val/loss': 0.17969653196632862, 'test/loss': 0.16809421498328447, '_timestamp': 1762790358.8389752}).
Epoch: 3, Steps: 133 | Train Loss: 0.3103235 Vali Loss: 0.1818864 Test Loss: 0.1638346
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.3053778 Vali Loss: 0.1784825 Test Loss: 0.1633019
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.3005581 Vali Loss: 0.1756583 Test Loss: 0.1625928
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2997102 Vali Loss: 0.1755492 Test Loss: 0.1627459
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.3011044 Vali Loss: 0.1802103 Test Loss: 0.1625762
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2988524 Vali Loss: 0.1790389 Test Loss: 0.1627058
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.3003350 Vali Loss: 0.1785838 Test Loss: 0.1625789
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2982941 Vali Loss: 0.1748197 Test Loss: 0.1626135
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2988713 Vali Loss: 0.1776161 Test Loss: 0.1626357
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_ABSA_H10_FEDformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 231
test shape: (231, 10, 1) (231, 10, 1)
test shape: (231, 10, 1) (231, 10, 1)


	mse:0.0005226313369348645, mae:0.017466643825173378, rmse:0.02286113239824772, r2:-0.13203060626983643, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0175, RMSE: 0.0229, RÂ²: -0.1320, MAPE: 2.07%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.560 MB of 0.560 MB uploadedwandb: \ 0.560 MB of 0.560 MB uploadedwandb: | 0.560 MB of 0.560 MB uploadedwandb: / 0.560 MB of 0.560 MB uploadedwandb: - 0.560 MB of 0.771 MB uploadedwandb: \ 0.659 MB of 0.771 MB uploadedwandb: | 0.771 MB of 0.771 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–â–‚â–â–‚â–â–â–
wandb:                 train/loss â–ˆâ–…â–‚â–‚â–ƒâ–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–‚â–‚â–†â–…â–…â–â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 15118342
wandb:     model/trainable_params 15118342
wandb:                  test/loss 0.16264
wandb:                 train/loss 0.29887
wandb:   val/directional_accuracy 51.94805
wandb:                   val/loss 0.17762
wandb:                    val/mae 0.01747
wandb:                   val/mape 207.49605
wandb:                    val/mse 0.00052
wandb:                     val/r2 -0.13203
wandb:                   val/rmse 0.02286
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/pypsd3k8
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_175823-pypsd3k8/logs
Completed: ABSA H=10

Training: FEDformer on ABSA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_180319-3f3om5uw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/3f3om5uw
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_ABSA_H22  Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
 fourier enhanced cross attention used!
modes_q=26, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/3f3om5uw
>>>>>>>start training : long_term_forecast_FEDformer_ABSA_H22_FEDformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 219
Epoch: 1, Steps: 132 | Train Loss: 0.3771516 Vali Loss: 0.1831842 Test Loss: 0.1601561
Validation loss decreased (inf --> 0.183184).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.37715155947389023, 'val/loss': 0.18318420435701097, 'test/loss': 0.16015607970101492, '_timestamp': 1762790631.5538683}).
Epoch: 2, Steps: 132 | Train Loss: 0.3295380 Vali Loss: 0.1861918 Test Loss: 0.1698990
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.32953800317464454, 'val/loss': 0.18619181428636825, 'test/loss': 0.1698989516922406, '_timestamp': 1762790656.433058}).
Epoch: 3, Steps: 132 | Train Loss: 0.3209046 Vali Loss: 0.1802809 Test Loss: 0.1584138
Validation loss decreased (0.183184 --> 0.180281).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.3161558 Vali Loss: 0.1803774 Test Loss: 0.1585955
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.3139923 Vali Loss: 0.1829771 Test Loss: 0.1580550
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.3133083 Vali Loss: 0.1827609 Test Loss: 0.1582016
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.3127827 Vali Loss: 0.1827958 Test Loss: 0.1575487
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.3120157 Vali Loss: 0.1828537 Test Loss: 0.1576006
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.3119666 Vali Loss: 0.1830986 Test Loss: 0.1574669
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.3120000 Vali Loss: 0.1830860 Test Loss: 0.1575189
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.3118269 Vali Loss: 0.1828645 Test Loss: 0.1575264
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.3120022 Vali Loss: 0.1830748 Test Loss: 0.1575122
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.3118482 Vali Loss: 0.1826944 Test Loss: 0.1575120
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_ABSA_H22_FEDformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 219
test shape: (219, 22, 1) (219, 22, 1)
test shape: (219, 22, 1) (219, 22, 1)


	mse:0.0004902607761323452, mae:0.01671029068529606, rmse:0.02214183285832405, r2:-0.04639995098114014, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0167, RMSE: 0.0221, RÂ²: -0.0464, MAPE: 1.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.639 MB of 0.640 MB uploadedwandb: \ 0.639 MB of 0.640 MB uploadedwandb: | 0.640 MB of 0.640 MB uploadedwandb: / 0.640 MB of 0.851 MB uploadedwandb: - 0.851 MB of 0.851 MB uploadedwandb: \ 0.851 MB of 0.851 MB uploadedwandb: | 0.851 MB of 0.851 MB uploadedwandb: / 0.851 MB of 0.851 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‡â–ˆâ–…â–†â–‚â–‚â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 15904774
wandb:     model/trainable_params 15904774
wandb:                  test/loss 0.15751
wandb:                 train/loss 0.31185
wandb:   val/directional_accuracy 50.31529
wandb:                   val/loss 0.18269
wandb:                    val/mae 0.01671
wandb:                   val/mape 174.64219
wandb:                    val/mse 0.00049
wandb:                     val/r2 -0.0464
wandb:                   val/rmse 0.02214
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/3f3om5uw
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_180319-3f3om5uw/logs
Completed: ABSA H=22

Training: FEDformer on ABSA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_180920-qdovycjy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/qdovycjy
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_ABSA_H50  Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 39]
 fourier enhanced cross attention used!
modes_q=32, index_q=[0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 21, 22, 24, 26, 28, 29, 30, 33, 34, 35, 36, 37, 38, 39]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/qdovycjy
>>>>>>>start training : long_term_forecast_FEDformer_ABSA_H50_FEDformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 191
Epoch: 1, Steps: 132 | Train Loss: 0.3870798 Vali Loss: 0.1930489 Test Loss: 0.1582153
Validation loss decreased (inf --> 0.193049).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.387079759980693, 'val/loss': 0.19304893910884857, 'test/loss': 0.15821529800693193, '_timestamp': 1762790995.4502273}).
Epoch: 2, Steps: 132 | Train Loss: 0.3490784 Vali Loss: 0.1833576 Test Loss: 0.1592530
Validation loss decreased (0.193049 --> 0.183358).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3490783996654279, 'val/loss': 0.18335758646329245, 'test/loss': 0.15925297886133194, '_timestamp': 1762791022.0738547}).
Epoch: 3, Steps: 132 | Train Loss: 0.3424042 Vali Loss: 0.1842250 Test Loss: 0.1585750
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.3395448 Vali Loss: 0.1819499 Test Loss: 0.1570926
Validation loss decreased (0.183358 --> 0.181950).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.3373497 Vali Loss: 0.1845348 Test Loss: 0.1565950
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.3351688 Vali Loss: 0.1833134 Test Loss: 0.1564815
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.3347396 Vali Loss: 0.1830146 Test Loss: 0.1569270
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.3356246 Vali Loss: 0.1828782 Test Loss: 0.1566610
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.3355017 Vali Loss: 0.1828845 Test Loss: 0.1568347
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.3349331 Vali Loss: 0.1829229 Test Loss: 0.1568413
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.3347291 Vali Loss: 0.1827765 Test Loss: 0.1568489
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.3353217 Vali Loss: 0.1828144 Test Loss: 0.1568723
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.3352991 Vali Loss: 0.1827821 Test Loss: 0.1568716
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.3399965 Vali Loss: 0.1827582 Test Loss: 0.1568680
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_ABSA_H50_FEDformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 191
test shape: (191, 50, 1) (191, 50, 1)
test shape: (191, 50, 1) (191, 50, 1)


	mse:0.0005094699445180595, mae:0.017036961391568184, rmse:0.022571440786123276, r2:-0.04723632335662842, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0170, RMSE: 0.0226, RÂ²: -0.0472, MAPE: 1.39%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.731 MB of 0.733 MB uploadedwandb: \ 0.733 MB of 0.733 MB uploadedwandb: | 0.733 MB of 0.945 MB uploadedwandb: / 0.911 MB of 0.945 MB uploadedwandb: - 0.945 MB of 0.945 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–â–â–‚â–‚â–â–â–‚â–‚â–†
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–â–ˆâ–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 13
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 16691206
wandb:     model/trainable_params 16691206
wandb:                  test/loss 0.15687
wandb:                 train/loss 0.34
wandb:   val/directional_accuracy 48.88343
wandb:                   val/loss 0.18276
wandb:                    val/mae 0.01704
wandb:                   val/mape 139.37676
wandb:                    val/mse 0.00051
wandb:                     val/r2 -0.04724
wandb:                   val/rmse 0.02257
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/qdovycjy
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_180920-qdovycjy/logs
Exception in thread ChkStopThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 286, in check_stop_status
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 268, in check_network_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 848, in deliver_network_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 840, in deliver_stop_status
    return self._deliver_network_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 510, in _deliver_network_status
    return self._deliver_stop_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 494, in _deliver_stop_status
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: ABSA H=50

Training: FEDformer on ABSA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_181602-mrd721vj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/mrd721vj
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_ABSA_H100 Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 8, 13, 16, 20, 22, 23, 24, 28, 29, 30, 31, 32, 34, 35, 37, 39, 43, 44, 46, 51, 52, 55, 56, 57, 62, 63, 64]
 fourier enhanced cross attention used!
modes_q=32, index_q=[0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 17, 18, 20, 21, 22, 23, 28, 30, 34, 36, 44, 45, 50, 53, 54, 55, 56, 57, 61, 63]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/mrd721vj
>>>>>>>start training : long_term_forecast_FEDformer_ABSA_H100_FEDformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 141
Epoch: 1, Steps: 130 | Train Loss: 0.4227879 Vali Loss: 0.1935150 Test Loss: 0.1643772
Validation loss decreased (inf --> 0.193515).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.42278791688955747, 'val/loss': 0.19351499676704406, 'test/loss': 0.1643771782517433, '_timestamp': 1762791396.9651318}).
Epoch: 2, Steps: 130 | Train Loss: 0.3832995 Vali Loss: 0.1942058 Test Loss: 0.1645653
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.38329952244575205, 'val/loss': 0.19420575797557832, 'test/loss': 0.16456532031297683, '_timestamp': 1762791423.1041188}).
Epoch: 3, Steps: 130 | Train Loss: 0.3768354 Vali Loss: 0.1904706 Test Loss: 0.1674540
Validation loss decreased (0.193515 --> 0.190471).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.3753969 Vali Loss: 0.1910722 Test Loss: 0.1654032
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.3726393 Vali Loss: 0.1883202 Test Loss: 0.1660667
Validation loss decreased (0.190471 --> 0.188320).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.3711974 Vali Loss: 0.1890968 Test Loss: 0.1658420
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.3706368 Vali Loss: 0.1892586 Test Loss: 0.1655548
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.3709668 Vali Loss: 0.1885881 Test Loss: 0.1659857
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.3707422 Vali Loss: 0.1890369 Test Loss: 0.1657587
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.3701249 Vali Loss: 0.1886069 Test Loss: 0.1657250
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.3703855 Vali Loss: 0.1886678 Test Loss: 0.1657055
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.3711731 Vali Loss: 0.1896152 Test Loss: 0.1657071
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.3721502 Vali Loss: 0.1888370 Test Loss: 0.1657141
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.3708494 Vali Loss: 0.1891910 Test Loss: 0.1657068
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.3702873 Vali Loss: 0.1893261 Test Loss: 0.1657099
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_ABSA_H100_FEDformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 141
test shape: (141, 100, 1) (141, 100, 1)
test shape: (141, 100, 1) (141, 100, 1)


	mse:0.0005350429564714432, mae:0.017556918784976006, rmse:0.0231309961527586, r2:-0.036837100982666016, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0176, RMSE: 0.0231, RÂ²: -0.0368, MAPE: 1.24%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.846 MB of 0.851 MB uploadedwandb: \ 0.846 MB of 0.851 MB uploadedwandb: | 0.846 MB of 0.851 MB uploadedwandb: / 0.846 MB of 0.851 MB uploadedwandb: - 0.851 MB of 0.851 MB uploadedwandb: \ 0.851 MB of 1.063 MB uploadedwandb: | 0.851 MB of 1.063 MB uploadedwandb: / 1.063 MB of 1.063 MB uploadedwandb: - 1.063 MB of 1.063 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–†â–„â–‚â–‚â–‚â–‚â–â–â–‚â–ƒâ–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–ˆâ–â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–„â–‚â–ƒâ–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 16691206
wandb:     model/trainable_params 16691206
wandb:                  test/loss 0.16571
wandb:                 train/loss 0.37029
wandb:   val/directional_accuracy 47.39595
wandb:                   val/loss 0.18933
wandb:                    val/mae 0.01756
wandb:                   val/mape 123.70164
wandb:                    val/mse 0.00054
wandb:                     val/r2 -0.03684
wandb:                   val/rmse 0.02313
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/mrd721vj
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_181602-mrd721vj/logs
Completed: ABSA H=100

Training: FEDformer on SASOL for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_182300-gsfqko9w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/gsfqko9w
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_SASOL_H3  Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
 fourier enhanced cross attention used!
modes_q=16, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/gsfqko9w
>>>>>>>start training : long_term_forecast_FEDformer_SASOL_H3_FEDformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 3
============================================================
train 3772
val 211
test 212
Epoch: 1, Steps: 118 | Train Loss: 0.3215327 Vali Loss: 0.1118112 Test Loss: 0.1576176
Validation loss decreased (inf --> 0.111811).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3215326814580772, 'val/loss': 0.11181117381368365, 'test/loss': 0.15761764773300715, '_timestamp': 1762791808.2489047}).
Epoch: 2, Steps: 118 | Train Loss: 0.2561249 Vali Loss: 0.1128064 Test Loss: 0.1606105
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2561249032111491, 'val/loss': 0.11280640001807894, 'test/loss': 0.16061050764151982, '_timestamp': 1762791829.0292702}).
Epoch: 3, Steps: 118 | Train Loss: 0.2378035 Vali Loss: 0.1029104 Test Loss: 0.1575167
Validation loss decreased (0.111811 --> 0.102910).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2301180 Vali Loss: 0.1034208 Test Loss: 0.1516290
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2259405 Vali Loss: 0.1044854 Test Loss: 0.1504731
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2261799 Vali Loss: 0.1038685 Test Loss: 0.1507733
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2238157 Vali Loss: 0.1019811 Test Loss: 0.1501162
Validation loss decreased (0.102910 --> 0.101981).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2238208 Vali Loss: 0.1026869 Test Loss: 0.1500773
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2235961 Vali Loss: 0.1011485 Test Loss: 0.1501463
Validation loss decreased (0.101981 --> 0.101149).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2239152 Vali Loss: 0.1039751 Test Loss: 0.1500520
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2239926 Vali Loss: 0.1029608 Test Loss: 0.1500664
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2238463 Vali Loss: 0.1033833 Test Loss: 0.1500730
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2231145 Vali Loss: 0.1043710 Test Loss: 0.1500732
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2233564 Vali Loss: 0.1042286 Test Loss: 0.1500773
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.2242278 Vali Loss: 0.1047080 Test Loss: 0.1500741
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.2228713 Vali Loss: 0.1035123 Test Loss: 0.1500724
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.2230671 Vali Loss: 0.1049261 Test Loss: 0.1500726
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.2228272 Vali Loss: 0.1008685 Test Loss: 0.1500724
Validation loss decreased (0.101149 --> 0.100868).  Saving model ...
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.2244122 Vali Loss: 0.1017115 Test Loss: 0.1500725
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.2244516 Vali Loss: 0.1057124 Test Loss: 0.1500724
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 118 | Train Loss: 0.2235668 Vali Loss: 0.1012552 Test Loss: 0.1500726
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 118 | Train Loss: 0.2222733 Vali Loss: 0.1040199 Test Loss: 0.1500724
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 118 | Train Loss: 0.2227082 Vali Loss: 0.1003216 Test Loss: 0.1500725
Validation loss decreased (0.100868 --> 0.100322).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 118 | Train Loss: 0.2229381 Vali Loss: 0.1035231 Test Loss: 0.1500726
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 118 | Train Loss: 0.2236774 Vali Loss: 0.1005298 Test Loss: 0.1500725
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 118 | Train Loss: 0.2237840 Vali Loss: 0.1050402 Test Loss: 0.1500725
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 118 | Train Loss: 0.2233508 Vali Loss: 0.1054436 Test Loss: 0.1500725
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 118 | Train Loss: 0.2233582 Vali Loss: 0.1032219 Test Loss: 0.1500725
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 118 | Train Loss: 0.2226906 Vali Loss: 0.1014940 Test Loss: 0.1500726
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 118 | Train Loss: 0.2241965 Vali Loss: 0.1016966 Test Loss: 0.1500726
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 118 | Train Loss: 0.2241432 Vali Loss: 0.1022127 Test Loss: 0.1500726
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 118 | Train Loss: 0.2228609 Vali Loss: 0.1025623 Test Loss: 0.1500726
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 118 | Train Loss: 0.2237543 Vali Loss: 0.1059252 Test Loss: 0.1500726
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_SASOL_H3_FEDformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 212
test shape: (212, 3, 1) (212, 3, 1)
test shape: (212, 3, 1) (212, 3, 1)


	mse:0.002273412188515067, mae:0.03536223992705345, rmse:0.047680310904979706, r2:-0.031960248947143555, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0354, RMSE: 0.0477, RÂ²: -0.0320, MAPE: 8683201.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.455 MB of 0.455 MB uploadedwandb: \ 0.455 MB of 0.455 MB uploadedwandb: | 0.455 MB of 0.455 MB uploadedwandb: / 0.455 MB of 0.455 MB uploadedwandb: - 0.455 MB of 0.455 MB uploadedwandb: \ 0.455 MB of 0.455 MB uploadedwandb: | 0.455 MB of 0.455 MB uploadedwandb: / 0.583 MB of 0.797 MB uploaded (0.002 MB deduped)wandb: - 0.583 MB of 0.797 MB uploaded (0.002 MB deduped)wandb: \ 0.797 MB of 0.797 MB uploaded (0.002 MB deduped)wandb: | 0.797 MB of 0.797 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–‚â–‚â–‚â–â–â–â–‚â–‚â–â–â–â–‚â–‚â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–…â–†â–…â–ƒâ–„â–‚â–†â–„â–…â–†â–†â–†â–…â–‡â–‚â–ƒâ–ˆâ–‚â–†â–â–…â–â–‡â–‡â–…â–‚â–ƒâ–ƒâ–„â–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 32
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 14594054
wandb:     model/trainable_params 14594054
wandb:                  test/loss 0.15007
wandb:                 train/loss 0.22375
wandb:   val/directional_accuracy 49.29245
wandb:                   val/loss 0.10593
wandb:                    val/mae 0.03536
wandb:                   val/mape 868320100.0
wandb:                    val/mse 0.00227
wandb:                     val/r2 -0.03196
wandb:                   val/rmse 0.04768
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/gsfqko9w
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_182300-gsfqko9w/logs
Completed: SASOL H=3

Training: FEDformer on SASOL for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_183438-wdeqajqt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/wdeqajqt
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_SASOL_H5  Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
 fourier enhanced cross attention used!
modes_q=17, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/wdeqajqt
>>>>>>>start training : long_term_forecast_FEDformer_SASOL_H5_FEDformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 5
============================================================
train 3770
val 209
test 210
Epoch: 1, Steps: 118 | Train Loss: 0.3182533 Vali Loss: 0.1152190 Test Loss: 0.1743495
Validation loss decreased (inf --> 0.115219).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3182532572645252, 'val/loss': 0.11521895762000765, 'test/loss': 0.17434952301638468, '_timestamp': 1762792506.8123603}).
Epoch: 2, Steps: 118 | Train Loss: 0.2525718 Vali Loss: 0.1054950 Test Loss: 0.1633656
Validation loss decreased (0.115219 --> 0.105495).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2525718423017001, 'val/loss': 0.10549501755407878, 'test/loss': 0.1633656056863921, '_timestamp': 1762792527.154571}).
Epoch: 3, Steps: 118 | Train Loss: 0.2365324 Vali Loss: 0.1113551 Test Loss: 0.1554949
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2298509 Vali Loss: 0.1115298 Test Loss: 0.1556115
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2258311 Vali Loss: 0.1047393 Test Loss: 0.1540709
Validation loss decreased (0.105495 --> 0.104739).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2236130 Vali Loss: 0.1038071 Test Loss: 0.1524812
Validation loss decreased (0.104739 --> 0.103807).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2243599 Vali Loss: 0.1051467 Test Loss: 0.1523700
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2234397 Vali Loss: 0.1033159 Test Loss: 0.1519847
Validation loss decreased (0.103807 --> 0.103316).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2232178 Vali Loss: 0.1088850 Test Loss: 0.1518455
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2222888 Vali Loss: 0.1053649 Test Loss: 0.1518831
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2228261 Vali Loss: 0.1058434 Test Loss: 0.1518876
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2223680 Vali Loss: 0.1078022 Test Loss: 0.1518894
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2223114 Vali Loss: 0.1049588 Test Loss: 0.1518738
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2229472 Vali Loss: 0.1078531 Test Loss: 0.1518727
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.2223586 Vali Loss: 0.1036814 Test Loss: 0.1518707
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.2224185 Vali Loss: 0.1085709 Test Loss: 0.1518696
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.2225427 Vali Loss: 0.1054724 Test Loss: 0.1518694
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.2231599 Vali Loss: 0.1040436 Test Loss: 0.1518693
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_SASOL_H5_FEDformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 210
test shape: (210, 5, 1) (210, 5, 1)
test shape: (210, 5, 1) (210, 5, 1)


	mse:0.0022890272084623575, mae:0.03542840853333473, rmse:0.04784378036856651, r2:-0.03137218952178955, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0354, RMSE: 0.0478, RÂ²: -0.0314, MAPE: 7979805.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.496 MB of 0.496 MB uploadedwandb: \ 0.496 MB of 0.496 MB uploadedwandb: | 0.496 MB of 0.708 MB uploadedwandb: / 0.708 MB of 0.708 MB uploadedwandb: - 0.708 MB of 0.708 MB uploadedwandb: \ 0.708 MB of 0.708 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ˆâ–…â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ˆâ–‚â–â–ƒâ–â–†â–ƒâ–ƒâ–…â–‚â–…â–â–…â–ƒâ–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 17
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 14725126
wandb:     model/trainable_params 14725126
wandb:                  test/loss 0.15187
wandb:                 train/loss 0.22316
wandb:   val/directional_accuracy 46.54762
wandb:                   val/loss 0.10404
wandb:                    val/mae 0.03543
wandb:                   val/mape 797980550.0
wandb:                    val/mse 0.00229
wandb:                     val/r2 -0.03137
wandb:                   val/rmse 0.04784
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/wdeqajqt
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_183438-wdeqajqt/logs
Completed: SASOL H=5

Training: FEDformer on SASOL for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_184117-2r1m5oqx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/2r1m5oqx
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_SASOL_H10 Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
 fourier enhanced cross attention used!
modes_q=20, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/2r1m5oqx
>>>>>>>start training : long_term_forecast_FEDformer_SASOL_H10_FEDformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 10
============================================================
train 3765
val 204
test 205
Epoch: 1, Steps: 118 | Train Loss: 0.3131852 Vali Loss: 0.1070019 Test Loss: 0.1643265
Validation loss decreased (inf --> 0.107002).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3131852112079071, 'val/loss': 0.10700190280164991, 'test/loss': 0.16432654857635498, '_timestamp': 1762792905.2699127}).
Epoch: 2, Steps: 118 | Train Loss: 0.2535113 Vali Loss: 0.1138536 Test Loss: 0.1634544
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25351129510140014, 'val/loss': 0.11385362382446017, 'test/loss': 0.1634543580668313, '_timestamp': 1762792926.3486075}).
Epoch: 3, Steps: 118 | Train Loss: 0.2397141 Vali Loss: 0.1092426 Test Loss: 0.1541265
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2326198 Vali Loss: 0.1070139 Test Loss: 0.1535082
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2299747 Vali Loss: 0.1072555 Test Loss: 0.1542485
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2282096 Vali Loss: 0.1111621 Test Loss: 0.1527556
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2276703 Vali Loss: 0.1054259 Test Loss: 0.1531413
Validation loss decreased (0.107002 --> 0.105426).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2271098 Vali Loss: 0.1060857 Test Loss: 0.1530506
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2264562 Vali Loss: 0.1090266 Test Loss: 0.1528663
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2268505 Vali Loss: 0.1087801 Test Loss: 0.1529791
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2261438 Vali Loss: 0.1058249 Test Loss: 0.1529320
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2262851 Vali Loss: 0.1055753 Test Loss: 0.1529244
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2270170 Vali Loss: 0.1096795 Test Loss: 0.1529216
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2265228 Vali Loss: 0.1060376 Test Loss: 0.1529196
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.2263861 Vali Loss: 0.1093138 Test Loss: 0.1529203
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.2264762 Vali Loss: 0.1061114 Test Loss: 0.1529204
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.2262161 Vali Loss: 0.1062029 Test Loss: 0.1529204
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_SASOL_H10_FEDformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 205
test shape: (205, 10, 1) (205, 10, 1)
test shape: (205, 10, 1) (205, 10, 1)


	mse:0.0023064357228577137, mae:0.035346612334251404, rmse:0.04802536591887474, r2:-0.03907501697540283, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0353, RMSE: 0.0480, RÂ²: -0.0391, MAPE: 7966269.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.542 MB of 0.543 MB uploadedwandb: \ 0.542 MB of 0.543 MB uploadedwandb: | 0.542 MB of 0.543 MB uploadedwandb: / 0.543 MB of 0.543 MB uploadedwandb: - 0.543 MB of 0.543 MB uploadedwandb: \ 0.543 MB of 0.754 MB uploadedwandb: | 0.754 MB of 0.754 MB uploadedwandb: / 0.754 MB of 0.754 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‡â–…â–ˆâ–â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–ƒâ–ƒâ–ˆâ–â–‚â–…â–…â–â–â–†â–‚â–†â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 15118342
wandb:     model/trainable_params 15118342
wandb:                  test/loss 0.15292
wandb:                 train/loss 0.22622
wandb:   val/directional_accuracy 47.91328
wandb:                   val/loss 0.1062
wandb:                    val/mae 0.03535
wandb:                   val/mape 796626900.0
wandb:                    val/mse 0.00231
wandb:                     val/r2 -0.03908
wandb:                   val/rmse 0.04803
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/2r1m5oqx
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_184117-2r1m5oqx/logs
Completed: SASOL H=10

Training: FEDformer on SASOL for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_184745-yhm2z4ij
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/yhm2z4ij
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_SASOL_H22 Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
 fourier enhanced cross attention used!
modes_q=26, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/yhm2z4ij
>>>>>>>start training : long_term_forecast_FEDformer_SASOL_H22_FEDformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 22
============================================================
train 3753
val 192
test 193
Epoch: 1, Steps: 118 | Train Loss: 0.3087892 Vali Loss: 0.1121079 Test Loss: 0.1629568
Validation loss decreased (inf --> 0.112108).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3087892446477534, 'val/loss': 0.11210791518290837, 'test/loss': 0.16295683809689113, '_timestamp': 1762793295.2812064}).
Epoch: 2, Steps: 118 | Train Loss: 0.2610342 Vali Loss: 0.1144920 Test Loss: 0.1610761
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2610341710811954, 'val/loss': 0.11449202274282773, 'test/loss': 0.1610760592988559, '_timestamp': 1762793317.8284934}).
Epoch: 3, Steps: 118 | Train Loss: 0.2548966 Vali Loss: 0.1097649 Test Loss: 0.1602358
Validation loss decreased (0.112108 --> 0.109765).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2479703 Vali Loss: 0.1096019 Test Loss: 0.1572854
Validation loss decreased (0.109765 --> 0.109602).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2450949 Vali Loss: 0.1102305 Test Loss: 0.1559469
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2420693 Vali Loss: 0.1105278 Test Loss: 0.1567875
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2413399 Vali Loss: 0.1089692 Test Loss: 0.1557897
Validation loss decreased (0.109602 --> 0.108969).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2414749 Vali Loss: 0.1090007 Test Loss: 0.1559983
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2414796 Vali Loss: 0.1092679 Test Loss: 0.1560290
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2406388 Vali Loss: 0.1091868 Test Loss: 0.1560395
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2408402 Vali Loss: 0.1091764 Test Loss: 0.1560282
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2407058 Vali Loss: 0.1091716 Test Loss: 0.1560338
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2406074 Vali Loss: 0.1091887 Test Loss: 0.1560281
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2398343 Vali Loss: 0.1091924 Test Loss: 0.1560319
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.2409894 Vali Loss: 0.1091890 Test Loss: 0.1560306
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.2405397 Vali Loss: 0.1091905 Test Loss: 0.1560296
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.2407876 Vali Loss: 0.1091912 Test Loss: 0.1560286
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_SASOL_H22_FEDformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 193
test shape: (193, 22, 1) (193, 22, 1)
test shape: (193, 22, 1) (193, 22, 1)


	mse:0.00231826095841825, mae:0.0351884625852108, rmse:0.048148322850465775, r2:-0.03303420543670654, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0352, RMSE: 0.0481, RÂ²: -0.0330, MAPE: 7555822.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.598 MB of 0.599 MB uploadedwandb: \ 0.599 MB of 0.599 MB uploadedwandb: | 0.599 MB of 0.810 MB uploadedwandb: / 0.810 MB of 0.810 MB uploadedwandb: - 0.810 MB of 0.810 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–ƒâ–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–„â–‡â–ˆâ–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 15904774
wandb:     model/trainable_params 15904774
wandb:                  test/loss 0.15603
wandb:                 train/loss 0.24079
wandb:   val/directional_accuracy 47.34764
wandb:                   val/loss 0.10919
wandb:                    val/mae 0.03519
wandb:                   val/mape 755582250.0
wandb:                    val/mse 0.00232
wandb:                     val/r2 -0.03303
wandb:                   val/rmse 0.04815
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/yhm2z4ij
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_184745-yhm2z4ij/logs
Completed: SASOL H=22

Training: FEDformer on SASOL for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_185436-anrtlo0u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/anrtlo0u
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_SASOL_H50 Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 39]
 fourier enhanced cross attention used!
modes_q=32, index_q=[0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 21, 22, 24, 26, 28, 29, 30, 33, 34, 35, 36, 37, 38, 39]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/anrtlo0u
>>>>>>>start training : long_term_forecast_FEDformer_SASOL_H50_FEDformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 50
============================================================
train 3725
val 164
test 165
Epoch: 1, Steps: 117 | Train Loss: 0.3332971 Vali Loss: 0.1074492 Test Loss: 0.1827289
Validation loss decreased (inf --> 0.107449).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.33329705129831266, 'val/loss': 0.1074491838614146, 'test/loss': 0.18272891268134117, '_timestamp': 1762793707.3490913}).
Epoch: 2, Steps: 117 | Train Loss: 0.2933133 Vali Loss: 0.1049471 Test Loss: 0.1809979
Validation loss decreased (0.107449 --> 0.104947).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.293313345593265, 'val/loss': 0.10494714602828026, 'test/loss': 0.18099789942304292, '_timestamp': 1762793730.8575778}).
Epoch: 3, Steps: 117 | Train Loss: 0.2874824 Vali Loss: 0.1078688 Test Loss: 0.1672140
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 117 | Train Loss: 0.2835894 Vali Loss: 0.1101987 Test Loss: 0.1679660
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 117 | Train Loss: 0.2813503 Vali Loss: 0.1057335 Test Loss: 0.1658507
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 117 | Train Loss: 0.2801725 Vali Loss: 0.1128206 Test Loss: 0.1647225
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 117 | Train Loss: 0.2786281 Vali Loss: 0.1137144 Test Loss: 0.1659190
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 117 | Train Loss: 0.2792325 Vali Loss: 0.1050076 Test Loss: 0.1656359
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 117 | Train Loss: 0.2780097 Vali Loss: 0.1100415 Test Loss: 0.1653567
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 117 | Train Loss: 0.2787112 Vali Loss: 0.1081505 Test Loss: 0.1653621
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 117 | Train Loss: 0.2782891 Vali Loss: 0.1042030 Test Loss: 0.1653640
Validation loss decreased (0.104947 --> 0.104203).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 117 | Train Loss: 0.2778854 Vali Loss: 0.1135031 Test Loss: 0.1653596
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 117 | Train Loss: 0.2775887 Vali Loss: 0.1058263 Test Loss: 0.1653561
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 117 | Train Loss: 0.2785892 Vali Loss: 0.1087557 Test Loss: 0.1653559
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 117 | Train Loss: 0.2779519 Vali Loss: 0.1142366 Test Loss: 0.1653557
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 117 | Train Loss: 0.2775983 Vali Loss: 0.1082964 Test Loss: 0.1653555
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 117 | Train Loss: 0.2789768 Vali Loss: 0.1114830 Test Loss: 0.1653551
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 117 | Train Loss: 0.2785998 Vali Loss: 0.1082994 Test Loss: 0.1653551
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 117 | Train Loss: 0.2782005 Vali Loss: 0.1127077 Test Loss: 0.1653549
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 117 | Train Loss: 0.2782786 Vali Loss: 0.1105378 Test Loss: 0.1653550
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 117 | Train Loss: 0.2776663 Vali Loss: 0.1100816 Test Loss: 0.1653549
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_SASOL_H50_FEDformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 165
test shape: (165, 50, 1) (165, 50, 1)
test shape: (165, 50, 1) (165, 50, 1)


	mse:0.0021086998749524355, mae:0.033745307475328445, rmse:0.04592058062553406, r2:-0.029255390167236328, dtw:Not calculated


VAL - MSE: 0.0021, MAE: 0.0337, RMSE: 0.0459, RÂ²: -0.0293, MAPE: 7387016.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.628 MB of 0.630 MB uploadedwandb: \ 0.630 MB of 0.630 MB uploadedwandb: | 0.630 MB of 0.843 MB uploadedwandb: / 0.630 MB of 0.843 MB uploadedwandb: - 0.843 MB of 0.843 MB uploadedwandb: \ 0.843 MB of 0.843 MB uploadedwandb: | 0.843 MB of 0.843 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–†â–ˆâ–ƒâ–â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–„â–ƒâ–‚â–‚â–â–‚â–â–â–â–‚â–â–â–‚â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–…â–‚â–‡â–ˆâ–‚â–…â–„â–â–‡â–‚â–„â–ˆâ–„â–†â–„â–‡â–…â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 16691206
wandb:     model/trainable_params 16691206
wandb:                  test/loss 0.16535
wandb:                 train/loss 0.27767
wandb:   val/directional_accuracy 45.61534
wandb:                   val/loss 0.11008
wandb:                    val/mae 0.03375
wandb:                   val/mape 738701650.0
wandb:                    val/mse 0.00211
wandb:                     val/r2 -0.02926
wandb:                   val/rmse 0.04592
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/anrtlo0u
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_185436-anrtlo0u/logs
Completed: SASOL H=50

Training: FEDformer on SASOL for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_190317-lqtbi3lx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/lqtbi3lx
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_SASOL_H100Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 8, 13, 16, 20, 22, 23, 24, 28, 29, 30, 31, 32, 34, 35, 37, 39, 43, 44, 46, 51, 52, 55, 56, 57, 62, 63, 64]
 fourier enhanced cross attention used!
modes_q=32, index_q=[0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 17, 18, 20, 21, 22, 23, 28, 30, 34, 36, 44, 45, 50, 53, 54, 55, 56, 57, 61, 63]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/lqtbi3lx
>>>>>>>start training : long_term_forecast_FEDformer_SASOL_H100_FEDformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 100
============================================================
train 3675
val 114
test 115
Epoch: 1, Steps: 115 | Train Loss: 0.3919973 Vali Loss: 0.1118401 Test Loss: 0.1808262
Validation loss decreased (inf --> 0.111840).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.39199733332447384, 'val/loss': 0.11184006184339523, 'test/loss': 0.18082618340849876, '_timestamp': 1762794228.1936588}).
Epoch: 2, Steps: 115 | Train Loss: 0.3517176 Vali Loss: 0.1206511 Test Loss: 0.1682203
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3517175845477892, 'val/loss': 0.12065106444060802, 'test/loss': 0.16822026669979095, '_timestamp': 1762794251.7846477}).
Epoch: 3, Steps: 115 | Train Loss: 0.3479139 Vali Loss: 0.1116264 Test Loss: 0.1755292
Validation loss decreased (0.111840 --> 0.111626).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 115 | Train Loss: 0.3455534 Vali Loss: 0.1170578 Test Loss: 0.1700764
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 115 | Train Loss: 0.3440851 Vali Loss: 0.1224810 Test Loss: 0.1707684
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 115 | Train Loss: 0.3435623 Vali Loss: 0.1180337 Test Loss: 0.1713781
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 115 | Train Loss: 0.3430728 Vali Loss: 0.1153611 Test Loss: 0.1717236
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 115 | Train Loss: 0.3428257 Vali Loss: 0.1156337 Test Loss: 0.1713424
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 115 | Train Loss: 0.3429289 Vali Loss: 0.1176138 Test Loss: 0.1711640
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 115 | Train Loss: 0.3425687 Vali Loss: 0.1154546 Test Loss: 0.1711039
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 115 | Train Loss: 0.3426715 Vali Loss: 0.1151498 Test Loss: 0.1710613
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 115 | Train Loss: 0.3425577 Vali Loss: 0.1162404 Test Loss: 0.1710428
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 115 | Train Loss: 0.3427461 Vali Loss: 0.1154318 Test Loss: 0.1710372
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_SASOL_H100_FEDformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 115
test shape: (115, 100, 1) (115, 100, 1)
test shape: (115, 100, 1) (115, 100, 1)


	mse:0.002012155717238784, mae:0.03304242342710495, rmse:0.04485705867409706, r2:-0.01397693157196045, dtw:Not calculated


VAL - MSE: 0.0020, MAE: 0.0330, RMSE: 0.0449, RÂ²: -0.0140, MAPE: 6335070.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.662 MB of 0.666 MB uploadedwandb: \ 0.662 MB of 0.666 MB uploadedwandb: | 0.662 MB of 0.666 MB uploadedwandb: / 0.666 MB of 0.666 MB uploadedwandb: - 0.666 MB of 0.878 MB uploadedwandb: \ 0.878 MB of 0.878 MB uploadedwandb: | 0.878 MB of 0.878 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–…â–ˆâ–…â–ƒâ–„â–…â–ƒâ–ƒâ–„â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 16691206
wandb:     model/trainable_params 16691206
wandb:                  test/loss 0.17104
wandb:                 train/loss 0.34275
wandb:   val/directional_accuracy 49.24023
wandb:                   val/loss 0.11543
wandb:                    val/mae 0.03304
wandb:                   val/mape 633507000.0
wandb:                    val/mse 0.00201
wandb:                     val/r2 -0.01398
wandb:                   val/rmse 0.04486
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/lqtbi3lx
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_190317-lqtbi3lx/logs
Completed: SASOL H=100

Training: FEDformer on DRD_GOLD for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_190848-nmpi30lt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/nmpi30lt
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_DRD_GOLD_H3Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
 fourier enhanced cross attention used!
modes_q=16, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/nmpi30lt
>>>>>>>start training : long_term_forecast_FEDformer_DRD_GOLD_H3_FEDformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.3625885 Vali Loss: 0.1697385 Test Loss: 0.1609889
Validation loss decreased (inf --> 0.169739).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.36258848426037266, 'val/loss': 0.16973852925002575, 'test/loss': 0.16098887100815773, '_timestamp': 1762794558.6622314}).
Epoch: 2, Steps: 133 | Train Loss: 0.2911779 Vali Loss: 0.1642010 Test Loss: 0.1519762
Validation loss decreased (0.169739 --> 0.164201).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.29117792396617115, 'val/loss': 0.16420095600187778, 'test/loss': 0.15197619050741196, '_timestamp': 1762794582.4994686}).
Epoch: 3, Steps: 133 | Train Loss: 0.2675660 Vali Loss: 0.1445876 Test Loss: 0.1378654
Validation loss decreased (0.164201 --> 0.144588).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2537536 Vali Loss: 0.1407027 Test Loss: 0.1335996
Validation loss decreased (0.144588 --> 0.140703).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2486012 Vali Loss: 0.1405370 Test Loss: 0.1347870
Validation loss decreased (0.140703 --> 0.140537).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2461385 Vali Loss: 0.1371079 Test Loss: 0.1303754
Validation loss decreased (0.140537 --> 0.137108).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2434635 Vali Loss: 0.1363668 Test Loss: 0.1310829
Validation loss decreased (0.137108 --> 0.136367).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2428823 Vali Loss: 0.1458104 Test Loss: 0.1306539
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2428586 Vali Loss: 0.1403508 Test Loss: 0.1303722
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2435889 Vali Loss: 0.1375640 Test Loss: 0.1303234
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2426146 Vali Loss: 0.1421406 Test Loss: 0.1303394
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2429064 Vali Loss: 0.1401808 Test Loss: 0.1303232
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2417522 Vali Loss: 0.1328360 Test Loss: 0.1303081
Validation loss decreased (0.136367 --> 0.132836).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2425277 Vali Loss: 0.1373780 Test Loss: 0.1303077
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2421990 Vali Loss: 0.1362421 Test Loss: 0.1303062
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2418018 Vali Loss: 0.1392759 Test Loss: 0.1303078
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2425318 Vali Loss: 0.1365370 Test Loss: 0.1303082
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2423490 Vali Loss: 0.1350341 Test Loss: 0.1303084
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2422563 Vali Loss: 0.1344376 Test Loss: 0.1303082
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2426646 Vali Loss: 0.1396539 Test Loss: 0.1303084
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2431918 Vali Loss: 0.1327429 Test Loss: 0.1303084
Validation loss decreased (0.132836 --> 0.132743).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2430592 Vali Loss: 0.1377825 Test Loss: 0.1303083
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2431015 Vali Loss: 0.1367527 Test Loss: 0.1303083
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2428922 Vali Loss: 0.1402670 Test Loss: 0.1303082
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2426124 Vali Loss: 0.1345945 Test Loss: 0.1303083
EarlyStopping counter: 4 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2436323 Vali Loss: 0.1373959 Test Loss: 0.1303084
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.2419817 Vali Loss: 0.1343437 Test Loss: 0.1303083
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.2432322 Vali Loss: 0.1363038 Test Loss: 0.1303083
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.2432129 Vali Loss: 0.1360349 Test Loss: 0.1303083
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 133 | Train Loss: 0.2430594 Vali Loss: 0.1330132 Test Loss: 0.1303083
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 133 | Train Loss: 0.2430257 Vali Loss: 0.1352681 Test Loss: 0.1303083
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_DRD_GOLD_H3_FEDformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.000982621219009161, mae:0.022295016795396805, rmse:0.03134679049253464, r2:-0.059574127197265625, dtw:Not calculated


VAL - MSE: 0.0010, MAE: 0.0223, RMSE: 0.0313, RÂ²: -0.0596, MAPE: 1764406.38%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.479 MB of 0.480 MB uploadedwandb: \ 0.479 MB of 0.480 MB uploadedwandb: | 0.479 MB of 0.480 MB uploadedwandb: / 0.480 MB of 0.480 MB uploadedwandb: - 0.480 MB of 0.480 MB uploadedwandb: \ 0.480 MB of 0.480 MB uploadedwandb: | 0.480 MB of 0.480 MB uploadedwandb: / 0.480 MB of 0.480 MB uploadedwandb: - 0.608 MB of 0.821 MB uploaded (0.002 MB deduped)wandb: \ 0.821 MB of 0.821 MB uploaded (0.002 MB deduped)wandb: | 0.821 MB of 0.821 MB uploaded (0.002 MB deduped)wandb: / 0.821 MB of 0.821 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–…â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–…â–…â–ƒâ–ƒâ–ˆâ–…â–„â–†â–…â–â–ƒâ–ƒâ–„â–ƒâ–‚â–‚â–…â–â–„â–ƒâ–…â–‚â–ƒâ–‚â–ƒâ–ƒâ–â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 30
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 14594054
wandb:     model/trainable_params 14594054
wandb:                  test/loss 0.13031
wandb:                 train/loss 0.24303
wandb:   val/directional_accuracy 56.96203
wandb:                   val/loss 0.13527
wandb:                    val/mae 0.0223
wandb:                   val/mape 176440637.5
wandb:                    val/mse 0.00098
wandb:                     val/r2 -0.05957
wandb:                   val/rmse 0.03135
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/nmpi30lt
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_190848-nmpi30lt/logs
Completed: DRD_GOLD H=3

Training: FEDformer on DRD_GOLD for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_192058-07m65gam
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/07m65gam
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_DRD_GOLD_H5Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
 fourier enhanced cross attention used!
modes_q=17, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/07m65gam
>>>>>>>start training : long_term_forecast_FEDformer_DRD_GOLD_H5_FEDformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.3586763 Vali Loss: 0.1709075 Test Loss: 0.1606587
Validation loss decreased (inf --> 0.170907).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.35867625026774586, 'val/loss': 0.17090749740600586, 'test/loss': 0.16065870970487595, '_timestamp': 1762795287.2742233}).
Epoch: 2, Steps: 133 | Train Loss: 0.2896636 Vali Loss: 0.1602974 Test Loss: 0.1467740
Validation loss decreased (0.170907 --> 0.160297).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.28966358718567325, 'val/loss': 0.1602973658591509, 'test/loss': 0.14677398838102818, '_timestamp': 1762795310.0456278}).
Epoch: 3, Steps: 133 | Train Loss: 0.2657383 Vali Loss: 0.1521970 Test Loss: 0.1424814
Validation loss decreased (0.160297 --> 0.152197).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2512589 Vali Loss: 0.1403253 Test Loss: 0.1333125
Validation loss decreased (0.152197 --> 0.140325).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2463990 Vali Loss: 0.1400400 Test Loss: 0.1331478
Validation loss decreased (0.140325 --> 0.140040).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2452404 Vali Loss: 0.1372758 Test Loss: 0.1332801
Validation loss decreased (0.140040 --> 0.137276).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2430987 Vali Loss: 0.1386036 Test Loss: 0.1330583
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2436254 Vali Loss: 0.1387855 Test Loss: 0.1314644
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2430413 Vali Loss: 0.1412355 Test Loss: 0.1319808
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2426039 Vali Loss: 0.1447783 Test Loss: 0.1323721
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2429290 Vali Loss: 0.1425601 Test Loss: 0.1321083
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2425280 Vali Loss: 0.1344933 Test Loss: 0.1320494
Validation loss decreased (0.137276 --> 0.134493).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2424614 Vali Loss: 0.1443352 Test Loss: 0.1320983
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2423853 Vali Loss: 0.1387747 Test Loss: 0.1321187
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2431484 Vali Loss: 0.1440371 Test Loss: 0.1321155
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2425749 Vali Loss: 0.1375088 Test Loss: 0.1321131
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2424851 Vali Loss: 0.1452818 Test Loss: 0.1321114
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2422629 Vali Loss: 0.1398406 Test Loss: 0.1321117
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2421481 Vali Loss: 0.1372210 Test Loss: 0.1321116
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2422672 Vali Loss: 0.1401109 Test Loss: 0.1321118
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2423036 Vali Loss: 0.1373000 Test Loss: 0.1321120
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2424442 Vali Loss: 0.1418352 Test Loss: 0.1321119
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_DRD_GOLD_H5_FEDformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.0009714191546663642, mae:0.022265497595071793, rmse:0.03116759844124317, r2:-0.04029273986816406, dtw:Not calculated


VAL - MSE: 0.0010, MAE: 0.0223, RMSE: 0.0312, RÂ²: -0.0403, MAPE: 2477431.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.509 MB of 0.509 MB uploadedwandb: \ 0.509 MB of 0.509 MB uploadedwandb: | 0.509 MB of 0.509 MB uploadedwandb: / 0.509 MB of 0.509 MB uploadedwandb: - 0.509 MB of 0.721 MB uploadedwandb: \ 0.721 MB of 0.721 MB uploadedwandb: | 0.721 MB of 0.721 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–„â–…â–„â–â–…â–ƒâ–…â–‚â–…â–ƒâ–‚â–ƒâ–‚â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 14725126
wandb:     model/trainable_params 14725126
wandb:                  test/loss 0.13211
wandb:                 train/loss 0.24244
wandb:   val/directional_accuracy 54.14894
wandb:                   val/loss 0.14184
wandb:                    val/mae 0.02227
wandb:                   val/mape 247743150.0
wandb:                    val/mse 0.00097
wandb:                     val/r2 -0.04029
wandb:                   val/rmse 0.03117
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/07m65gam
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_192058-07m65gam/logs
Exception in thread Exception in thread IntMsgThrChkStopThr:
:
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 300, in check_internal_messages
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 286, in check_stop_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 856, in deliver_internal_messages
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 840, in deliver_stop_status
    return self._deliver_internal_messages(internal_message)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 516, in _deliver_internal_messages
    return self._deliver_stop_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 494, in _deliver_stop_status
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: DRD_GOLD H=5

Training: FEDformer on DRD_GOLD for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_192949-lzfdcqbj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/lzfdcqbj
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_DRD_GOLD_H10Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
 fourier enhanced cross attention used!
modes_q=20, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/lzfdcqbj
>>>>>>>start training : long_term_forecast_FEDformer_DRD_GOLD_H10_FEDformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.3559467 Vali Loss: 0.1816657 Test Loss: 0.1573196
Validation loss decreased (inf --> 0.181666).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.35594665768899414, 'val/loss': 0.18166570737957954, 'test/loss': 0.15731961280107498, '_timestamp': 1762795819.4851215}).
Epoch: 2, Steps: 133 | Train Loss: 0.2934384 Vali Loss: 0.1645599 Test Loss: 0.1522733
Validation loss decreased (0.181666 --> 0.164560).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2934383553893943, 'val/loss': 0.16455990634858608, 'test/loss': 0.15227330848574638, '_timestamp': 1762795842.9224186}).
Epoch: 3, Steps: 133 | Train Loss: 0.2738748 Vali Loss: 0.1657779 Test Loss: 0.1467472
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2628061 Vali Loss: 0.1555534 Test Loss: 0.1429476
Validation loss decreased (0.164560 --> 0.155553).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2584098 Vali Loss: 0.1453479 Test Loss: 0.1419975
Validation loss decreased (0.155553 --> 0.145348).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2559160 Vali Loss: 0.1541663 Test Loss: 0.1398916
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2557034 Vali Loss: 0.1578289 Test Loss: 0.1394148
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2545200 Vali Loss: 0.1546451 Test Loss: 0.1396713
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2545643 Vali Loss: 0.1565346 Test Loss: 0.1398150
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2545342 Vali Loss: 0.1507167 Test Loss: 0.1396542
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2541888 Vali Loss: 0.1493040 Test Loss: 0.1395517
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2546066 Vali Loss: 0.1495593 Test Loss: 0.1395665
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2543227 Vali Loss: 0.1494157 Test Loss: 0.1395983
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2528589 Vali Loss: 0.1487547 Test Loss: 0.1395896
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2541429 Vali Loss: 0.1613919 Test Loss: 0.1395876
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_DRD_GOLD_H10_FEDformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.0009872142691165209, mae:0.022489136084914207, rmse:0.031419966369867325, r2:-0.04107677936553955, dtw:Not calculated


VAL - MSE: 0.0010, MAE: 0.0225, RMSE: 0.0314, RÂ²: -0.0411, MAPE: 2002224.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.524 MB of 0.525 MB uploadedwandb: \ 0.524 MB of 0.525 MB uploadedwandb: | 0.524 MB of 0.525 MB uploadedwandb: / 0.524 MB of 0.525 MB uploadedwandb: - 0.525 MB of 0.525 MB uploadedwandb: \ 0.525 MB of 0.525 MB uploadedwandb: | 0.525 MB of 0.736 MB uploadedwandb: / 0.736 MB of 0.736 MB uploadedwandb: - 0.736 MB of 0.736 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–ƒâ–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–â–„â–…â–„â–…â–ƒâ–‚â–‚â–‚â–‚â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 15118342
wandb:     model/trainable_params 15118342
wandb:                  test/loss 0.13959
wandb:                 train/loss 0.25414
wandb:   val/directional_accuracy 54.15459
wandb:                   val/loss 0.16139
wandb:                    val/mae 0.02249
wandb:                   val/mape 200222475.0
wandb:                    val/mse 0.00099
wandb:                     val/r2 -0.04108
wandb:                   val/rmse 0.03142
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/lzfdcqbj
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_192949-lzfdcqbj/logs
Completed: DRD_GOLD H=10

Training: FEDformer on DRD_GOLD for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_193617-lwpdjakr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/lwpdjakr
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_DRD_GOLD_H22Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
 fourier enhanced cross attention used!
modes_q=26, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/lwpdjakr
>>>>>>>start training : long_term_forecast_FEDformer_DRD_GOLD_H22_FEDformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.3613283 Vali Loss: 0.2034837 Test Loss: 0.1704533
Validation loss decreased (inf --> 0.203484).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3613282803333167, 'val/loss': 0.20348373906952993, 'test/loss': 0.1704532504081726, '_timestamp': 1762796208.769593}).
Epoch: 2, Steps: 132 | Train Loss: 0.3125092 Vali Loss: 0.1953812 Test Loss: 0.1635573
Validation loss decreased (0.203484 --> 0.195381).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3125092016928124, 'val/loss': 0.19538120286805288, 'test/loss': 0.16355731870446885, '_timestamp': 1762796233.6340904}).
Epoch: 3, Steps: 132 | Train Loss: 0.2927360 Vali Loss: 0.1790701 Test Loss: 0.1562784
Validation loss decreased (0.195381 --> 0.179070).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2804167 Vali Loss: 0.1764977 Test Loss: 0.1549297
Validation loss decreased (0.179070 --> 0.176498).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2744593 Vali Loss: 0.1701780 Test Loss: 0.1486635
Validation loss decreased (0.176498 --> 0.170178).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2723888 Vali Loss: 0.1688141 Test Loss: 0.1494629
Validation loss decreased (0.170178 --> 0.168814).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2716738 Vali Loss: 0.1705110 Test Loss: 0.1506381
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2704605 Vali Loss: 0.1705285 Test Loss: 0.1508542
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2698982 Vali Loss: 0.1706632 Test Loss: 0.1506219
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2702335 Vali Loss: 0.1710908 Test Loss: 0.1507710
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2700267 Vali Loss: 0.1700764 Test Loss: 0.1507881
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2701481 Vali Loss: 0.1702333 Test Loss: 0.1507127
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2703191 Vali Loss: 0.1704415 Test Loss: 0.1507012
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2703036 Vali Loss: 0.1709137 Test Loss: 0.1507038
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2701508 Vali Loss: 0.1706477 Test Loss: 0.1507015
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2698582 Vali Loss: 0.1706590 Test Loss: 0.1506953
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_DRD_GOLD_H22_FEDformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.0009702452225610614, mae:0.022127844393253326, rmse:0.03114875964820385, r2:-0.018865466117858887, dtw:Not calculated


VAL - MSE: 0.0010, MAE: 0.0221, RMSE: 0.0311, RÂ²: -0.0189, MAPE: 2568968.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.601 MB of 0.602 MB uploadedwandb: \ 0.601 MB of 0.602 MB uploadedwandb: | 0.602 MB of 0.602 MB uploadedwandb: / 0.602 MB of 0.602 MB uploadedwandb: - 0.602 MB of 0.813 MB uploadedwandb: \ 0.813 MB of 0.813 MB uploadedwandb: | 0.813 MB of 0.813 MB uploadedwandb: / 0.813 MB of 0.813 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‡â–â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–„â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–‚â–â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 15904774
wandb:     model/trainable_params 15904774
wandb:                  test/loss 0.1507
wandb:                 train/loss 0.26986
wandb:   val/directional_accuracy 56.20358
wandb:                   val/loss 0.17066
wandb:                    val/mae 0.02213
wandb:                   val/mape 256896825.0
wandb:                    val/mse 0.00097
wandb:                     val/r2 -0.01887
wandb:                   val/rmse 0.03115
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/lwpdjakr
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_193617-lwpdjakr/logs
Completed: DRD_GOLD H=22

Training: FEDformer on DRD_GOLD for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_194332-qkzyo6ic
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/qkzyo6ic
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_DRD_GOLD_H50Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 39]
 fourier enhanced cross attention used!
modes_q=32, index_q=[0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 21, 22, 24, 26, 28, 29, 30, 33, 34, 35, 36, 37, 38, 39]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/qkzyo6ic
>>>>>>>start training : long_term_forecast_FEDformer_DRD_GOLD_H50_FEDformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.3973857 Vali Loss: 0.2439786 Test Loss: 0.1839645
Validation loss decreased (inf --> 0.243979).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.39738566744508164, 'val/loss': 0.24397861460844675, 'test/loss': 0.18396447598934174, '_timestamp': 1762796645.4607666}).
Epoch: 2, Steps: 132 | Train Loss: 0.3585074 Vali Loss: 0.2355136 Test Loss: 0.1760980
Validation loss decreased (0.243979 --> 0.235514).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.35850740935314784, 'val/loss': 0.23551355799039206, 'test/loss': 0.1760979543129603, '_timestamp': 1762796671.8173077}).
Epoch: 3, Steps: 132 | Train Loss: 0.3474926 Vali Loss: 0.2295914 Test Loss: 0.1733571
Validation loss decreased (0.235514 --> 0.229591).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.3374966 Vali Loss: 0.2302347 Test Loss: 0.1765997
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.3323920 Vali Loss: 0.2239138 Test Loss: 0.1724159
Validation loss decreased (0.229591 --> 0.223914).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.3290517 Vali Loss: 0.2256924 Test Loss: 0.1747534
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.3277712 Vali Loss: 0.2222935 Test Loss: 0.1714574
Validation loss decreased (0.223914 --> 0.222294).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.3276975 Vali Loss: 0.2208117 Test Loss: 0.1694910
Validation loss decreased (0.222294 --> 0.220812).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.3266573 Vali Loss: 0.2211117 Test Loss: 0.1697462
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.3257792 Vali Loss: 0.2214949 Test Loss: 0.1702243
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.3270513 Vali Loss: 0.2207583 Test Loss: 0.1701349
Validation loss decreased (0.220812 --> 0.220758).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.3291182 Vali Loss: 0.2211719 Test Loss: 0.1701854
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.3305536 Vali Loss: 0.2208878 Test Loss: 0.1701382
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.3259633 Vali Loss: 0.2210063 Test Loss: 0.1701850
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.3258465 Vali Loss: 0.2213162 Test Loss: 0.1701858
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.3270610 Vali Loss: 0.2210773 Test Loss: 0.1701920
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.3284423 Vali Loss: 0.2214106 Test Loss: 0.1701953
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.3264380 Vali Loss: 0.2210406 Test Loss: 0.1701975
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.3255300 Vali Loss: 0.2209681 Test Loss: 0.1701982
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.3263403 Vali Loss: 0.2211871 Test Loss: 0.1701982
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.3254907 Vali Loss: 0.2215320 Test Loss: 0.1701982
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_DRD_GOLD_H50_FEDformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.0009484099573455751, mae:0.021927349269390106, rmse:0.030796265229582787, r2:-0.018500208854675293, dtw:Not calculated


VAL - MSE: 0.0009, MAE: 0.0219, RMSE: 0.0308, RÂ²: -0.0185, MAPE: 2146187.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.717 MB of 0.719 MB uploadedwandb: \ 0.717 MB of 0.719 MB uploadedwandb: | 0.719 MB of 0.719 MB uploadedwandb: / 0.719 MB of 0.719 MB uploadedwandb: - 0.719 MB of 0.931 MB uploadedwandb: \ 0.931 MB of 0.931 MB uploadedwandb: | 0.931 MB of 0.931 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–ˆâ–„â–†â–ƒâ–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–‚â–ƒâ–â–â–â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ˆâ–ƒâ–…â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 16691206
wandb:     model/trainable_params 16691206
wandb:                  test/loss 0.1702
wandb:                 train/loss 0.32549
wandb:   val/directional_accuracy 51.9334
wandb:                   val/loss 0.22153
wandb:                    val/mae 0.02193
wandb:                   val/mape 214618750.0
wandb:                    val/mse 0.00095
wandb:                     val/r2 -0.0185
wandb:                   val/rmse 0.0308
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/qkzyo6ic
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_194332-qkzyo6ic/logs
Completed: DRD_GOLD H=50

Training: FEDformer on DRD_GOLD for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_195312-da3tsn1u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/da3tsn1u
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_DRD_GOLD_H100Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 8, 13, 16, 20, 22, 23, 24, 28, 29, 30, 31, 32, 34, 35, 37, 39, 43, 44, 46, 51, 52, 55, 56, 57, 62, 63, 64]
 fourier enhanced cross attention used!
modes_q=32, index_q=[0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 17, 18, 20, 21, 22, 23, 28, 30, 34, 36, 44, 45, 50, 53, 54, 55, 56, 57, 61, 63]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/da3tsn1u
>>>>>>>start training : long_term_forecast_FEDformer_DRD_GOLD_H100_FEDformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.4803990 Vali Loss: 0.2728543 Test Loss: 0.1823064
Validation loss decreased (inf --> 0.272854).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.480398964881897, 'val/loss': 0.27285430431365965, 'test/loss': 0.18230642676353453, '_timestamp': 1762797226.0686688}).
Epoch: 2, Steps: 130 | Train Loss: 0.4431061 Vali Loss: 0.2938591 Test Loss: 0.2237817
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.44310606167866634, 'val/loss': 0.29385905265808104, 'test/loss': 0.22378169000148773, '_timestamp': 1762797251.7388434}).
Epoch: 3, Steps: 130 | Train Loss: 0.4363634 Vali Loss: 0.2751956 Test Loss: 0.1829370
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.4315185 Vali Loss: 0.2884802 Test Loss: 0.2155410
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.4284980 Vali Loss: 0.2828287 Test Loss: 0.2023043
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.4267955 Vali Loss: 0.2763706 Test Loss: 0.1938002
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.4266455 Vali Loss: 0.2890957 Test Loss: 0.2038389
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.4257389 Vali Loss: 0.2831332 Test Loss: 0.2019591
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.4265546 Vali Loss: 0.2797531 Test Loss: 0.2013328
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.4250300 Vali Loss: 0.2809476 Test Loss: 0.2012889
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.4253376 Vali Loss: 0.2826383 Test Loss: 0.2007861
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_DRD_GOLD_H100_FEDformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0009211102151311934, mae:0.021335715427994728, rmse:0.030349798500537872, r2:-0.019439220428466797, dtw:Not calculated


VAL - MSE: 0.0009, MAE: 0.0213, RMSE: 0.0303, RÂ²: -0.0194, MAPE: 1786142.38%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.793 MB of 0.798 MB uploadedwandb: \ 0.793 MB of 0.798 MB uploadedwandb: | 0.793 MB of 0.798 MB uploadedwandb: / 0.798 MB of 0.798 MB uploadedwandb: - 0.798 MB of 0.798 MB uploadedwandb: \ 0.798 MB of 1.009 MB uploadedwandb: | 1.009 MB of 1.009 MB uploadedwandb: / 1.009 MB of 1.009 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–…â–ƒâ–…â–…â–…â–…â–…
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ˆâ–…â–‚â–ˆâ–…â–ƒâ–„â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 16691206
wandb:     model/trainable_params 16691206
wandb:                  test/loss 0.20079
wandb:                 train/loss 0.42534
wandb:   val/directional_accuracy 52.99423
wandb:                   val/loss 0.28264
wandb:                    val/mae 0.02134
wandb:                   val/mape 178614237.5
wandb:                    val/mse 0.00092
wandb:                     val/r2 -0.01944
wandb:                   val/rmse 0.03035
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/da3tsn1u
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_195312-da3tsn1u/logs
Completed: DRD_GOLD H=100

Training: FEDformer on ANGLO_AMERICAN for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_195832-k8dqd8wf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/k8dqd8wf
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_ANGLO_AMERICAN_H3Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
 fourier enhanced cross attention used!
modes_q=16, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/k8dqd8wf
>>>>>>>start training : long_term_forecast_FEDformer_ANGLO_AMERICAN_H3_FEDformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 3
============================================================
train 778
val 44
test 46
Epoch: 1, Steps: 25 | Train Loss: 0.5111145 Vali Loss: 0.5967873 Test Loss: 0.8196713
Validation loss decreased (inf --> 0.596787).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 25 | Train Loss: 0.3903458 Vali Loss: 0.5732873 Test Loss: 0.7200160
Validation loss decreased (0.596787 --> 0.573287).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.5111145102977752, 'val/loss': 0.596787303686142, 'test/loss': 0.8196713179349899, '_timestamp': 1762797525.449618}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3903458273410797, 'val/loss': 0.573287308216095, 'test/loss': 0.7200159579515457, '_timestamp': 1762797530.988957}).
Epoch: 3, Steps: 25 | Train Loss: 0.3699744 Vali Loss: 0.5151207 Test Loss: 0.7023318
Validation loss decreased (0.573287 --> 0.515121).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 25 | Train Loss: 0.3530189 Vali Loss: 0.5104678 Test Loss: 0.6992840
Validation loss decreased (0.515121 --> 0.510468).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 25 | Train Loss: 0.3444280 Vali Loss: 0.5478569 Test Loss: 0.7002182
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 25 | Train Loss: 0.3453913 Vali Loss: 0.5150166 Test Loss: 0.6927065
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 25 | Train Loss: 0.3429491 Vali Loss: 0.5050563 Test Loss: 0.6917277
Validation loss decreased (0.510468 --> 0.505056).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 25 | Train Loss: 0.3397076 Vali Loss: 0.4875700 Test Loss: 0.6910610
Validation loss decreased (0.505056 --> 0.487570).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 25 | Train Loss: 0.3376353 Vali Loss: 0.5136960 Test Loss: 0.6918152
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 25 | Train Loss: 0.3378848 Vali Loss: 0.4985998 Test Loss: 0.6915244
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 25 | Train Loss: 0.3341474 Vali Loss: 0.4807882 Test Loss: 0.6915061
Validation loss decreased (0.487570 --> 0.480788).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 25 | Train Loss: 0.3346221 Vali Loss: 0.5027460 Test Loss: 0.6915876
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 25 | Train Loss: 0.3493422 Vali Loss: 0.4938613 Test Loss: 0.6916089
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 25 | Train Loss: 0.3332831 Vali Loss: 0.5137595 Test Loss: 0.6915492
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 25 | Train Loss: 0.3366820 Vali Loss: 0.4893388 Test Loss: 0.6915593
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 25 | Train Loss: 0.3413956 Vali Loss: 0.5190231 Test Loss: 0.6915638
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 25 | Train Loss: 0.3377754 Vali Loss: 0.5295949 Test Loss: 0.6915648
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 25 | Train Loss: 0.3341551 Vali Loss: 0.5367500 Test Loss: 0.6915632
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 25 | Train Loss: 0.3414728 Vali Loss: 0.4984001 Test Loss: 0.6915633
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 25 | Train Loss: 0.3388937 Vali Loss: 0.5309120 Test Loss: 0.6915634
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 25 | Train Loss: 0.3411699 Vali Loss: 0.4804430 Test Loss: 0.6915644
Validation loss decreased (0.480788 --> 0.480443).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 25 | Train Loss: 0.3370946 Vali Loss: 0.5063257 Test Loss: 0.6915633
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 25 | Train Loss: 0.3403708 Vali Loss: 0.4981377 Test Loss: 0.6915637
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 25 | Train Loss: 0.3350003 Vali Loss: 0.4938947 Test Loss: 0.6915633
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 25 | Train Loss: 0.3367886 Vali Loss: 0.5356777 Test Loss: 0.6915634
EarlyStopping counter: 4 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 25 | Train Loss: 0.3379885 Vali Loss: 0.5183783 Test Loss: 0.6915639
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 25 | Train Loss: 0.3410189 Vali Loss: 0.5040632 Test Loss: 0.6915638
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 25 | Train Loss: 0.3298768 Vali Loss: 0.5308515 Test Loss: 0.6915638
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 25 | Train Loss: 0.3462381 Vali Loss: 0.5249684 Test Loss: 0.6915638
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 25 | Train Loss: 0.3342328 Vali Loss: 0.5211742 Test Loss: 0.6915638
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 25 | Train Loss: 0.3363484 Vali Loss: 0.5271576 Test Loss: 0.6915638
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_ANGLO_AMERICAN_H3_FEDformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 46
test shape: (46, 3, 1) (46, 3, 1)
test shape: (46, 3, 1) (46, 3, 1)


	mse:0.006657760590314865, mae:0.05663667246699333, rmse:0.08159510046243668, r2:0.017288565635681152, dtw:Not calculated


VAL - MSE: 0.0067, MAE: 0.0566, RMSE: 0.0816, RÂ²: 0.0173, MAPE: 69154608.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.437 MB of 0.437 MB uploadedwandb: \ 0.437 MB of 0.437 MB uploadedwandb: | 0.437 MB of 0.437 MB uploadedwandb: / 0.437 MB of 0.437 MB uploadedwandb: - 0.437 MB of 0.437 MB uploadedwandb: \ 0.437 MB of 0.437 MB uploadedwandb: | 0.437 MB of 0.437 MB uploadedwandb: / 0.437 MB of 0.437 MB uploadedwandb: - 0.566 MB of 0.780 MB uploaded (0.002 MB deduped)wandb: \ 0.780 MB of 0.780 MB uploaded (0.002 MB deduped)wandb: | 0.780 MB of 0.780 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–†â–‡â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–„â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–ƒâ–â–„â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–„â–ˆâ–…â–„â–‚â–„â–ƒâ–â–ƒâ–‚â–„â–‚â–…â–†â–‡â–ƒâ–†â–â–„â–ƒâ–‚â–‡â–…â–ƒâ–†â–†â–…â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 30
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 14594054
wandb:     model/trainable_params 14594054
wandb:                  test/loss 0.69156
wandb:                 train/loss 0.33635
wandb:   val/directional_accuracy 52.17391
wandb:                   val/loss 0.52716
wandb:                    val/mae 0.05664
wandb:                   val/mape 6915460800.0
wandb:                    val/mse 0.00666
wandb:                     val/r2 0.01729
wandb:                   val/rmse 0.0816
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/k8dqd8wf
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_195832-k8dqd8wf/logs
Completed: ANGLO_AMERICAN H=3

Training: FEDformer on ANGLO_AMERICAN for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_200155-mseyce10
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/mseyce10
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_ANGLO_AMERICAN_H5Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
 fourier enhanced cross attention used!
modes_q=17, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/mseyce10
>>>>>>>start training : long_term_forecast_FEDformer_ANGLO_AMERICAN_H5_FEDformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 5
============================================================
train 776
val 42
test 44
Epoch: 1, Steps: 25 | Train Loss: 0.5140736 Vali Loss: 0.6296473 Test Loss: 0.9108241
Validation loss decreased (inf --> 0.629647).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 25 | Train Loss: 0.4062652 Vali Loss: 0.4891082 Test Loss: 0.7421819
Validation loss decreased (0.629647 --> 0.489108).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.5140736019611358, 'val/loss': 0.6296472549438477, 'test/loss': 0.9108240604400635, '_timestamp': 1762797728.8512828}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.4062651777267456, 'val/loss': 0.48910820484161377, 'test/loss': 0.7421818822622299, '_timestamp': 1762797734.3447752}).
Epoch: 3, Steps: 25 | Train Loss: 0.3752216 Vali Loss: 0.5908710 Test Loss: 0.7726183
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 25 | Train Loss: 0.3575213 Vali Loss: 0.5014559 Test Loss: 0.7254082
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 25 | Train Loss: 0.3524000 Vali Loss: 0.5333034 Test Loss: 0.7379810
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 25 | Train Loss: 0.3452733 Vali Loss: 0.5394423 Test Loss: 0.7319578
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 25 | Train Loss: 0.3490253 Vali Loss: 0.5411966 Test Loss: 0.7343373
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 25 | Train Loss: 0.3468458 Vali Loss: 0.5256296 Test Loss: 0.7324878
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 25 | Train Loss: 0.3464033 Vali Loss: 0.4968327 Test Loss: 0.7324867
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 25 | Train Loss: 0.3465905 Vali Loss: 0.5072261 Test Loss: 0.7329124
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 25 | Train Loss: 0.3482745 Vali Loss: 0.5061537 Test Loss: 0.7329838
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 25 | Train Loss: 0.3459664 Vali Loss: 0.5174613 Test Loss: 0.7327534
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_ANGLO_AMERICAN_H5_FEDformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 44
test shape: (44, 5, 1) (44, 5, 1)
test shape: (44, 5, 1) (44, 5, 1)


	mse:0.006809668615460396, mae:0.05712544545531273, rmse:0.08252071589231491, r2:0.034352242946624756, dtw:Not calculated


VAL - MSE: 0.0068, MAE: 0.0571, RMSE: 0.0825, RÂ²: 0.0344, MAPE: 51814712.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.476 MB of 0.477 MB uploadedwandb: \ 0.476 MB of 0.477 MB uploadedwandb: | 0.476 MB of 0.477 MB uploadedwandb: / 0.477 MB of 0.477 MB uploadedwandb: - 0.477 MB of 0.477 MB uploadedwandb: \ 0.477 MB of 0.687 MB uploadedwandb: | 0.687 MB of 0.687 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–„â–ƒâ–â–‚â–â–â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–„â–„â–„â–ƒâ–â–‚â–‚â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 14725126
wandb:     model/trainable_params 14725126
wandb:                  test/loss 0.73275
wandb:                 train/loss 0.34597
wandb:   val/directional_accuracy 57.95455
wandb:                   val/loss 0.51746
wandb:                    val/mae 0.05713
wandb:                   val/mape 5181471200.0
wandb:                    val/mse 0.00681
wandb:                     val/r2 0.03435
wandb:                   val/rmse 0.08252
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/mseyce10
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_200155-mseyce10/logs
Completed: ANGLO_AMERICAN H=5

Training: FEDformer on ANGLO_AMERICAN for H=10
502 response executing GraphQL.

<html><head>
<meta http-equiv="content-type" content="text/html;charset=utf-8">
<title>502 Server Error</title>
</head>
<body text=#000000 bgcolor=#ffffff>
<h1>Error: Server Error</h1>
<h2>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds.</h2>
<h2></h2>
</body></html>

wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_200339-9q7z0qar
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/9q7z0qar
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_ANGLO_AMERICAN_H10Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
 fourier enhanced cross attention used!
modes_q=20, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/9q7z0qar
>>>>>>>start training : long_term_forecast_FEDformer_ANGLO_AMERICAN_H10_FEDformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 10
============================================================
train 771
val 37
test 39
Epoch: 1, Steps: 25 | Train Loss: 0.5234021 Vali Loss: 0.5933033 Test Loss: 0.8022022
Validation loss decreased (inf --> 0.593303).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 25 | Train Loss: 0.4012199 Vali Loss: 0.5255281 Test Loss: 0.7746810
Validation loss decreased (0.593303 --> 0.525528).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.5234021282196045, 'val/loss': 0.5933032631874084, 'test/loss': 0.8022021949291229, '_timestamp': 1762797833.8301914}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.40121987104415896, 'val/loss': 0.5255280584096909, 'test/loss': 0.7746809720993042, '_timestamp': 1762797839.5394223}).
Epoch: 3, Steps: 25 | Train Loss: 0.3744897 Vali Loss: 0.5793689 Test Loss: 0.8112312
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 25 | Train Loss: 0.3651633 Vali Loss: 0.5767512 Test Loss: 0.7578502
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 25 | Train Loss: 0.3650105 Vali Loss: 0.5829510 Test Loss: 0.7724671
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 25 | Train Loss: 0.3569320 Vali Loss: 0.5602390 Test Loss: 0.7717183
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 25 | Train Loss: 0.3524476 Vali Loss: 0.5773094 Test Loss: 0.7695250
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 25 | Train Loss: 0.3594794 Vali Loss: 0.5844042 Test Loss: 0.7696909
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 25 | Train Loss: 0.3627231 Vali Loss: 0.6101928 Test Loss: 0.7701739
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 25 | Train Loss: 0.3502085 Vali Loss: 0.6086090 Test Loss: 0.7698178
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 25 | Train Loss: 0.3568903 Vali Loss: 0.5840585 Test Loss: 0.7696520
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 25 | Train Loss: 0.3541471 Vali Loss: 0.5837511 Test Loss: 0.7695969
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_ANGLO_AMERICAN_H10_FEDformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 39
test shape: (39, 10, 1) (39, 10, 1)
test shape: (39, 10, 1) (39, 10, 1)


	mse:0.007617066614329815, mae:0.06126415729522705, rmse:0.08727581053972244, r2:0.016963064670562744, dtw:Not calculated


VAL - MSE: 0.0076, MAE: 0.0613, RMSE: 0.0873, RÂ²: 0.0170, MAPE: 29627286.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.511 MB of 0.512 MB uploadedwandb: \ 0.511 MB of 0.512 MB uploadedwandb: | 0.512 MB of 0.512 MB uploadedwandb: / 0.512 MB of 0.512 MB uploadedwandb: - 0.512 MB of 0.723 MB uploadedwandb: \ 0.673 MB of 0.723 MB uploadedwandb: | 0.723 MB of 0.723 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–…â–…â–ƒâ–‚â–„â–…â–â–ƒâ–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–ƒâ–„â–â–ƒâ–„â–ˆâ–ˆâ–„â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 15118342
wandb:     model/trainable_params 15118342
wandb:                  test/loss 0.7696
wandb:                 train/loss 0.35415
wandb:   val/directional_accuracy 39.60114
wandb:                   val/loss 0.58375
wandb:                    val/mae 0.06126
wandb:                   val/mape 2962728600.0
wandb:                    val/mse 0.00762
wandb:                     val/r2 0.01696
wandb:                   val/rmse 0.08728
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/9q7z0qar
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_200339-9q7z0qar/logs
Completed: ANGLO_AMERICAN H=10

Training: FEDformer on ANGLO_AMERICAN for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_200515-g3m59la5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/g3m59la5
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_ANGLO_AMERICAN_H22Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
 fourier enhanced cross attention used!
modes_q=26, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/g3m59la5
>>>>>>>start training : long_term_forecast_FEDformer_ANGLO_AMERICAN_H22_FEDformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 22
============================================================
train 759
val 25
test 27
Epoch: 1, Steps: 24 | Train Loss: 0.5216068 Vali Loss: 0.7315589 Test Loss: 1.2329650
Validation loss decreased (inf --> 0.731559).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 24 | Train Loss: 0.4249163 Vali Loss: 0.6944593 Test Loss: 1.1961086
Validation loss decreased (0.731559 --> 0.694459).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.521606786797444, 'val/loss': 0.7315588593482971, 'test/loss': 1.2329649925231934, '_timestamp': 1762797929.601575}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.4249163046479225, 'val/loss': 0.6944593191146851, 'test/loss': 1.1961085796356201, '_timestamp': 1762797935.436617}).
Epoch: 3, Steps: 24 | Train Loss: 0.3977551 Vali Loss: 0.7013518 Test Loss: 1.2002499
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 24 | Train Loss: 0.3871457 Vali Loss: 0.7002575 Test Loss: 1.1967117
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 24 | Train Loss: 0.3812961 Vali Loss: 0.6921358 Test Loss: 1.1902627
Validation loss decreased (0.694459 --> 0.692136).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 24 | Train Loss: 0.3789680 Vali Loss: 0.6951120 Test Loss: 1.1950489
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 24 | Train Loss: 0.3771990 Vali Loss: 0.6961331 Test Loss: 1.1979948
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 24 | Train Loss: 0.3784467 Vali Loss: 0.6939482 Test Loss: 1.1949279
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 24 | Train Loss: 0.3777223 Vali Loss: 0.6942526 Test Loss: 1.1951240
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 24 | Train Loss: 0.3768785 Vali Loss: 0.6943828 Test Loss: 1.1953340
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 24 | Train Loss: 0.3753562 Vali Loss: 0.6941126 Test Loss: 1.1951112
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 24 | Train Loss: 0.3759250 Vali Loss: 0.6941802 Test Loss: 1.1951672
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 24 | Train Loss: 0.3752346 Vali Loss: 0.6942058 Test Loss: 1.1951874
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 24 | Train Loss: 0.3755761 Vali Loss: 0.6941950 Test Loss: 1.1951728
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 24 | Train Loss: 0.3750108 Vali Loss: 0.6941743 Test Loss: 1.1951551
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_FEDformer_ANGLO_AMERICAN_H22_FEDformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 27
test shape: (27, 22, 1) (27, 22, 1)
test shape: (27, 22, 1) (27, 22, 1)


	mse:0.007905102334916592, mae:0.06384728103876114, rmse:0.08891063928604126, r2:-0.006872415542602539, dtw:Not calculated


VAL - MSE: 0.0079, MAE: 0.0638, RMSE: 0.0889, RÂ²: -0.0069, MAPE: 19105264.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.484 MB of 0.485 MB uploadedwandb: \ 0.484 MB of 0.485 MB uploadedwandb: | 0.484 MB of 0.485 MB uploadedwandb: / 0.485 MB of 0.485 MB uploadedwandb: - 0.485 MB of 0.697 MB uploadedwandb: \ 0.697 MB of 0.697 MB uploadedwandb: | 0.697 MB of 0.697 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–†â–â–„â–†â–„â–„â–…â–„â–„â–„â–„â–„
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‡â–â–ƒâ–„â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 15904774
wandb:     model/trainable_params 15904774
wandb:                  test/loss 1.19516
wandb:                 train/loss 0.37501
wandb:   val/directional_accuracy 54.85009
wandb:                   val/loss 0.69417
wandb:                    val/mae 0.06385
wandb:                   val/mape 1910526400.0
wandb:                    val/mse 0.00791
wandb:                     val/r2 -0.00687
wandb:                   val/rmse 0.08891
wandb: 
wandb: ðŸš€ View run FEDformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/g3m59la5
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_200515-g3m59la5/logs
Completed: ANGLO_AMERICAN H=22

Training: FEDformer on ANGLO_AMERICAN for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_200715-urupgnhr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/urupgnhr
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_ANGLO_AMERICAN_H50Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 39]
 fourier enhanced cross attention used!
modes_q=32, index_q=[0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 21, 22, 24, 26, 28, 29, 30, 33, 34, 35, 36, 37, 38, 39]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/urupgnhr
>>>>>>>start training : long_term_forecast_FEDformer_ANGLO_AMERICAN_H50_FEDformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 50
============================================================
train 731
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.005 MB of 0.024 MB uploadedwandb: / 0.005 MB of 0.024 MB uploadedwandb: - 0.024 MB of 0.024 MB uploadedwandb: ðŸš€ View run FEDformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/urupgnhr
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_200715-urupgnhr/logs
Traceback (most recent call last):
  File "run.py", line 207, in <module>
    exp.train(setting)
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 93, in train
    vali_data, vali_loader = self._get_data(flag='val')
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 43, in _get_data
    data_set, data_loader = data_provider(self.args, flag)
  File "/home-mscluster/bkodze/research_project/forecast-research/data_provider/data_factory.py", line 79, in data_provider
    print(flag, len(data_set))
ValueError: __len__() should return >= 0
Completed: ANGLO_AMERICAN H=50

Training: FEDformer on ANGLO_AMERICAN for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_200740-qmqkr9gi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FEDformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/qmqkr9gi
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           FEDformer_ANGLO_AMERICAN_H100Model:              FEDformer           

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                FEDformer_Exp       Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fourier enhanced block used!
modes=32, index=[0, 1, 2, 3, 4, 8, 13, 16, 20, 22, 23, 24, 28, 29, 30, 31, 32, 34, 35, 37, 39, 43, 44, 46, 51, 52, 55, 56, 57, 62, 63, 64]
 fourier enhanced cross attention used!
modes_q=32, index_q=[0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 17, 18, 20, 21, 22, 23, 28, 30, 34, 36, 44, 45, 50, 53, 54, 55, 56, 57, 61, 63]
modes_kv=30, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
âœ… W&B initialized: FEDformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/qmqkr9gi
>>>>>>>start training : long_term_forecast_FEDformer_ANGLO_AMERICAN_H100_FEDformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_FEDformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 100
============================================================
train 681
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.005 MB of 0.007 MB uploadedwandb: / 0.024 MB of 0.024 MB uploadedwandb: ðŸš€ View run FEDformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/qmqkr9gi
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_200740-qmqkr9gi/logs
Traceback (most recent call last):
  File "run.py", line 207, in <module>
    exp.train(setting)
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 93, in train
    vali_data, vali_loader = self._get_data(flag='val')
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 43, in _get_data
    data_set, data_loader = data_provider(self.args, flag)
  File "/home-mscluster/bkodze/research_project/forecast-research/data_provider/data_factory.py", line 79, in data_provider
    print(flag, len(data_set))
ValueError: __len__() should return >= 0
Completed: ANGLO_AMERICAN H=100

FEDformer training completed for all datasets!
