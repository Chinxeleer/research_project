##############################################################################
# Training Informer Model on All Datasets
##############################################################################
Training: Informer on NVIDIA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_091426-87qsggi6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/87qsggi6
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NVIDIA_H3  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/87qsggi6
>>>>>>>start training : long_term_forecast_Informer_NVIDIA_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
Overriding target from 'OT' to 'close' for stock data
val 237
Overriding target from 'OT' to 'close' for stock data
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.3091627 Vali Loss: 0.2386510 Test Loss: 1.0307190
Validation loss decreased (inf --> 0.238651).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.30916271599611844, 'val/loss': 0.2386509608477354, 'test/loss': 1.0307190492749214, '_timestamp': 1762326881.9052052}).
Epoch: 2, Steps: 133 | Train Loss: 0.2475854 Vali Loss: 0.2616613 Test Loss: 1.2668754
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2334936 Vali Loss: 0.1898537 Test Loss: 1.0493783
Validation loss decreased (0.238651 --> 0.189854).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24758538581374892, 'val/loss': 0.26166126132011414, 'test/loss': 1.2668754383921623, '_timestamp': 1762326888.8468897}).
Epoch: 4, Steps: 133 | Train Loss: 0.2292731 Vali Loss: 0.1882774 Test Loss: 1.0000950
Validation loss decreased (0.189854 --> 0.188277).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2262834 Vali Loss: 0.1904290 Test Loss: 0.9850689
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2243169 Vali Loss: 0.1855889 Test Loss: 0.9838178
Validation loss decreased (0.188277 --> 0.185589).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2242645 Vali Loss: 0.1854032 Test Loss: 1.0002660
Validation loss decreased (0.185589 --> 0.185403).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2228799 Vali Loss: 0.1899752 Test Loss: 1.0015444
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2234300 Vali Loss: 0.1857134 Test Loss: 0.9860108
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2230344 Vali Loss: 0.1863485 Test Loss: 1.0031455
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2235396 Vali Loss: 0.1888133 Test Loss: 0.9961557
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2232166 Vali Loss: 0.1879768 Test Loss: 1.0105154
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2232636 Vali Loss: 0.1901637 Test Loss: 1.0061088
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2225523 Vali Loss: 0.1847500 Test Loss: 0.9893156
Validation loss decreased (0.185403 --> 0.184750).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2234685 Vali Loss: 0.2038938 Test Loss: 0.9900539
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2230090 Vali Loss: 0.1824271 Test Loss: 1.0009633
Validation loss decreased (0.184750 --> 0.182427).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2229394 Vali Loss: 0.1967366 Test Loss: 0.9979952
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2228805 Vali Loss: 0.1887333 Test Loss: 1.0078470
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2227243 Vali Loss: 0.1873033 Test Loss: 0.9831793
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2227241 Vali Loss: 0.1864374 Test Loss: 1.0004017
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2222049 Vali Loss: 0.1868288 Test Loss: 0.9824001
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2226677 Vali Loss: 0.1825155 Test Loss: 0.9960773
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2226594 Vali Loss: 0.2050414 Test Loss: 1.0013706
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2230768 Vali Loss: 0.1867340 Test Loss: 0.9995435
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2235330 Vali Loss: 0.2068599 Test Loss: 0.9967552
EarlyStopping counter: 9 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2228108 Vali Loss: 0.1872728 Test Loss: 1.0104721
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NVIDIA_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.0011434670304879546, mae:0.02538158744573593, rmse:0.03381519019603729, r2:-0.01932680606842041, dtw:Not calculated


VAL - MSE: 0.0011, MAE: 0.0254, RMSE: 0.0338, RÂ²: -0.0193, MAPE: 77078.54%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.472 MB of 0.472 MB uploadedwandb: \ 0.472 MB of 0.472 MB uploadedwandb: | 0.472 MB of 0.472 MB uploadedwandb: / 0.472 MB of 0.472 MB uploadedwandb: - 0.472 MB of 0.472 MB uploadedwandb: \ 0.472 MB of 0.472 MB uploadedwandb: | 0.472 MB of 0.472 MB uploadedwandb: / 0.472 MB of 0.472 MB uploadedwandb: - 0.472 MB of 0.472 MB uploadedwandb: \ 0.654 MB of 0.954 MB uploaded (0.002 MB deduped)wandb: | 0.654 MB of 0.954 MB uploaded (0.002 MB deduped)wandb: / 0.954 MB of 0.954 MB uploaded (0.002 MB deduped)wandb: - 0.954 MB of 0.954 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–â–ƒâ–ƒâ–â–ƒâ–‚â–„â–ƒâ–‚â–‚â–ƒâ–ƒâ–„â–â–ƒâ–â–‚â–ƒâ–ƒâ–ƒâ–„
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–‚â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–‡â–â–…â–ƒâ–‚â–‚â–‚â–â–‡â–‚â–ˆâ–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 25
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 1.01047
wandb:                 train/loss 0.22281
wandb:   val/directional_accuracy 49.36709
wandb:                   val/loss 0.18727
wandb:                    val/mae 0.02538
wandb:                   val/mape 7707853.90625
wandb:                    val/mse 0.00114
wandb:                     val/r2 -0.01933
wandb:                   val/rmse 0.03382
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/87qsggi6
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_091426-87qsggi6/logs
Completed: NVIDIA H=3

Training: Informer on NVIDIA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_091759-5c8rz7f3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/5c8rz7f3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NVIDIA_H5  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/5c8rz7f3
>>>>>>>start training : long_term_forecast_Informer_NVIDIA_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
Overriding target from 'OT' to 'close' for stock data
val 235
Overriding target from 'OT' to 'close' for stock data
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.3086393 Vali Loss: 0.2133219 Test Loss: 1.1703456
Validation loss decreased (inf --> 0.213322).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2497776 Vali Loss: 0.2259409 Test Loss: 1.2233291
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3086393351169457, 'val/loss': 0.2133218739181757, 'test/loss': 1.1703456081449986, '_timestamp': 1762327092.6853082}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24977758179481765, 'val/loss': 0.22594092041254044, 'test/loss': 1.2233290821313858, '_timestamp': 1762327099.7385664}).
Epoch: 3, Steps: 133 | Train Loss: 0.2377220 Vali Loss: 0.2202146 Test Loss: 1.0616315
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2322677 Vali Loss: 0.2328399 Test Loss: 1.1444872
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2291415 Vali Loss: 0.2040748 Test Loss: 1.0409104
Validation loss decreased (0.213322 --> 0.204075).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2276062 Vali Loss: 0.1989951 Test Loss: 1.0409010
Validation loss decreased (0.204075 --> 0.198995).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2274306 Vali Loss: 0.2015639 Test Loss: 0.9955462
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2268054 Vali Loss: 0.1953332 Test Loss: 1.0117012
Validation loss decreased (0.198995 --> 0.195333).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2264946 Vali Loss: 0.1928121 Test Loss: 1.0113226
Validation loss decreased (0.195333 --> 0.192812).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2276296 Vali Loss: 0.1955704 Test Loss: 1.0096503
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2263626 Vali Loss: 0.1903740 Test Loss: 1.0153432
Validation loss decreased (0.192812 --> 0.190374).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2263060 Vali Loss: 0.2100768 Test Loss: 1.0039419
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2261078 Vali Loss: 0.2247901 Test Loss: 1.0137502
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2269123 Vali Loss: 0.2118764 Test Loss: 1.0115886
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2263575 Vali Loss: 0.1909207 Test Loss: 1.0167105
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2262206 Vali Loss: 0.1919245 Test Loss: 1.0005437
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2263979 Vali Loss: 0.1902999 Test Loss: 1.0031218
Validation loss decreased (0.190374 --> 0.190300).  Saving model ...
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2273067 Vali Loss: 0.2012740 Test Loss: 1.0106612
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2260303 Vali Loss: 0.1881285 Test Loss: 1.0035047
Validation loss decreased (0.190300 --> 0.188128).  Saving model ...
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2266780 Vali Loss: 0.1935555 Test Loss: 1.0067801
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2265124 Vali Loss: 0.1960886 Test Loss: 1.0100955
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2261006 Vali Loss: 0.1898551 Test Loss: 1.0041098
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2263127 Vali Loss: 0.2095511 Test Loss: 0.9929803
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2256651 Vali Loss: 0.1901185 Test Loss: 1.0028808
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2264688 Vali Loss: 0.1904478 Test Loss: 1.0040978
EarlyStopping counter: 6 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2259801 Vali Loss: 0.2001262 Test Loss: 1.0102411
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.2258092 Vali Loss: 0.1915968 Test Loss: 1.0094807
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.2257182 Vali Loss: 0.1902151 Test Loss: 1.0003460
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.2268554 Vali Loss: 0.1986740 Test Loss: 1.0042118
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NVIDIA_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.0011464025592431426, mae:0.025391150265932083, rmse:0.03385856747627258, r2:-0.015341997146606445, dtw:Not calculated


VAL - MSE: 0.0011, MAE: 0.0254, RMSE: 0.0339, RÂ²: -0.0153, MAPE: 102681.45%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.528 MB of 0.528 MB uploadedwandb: \ 0.528 MB of 0.528 MB uploadedwandb: | 0.528 MB of 0.528 MB uploadedwandb: / 0.528 MB of 0.528 MB uploadedwandb: - 0.528 MB of 0.829 MB uploadedwandb: \ 0.528 MB of 0.829 MB uploadedwandb: | 0.829 MB of 0.829 MB uploadedwandb: / 0.829 MB of 0.829 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–„â–ˆâ–ƒâ–ƒâ–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–‚â–â–â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–ˆâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–„â–‡â–…â–â–‚â–â–ƒâ–â–‚â–‚â–â–„â–â–â–ƒâ–‚â–â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 28
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 1.00421
wandb:                 train/loss 0.22686
wandb:   val/directional_accuracy 52.23404
wandb:                   val/loss 0.19867
wandb:                    val/mae 0.02539
wandb:                   val/mape 10268145.3125
wandb:                    val/mse 0.00115
wandb:                     val/r2 -0.01534
wandb:                   val/rmse 0.03386
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/5c8rz7f3
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_091759-5c8rz7f3/logs
Completed: NVIDIA H=5

Training: Informer on NVIDIA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_092145-11y9x8wg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/11y9x8wg
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NVIDIA_H10 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/11y9x8wg
>>>>>>>start training : long_term_forecast_Informer_NVIDIA_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
Overriding target from 'OT' to 'close' for stock data
val 230
Overriding target from 'OT' to 'close' for stock data
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.3094469 Vali Loss: 0.2184820 Test Loss: 1.3110253
Validation loss decreased (inf --> 0.218482).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2518733 Vali Loss: 0.2108495 Test Loss: 1.2170221
Validation loss decreased (0.218482 --> 0.210850).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3094468782270761, 'val/loss': 0.2184820342808962, 'test/loss': 1.3110253289341927, '_timestamp': 1762327318.9823716}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25187330306472633, 'val/loss': 0.21084953658282757, 'test/loss': 1.217022106051445, '_timestamp': 1762327326.0381107}).
Epoch: 3, Steps: 133 | Train Loss: 0.2415205 Vali Loss: 0.2155203 Test Loss: 1.1628659
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2351060 Vali Loss: 0.2221273 Test Loss: 1.0660771
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2331866 Vali Loss: 0.2129051 Test Loss: 1.0258478
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2325758 Vali Loss: 0.2117218 Test Loss: 1.1305629
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2303088 Vali Loss: 0.2046500 Test Loss: 1.1051424
Validation loss decreased (0.210850 --> 0.204650).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2304980 Vali Loss: 0.2284253 Test Loss: 1.1070698
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2300591 Vali Loss: 0.2108719 Test Loss: 1.0952316
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2292057 Vali Loss: 0.2227708 Test Loss: 1.1117680
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2297246 Vali Loss: 0.2174178 Test Loss: 1.0970428
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2292885 Vali Loss: 0.2090507 Test Loss: 1.0905135
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2294455 Vali Loss: 0.2030957 Test Loss: 1.1106998
Validation loss decreased (0.204650 --> 0.203096).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2287265 Vali Loss: 0.2098454 Test Loss: 1.1066259
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2299499 Vali Loss: 0.2065459 Test Loss: 1.1055630
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2295014 Vali Loss: 0.2221672 Test Loss: 1.1087991
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2290087 Vali Loss: 0.2058259 Test Loss: 1.1023541
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2291079 Vali Loss: 0.2097019 Test Loss: 1.0922299
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2294972 Vali Loss: 0.2078460 Test Loss: 1.1028402
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2296475 Vali Loss: 0.2088435 Test Loss: 1.1106855
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2286967 Vali Loss: 0.2049549 Test Loss: 1.1027777
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2285430 Vali Loss: 0.2043884 Test Loss: 1.1086891
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2293502 Vali Loss: 0.2076041 Test Loss: 1.1053329
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NVIDIA_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.001165130757726729, mae:0.02561333030462265, rmse:0.03413401171565056, r2:-0.015696406364440918, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0256, RMSE: 0.0341, RÂ²: -0.0157, MAPE: 74512.35%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.545 MB of 0.546 MB uploadedwandb: \ 0.546 MB of 0.546 MB uploadedwandb: | 0.546 MB of 0.546 MB uploadedwandb: / 0.546 MB of 0.846 MB uploadedwandb: - 0.546 MB of 0.846 MB uploadedwandb: \ 0.846 MB of 0.846 MB uploadedwandb: | 0.846 MB of 0.846 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–†â–…â–…â–…â–…â–…â–„â–…â–…â–…â–…â–…â–„â–…â–…â–…â–…â–…
wandb:                 train/loss â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–‚â–‚â–â–â–‚â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–†â–„â–ƒâ–â–ˆâ–ƒâ–†â–…â–ƒâ–â–ƒâ–‚â–†â–‚â–ƒâ–‚â–ƒâ–‚â–â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 1.10533
wandb:                 train/loss 0.22935
wandb:   val/directional_accuracy 52.1256
wandb:                   val/loss 0.2076
wandb:                    val/mae 0.02561
wandb:                   val/mape 7451235.15625
wandb:                    val/mse 0.00117
wandb:                     val/r2 -0.0157
wandb:                   val/rmse 0.03413
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/11y9x8wg
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_092145-11y9x8wg/logs
Completed: NVIDIA H=10

Training: Informer on NVIDIA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_092449-ipn6mfom
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/ipn6mfom
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NVIDIA_H22 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/ipn6mfom
>>>>>>>start training : long_term_forecast_Informer_NVIDIA_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
Overriding target from 'OT' to 'close' for stock data
val 218
Overriding target from 'OT' to 'close' for stock data
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.3124663 Vali Loss: 0.2085607 Test Loss: 1.3510074
Validation loss decreased (inf --> 0.208561).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2559836 Vali Loss: 0.2609294 Test Loss: 1.3916083
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3124663369222121, 'val/loss': 0.20856066261019027, 'test/loss': 1.3510074487754278, '_timestamp': 1762327502.3328805}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2559836409760244, 'val/loss': 0.2609293929168156, 'test/loss': 1.3916083233697074, '_timestamp': 1762327509.508853}).
Epoch: 3, Steps: 132 | Train Loss: 0.2445009 Vali Loss: 0.2551844 Test Loss: 1.4226154
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2394539 Vali Loss: 0.2239224 Test Loss: 1.2365327
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2363504 Vali Loss: 0.2258874 Test Loss: 1.2331060
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2350848 Vali Loss: 0.2267281 Test Loss: 1.2697990
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2338498 Vali Loss: 0.2253600 Test Loss: 1.2571967
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2336700 Vali Loss: 0.2263902 Test Loss: 1.2450960
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2333884 Vali Loss: 0.2248933 Test Loss: 1.2478101
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2334354 Vali Loss: 0.2246879 Test Loss: 1.2395794
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2331549 Vali Loss: 0.2217535 Test Loss: 1.2436250
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NVIDIA_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.0012063556350767612, mae:0.026019880548119545, rmse:0.0347326323390007, r2:-0.022460460662841797, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0260, RMSE: 0.0347, RÂ²: -0.0225, MAPE: 176623.45%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.585 MB of 0.586 MB uploadedwandb: \ 0.586 MB of 0.586 MB uploadedwandb: | 0.586 MB of 0.586 MB uploadedwandb: / 0.586 MB of 0.885 MB uploadedwandb: - 0.885 MB of 0.885 MB uploadedwandb: \ 0.885 MB of 0.885 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–â–‚â–‚â–â–‚â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–‚â–‚â–‚â–‚â–‚â–‚â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 1.24362
wandb:                 train/loss 0.23315
wandb:   val/directional_accuracy 50.04369
wandb:                   val/loss 0.22175
wandb:                    val/mae 0.02602
wandb:                   val/mape 17662345.3125
wandb:                    val/mse 0.00121
wandb:                     val/r2 -0.02246
wandb:                   val/rmse 0.03473
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/ipn6mfom
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_092449-ipn6mfom/logs
Completed: NVIDIA H=22

Training: Informer on NVIDIA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_092633-fc9iws8y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/fc9iws8y
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NVIDIA_H50 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/fc9iws8y
>>>>>>>start training : long_term_forecast_Informer_NVIDIA_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
Overriding target from 'OT' to 'close' for stock data
val 190
Overriding target from 'OT' to 'close' for stock data
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.3227387 Vali Loss: 0.2319628 Test Loss: 1.4013838
Validation loss decreased (inf --> 0.231963).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2645985 Vali Loss: 0.2682027 Test Loss: 1.4320482
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.32273873483592813, 'val/loss': 0.23196280499299368, 'test/loss': 1.4013838370641072, '_timestamp': 1762327607.0409958}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26459847853490803, 'val/loss': 0.26820274690787, 'test/loss': 1.4320482313632965, '_timestamp': 1762327614.0548759}).
Epoch: 3, Steps: 132 | Train Loss: 0.2530927 Vali Loss: 0.2716834 Test Loss: 1.5467180
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2472397 Vali Loss: 0.2772574 Test Loss: 1.4658712
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2427654 Vali Loss: 0.2744897 Test Loss: 1.4552205
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2433093 Vali Loss: 0.2736629 Test Loss: 1.4475216
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2444665 Vali Loss: 0.2625725 Test Loss: 1.4490473
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2413239 Vali Loss: 0.2887430 Test Loss: 1.4541839
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2461334 Vali Loss: 0.2683958 Test Loss: 1.4535539
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2397059 Vali Loss: 0.2818458 Test Loss: 1.4387239
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2441727 Vali Loss: 0.2834600 Test Loss: 1.4447539
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NVIDIA_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.001221248647198081, mae:0.02656562626361847, rmse:0.034946367144584656, r2:-0.02626621723175049, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0266, RMSE: 0.0349, RÂ²: -0.0263, MAPE: 234145.31%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.623 MB of 0.625 MB uploadedwandb: \ 0.623 MB of 0.625 MB uploadedwandb: | 0.625 MB of 0.625 MB uploadedwandb: / 0.625 MB of 0.625 MB uploadedwandb: - 0.625 MB of 0.923 MB uploadedwandb: \ 0.923 MB of 0.923 MB uploadedwandb: | 0.923 MB of 0.923 MB uploadedwandb: / 0.923 MB of 0.923 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–ƒâ–‚â–„â–â–ƒ
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–…â–„â–„â–â–ˆâ–ƒâ–†â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 1.44475
wandb:                 train/loss 0.24417
wandb:   val/directional_accuracy 49.88185
wandb:                   val/loss 0.28346
wandb:                    val/mae 0.02657
wandb:                   val/mape 23414531.25
wandb:                    val/mse 0.00122
wandb:                     val/r2 -0.02627
wandb:                   val/rmse 0.03495
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/fc9iws8y
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_092633-fc9iws8y/logs
Completed: NVIDIA H=50

Training: Informer on NVIDIA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_092821-483f55vy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/483f55vy
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NVIDIA_H100Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/483f55vy
>>>>>>>start training : long_term_forecast_Informer_NVIDIA_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
Overriding target from 'OT' to 'close' for stock data
val 140
Overriding target from 'OT' to 'close' for stock data
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.3292054 Vali Loss: 0.2998421 Test Loss: 1.4688081
Validation loss decreased (inf --> 0.299842).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3292054410164173, 'val/loss': 0.2998421132564545, 'test/loss': 1.468808114528656, '_timestamp': 1762327715.1061392}).
Epoch: 2, Steps: 130 | Train Loss: 0.2682046 Vali Loss: 0.3285151 Test Loss: 1.7711438
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.2528865 Vali Loss: 0.2475196 Test Loss: 1.6159348
Validation loss decreased (0.299842 --> 0.247520).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2475895 Vali Loss: 0.2742248 Test Loss: 1.6054734
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2682046428322792, 'val/loss': 0.32851505279541016, 'test/loss': 1.7711437940597534, '_timestamp': 1762327723.4051263}).
Epoch: 5, Steps: 130 | Train Loss: 0.2442913 Vali Loss: 0.2666104 Test Loss: 1.5793064
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2430202 Vali Loss: 0.2630494 Test Loss: 1.6191652
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2422130 Vali Loss: 0.2640088 Test Loss: 1.5779566
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2418351 Vali Loss: 0.2656010 Test Loss: 1.5884060
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2412691 Vali Loss: 0.2662013 Test Loss: 1.5788440
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2415381 Vali Loss: 0.2601071 Test Loss: 1.5731803
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2418475 Vali Loss: 0.2644680 Test Loss: 1.5814222
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2410598 Vali Loss: 0.2631018 Test Loss: 1.5806866
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2412576 Vali Loss: 0.2585910 Test Loss: 1.5785077
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NVIDIA_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0012926521012559533, mae:0.027471665292978287, rmse:0.03595346957445145, r2:-0.004067659378051758, dtw:Not calculated


VAL - MSE: 0.0013, MAE: 0.0275, RMSE: 0.0360, RÂ²: -0.0041, MAPE: 84639.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.650 MB of 0.655 MB uploadedwandb: \ 0.650 MB of 0.655 MB uploadedwandb: | 0.650 MB of 0.655 MB uploadedwandb: / 0.655 MB of 0.655 MB uploadedwandb: - 0.655 MB of 0.655 MB uploadedwandb: \ 0.655 MB of 0.953 MB uploadedwandb: | 0.953 MB of 0.953 MB uploadedwandb: / 0.953 MB of 0.953 MB uploadedwandb: - 0.953 MB of 0.953 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–†â–‚â–ˆâ–‚â–ƒâ–‚â–â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ˆâ–†â–…â–…â–†â–†â–„â–…â–…â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 1.57851
wandb:                 train/loss 0.24126
wandb:   val/directional_accuracy 50.22367
wandb:                   val/loss 0.25859
wandb:                    val/mae 0.02747
wandb:                   val/mape 8463900.0
wandb:                    val/mse 0.00129
wandb:                     val/r2 -0.00407
wandb:                   val/rmse 0.03595
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/483f55vy
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_092821-483f55vy/logs
Completed: NVIDIA H=100

Training: Informer on APPLE for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_093023-gornikpm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/gornikpm
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_APPLE_H3   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/gornikpm
>>>>>>>start training : long_term_forecast_Informer_APPLE_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
Overriding target from 'OT' to 'close' for stock data
val 237
Overriding target from 'OT' to 'close' for stock data
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.2921719 Vali Loss: 0.1161840 Test Loss: 0.1721903
Validation loss decreased (inf --> 0.116184).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2375773 Vali Loss: 0.1085540 Test Loss: 0.1655686
Validation loss decreased (0.116184 --> 0.108554).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.292171914438556, 'val/loss': 0.11618401855230331, 'test/loss': 0.17219032440334558, '_timestamp': 1762327836.690499}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23757725237007427, 'val/loss': 0.1085540410131216, 'test/loss': 0.1655685557052493, '_timestamp': 1762327843.7305686}).
Epoch: 3, Steps: 133 | Train Loss: 0.2228303 Vali Loss: 0.0906291 Test Loss: 0.1385305
Validation loss decreased (0.108554 --> 0.090629).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2171866 Vali Loss: 0.0878135 Test Loss: 0.1417708
Validation loss decreased (0.090629 --> 0.087814).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2161124 Vali Loss: 0.0880278 Test Loss: 0.1408333
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2137204 Vali Loss: 0.0861675 Test Loss: 0.1403445
Validation loss decreased (0.087814 --> 0.086167).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2123911 Vali Loss: 0.0923590 Test Loss: 0.1445434
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2124054 Vali Loss: 0.0937718 Test Loss: 0.1433165
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2123571 Vali Loss: 0.0889134 Test Loss: 0.1424889
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2116852 Vali Loss: 0.0874269 Test Loss: 0.1424803
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2119544 Vali Loss: 0.0891221 Test Loss: 0.1429622
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2120218 Vali Loss: 0.0869650 Test Loss: 0.1417813
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2117611 Vali Loss: 0.0880920 Test Loss: 0.1425936
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2111640 Vali Loss: 0.0912270 Test Loss: 0.1455072
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2117843 Vali Loss: 0.0878513 Test Loss: 0.1416448
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2116782 Vali Loss: 0.0883331 Test Loss: 0.1431708
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_APPLE_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.00020288318046368659, mae:0.010402326472103596, rmse:0.014243707060813904, r2:-0.01470637321472168, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0104, RMSE: 0.0142, RÂ²: -0.0147, MAPE: 499258.72%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.468 MB of 0.468 MB uploadedwandb: \ 0.468 MB of 0.468 MB uploadedwandb: | 0.468 MB of 0.468 MB uploadedwandb: / 0.468 MB of 0.468 MB uploadedwandb: - 0.468 MB of 0.468 MB uploadedwandb: \ 0.468 MB of 0.468 MB uploadedwandb: | 0.650 MB of 0.949 MB uploaded (0.002 MB deduped)wandb: / 0.650 MB of 0.949 MB uploaded (0.002 MB deduped)wandb: - 0.949 MB of 0.949 MB uploaded (0.002 MB deduped)wandb: \ 0.949 MB of 0.949 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–„â–ƒâ–ƒâ–‡â–†â–…â–…â–…â–„â–…â–ˆâ–„â–†
wandb:                 train/loss â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–â–â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–ƒâ–ƒâ–â–‡â–ˆâ–„â–‚â–„â–‚â–ƒâ–†â–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.14317
wandb:                 train/loss 0.21168
wandb:   val/directional_accuracy 50.42194
wandb:                   val/loss 0.08833
wandb:                    val/mae 0.0104
wandb:                   val/mape 49925871.875
wandb:                    val/mse 0.0002
wandb:                     val/r2 -0.01471
wandb:                   val/rmse 0.01424
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/gornikpm
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_093023-gornikpm/logs
Completed: APPLE H=3

Training: Informer on APPLE for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_093245-m3sgfnjy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/m3sgfnjy
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_APPLE_H5   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/m3sgfnjy
>>>>>>>start training : long_term_forecast_Informer_APPLE_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
Overriding target from 'OT' to 'close' for stock data
val 235
Overriding target from 'OT' to 'close' for stock data
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.2903933 Vali Loss: 0.1084273 Test Loss: 0.1675299
Validation loss decreased (inf --> 0.108427).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2383905 Vali Loss: 0.0916691 Test Loss: 0.1542204
Validation loss decreased (0.108427 --> 0.091669).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2903932877501151, 'val/loss': 0.10842730477452278, 'test/loss': 0.1675299108028412, '_timestamp': 1762327978.317476}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2383905185344524, 'val/loss': 0.09166914131492376, 'test/loss': 0.15422035567462444, '_timestamp': 1762327985.3457997}).
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2903932877501151, 'val/loss': 0.10842730477452278, 'test/loss': 0.1675299108028412, '_timestamp': 1762327978.317476}).
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
Epoch: 3, Steps: 133 | Train Loss: 0.2250918 Vali Loss: 0.0928962 Test Loss: 0.1544838
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2189587 Vali Loss: 0.0937206 Test Loss: 0.1642506
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2164054 Vali Loss: 0.0984868 Test Loss: 0.1655766
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2145358 Vali Loss: 0.0876462 Test Loss: 0.1513746
Validation loss decreased (0.091669 --> 0.087646).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2144432 Vali Loss: 0.0950121 Test Loss: 0.1600907
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2137066 Vali Loss: 0.0935856 Test Loss: 0.1574588
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2132194 Vali Loss: 0.0957437 Test Loss: 0.1586930
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2139853 Vali Loss: 0.0928500 Test Loss: 0.1577542
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2132375 Vali Loss: 0.0941314 Test Loss: 0.1595789
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2127209 Vali Loss: 0.0920608 Test Loss: 0.1571811
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2135704 Vali Loss: 0.0911582 Test Loss: 0.1564486
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2133769 Vali Loss: 0.0908069 Test Loss: 0.1554767
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2134628 Vali Loss: 0.0951295 Test Loss: 0.1583086
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2137835 Vali Loss: 0.0948517 Test Loss: 0.1577714
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_APPLE_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.0002029397146543488, mae:0.010331438854336739, rmse:0.014245690777897835, r2:-0.01035618782043457, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0103, RMSE: 0.0142, RÂ²: -0.0104, MAPE: 681029.62%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.494 MB of 0.494 MB uploadedwandb: \ 0.494 MB of 0.494 MB uploadedwandb: | 0.494 MB of 0.494 MB uploadedwandb: / 0.494 MB of 0.494 MB uploadedwandb: - 0.494 MB of 0.793 MB uploadedwandb: \ 0.793 MB of 0.793 MB uploadedwandb: | 0.793 MB of 0.793 MB uploadedwandb: / 0.793 MB of 0.793 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ƒâ–‡â–ˆâ–â–…â–„â–…â–„â–…â–„â–„â–ƒâ–„â–„
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–…â–ˆâ–â–†â–…â–†â–„â–…â–„â–ƒâ–ƒâ–†â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.15777
wandb:                 train/loss 0.21378
wandb:   val/directional_accuracy 49.25532
wandb:                   val/loss 0.09485
wandb:                    val/mae 0.01033
wandb:                   val/mape 68102962.5
wandb:                    val/mse 0.0002
wandb:                     val/r2 -0.01036
wandb:                   val/rmse 0.01425
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/m3sgfnjy
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_093245-m3sgfnjy/logs
Completed: APPLE H=5

Training: Informer on APPLE for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_093503-pqw241z3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/pqw241z3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_APPLE_H10  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/pqw241z3
>>>>>>>start training : long_term_forecast_Informer_APPLE_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
Overriding target from 'OT' to 'close' for stock data
val 230
Overriding target from 'OT' to 'close' for stock data
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2938560 Vali Loss: 0.1558994 Test Loss: 0.2002912
Validation loss decreased (inf --> 0.155899).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2393220 Vali Loss: 0.0977976 Test Loss: 0.1640651
Validation loss decreased (0.155899 --> 0.097798).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29385601320212945, 'val/loss': 0.155899440869689, 'test/loss': 0.20029121078550816, '_timestamp': 1762328116.7123165}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23932199224941714, 'val/loss': 0.09779764246195555, 'test/loss': 0.16406513191759586, '_timestamp': 1762328123.7611444}).
Epoch: 3, Steps: 133 | Train Loss: 0.2274409 Vali Loss: 0.1026492 Test Loss: 0.1721406
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2207400 Vali Loss: 0.1257407 Test Loss: 0.1975660
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2180779 Vali Loss: 0.1075726 Test Loss: 0.1740623
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2188867 Vali Loss: 0.1150668 Test Loss: 0.1868644
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2161975 Vali Loss: 0.1126501 Test Loss: 0.1821052
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2154781 Vali Loss: 0.1114156 Test Loss: 0.1816169
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2154626 Vali Loss: 0.1096651 Test Loss: 0.1811897
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2155977 Vali Loss: 0.1075907 Test Loss: 0.1793057
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2153665 Vali Loss: 0.1120379 Test Loss: 0.1799906
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2151291 Vali Loss: 0.1105797 Test Loss: 0.1780780
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_APPLE_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.0002132332738256082, mae:0.010758612304925919, rmse:0.014602509327232838, r2:-0.0531768798828125, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0108, RMSE: 0.0146, RÂ²: -0.0532, MAPE: 678354.12%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.581 MB of 0.582 MB uploadedwandb: \ 0.581 MB of 0.582 MB uploadedwandb: | 0.581 MB of 0.582 MB uploadedwandb: / 0.582 MB of 0.582 MB uploadedwandb: - 0.582 MB of 0.880 MB uploadedwandb: \ 0.876 MB of 0.880 MB uploadedwandb: | 0.880 MB of 0.880 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–‚â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–„â–ƒâ–ƒâ–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ˆâ–‚â–…â–„â–„â–ƒâ–‚â–„â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.17808
wandb:                 train/loss 0.21513
wandb:   val/directional_accuracy 49.80676
wandb:                   val/loss 0.11058
wandb:                    val/mae 0.01076
wandb:                   val/mape 67835412.5
wandb:                    val/mse 0.00021
wandb:                     val/r2 -0.05318
wandb:                   val/rmse 0.0146
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/pqw241z3
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_093503-pqw241z3/logs
Completed: APPLE H=10

Training: Informer on APPLE for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_093655-uee1mrkc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/uee1mrkc
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_APPLE_H22  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/uee1mrkc
>>>>>>>start training : long_term_forecast_Informer_APPLE_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
Overriding target from 'OT' to 'close' for stock data
val 218
Overriding target from 'OT' to 'close' for stock data
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2958149 Vali Loss: 0.1378669 Test Loss: 0.2068474
Validation loss decreased (inf --> 0.137867).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2415595 Vali Loss: 0.1423573 Test Loss: 0.2226387
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29581486162814224, 'val/loss': 0.1378668567964009, 'test/loss': 0.2068473505122321, '_timestamp': 1762328227.9892836}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24155954124801088, 'val/loss': 0.14235726850373404, 'test/loss': 0.2226387390068599, '_timestamp': 1762328235.0344887}).
Epoch: 3, Steps: 132 | Train Loss: 0.2301599 Vali Loss: 0.1123131 Test Loss: 0.1978163
Validation loss decreased (0.137867 --> 0.112313).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2241736 Vali Loss: 0.1063255 Test Loss: 0.1883966
Validation loss decreased (0.112313 --> 0.106325).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2208580 Vali Loss: 0.1157942 Test Loss: 0.1970564
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2196220 Vali Loss: 0.1197859 Test Loss: 0.2052569
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2183702 Vali Loss: 0.1158305 Test Loss: 0.2009054
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2180467 Vali Loss: 0.1211255 Test Loss: 0.2060044
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2179469 Vali Loss: 0.1201018 Test Loss: 0.2051134
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2177121 Vali Loss: 0.1185022 Test Loss: 0.2030965
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2180267 Vali Loss: 0.1185959 Test Loss: 0.2034291
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2175584 Vali Loss: 0.1202709 Test Loss: 0.2047205
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2177948 Vali Loss: 0.1186492 Test Loss: 0.2040075
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2173992 Vali Loss: 0.1190190 Test Loss: 0.2031931
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_APPLE_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.00020903412951156497, mae:0.010364471934735775, rmse:0.014458012767136097, r2:-0.007371664047241211, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0104, RMSE: 0.0145, RÂ²: -0.0074, MAPE: 629271.69%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.635 MB of 0.636 MB uploadedwandb: \ 0.635 MB of 0.636 MB uploadedwandb: | 0.636 MB of 0.636 MB uploadedwandb: / 0.636 MB of 0.935 MB uploadedwandb: - 0.935 MB of 0.935 MB uploadedwandb: \ 0.935 MB of 0.935 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–â–„â–ˆâ–†â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–â–…â–‡â–…â–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 13
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.20319
wandb:                 train/loss 0.2174
wandb:   val/directional_accuracy 48.99519
wandb:                   val/loss 0.11902
wandb:                    val/mae 0.01036
wandb:                   val/mape 62927168.75
wandb:                    val/mse 0.00021
wandb:                     val/r2 -0.00737
wandb:                   val/rmse 0.01446
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/uee1mrkc
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_093655-uee1mrkc/logs
Completed: APPLE H=22

Training: Informer on APPLE for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_093858-ybpclo8g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/ybpclo8g
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_APPLE_H50  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/ybpclo8g
>>>>>>>start training : long_term_forecast_Informer_APPLE_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
Overriding target from 'OT' to 'close' for stock data
val 190
Overriding target from 'OT' to 'close' for stock data
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.3044715 Vali Loss: 0.1639417 Test Loss: 0.2487009
Validation loss decreased (inf --> 0.163942).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2526483 Vali Loss: 0.1595994 Test Loss: 0.2555627
Validation loss decreased (0.163942 --> 0.159599).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3044714964926243, 'val/loss': 0.16394173602263132, 'test/loss': 0.24870085219542185, '_timestamp': 1762328351.2103279}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2526482650727937, 'val/loss': 0.15959940354029337, 'test/loss': 0.2555626903971036, '_timestamp': 1762328358.2901173}).
Epoch: 3, Steps: 132 | Train Loss: 0.2388640 Vali Loss: 0.2129456 Test Loss: 0.3329968
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2315003 Vali Loss: 0.1454553 Test Loss: 0.2615252
Validation loss decreased (0.159599 --> 0.145455).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2265529 Vali Loss: 0.1593222 Test Loss: 0.2770436
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2254117 Vali Loss: 0.1669842 Test Loss: 0.2908175
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2245231 Vali Loss: 0.1635249 Test Loss: 0.2835263
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2255937 Vali Loss: 0.1585879 Test Loss: 0.2827323
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2248238 Vali Loss: 0.1578921 Test Loss: 0.2821484
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2254520 Vali Loss: 0.1705517 Test Loss: 0.2940589
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2232827 Vali Loss: 0.1583966 Test Loss: 0.2801018
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2251645 Vali Loss: 0.1453830 Test Loss: 0.2636507
Validation loss decreased (0.145455 --> 0.145383).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2245256 Vali Loss: 0.1534472 Test Loss: 0.2752976
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2259858 Vali Loss: 0.1486436 Test Loss: 0.2674757
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2238897 Vali Loss: 0.1565980 Test Loss: 0.2761066
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2233156 Vali Loss: 0.1601983 Test Loss: 0.2809057
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2265053 Vali Loss: 0.1507136 Test Loss: 0.2708339
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2241362 Vali Loss: 0.1539360 Test Loss: 0.2769183
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2240319 Vali Loss: 0.1641543 Test Loss: 0.2868728
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2256992 Vali Loss: 0.1527796 Test Loss: 0.2765980
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2240981 Vali Loss: 0.1577535 Test Loss: 0.2784514
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2234209 Vali Loss: 0.1635222 Test Loss: 0.2859566
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_APPLE_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.0002235566935269162, mae:0.010754439979791641, rmse:0.014951812103390694, r2:-0.007579445838928223, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0108, RMSE: 0.0150, RÂ²: -0.0076, MAPE: 187271.55%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.678 MB of 0.680 MB uploadedwandb: \ 0.678 MB of 0.680 MB uploadedwandb: | 0.678 MB of 0.680 MB uploadedwandb: / 0.678 MB of 0.680 MB uploadedwandb: - 0.678 MB of 0.680 MB uploadedwandb: \ 0.680 MB of 0.680 MB uploadedwandb: | 0.680 MB of 0.980 MB uploadedwandb: / 0.980 MB of 0.980 MB uploadedwandb: - 0.980 MB of 0.980 MB uploadedwandb: \ 0.980 MB of 0.980 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–ƒâ–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–…â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–‚â–â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–‚â–ƒâ–ƒâ–‚â–‚â–„â–‚â–â–‚â–â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.28596
wandb:                 train/loss 0.22342
wandb:   val/directional_accuracy 49.82814
wandb:                   val/loss 0.16352
wandb:                    val/mae 0.01075
wandb:                   val/mape 18727154.6875
wandb:                    val/mse 0.00022
wandb:                     val/r2 -0.00758
wandb:                   val/rmse 0.01495
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/ybpclo8g
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_093858-ybpclo8g/logs
Completed: APPLE H=50

Training: Informer on APPLE for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_094206-vvo7so6i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/vvo7so6i
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_APPLE_H100 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/vvo7so6i
>>>>>>>start training : long_term_forecast_Informer_APPLE_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
Overriding target from 'OT' to 'close' for stock data
val 140
Overriding target from 'OT' to 'close' for stock data
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.3123877 Vali Loss: 0.1468366 Test Loss: 0.2402724
Validation loss decreased (inf --> 0.146837).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2573054 Vali Loss: 0.1603188 Test Loss: 0.2827405
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3123877445092568, 'val/loss': 0.14683664441108704, 'test/loss': 0.24027238488197328, '_timestamp': 1762328538.794135}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25730536729097364, 'val/loss': 0.16031875610351562, 'test/loss': 0.2827404886484146, '_timestamp': 1762328545.630971}).
Epoch: 3, Steps: 130 | Train Loss: 0.2420557 Vali Loss: 0.1702715 Test Loss: 0.3268284
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2342555 Vali Loss: 0.1914256 Test Loss: 0.3421255
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2304066 Vali Loss: 0.1779629 Test Loss: 0.3243062
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2286348 Vali Loss: 0.1646961 Test Loss: 0.3125268
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2282347 Vali Loss: 0.1738885 Test Loss: 0.3230384
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2271399 Vali Loss: 0.1708591 Test Loss: 0.3176633
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2268696 Vali Loss: 0.1736606 Test Loss: 0.3211459
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2273577 Vali Loss: 0.1716917 Test Loss: 0.3184152
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2269818 Vali Loss: 0.1721294 Test Loss: 0.3242616
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_APPLE_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0002402796526439488, mae:0.011113891378045082, rmse:0.01550095621496439, r2:-0.017652034759521484, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0111, RMSE: 0.0155, RÂ²: -0.0177, MAPE: 191316.69%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.700 MB of 0.705 MB uploadedwandb: \ 0.700 MB of 0.705 MB uploadedwandb: | 0.700 MB of 0.705 MB uploadedwandb: / 0.705 MB of 0.705 MB uploadedwandb: - 0.705 MB of 1.004 MB uploadedwandb: \ 0.705 MB of 1.004 MB uploadedwandb: | 1.004 MB of 1.004 MB uploadedwandb: / 1.004 MB of 1.004 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–„â–ˆâ–„â–â–ƒâ–‚â–ƒâ–‚â–„
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–ˆâ–„â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.32426
wandb:                 train/loss 0.22698
wandb:   val/directional_accuracy 50.39683
wandb:                   val/loss 0.17213
wandb:                    val/mae 0.01111
wandb:                   val/mape 19131668.75
wandb:                    val/mse 0.00024
wandb:                     val/r2 -0.01765
wandb:                   val/rmse 0.0155
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/vvo7so6i
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_094206-vvo7so6i/logs
Completed: APPLE H=100

Training: Informer on SP500 for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_094350-rp8a9ez4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/rp8a9ez4
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SP500_H3   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/rp8a9ez4
>>>>>>>start training : long_term_forecast_Informer_SP500_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 3
============================================================
train 4241
Overriding target from 'OT' to 'close' for stock data
val 237
Overriding target from 'OT' to 'close' for stock data
test 238
Epoch: 1, Steps: 133 | Train Loss: 0.2327026 Vali Loss: 0.0793389 Test Loss: 0.1063711
Validation loss decreased (inf --> 0.079339).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.1928656 Vali Loss: 0.0870278 Test Loss: 0.0900754
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23270255932234285, 'val/loss': 0.07933887466788292, 'test/loss': 0.10637106513604522, '_timestamp': 1762328643.1311896}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19286563208228663, 'val/loss': 0.08702779375016689, 'test/loss': 0.09007539227604866, '_timestamp': 1762328650.0610268}).
Epoch: 3, Steps: 133 | Train Loss: 0.1802906 Vali Loss: 0.0787380 Test Loss: 0.0812881
Validation loss decreased (0.079339 --> 0.078738).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1781112 Vali Loss: 0.0684139 Test Loss: 0.0925748
Validation loss decreased (0.078738 --> 0.068414).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1753137 Vali Loss: 0.0678479 Test Loss: 0.0926409
Validation loss decreased (0.068414 --> 0.067848).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1728491 Vali Loss: 0.0689091 Test Loss: 0.1015132
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1724153 Vali Loss: 0.0681911 Test Loss: 0.1001152
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1724322 Vali Loss: 0.0680315 Test Loss: 0.0928862
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1727966 Vali Loss: 0.0672874 Test Loss: 0.0974066
Validation loss decreased (0.067848 --> 0.067287).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1718402 Vali Loss: 0.0660761 Test Loss: 0.0973964
Validation loss decreased (0.067287 --> 0.066076).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1719961 Vali Loss: 0.0661661 Test Loss: 0.0953453
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1715414 Vali Loss: 0.0665227 Test Loss: 0.0927635
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1712939 Vali Loss: 0.0665126 Test Loss: 0.0918137
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1724121 Vali Loss: 0.0662666 Test Loss: 0.0953119
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1724898 Vali Loss: 0.0696129 Test Loss: 0.0904151
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1709320 Vali Loss: 0.0655282 Test Loss: 0.0931669
Validation loss decreased (0.066076 --> 0.065528).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1711419 Vali Loss: 0.0683908 Test Loss: 0.0931479
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1723677 Vali Loss: 0.0667372 Test Loss: 0.0916865
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1719557 Vali Loss: 0.0671513 Test Loss: 0.0963374
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1713229 Vali Loss: 0.0665662 Test Loss: 0.0949296
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1714656 Vali Loss: 0.0666626 Test Loss: 0.0938060
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1732265 Vali Loss: 0.0668490 Test Loss: 0.0934572
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1712300 Vali Loss: 0.0681921 Test Loss: 0.0948314
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.1709961 Vali Loss: 0.0661219 Test Loss: 0.0914817
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.1711836 Vali Loss: 0.0663758 Test Loss: 0.0917705
EarlyStopping counter: 9 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.1718974 Vali Loss: 0.0662575 Test Loss: 0.0923862
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SP500_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 238
test shape: (238, 3, 1) (238, 3, 1)
test shape: (238, 3, 1) (238, 3, 1)


	mse:6.778716488042846e-05, mae:0.006186097860336304, rmse:0.008233295753598213, r2:-0.04269886016845703, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0062, RMSE: 0.0082, RÂ²: -0.0427, MAPE: 2.43%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.493 MB of 0.493 MB uploadedwandb: \ 0.493 MB of 0.493 MB uploadedwandb: | 0.493 MB of 0.493 MB uploadedwandb: / 0.493 MB of 0.493 MB uploadedwandb: - 0.493 MB of 0.493 MB uploadedwandb: \ 0.493 MB of 0.493 MB uploadedwandb: | 0.493 MB of 0.493 MB uploadedwandb: / 0.493 MB of 0.493 MB uploadedwandb: - 0.675 MB of 0.976 MB uploaded (0.002 MB deduped)wandb: \ 0.675 MB of 0.976 MB uploaded (0.002 MB deduped)wandb: | 0.976 MB of 0.976 MB uploaded (0.002 MB deduped)wandb: / 0.976 MB of 0.976 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–…â–…â–ˆâ–ˆâ–…â–‡â–‡â–†â–…â–…â–†â–„â–…â–…â–…â–†â–†â–…â–…â–†â–…â–…â–…
wandb:                 train/loss â–ˆâ–†â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–â–â–‚â–‚â–â–â–ƒâ–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–â–â–‚â–‚â–â–ƒâ–â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 25
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.09239
wandb:                 train/loss 0.1719
wandb:   val/directional_accuracy 52.52101
wandb:                   val/loss 0.06626
wandb:                    val/mae 0.00619
wandb:                   val/mape 243.35685
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.0427
wandb:                   val/rmse 0.00823
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/rp8a9ez4
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_094350-rp8a9ez4/logs
Completed: SP500 H=3

Training: Informer on SP500 for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_094723-setahr95
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/setahr95
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SP500_H5   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/setahr95
>>>>>>>start training : long_term_forecast_Informer_SP500_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 5
============================================================
train 4239
Overriding target from 'OT' to 'close' for stock data
val 235
Overriding target from 'OT' to 'close' for stock data
test 236
Epoch: 1, Steps: 133 | Train Loss: 0.2364729 Vali Loss: 0.0944512 Test Loss: 0.1269119
Validation loss decreased (inf --> 0.094451).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.1901457 Vali Loss: 0.1097286 Test Loss: 0.0821484
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23647294434389673, 'val/loss': 0.09445123095065355, 'test/loss': 0.12691188789904118, '_timestamp': 1762328856.3841553}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19014573186860048, 'val/loss': 0.10972855612635612, 'test/loss': 0.0821483638137579, '_timestamp': 1762328863.8964999}).
Epoch: 3, Steps: 133 | Train Loss: 0.1833342 Vali Loss: 0.0685781 Test Loss: 0.0991838
Validation loss decreased (0.094451 --> 0.068578).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1813650 Vali Loss: 0.0722221 Test Loss: 0.1109976
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1781525 Vali Loss: 0.0690553 Test Loss: 0.0850305
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1769846 Vali Loss: 0.0687392 Test Loss: 0.0854651
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1763635 Vali Loss: 0.0706079 Test Loss: 0.0894955
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1761797 Vali Loss: 0.0683416 Test Loss: 0.0892072
Validation loss decreased (0.068578 --> 0.068342).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1766264 Vali Loss: 0.0694556 Test Loss: 0.0904678
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1769082 Vali Loss: 0.0668947 Test Loss: 0.0897658
Validation loss decreased (0.068342 --> 0.066895).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1765106 Vali Loss: 0.0698877 Test Loss: 0.0906585
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1759130 Vali Loss: 0.0678172 Test Loss: 0.0882047
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1763520 Vali Loss: 0.0686387 Test Loss: 0.0903763
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1771947 Vali Loss: 0.0677212 Test Loss: 0.0879537
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1768434 Vali Loss: 0.0689103 Test Loss: 0.0891021
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1764332 Vali Loss: 0.0683552 Test Loss: 0.0893513
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1765857 Vali Loss: 0.0666980 Test Loss: 0.0876825
Validation loss decreased (0.066895 --> 0.066698).  Saving model ...
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1790082 Vali Loss: 0.0692159 Test Loss: 0.0897060
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1761517 Vali Loss: 0.0682753 Test Loss: 0.0877908
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1764323 Vali Loss: 0.0672909 Test Loss: 0.0887847
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1765286 Vali Loss: 0.0676215 Test Loss: 0.0886273
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1766055 Vali Loss: 0.0692133 Test Loss: 0.0883044
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1761581 Vali Loss: 0.0678189 Test Loss: 0.0880108
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.1770506 Vali Loss: 0.0667964 Test Loss: 0.0924135
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.1764829 Vali Loss: 0.0690735 Test Loss: 0.0904377
EarlyStopping counter: 8 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.1763884 Vali Loss: 0.0674701 Test Loss: 0.0900337
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.1767145 Vali Loss: 0.0669422 Test Loss: 0.0898478
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SP500_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 236
test shape: (236, 5, 1) (236, 5, 1)
test shape: (236, 5, 1) (236, 5, 1)


	mse:6.722804391756654e-05, mae:0.0061162482015788555, rmse:0.008199270814657211, r2:-0.03489387035369873, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0082, RÂ²: -0.0349, MAPE: 1.66%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.523 MB of 0.523 MB uploadedwandb: \ 0.523 MB of 0.523 MB uploadedwandb: | 0.523 MB of 0.523 MB uploadedwandb: / 0.523 MB of 0.824 MB uploadedwandb: - 0.523 MB of 0.824 MB uploadedwandb: \ 0.824 MB of 0.824 MB uploadedwandb: | 0.824 MB of 0.824 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–ˆâ–â–â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–†â–ƒâ–‚â–â–â–‚â–‚â–‚â–â–â–‚â–‚â–â–‚â–„â–â–â–‚â–‚â–â–‚â–‚â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–ˆâ–„â–„â–†â–ƒâ–„â–â–…â–‚â–ƒâ–‚â–„â–ƒâ–â–„â–ƒâ–‚â–‚â–„â–‚â–â–„â–‚â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 26
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.08985
wandb:                 train/loss 0.17671
wandb:   val/directional_accuracy 48.51695
wandb:                   val/loss 0.06694
wandb:                    val/mae 0.00612
wandb:                   val/mape 166.25348
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.03489
wandb:                   val/rmse 0.0082
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/setahr95
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_094723-setahr95/logs
Completed: SP500 H=5

Training: Informer on SP500 for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_095053-855f8zx7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/855f8zx7
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SP500_H10  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/855f8zx7
>>>>>>>start training : long_term_forecast_Informer_SP500_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 10
============================================================
train 4234
Overriding target from 'OT' to 'close' for stock data
val 230
Overriding target from 'OT' to 'close' for stock data
test 231
Epoch: 1, Steps: 133 | Train Loss: 0.2345299 Vali Loss: 0.0880020 Test Loss: 0.0991801
Validation loss decreased (inf --> 0.088002).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.1890880 Vali Loss: 0.0739570 Test Loss: 0.0951309
Validation loss decreased (0.088002 --> 0.073957).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23452986732013242, 'val/loss': 0.08800197392702103, 'test/loss': 0.09918005391955376, '_timestamp': 1762329065.5329466}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.18908804481414923, 'val/loss': 0.07395704556256533, 'test/loss': 0.09513094229623675, '_timestamp': 1762329072.580986}).
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23452986732013242, 'val/loss': 0.08800197392702103, 'test/loss': 0.09918005391955376, '_timestamp': 1762329065.5329466}).
Epoch: 3, Steps: 133 | Train Loss: 0.1824829 Vali Loss: 0.0691397 Test Loss: 0.1073999
Validation loss decreased (0.073957 --> 0.069140).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1790985 Vali Loss: 0.0677033 Test Loss: 0.1114915
Validation loss decreased (0.069140 --> 0.067703).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1774705 Vali Loss: 0.0663873 Test Loss: 0.1021970
Validation loss decreased (0.067703 --> 0.066387).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1767566 Vali Loss: 0.0670230 Test Loss: 0.1041008
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1767348 Vali Loss: 0.0646873 Test Loss: 0.1064259
Validation loss decreased (0.066387 --> 0.064687).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1762180 Vali Loss: 0.0681104 Test Loss: 0.0993293
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1765180 Vali Loss: 0.0682836 Test Loss: 0.0990435
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1762416 Vali Loss: 0.0671853 Test Loss: 0.0989766
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1761855 Vali Loss: 0.0664010 Test Loss: 0.1020648
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1762214 Vali Loss: 0.0663471 Test Loss: 0.0974356
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1764977 Vali Loss: 0.0679509 Test Loss: 0.0995088
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1752265 Vali Loss: 0.0681499 Test Loss: 0.1017904
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1779350 Vali Loss: 0.0664360 Test Loss: 0.0981779
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1764036 Vali Loss: 0.0690095 Test Loss: 0.0982643
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1766759 Vali Loss: 0.0688984 Test Loss: 0.1024936
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SP500_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 231
test shape: (231, 10, 1) (231, 10, 1)
test shape: (231, 10, 1) (231, 10, 1)


	mse:6.647268310189247e-05, mae:0.006060549523681402, rmse:0.008153078146278858, r2:-0.022612690925598145, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0082, RÂ²: -0.0226, MAPE: 1.48%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.534 MB of 0.535 MB uploadedwandb: \ 0.535 MB of 0.535 MB uploadedwandb: | 0.535 MB of 0.834 MB uploadedwandb: / 0.535 MB of 0.834 MB uploadedwandb: - 0.834 MB of 0.834 MB uploadedwandb: \ 0.834 MB of 0.834 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–†â–ˆâ–ƒâ–„â–…â–‚â–‚â–‚â–ƒâ–â–‚â–ƒâ–â–â–„
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–„â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–„â–…â–â–†â–‡â–…â–„â–„â–†â–†â–„â–ˆâ–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.10249
wandb:                 train/loss 0.17668
wandb:   val/directional_accuracy 48.29245
wandb:                   val/loss 0.0689
wandb:                    val/mae 0.00606
wandb:                   val/mape 148.14092
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.02261
wandb:                   val/rmse 0.00815
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/855f8zx7
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_095053-855f8zx7/logs
Completed: SP500 H=10

Training: Informer on SP500 for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_095319-k1mpclq3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/k1mpclq3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SP500_H22  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/k1mpclq3
>>>>>>>start training : long_term_forecast_Informer_SP500_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 22
============================================================
train 4222
Overriding target from 'OT' to 'close' for stock data
val 218
Overriding target from 'OT' to 'close' for stock data
test 219
Epoch: 1, Steps: 132 | Train Loss: 0.2366720 Vali Loss: 0.0755214 Test Loss: 0.1389793
Validation loss decreased (inf --> 0.075521).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.1902805 Vali Loss: 0.0760245 Test Loss: 0.0834075
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23667204729986913, 'val/loss': 0.07552139461040497, 'test/loss': 0.13897925083126342, '_timestamp': 1762329213.3089457}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19028054183405457, 'val/loss': 0.0760245259319033, 'test/loss': 0.08340750208922795, '_timestamp': 1762329220.3258026}).
Epoch: 3, Steps: 132 | Train Loss: 0.1838767 Vali Loss: 0.0732109 Test Loss: 0.0871006
Validation loss decreased (0.075521 --> 0.073211).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.1813198 Vali Loss: 0.0728739 Test Loss: 0.0900322
Validation loss decreased (0.073211 --> 0.072874).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.1798789 Vali Loss: 0.0711875 Test Loss: 0.0882660
Validation loss decreased (0.072874 --> 0.071188).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.1790808 Vali Loss: 0.0695452 Test Loss: 0.0910883
Validation loss decreased (0.071188 --> 0.069545).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.1789675 Vali Loss: 0.0706786 Test Loss: 0.0893677
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.1788121 Vali Loss: 0.0691080 Test Loss: 0.0958066
Validation loss decreased (0.069545 --> 0.069108).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.1787085 Vali Loss: 0.0687035 Test Loss: 0.0972134
Validation loss decreased (0.069108 --> 0.068704).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.1785474 Vali Loss: 0.0689396 Test Loss: 0.0950130
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.1783638 Vali Loss: 0.0688241 Test Loss: 0.0971894
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.1785715 Vali Loss: 0.0686037 Test Loss: 0.0968575
Validation loss decreased (0.068704 --> 0.068604).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.1784812 Vali Loss: 0.0689220 Test Loss: 0.0972775
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.1786114 Vali Loss: 0.0686498 Test Loss: 0.0971584
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.1784434 Vali Loss: 0.0686995 Test Loss: 0.0983894
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.1782212 Vali Loss: 0.0688526 Test Loss: 0.0969939
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.1786028 Vali Loss: 0.0691902 Test Loss: 0.0946112
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.1785753 Vali Loss: 0.0689920 Test Loss: 0.0962599
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.1785759 Vali Loss: 0.0688845 Test Loss: 0.0970184
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.1785275 Vali Loss: 0.0688216 Test Loss: 0.0964242
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.1785204 Vali Loss: 0.0688329 Test Loss: 0.0972900
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.1787097 Vali Loss: 0.0686864 Test Loss: 0.0988114
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SP500_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 219
test shape: (219, 22, 1) (219, 22, 1)
test shape: (219, 22, 1) (219, 22, 1)


	mse:6.563603528775275e-05, mae:0.00604445394128561, rmse:0.008101606741547585, r2:-0.02804577350616455, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0060, RMSE: 0.0081, RÂ²: -0.0280, MAPE: 1.53%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.608 MB of 0.609 MB uploadedwandb: \ 0.608 MB of 0.609 MB uploadedwandb: | 0.608 MB of 0.609 MB uploadedwandb: / 0.609 MB of 0.609 MB uploadedwandb: - 0.609 MB of 0.910 MB uploadedwandb: \ 0.910 MB of 0.910 MB uploadedwandb: | 0.910 MB of 0.910 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ƒâ–‚â–ƒâ–‚â–†â–‡â–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–…â–†â–‡â–‡â–‡â–ˆ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‡â–…â–‚â–„â–‚â–â–‚â–â–â–â–â–â–â–‚â–‚â–â–â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.09881
wandb:                 train/loss 0.17871
wandb:   val/directional_accuracy 50.59796
wandb:                   val/loss 0.06869
wandb:                    val/mae 0.00604
wandb:                   val/mape 153.49462
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.02805
wandb:                   val/rmse 0.0081
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/k1mpclq3
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_095319-k1mpclq3/logs
Completed: SP500 H=22

Training: Informer on SP500 for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_095619-hx8q052v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/hx8q052v
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SP500_H50  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/hx8q052v
>>>>>>>start training : long_term_forecast_Informer_SP500_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 50
============================================================
train 4194
Overriding target from 'OT' to 'close' for stock data
val 190
Overriding target from 'OT' to 'close' for stock data
test 191
Epoch: 1, Steps: 132 | Train Loss: 0.2440268 Vali Loss: 0.0843844 Test Loss: 0.1067435
Validation loss decreased (inf --> 0.084384).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.1944456 Vali Loss: 0.0823859 Test Loss: 0.1824915
Validation loss decreased (0.084384 --> 0.082386).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.24402680377842803, 'val/loss': 0.08438443019986153, 'test/loss': 0.10674345244963963, '_timestamp': 1762329392.7212818}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.1944455663588914, 'val/loss': 0.08238586535056432, 'test/loss': 0.18249151731530824, '_timestamp': 1762329399.695749}).
Epoch: 3, Steps: 132 | Train Loss: 0.1889463 Vali Loss: 0.0722366 Test Loss: 0.1080429
Validation loss decreased (0.082386 --> 0.072237).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.1842707 Vali Loss: 0.0702234 Test Loss: 0.1131524
Validation loss decreased (0.072237 --> 0.070223).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.1819808 Vali Loss: 0.0710338 Test Loss: 0.1135405
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.1827781 Vali Loss: 0.0696363 Test Loss: 0.1121271
Validation loss decreased (0.070223 --> 0.069636).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.1837990 Vali Loss: 0.0697055 Test Loss: 0.1137700
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.1817100 Vali Loss: 0.0697459 Test Loss: 0.1219677
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.1825403 Vali Loss: 0.0701028 Test Loss: 0.1183644
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.1817006 Vali Loss: 0.0697973 Test Loss: 0.1147237
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.1844963 Vali Loss: 0.0695085 Test Loss: 0.1170355
Validation loss decreased (0.069636 --> 0.069508).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.1809059 Vali Loss: 0.0699519 Test Loss: 0.1142476
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.1816620 Vali Loss: 0.0699386 Test Loss: 0.1144830
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.1807122 Vali Loss: 0.0698713 Test Loss: 0.1139771
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.1811694 Vali Loss: 0.0695398 Test Loss: 0.1171021
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.1812244 Vali Loss: 0.0698132 Test Loss: 0.1145959
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.1809441 Vali Loss: 0.0701052 Test Loss: 0.1133712
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.1817507 Vali Loss: 0.0700668 Test Loss: 0.1165173
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.1877922 Vali Loss: 0.0697699 Test Loss: 0.1136577
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.1896178 Vali Loss: 0.0695140 Test Loss: 0.1160125
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.1815235 Vali Loss: 0.0697195 Test Loss: 0.1151917
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SP500_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 191
test shape: (191, 50, 1) (191, 50, 1)
test shape: (191, 50, 1) (191, 50, 1)


	mse:6.616925384150818e-05, mae:0.006062474101781845, rmse:0.008134448900818825, r2:-0.017479300498962402, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0081, RÂ²: -0.0175, MAPE: 1.35%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.676 MB of 0.678 MB uploadedwandb: \ 0.676 MB of 0.678 MB uploadedwandb: | 0.678 MB of 0.678 MB uploadedwandb: / 0.678 MB of 0.978 MB uploadedwandb: - 0.678 MB of 0.978 MB uploadedwandb: \ 0.978 MB of 0.978 MB uploadedwandb: | 0.978 MB of 0.978 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–„â–„â–ƒâ–„â–ˆâ–†â–„â–†â–„â–„â–„â–†â–„â–„â–…â–„â–…â–…
wandb:                 train/loss â–‡â–„â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–„â–â–‚â–â–â–â–â–‚â–‡â–ˆâ–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–…â–â–‚â–‚â–ƒâ–‚â–â–‚â–‚â–‚â–â–‚â–ƒâ–‚â–‚â–â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.11519
wandb:                 train/loss 0.18152
wandb:   val/directional_accuracy 49.36425
wandb:                   val/loss 0.06972
wandb:                    val/mae 0.00606
wandb:                   val/mape 134.54211
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.01748
wandb:                   val/rmse 0.00813
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/hx8q052v
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_095619-hx8q052v/logs
Completed: SP500 H=50

Training: Informer on SP500 for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_095909-3stmh0tw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/3stmh0tw
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SP500_H100 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/3stmh0tw
>>>>>>>start training : long_term_forecast_Informer_SP500_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 100
============================================================
train 4144
Overriding target from 'OT' to 'close' for stock data
val 140
Overriding target from 'OT' to 'close' for stock data
test 141
Epoch: 1, Steps: 130 | Train Loss: 0.2515850 Vali Loss: 0.0759513 Test Loss: 0.2098040
Validation loss decreased (inf --> 0.075951).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2017786 Vali Loss: 0.0709716 Test Loss: 0.1798622
Validation loss decreased (0.075951 --> 0.070972).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.25158501084034257, 'val/loss': 0.07595128864049912, 'test/loss': 0.20980403423309327, '_timestamp': 1762329563.2321088}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.20177863750320216, 'val/loss': 0.07097160220146179, 'test/loss': 0.17986224740743637, '_timestamp': 1762329570.0756526}).
Epoch: 3, Steps: 130 | Train Loss: 0.1898974 Vali Loss: 0.0669311 Test Loss: 0.1656674
Validation loss decreased (0.070972 --> 0.066931).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.1857751 Vali Loss: 0.0698870 Test Loss: 0.1906624
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.1839737 Vali Loss: 0.0651253 Test Loss: 0.1780373
Validation loss decreased (0.066931 --> 0.065125).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.1830594 Vali Loss: 0.0666102 Test Loss: 0.1680609
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.1829860 Vali Loss: 0.0659746 Test Loss: 0.1655901
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.1832125 Vali Loss: 0.0654839 Test Loss: 0.1674834
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.1822623 Vali Loss: 0.0660068 Test Loss: 0.1715207
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.1824187 Vali Loss: 0.0661970 Test Loss: 0.1668413
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.1826789 Vali Loss: 0.0660948 Test Loss: 0.1644783
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.1822550 Vali Loss: 0.0654407 Test Loss: 0.1665402
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.1823958 Vali Loss: 0.0661259 Test Loss: 0.1674731
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.1820707 Vali Loss: 0.0662409 Test Loss: 0.1686863
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.1827592 Vali Loss: 0.0653476 Test Loss: 0.1649369
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SP500_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 141
test shape: (141, 100, 1) (141, 100, 1)
test shape: (141, 100, 1) (141, 100, 1)


	mse:7.000836922088638e-05, mae:0.006225295830518007, rmse:0.008367100730538368, r2:-0.021895527839660645, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0062, RMSE: 0.0084, RÂ²: -0.0219, MAPE: 1.54%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.766 MB of 0.771 MB uploadedwandb: \ 0.766 MB of 0.771 MB uploadedwandb: | 0.771 MB of 0.771 MB uploadedwandb: / 0.771 MB of 1.070 MB uploadedwandb: - 1.070 MB of 1.070 MB uploadedwandb: \ 1.070 MB of 1.070 MB uploadedwandb: | 1.070 MB of 1.070 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–…â–‚â–â–‚â–ƒâ–‚â–â–‚â–‚â–‚â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–‚â–â–â–‚â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–ˆâ–â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–â–‚â–ƒâ–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.16494
wandb:                 train/loss 0.18276
wandb:   val/directional_accuracy 49.27287
wandb:                   val/loss 0.06535
wandb:                    val/mae 0.00623
wandb:                   val/mape 153.75435
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.0219
wandb:                   val/rmse 0.00837
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/3stmh0tw
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_095909-3stmh0tw/logs
Completed: SP500 H=100

Training: Informer on NASDAQ for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_100125-0i20vvtm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/0i20vvtm
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NASDAQ_H3  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/0i20vvtm
>>>>>>>start training : long_term_forecast_Informer_NASDAQ_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
Overriding target from 'OT' to 'close' for stock data
val 237
Overriding target from 'OT' to 'close' for stock data
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.3077824 Vali Loss: 0.1949568 Test Loss: 0.1632546
Validation loss decreased (inf --> 0.194957).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2525309 Vali Loss: 0.1630140 Test Loss: 0.1533888
Validation loss decreased (0.194957 --> 0.163014).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3077824344312338, 'val/loss': 0.1949568185955286, 'test/loss': 0.163254558108747, '_timestamp': 1762329698.1115565}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25253085971326755, 'val/loss': 0.16301395557820797, 'test/loss': 0.15338875260204077, '_timestamp': 1762329705.0904367}).
Epoch: 3, Steps: 133 | Train Loss: 0.2370147 Vali Loss: 0.1388907 Test Loss: 0.1293801
Validation loss decreased (0.163014 --> 0.138891).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2316591 Vali Loss: 0.1432344 Test Loss: 0.1246746
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2290655 Vali Loss: 0.1482440 Test Loss: 0.1397814
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2279051 Vali Loss: 0.1498205 Test Loss: 0.1271718
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2267194 Vali Loss: 0.1450317 Test Loss: 0.1268191
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2258625 Vali Loss: 0.1424349 Test Loss: 0.1295570
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2257222 Vali Loss: 0.1406495 Test Loss: 0.1263031
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2257198 Vali Loss: 0.1417582 Test Loss: 0.1245300
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2259059 Vali Loss: 0.1415352 Test Loss: 0.1221705
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2252285 Vali Loss: 0.1384603 Test Loss: 0.1244343
Validation loss decreased (0.138891 --> 0.138460).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2260524 Vali Loss: 0.1428129 Test Loss: 0.1260301
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2254197 Vali Loss: 0.1439871 Test Loss: 0.1256686
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2256599 Vali Loss: 0.1377016 Test Loss: 0.1238730
Validation loss decreased (0.138460 --> 0.137702).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2255107 Vali Loss: 0.1420474 Test Loss: 0.1244701
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2259803 Vali Loss: 0.1423418 Test Loss: 0.1275398
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2254234 Vali Loss: 0.1383225 Test Loss: 0.1236124
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2250733 Vali Loss: 0.1406966 Test Loss: 0.1265213
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2256636 Vali Loss: 0.1376946 Test Loss: 0.1234381
Validation loss decreased (0.137702 --> 0.137695).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2248285 Vali Loss: 0.1442507 Test Loss: 0.1273238
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2252313 Vali Loss: 0.1359382 Test Loss: 0.1248804
Validation loss decreased (0.137695 --> 0.135938).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2247208 Vali Loss: 0.1391135 Test Loss: 0.1238171
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2254337 Vali Loss: 0.1387092 Test Loss: 0.1263602
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2250465 Vali Loss: 0.1434885 Test Loss: 0.1259993
EarlyStopping counter: 3 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2248393 Vali Loss: 0.1386479 Test Loss: 0.1259343
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.2249180 Vali Loss: 0.1427590 Test Loss: 0.1245908
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.2246331 Vali Loss: 0.1442754 Test Loss: 0.1253768
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.2272378 Vali Loss: 0.1373529 Test Loss: 0.1219462
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 133 | Train Loss: 0.2249871 Vali Loss: 0.1366049 Test Loss: 0.1233524
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 133 | Train Loss: 0.2252823 Vali Loss: 0.1406245 Test Loss: 0.1264751
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 133 | Train Loss: 0.2249990 Vali Loss: 0.1391937 Test Loss: 0.1261746
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NASDAQ_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.0001416398154105991, mae:0.008692719042301178, rmse:0.011901252903044224, r2:-0.040636539459228516, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0087, RMSE: 0.0119, RÂ²: -0.0406, MAPE: 1002728.69%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.490 MB of 0.490 MB uploadedwandb: \ 0.490 MB of 0.490 MB uploadedwandb: | 0.490 MB of 0.490 MB uploadedwandb: / 0.490 MB of 0.490 MB uploadedwandb: - 0.490 MB of 0.490 MB uploadedwandb: \ 0.490 MB of 0.490 MB uploadedwandb: | 0.490 MB of 0.490 MB uploadedwandb: / 0.672 MB of 0.973 MB uploaded (0.002 MB deduped)wandb: - 0.973 MB of 0.973 MB uploaded (0.002 MB deduped)wandb: \ 0.973 MB of 0.973 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–„â–‚â–ˆâ–ƒâ–ƒâ–„â–ƒâ–‚â–â–‚â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–â–‚â–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–…â–‡â–ˆâ–†â–„â–ƒâ–„â–„â–‚â–„â–…â–‚â–„â–„â–‚â–ƒâ–‚â–…â–â–ƒâ–‚â–…â–‚â–„â–…â–‚â–â–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 31
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.12617
wandb:                 train/loss 0.225
wandb:   val/directional_accuracy 48.10127
wandb:                   val/loss 0.13919
wandb:                    val/mae 0.00869
wandb:                   val/mape 100272868.75
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.04064
wandb:                   val/rmse 0.0119
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/0i20vvtm
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_100125-0i20vvtm/logs
Completed: NASDAQ H=3

Training: Informer on NASDAQ for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_100530-zi9t16gi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/zi9t16gi
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NASDAQ_H5  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/zi9t16gi
>>>>>>>start training : long_term_forecast_Informer_NASDAQ_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
Overriding target from 'OT' to 'close' for stock data
val 235
Overriding target from 'OT' to 'close' for stock data
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.3106390 Vali Loss: 0.1691474 Test Loss: 0.1469517
Validation loss decreased (inf --> 0.169147).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2555142 Vali Loss: 0.1588576 Test Loss: 0.1566088
Validation loss decreased (0.169147 --> 0.158858).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3106390273660645, 'val/loss': 0.1691474039107561, 'test/loss': 0.14695168565958738, '_timestamp': 1762329943.3243518}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2555141907213326, 'val/loss': 0.15885763429105282, 'test/loss': 0.15660879760980606, '_timestamp': 1762329950.2859576}).
Epoch: 3, Steps: 133 | Train Loss: 0.2429779 Vali Loss: 0.1594246 Test Loss: 0.1371341
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2364583 Vali Loss: 0.1524597 Test Loss: 0.1603189
Validation loss decreased (0.158858 --> 0.152460).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2337279 Vali Loss: 0.1414189 Test Loss: 0.1365056
Validation loss decreased (0.152460 --> 0.141419).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2316712 Vali Loss: 0.1444940 Test Loss: 0.1306106
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2312144 Vali Loss: 0.1424503 Test Loss: 0.1330829
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2305412 Vali Loss: 0.1448154 Test Loss: 0.1366060
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2301672 Vali Loss: 0.1475980 Test Loss: 0.1360298
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2316682 Vali Loss: 0.1504809 Test Loss: 0.1327700
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2312565 Vali Loss: 0.1411192 Test Loss: 0.1321662
Validation loss decreased (0.141419 --> 0.141119).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2304132 Vali Loss: 0.1460258 Test Loss: 0.1373718
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2308698 Vali Loss: 0.1443813 Test Loss: 0.1303615
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2304103 Vali Loss: 0.1562988 Test Loss: 0.1415566
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2306601 Vali Loss: 0.1417020 Test Loss: 0.1307008
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2295856 Vali Loss: 0.1441726 Test Loss: 0.1365857
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2302477 Vali Loss: 0.1457313 Test Loss: 0.1377757
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2304462 Vali Loss: 0.1462851 Test Loss: 0.1414279
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2300847 Vali Loss: 0.1425716 Test Loss: 0.1345757
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2310508 Vali Loss: 0.1406973 Test Loss: 0.1321793
Validation loss decreased (0.141119 --> 0.140697).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2311844 Vali Loss: 0.1420146 Test Loss: 0.1359862
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2307840 Vali Loss: 0.1513065 Test Loss: 0.1328012
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2302437 Vali Loss: 0.1560500 Test Loss: 0.1432664
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2298884 Vali Loss: 0.1400838 Test Loss: 0.1323250
Validation loss decreased (0.140697 --> 0.140084).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2309239 Vali Loss: 0.1421745 Test Loss: 0.1342134
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2304779 Vali Loss: 0.1484633 Test Loss: 0.1309566
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.2298562 Vali Loss: 0.1535262 Test Loss: 0.1367573
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.2302759 Vali Loss: 0.1519057 Test Loss: 0.1352344
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.2306605 Vali Loss: 0.1435933 Test Loss: 0.1361718
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 133 | Train Loss: 0.2295894 Vali Loss: 0.1409461 Test Loss: 0.1323746
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 133 | Train Loss: 0.2304472 Vali Loss: 0.1434947 Test Loss: 0.1358591
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 133 | Train Loss: 0.2314089 Vali Loss: 0.1386128 Test Loss: 0.1325813
Validation loss decreased (0.140084 --> 0.138613).  Saving model ...
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 133 | Train Loss: 0.2299489 Vali Loss: 0.1432758 Test Loss: 0.1356120
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 133 | Train Loss: 0.2306874 Vali Loss: 0.1526731 Test Loss: 0.1328238
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.1641532182693482e-14
Epoch: 35, Steps: 133 | Train Loss: 0.2310012 Vali Loss: 0.1473257 Test Loss: 0.1386862
EarlyStopping counter: 3 out of 10
Updating learning rate to 5.820766091346741e-15
Epoch: 36, Steps: 133 | Train Loss: 0.2312332 Vali Loss: 0.1545133 Test Loss: 0.1343130
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.9103830456733705e-15
Epoch: 37, Steps: 133 | Train Loss: 0.2308118 Vali Loss: 0.1419451 Test Loss: 0.1343791
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.4551915228366853e-15
Epoch: 38, Steps: 133 | Train Loss: 0.2301028 Vali Loss: 0.1449082 Test Loss: 0.1401267
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.275957614183426e-16
Epoch: 39, Steps: 133 | Train Loss: 0.2298746 Vali Loss: 0.1456002 Test Loss: 0.1352273
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.637978807091713e-16
Epoch: 40, Steps: 133 | Train Loss: 0.2302834 Vali Loss: 0.1416145 Test Loss: 0.1326743
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.8189894035458566e-16
Epoch: 41, Steps: 133 | Train Loss: 0.2305359 Vali Loss: 0.1477149 Test Loss: 0.1401437
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.094947017729283e-17
Epoch: 42, Steps: 133 | Train Loss: 0.2298382 Vali Loss: 0.1543447 Test Loss: 0.1352427
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NASDAQ_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.00014330382691696286, mae:0.008750160224735737, rmse:0.01197095774114132, r2:-0.04752027988433838, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0088, RMSE: 0.0120, RÂ²: -0.0475, MAPE: 2260662.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.526 MB of 0.527 MB uploadedwandb: \ 0.526 MB of 0.527 MB uploadedwandb: | 0.527 MB of 0.527 MB uploadedwandb: / 0.527 MB of 0.527 MB uploadedwandb: - 0.527 MB of 0.829 MB uploadedwandb: \ 0.829 MB of 0.829 MB uploadedwandb: | 0.829 MB of 0.829 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ƒâ–ˆâ–‚â–â–‚â–‚â–‚â–‚â–â–ƒâ–â–„â–â–‚â–ƒâ–„â–‚â–â–‚â–‚â–„â–â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–ƒâ–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–‚â–‚â–â–‚â–â–‚â–â–â–â–â–‚â–‚â–‚â–â–â–‚â–â–â–â–‚â–â–â–‚â–â–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–‚â–ƒâ–‚â–ƒâ–„â–…â–‚â–ƒâ–ƒâ–‡â–‚â–ƒâ–ƒâ–„â–‚â–‚â–‚â–…â–‡â–â–‚â–„â–†â–…â–ƒâ–‚â–ƒâ–â–ƒâ–†â–„â–†â–‚â–ƒâ–ƒâ–‚â–„â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 41
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.13524
wandb:                 train/loss 0.22984
wandb:   val/directional_accuracy 49.14894
wandb:                   val/loss 0.15434
wandb:                    val/mae 0.00875
wandb:                   val/mape 226066200.0
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.04752
wandb:                   val/rmse 0.01197
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/zi9t16gi
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_100530-zi9t16gi/logs
Completed: NASDAQ H=5

Training: Informer on NASDAQ for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_101042-h3ss4cuo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/h3ss4cuo
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NASDAQ_H10 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/h3ss4cuo
>>>>>>>start training : long_term_forecast_Informer_NASDAQ_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
Overriding target from 'OT' to 'close' for stock data
val 230
Overriding target from 'OT' to 'close' for stock data
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.3080606 Vali Loss: 0.1767334 Test Loss: 0.1619888
Validation loss decreased (inf --> 0.176733).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2575604 Vali Loss: 0.1537560 Test Loss: 0.1452658
Validation loss decreased (0.176733 --> 0.153756).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3080606099806334, 'val/loss': 0.17673338577151299, 'test/loss': 0.16198883671313524, '_timestamp': 1762330255.1835415}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2575604173922001, 'val/loss': 0.15375598333775997, 'test/loss': 0.14526577480137348, '_timestamp': 1762330262.262344}).
Epoch: 3, Steps: 133 | Train Loss: 0.2463340 Vali Loss: 0.1634562 Test Loss: 0.1637344
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2385632 Vali Loss: 0.1700789 Test Loss: 0.1794181
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2379663 Vali Loss: 0.1563572 Test Loss: 0.1591998
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2374130 Vali Loss: 0.1501429 Test Loss: 0.1508935
Validation loss decreased (0.153756 --> 0.150143).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2346326 Vali Loss: 0.1513509 Test Loss: 0.1525877
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2345126 Vali Loss: 0.1613746 Test Loss: 0.1662008
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2349126 Vali Loss: 0.1596254 Test Loss: 0.1618180
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2340391 Vali Loss: 0.1544101 Test Loss: 0.1526637
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2339570 Vali Loss: 0.1534109 Test Loss: 0.1550179
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2343857 Vali Loss: 0.1600572 Test Loss: 0.1665288
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2339293 Vali Loss: 0.1525306 Test Loss: 0.1477940
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2338306 Vali Loss: 0.1566399 Test Loss: 0.1597765
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2342729 Vali Loss: 0.1523280 Test Loss: 0.1529214
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2345741 Vali Loss: 0.1517469 Test Loss: 0.1503207
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NASDAQ_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.0001443446526536718, mae:0.008766132406890392, rmse:0.012014351785182953, r2:-0.042756080627441406, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0088, RMSE: 0.0120, RÂ²: -0.0428, MAPE: 1907793.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.586 MB of 0.586 MB uploadedwandb: \ 0.586 MB of 0.586 MB uploadedwandb: | 0.586 MB of 0.586 MB uploadedwandb: / 0.586 MB of 0.586 MB uploadedwandb: - 0.586 MB of 0.586 MB uploadedwandb: \ 0.586 MB of 0.885 MB uploadedwandb: | 0.885 MB of 0.885 MB uploadedwandb: / 0.885 MB of 0.885 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–ˆâ–„â–‚â–‚â–…â–„â–‚â–ƒâ–…â–â–„â–‚â–‚
wandb:                 train/loss â–ˆâ–„â–ƒâ–ƒâ–â–â–‚â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–ˆâ–ƒâ–â–â–…â–„â–‚â–‚â–„â–‚â–ƒâ–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.15032
wandb:                 train/loss 0.23457
wandb:   val/directional_accuracy 50.0
wandb:                   val/loss 0.15175
wandb:                    val/mae 0.00877
wandb:                   val/mape 190779325.0
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.04276
wandb:                   val/rmse 0.01201
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/h3ss4cuo
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_101042-h3ss4cuo/logs
Completed: NASDAQ H=10

Training: Informer on NASDAQ for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_101300-pa9zd0y6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/pa9zd0y6
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NASDAQ_H22 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/pa9zd0y6
>>>>>>>start training : long_term_forecast_Informer_NASDAQ_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
Overriding target from 'OT' to 'close' for stock data
val 218
Overriding target from 'OT' to 'close' for stock data
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.3129723 Vali Loss: 0.2220531 Test Loss: 0.1798717
Validation loss decreased (inf --> 0.222053).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2618064 Vali Loss: 0.1757144 Test Loss: 0.1575269
Validation loss decreased (0.222053 --> 0.175714).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.31297232978271716, 'val/loss': 0.22205314253057754, 'test/loss': 0.17987167515924998, '_timestamp': 1762330392.782871}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26180643531860726, 'val/loss': 0.1757143693310874, 'test/loss': 0.15752685495785304, '_timestamp': 1762330399.7090535}).
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.31297232978271716, 'val/loss': 0.22205314253057754, 'test/loss': 0.17987167515924998, '_timestamp': 1762330392.782871}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26180643531860726, 'val/loss': 0.1757143693310874, 'test/loss': 0.15752685495785304, '_timestamp': 1762330399.7090535}).
Epoch: 3, Steps: 132 | Train Loss: 0.2489283 Vali Loss: 0.1602485 Test Loss: 0.1467378
Validation loss decreased (0.175714 --> 0.160248).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2450952 Vali Loss: 0.1612100 Test Loss: 0.1372073
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2417915 Vali Loss: 0.1599323 Test Loss: 0.1438634
Validation loss decreased (0.160248 --> 0.159932).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2400517 Vali Loss: 0.1591954 Test Loss: 0.1419691
Validation loss decreased (0.159932 --> 0.159195).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2391418 Vali Loss: 0.1587732 Test Loss: 0.1388211
Validation loss decreased (0.159195 --> 0.158773).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2389090 Vali Loss: 0.1595715 Test Loss: 0.1410583
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2384683 Vali Loss: 0.1592214 Test Loss: 0.1430631
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2383355 Vali Loss: 0.1620140 Test Loss: 0.1411690
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2386180 Vali Loss: 0.1587169 Test Loss: 0.1400027
Validation loss decreased (0.158773 --> 0.158717).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2383851 Vali Loss: 0.1604352 Test Loss: 0.1411337
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2383440 Vali Loss: 0.1595085 Test Loss: 0.1414471
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2384049 Vali Loss: 0.1603552 Test Loss: 0.1408272
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2388807 Vali Loss: 0.1585895 Test Loss: 0.1424021
Validation loss decreased (0.158717 --> 0.158589).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2382505 Vali Loss: 0.1604753 Test Loss: 0.1391642
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2381640 Vali Loss: 0.1615322 Test Loss: 0.1378218
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2379046 Vali Loss: 0.1591004 Test Loss: 0.1411698
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2382854 Vali Loss: 0.1593235 Test Loss: 0.1420863
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2383426 Vali Loss: 0.1601405 Test Loss: 0.1418197
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2386379 Vali Loss: 0.1591790 Test Loss: 0.1430752
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2382807 Vali Loss: 0.1608040 Test Loss: 0.1397913
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.2381002 Vali Loss: 0.1584125 Test Loss: 0.1406095
Validation loss decreased (0.158589 --> 0.158412).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 132 | Train Loss: 0.2384256 Vali Loss: 0.1599745 Test Loss: 0.1404249
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 132 | Train Loss: 0.2384724 Vali Loss: 0.1583472 Test Loss: 0.1410537
Validation loss decreased (0.158412 --> 0.158347).  Saving model ...
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 132 | Train Loss: 0.2383450 Vali Loss: 0.1604515 Test Loss: 0.1420778
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 132 | Train Loss: 0.2381868 Vali Loss: 0.1592111 Test Loss: 0.1392875
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 132 | Train Loss: 0.2381812 Vali Loss: 0.1612382 Test Loss: 0.1387618
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 132 | Train Loss: 0.2381473 Vali Loss: 0.1583552 Test Loss: 0.1420486
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 132 | Train Loss: 0.2380857 Vali Loss: 0.1592148 Test Loss: 0.1414390
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 132 | Train Loss: 0.2380821 Vali Loss: 0.1596144 Test Loss: 0.1412018
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 132 | Train Loss: 0.2382875 Vali Loss: 0.1593614 Test Loss: 0.1407171
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 132 | Train Loss: 0.2383030 Vali Loss: 0.1605980 Test Loss: 0.1391850
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 132 | Train Loss: 0.2383424 Vali Loss: 0.1593536 Test Loss: 0.1415210
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1641532182693482e-14
Epoch: 35, Steps: 132 | Train Loss: 0.2385206 Vali Loss: 0.1606672 Test Loss: 0.1413107
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NASDAQ_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.0001522413658676669, mae:0.009064637124538422, rmse:0.012338613159954548, r2:-0.08801746368408203, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0091, RMSE: 0.0123, RÂ²: -0.0880, MAPE: 3152489.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.655 MB of 0.656 MB uploadedwandb: \ 0.655 MB of 0.656 MB uploadedwandb: | 0.655 MB of 0.656 MB uploadedwandb: / 0.656 MB of 0.656 MB uploadedwandb: - 0.656 MB of 0.958 MB uploadedwandb: \ 0.958 MB of 0.958 MB uploadedwandb: | 0.958 MB of 0.958 MB uploadedwandb: / 0.958 MB of 0.958 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–†â–„â–‚â–„â–…â–„â–ƒâ–„â–„â–„â–…â–‚â–â–„â–…â–„â–…â–ƒâ–ƒâ–ƒâ–„â–…â–ƒâ–‚â–…â–„â–„â–„â–‚â–„â–„
wandb:                 train/loss â–ˆâ–†â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–†â–„â–ƒâ–‚â–ƒâ–ƒâ–ˆâ–‚â–…â–ƒâ–…â–â–…â–‡â–‚â–ƒâ–„â–ƒâ–†â–â–„â–â–…â–ƒâ–‡â–â–ƒâ–ƒâ–ƒâ–…â–ƒâ–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 34
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.14131
wandb:                 train/loss 0.23852
wandb:   val/directional_accuracy 51.24509
wandb:                   val/loss 0.16067
wandb:                    val/mae 0.00906
wandb:                   val/mape 315248975.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.08802
wandb:                   val/rmse 0.01234
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/pa9zd0y6
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_101300-pa9zd0y6/logs
Completed: NASDAQ H=22

Training: Informer on NASDAQ for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_101729-koqgbeqk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/koqgbeqk
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NASDAQ_H50 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/koqgbeqk
>>>>>>>start training : long_term_forecast_Informer_NASDAQ_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
Overriding target from 'OT' to 'close' for stock data
val 190
Overriding target from 'OT' to 'close' for stock data
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.3185453 Vali Loss: 0.2267942 Test Loss: 0.2144434
Validation loss decreased (inf --> 0.226794).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2662268 Vali Loss: 0.1846114 Test Loss: 0.1690539
Validation loss decreased (0.226794 --> 0.184611).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.31854532049460843, 'val/loss': 0.2267941733201345, 'test/loss': 0.21444339056809744, '_timestamp': 1762330664.4005277}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2662267599706397, 'val/loss': 0.18461135029792786, 'test/loss': 0.16905392706394196, '_timestamp': 1762330671.4464426}).
Epoch: 3, Steps: 132 | Train Loss: 0.2546560 Vali Loss: 0.2097767 Test Loss: 0.1538574
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2493018 Vali Loss: 0.2030265 Test Loss: 0.1614231
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2440867 Vali Loss: 0.1873689 Test Loss: 0.1632351
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2437545 Vali Loss: 0.1874639 Test Loss: 0.1642251
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2428708 Vali Loss: 0.1928025 Test Loss: 0.1612837
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2418833 Vali Loss: 0.1799362 Test Loss: 0.1750312
Validation loss decreased (0.184611 --> 0.179936).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2437186 Vali Loss: 0.1817404 Test Loss: 0.1719098
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2406778 Vali Loss: 0.1823956 Test Loss: 0.1698546
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2411554 Vali Loss: 0.1793503 Test Loss: 0.1712331
Validation loss decreased (0.179936 --> 0.179350).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2428591 Vali Loss: 0.2009104 Test Loss: 0.1617414
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2414512 Vali Loss: 0.1881329 Test Loss: 0.1651214
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2429156 Vali Loss: 0.1962709 Test Loss: 0.1621830
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2433558 Vali Loss: 0.1947666 Test Loss: 0.1615652
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2406648 Vali Loss: 0.1820584 Test Loss: 0.1686766
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2427394 Vali Loss: 0.1913865 Test Loss: 0.1646124
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2449989 Vali Loss: 0.1906794 Test Loss: 0.1667242
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2411483 Vali Loss: 0.1819839 Test Loss: 0.1695161
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2414707 Vali Loss: 0.1807049 Test Loss: 0.1733452
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2408278 Vali Loss: 0.1811910 Test Loss: 0.1694374
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NASDAQ_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.00016085941751953214, mae:0.009289588779211044, rmse:0.012683036737143993, r2:-0.11055755615234375, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0093, RMSE: 0.0127, RÂ²: -0.1106, MAPE: 3404209.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.709 MB of 0.712 MB uploadedwandb: \ 0.709 MB of 0.712 MB uploadedwandb: | 0.712 MB of 0.712 MB uploadedwandb: / 0.712 MB of 0.712 MB uploadedwandb: - 0.712 MB of 1.012 MB uploadedwandb: \ 1.012 MB of 1.012 MB uploadedwandb: | 1.012 MB of 1.012 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–„â–„â–„â–ƒâ–ˆâ–‡â–†â–‡â–„â–…â–„â–„â–†â–…â–…â–†â–‡â–†
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–ƒâ–â–â–‚â–â–‚â–‚â–â–‚â–ƒâ–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–ƒâ–ƒâ–„â–â–‚â–‚â–â–†â–ƒâ–…â–…â–‚â–„â–„â–‚â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.16944
wandb:                 train/loss 0.24083
wandb:   val/directional_accuracy 50.67669
wandb:                   val/loss 0.18119
wandb:                    val/mae 0.00929
wandb:                   val/mape 340420900.0
wandb:                    val/mse 0.00016
wandb:                     val/r2 -0.11056
wandb:                   val/rmse 0.01268
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/koqgbeqk
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_101729-koqgbeqk/logs
Completed: NASDAQ H=50

Training: Informer on NASDAQ for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_102031-x2okfnv7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/x2okfnv7
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NASDAQ_H100Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/x2okfnv7
>>>>>>>start training : long_term_forecast_Informer_NASDAQ_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
Overriding target from 'OT' to 'close' for stock data
val 140
Overriding target from 'OT' to 'close' for stock data
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.3241454 Vali Loss: 0.3724214 Test Loss: 0.2884367
Validation loss decreased (inf --> 0.372421).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2673405 Vali Loss: 0.3380865 Test Loss: 0.3163501
Validation loss decreased (0.372421 --> 0.338086).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.32414535008943995, 'val/loss': 0.3724213719367981, 'test/loss': 0.28843667209148405, '_timestamp': 1762330845.3752894}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26734050959348676, 'val/loss': 0.3380864918231964, 'test/loss': 0.3163500964641571, '_timestamp': 1762330852.252773}).
Epoch: 3, Steps: 130 | Train Loss: 0.2541393 Vali Loss: 0.3624814 Test Loss: 0.3452755
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2486577 Vali Loss: 0.2987718 Test Loss: 0.3397692
Validation loss decreased (0.338086 --> 0.298772).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2454588 Vali Loss: 0.3038407 Test Loss: 0.4161068
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2442000 Vali Loss: 0.2931030 Test Loss: 0.3716716
Validation loss decreased (0.298772 --> 0.293103).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2439214 Vali Loss: 0.2950933 Test Loss: 0.3865936
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2433421 Vali Loss: 0.2952186 Test Loss: 0.3897396
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2431211 Vali Loss: 0.2947916 Test Loss: 0.3900996
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2434776 Vali Loss: 0.2923197 Test Loss: 0.3847031
Validation loss decreased (0.293103 --> 0.292320).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2432778 Vali Loss: 0.2988682 Test Loss: 0.3957010
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2425431 Vali Loss: 0.2954476 Test Loss: 0.4047643
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2428796 Vali Loss: 0.2922035 Test Loss: 0.3856071
Validation loss decreased (0.292320 --> 0.292204).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.2432401 Vali Loss: 0.2948864 Test Loss: 0.3855068
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.2426393 Vali Loss: 0.2937298 Test Loss: 0.3872716
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 130 | Train Loss: 0.2428604 Vali Loss: 0.2979444 Test Loss: 0.3860533
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 130 | Train Loss: 0.2429326 Vali Loss: 0.2995451 Test Loss: 0.3913903
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 130 | Train Loss: 0.2430226 Vali Loss: 0.2964785 Test Loss: 0.3901422
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 130 | Train Loss: 0.2427541 Vali Loss: 0.3009649 Test Loss: 0.4113952
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 130 | Train Loss: 0.2434196 Vali Loss: 0.2946448 Test Loss: 0.3936876
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 130 | Train Loss: 0.2423066 Vali Loss: 0.2923852 Test Loss: 0.3956997
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 130 | Train Loss: 0.2427419 Vali Loss: 0.2970630 Test Loss: 0.3947679
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 130 | Train Loss: 0.2437999 Vali Loss: 0.2951986 Test Loss: 0.3874542
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NASDAQ_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0001600287650944665, mae:0.008905877359211445, rmse:0.012650247663259506, r2:-0.0550839900970459, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0089, RMSE: 0.0127, RÂ²: -0.0551, MAPE: 1970883.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.746 MB of 0.751 MB uploadedwandb: \ 0.746 MB of 0.751 MB uploadedwandb: | 0.751 MB of 0.751 MB uploadedwandb: / 0.751 MB of 0.751 MB uploadedwandb: - 0.751 MB of 1.052 MB uploadedwandb: \ 1.052 MB of 1.052 MB uploadedwandb: | 1.052 MB of 1.052 MB uploadedwandb: / 1.052 MB of 1.052 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‚â–â–ˆâ–„â–…â–†â–†â–…â–†â–‡â–…â–…â–…â–…â–†â–†â–ˆâ–†â–†â–†â–…
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–â–â–‚â–â–â–â–â–â–‚â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–‚â–‚â–â–‚â–â–â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.38745
wandb:                 train/loss 0.2438
wandb:   val/directional_accuracy 49.92063
wandb:                   val/loss 0.2952
wandb:                    val/mae 0.00891
wandb:                   val/mape 197088350.0
wandb:                    val/mse 0.00016
wandb:                     val/r2 -0.05508
wandb:                   val/rmse 0.01265
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/x2okfnv7
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_102031-x2okfnv7/logs
Completed: NASDAQ H=100

Training: Informer on ABSA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_102346-669b24r8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/669b24r8
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ABSA_H3    Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/669b24r8
>>>>>>>start training : long_term_forecast_Informer_ABSA_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 3
============================================================
train 4240
Overriding target from 'OT' to 'close' for stock data
val 237
Overriding target from 'OT' to 'close' for stock data
test 238
Epoch: 1, Steps: 133 | Train Loss: 0.3538334 Vali Loss: 0.1907939 Test Loss: 0.1816303
Validation loss decreased (inf --> 0.190794).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2906186 Vali Loss: 0.2098133 Test Loss: 0.2011807
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3538333850919752, 'val/loss': 0.19079393707215786, 'test/loss': 0.18163029477000237, '_timestamp': 1762331042.191291}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2906186223254168, 'val/loss': 0.20981325395405293, 'test/loss': 0.20118067786097527, '_timestamp': 1762331049.261276}).
Epoch: 3, Steps: 133 | Train Loss: 0.2766829 Vali Loss: 0.1733353 Test Loss: 0.1626899
Validation loss decreased (0.190794 --> 0.173335).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2704151 Vali Loss: 0.1650749 Test Loss: 0.1547996
Validation loss decreased (0.173335 --> 0.165075).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2655178 Vali Loss: 0.1646129 Test Loss: 0.1559412
Validation loss decreased (0.165075 --> 0.164613).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2650573 Vali Loss: 0.1633187 Test Loss: 0.1536248
Validation loss decreased (0.164613 --> 0.163319).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2624003 Vali Loss: 0.1665085 Test Loss: 0.1561328
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2604408 Vali Loss: 0.1598749 Test Loss: 0.1555360
Validation loss decreased (0.163319 --> 0.159875).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2619933 Vali Loss: 0.1668136 Test Loss: 0.1550550
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2627610 Vali Loss: 0.1625895 Test Loss: 0.1546174
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2609787 Vali Loss: 0.1621103 Test Loss: 0.1545482
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2601771 Vali Loss: 0.1608967 Test Loss: 0.1554554
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2629390 Vali Loss: 0.1637048 Test Loss: 0.1551140
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2596686 Vali Loss: 0.1650058 Test Loss: 0.1543425
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2628323 Vali Loss: 0.1698143 Test Loss: 0.1558643
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2614460 Vali Loss: 0.1672746 Test Loss: 0.1542052
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2617158 Vali Loss: 0.1633336 Test Loss: 0.1555008
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2614843 Vali Loss: 0.1615587 Test Loss: 0.1548118
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ABSA_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 238
test shape: (238, 3, 1) (238, 3, 1)
test shape: (238, 3, 1) (238, 3, 1)


	mse:0.0004646394809242338, mae:0.016417047008872032, rmse:0.021555498242378235, r2:-0.02037990093231201, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0164, RMSE: 0.0216, RÂ²: -0.0204, MAPE: 1.24%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.496 MB of 0.496 MB uploadedwandb: \ 0.496 MB of 0.496 MB uploadedwandb: | 0.496 MB of 0.496 MB uploadedwandb: / 0.496 MB of 0.496 MB uploadedwandb: - 0.496 MB of 0.496 MB uploadedwandb: \ 0.496 MB of 0.496 MB uploadedwandb: | 0.496 MB of 0.496 MB uploadedwandb: / 0.496 MB of 0.496 MB uploadedwandb: - 0.496 MB of 0.496 MB uploadedwandb: \ 0.678 MB of 0.977 MB uploaded (0.002 MB deduped)wandb: | 0.977 MB of 0.977 MB uploaded (0.002 MB deduped)wandb: / 0.977 MB of 0.977 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–ƒâ–â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–ƒâ–ƒâ–„â–â–…â–‚â–‚â–‚â–ƒâ–„â–†â–…â–ƒâ–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 17
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.15481
wandb:                 train/loss 0.26148
wandb:   val/directional_accuracy 45.79832
wandb:                   val/loss 0.16156
wandb:                    val/mae 0.01642
wandb:                   val/mape 124.12906
wandb:                    val/mse 0.00046
wandb:                     val/r2 -0.02038
wandb:                   val/rmse 0.02156
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/669b24r8
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_102346-669b24r8/logs
Completed: ABSA H=3

Training: Informer on ABSA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_102633-l8wzzzgw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/l8wzzzgw
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ABSA_H5    Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/l8wzzzgw
>>>>>>>start training : long_term_forecast_Informer_ABSA_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 5
============================================================
train 4238
Overriding target from 'OT' to 'close' for stock data
val 235
Overriding target from 'OT' to 'close' for stock data
test 236
Epoch: 1, Steps: 133 | Train Loss: 0.3626935 Vali Loss: 0.1771465 Test Loss: 0.1653778
Validation loss decreased (inf --> 0.177146).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2963639 Vali Loss: 0.1777654 Test Loss: 0.1684242
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3626935361023236, 'val/loss': 0.17714649811387062, 'test/loss': 0.16537776961922646, '_timestamp': 1762331207.897109}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2963639448459883, 'val/loss': 0.17776535265147686, 'test/loss': 0.16842418815940619, '_timestamp': 1762331214.926754}).
Epoch: 3, Steps: 133 | Train Loss: 0.2840406 Vali Loss: 0.1685021 Test Loss: 0.1606641
Validation loss decreased (0.177146 --> 0.168502).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2765966 Vali Loss: 0.1701370 Test Loss: 0.1679643
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2721973 Vali Loss: 0.1670443 Test Loss: 0.1616056
Validation loss decreased (0.168502 --> 0.167044).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2698767 Vali Loss: 0.1651218 Test Loss: 0.1608762
Validation loss decreased (0.167044 --> 0.165122).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2687917 Vali Loss: 0.1674602 Test Loss: 0.1609623
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2684563 Vali Loss: 0.1730333 Test Loss: 0.1608789
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2678111 Vali Loss: 0.1705555 Test Loss: 0.1606274
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2679538 Vali Loss: 0.1663207 Test Loss: 0.1611013
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2676519 Vali Loss: 0.1653123 Test Loss: 0.1604391
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2670753 Vali Loss: 0.1670815 Test Loss: 0.1618102
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2695624 Vali Loss: 0.1589561 Test Loss: 0.1602688
Validation loss decreased (0.165122 --> 0.158956).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2681522 Vali Loss: 0.1684422 Test Loss: 0.1619271
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2664985 Vali Loss: 0.1666944 Test Loss: 0.1607899
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2669210 Vali Loss: 0.1644884 Test Loss: 0.1615177
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2671397 Vali Loss: 0.1645006 Test Loss: 0.1608752
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2672625 Vali Loss: 0.1647818 Test Loss: 0.1609101
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2677318 Vali Loss: 0.1644030 Test Loss: 0.1612665
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2685653 Vali Loss: 0.1639257 Test Loss: 0.1603506
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2690866 Vali Loss: 0.1635242 Test Loss: 0.1609735
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2677868 Vali Loss: 0.1650589 Test Loss: 0.1610334
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2675830 Vali Loss: 0.1634861 Test Loss: 0.1620060
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ABSA_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 236
test shape: (236, 5, 1) (236, 5, 1)
test shape: (236, 5, 1) (236, 5, 1)


	mse:0.00046318420208990574, mae:0.016368968412280083, rmse:0.021521715447306633, r2:-0.011269807815551758, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0164, RMSE: 0.0215, RÂ²: -0.0113, MAPE: 1.31%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.527 MB of 0.528 MB uploadedwandb: \ 0.527 MB of 0.528 MB uploadedwandb: | 0.528 MB of 0.528 MB uploadedwandb: / 0.528 MB of 0.528 MB uploadedwandb: - 0.528 MB of 0.827 MB uploadedwandb: \ 0.827 MB of 0.827 MB uploadedwandb: | 0.827 MB of 0.827 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–ƒâ–â–‚â–‚â–‚â–‚â–â–‚â–‚â–ƒ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–â–â–â–â–â–‚â–‚â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–‡â–…â–„â–…â–ˆâ–‡â–…â–„â–…â–â–†â–…â–„â–„â–„â–„â–ƒâ–ƒâ–„â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.16201
wandb:                 train/loss 0.26758
wandb:   val/directional_accuracy 51.48305
wandb:                   val/loss 0.16349
wandb:                    val/mae 0.01637
wandb:                   val/mape 131.39291
wandb:                    val/mse 0.00046
wandb:                     val/r2 -0.01127
wandb:                   val/rmse 0.02152
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/l8wzzzgw
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_102633-l8wzzzgw/logs
Completed: ABSA H=5

Training: Informer on ABSA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_102945-oq5a1wpy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/oq5a1wpy
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ABSA_H10   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/oq5a1wpy
>>>>>>>start training : long_term_forecast_Informer_ABSA_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 10
============================================================
train 4233
Overriding target from 'OT' to 'close' for stock data
val 230
Overriding target from 'OT' to 'close' for stock data
test 231
Epoch: 1, Steps: 133 | Train Loss: 0.3640656 Vali Loss: 0.1836680 Test Loss: 0.1726294
Validation loss decreased (inf --> 0.183668).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.3005311 Vali Loss: 0.1849930 Test Loss: 0.1844733
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.364065610824671, 'val/loss': 0.18366803042590618, 'test/loss': 0.1726294346153736, '_timestamp': 1762331400.3342364}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.30053107944646273, 'val/loss': 0.18499296717345715, 'test/loss': 0.18447328452020884, '_timestamp': 1762331407.4374168}).
Epoch: 3, Steps: 133 | Train Loss: 0.2877452 Vali Loss: 0.1684868 Test Loss: 0.1643979
Validation loss decreased (0.183668 --> 0.168487).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2794237 Vali Loss: 0.1772334 Test Loss: 0.1683654
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2761422 Vali Loss: 0.1724915 Test Loss: 0.1694590
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2743410 Vali Loss: 0.1719521 Test Loss: 0.1689842
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2716317 Vali Loss: 0.1704054 Test Loss: 0.1681143
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2704697 Vali Loss: 0.1759858 Test Loss: 0.1685918
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2710921 Vali Loss: 0.1781667 Test Loss: 0.1701377
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2705507 Vali Loss: 0.1749562 Test Loss: 0.1678985
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2697225 Vali Loss: 0.1682974 Test Loss: 0.1666131
Validation loss decreased (0.168487 --> 0.168297).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2708494 Vali Loss: 0.1706520 Test Loss: 0.1692541
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2710664 Vali Loss: 0.1701331 Test Loss: 0.1675892
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2700766 Vali Loss: 0.1764775 Test Loss: 0.1681849
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2703327 Vali Loss: 0.1669861 Test Loss: 0.1669131
Validation loss decreased (0.168297 --> 0.166986).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2701111 Vali Loss: 0.1709449 Test Loss: 0.1675040
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2699343 Vali Loss: 0.1738457 Test Loss: 0.1674984
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2696739 Vali Loss: 0.1746538 Test Loss: 0.1675672
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2697069 Vali Loss: 0.1750365 Test Loss: 0.1692417
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2707106 Vali Loss: 0.1691385 Test Loss: 0.1677036
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2695355 Vali Loss: 0.1728238 Test Loss: 0.1677581
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2694715 Vali Loss: 0.1754956 Test Loss: 0.1684621
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2693778 Vali Loss: 0.1713109 Test Loss: 0.1682842
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2709154 Vali Loss: 0.1698934 Test Loss: 0.1674674
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2691870 Vali Loss: 0.1733104 Test Loss: 0.1683208
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ABSA_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 231
test shape: (231, 10, 1) (231, 10, 1)
test shape: (231, 10, 1) (231, 10, 1)


	mse:0.0004688469343818724, mae:0.016442082822322845, rmse:0.02165287360548973, r2:-0.015532493591308594, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0164, RMSE: 0.0217, RÂ²: -0.0155, MAPE: 1.15%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.558 MB of 0.558 MB uploadedwandb: \ 0.558 MB of 0.558 MB uploadedwandb: | 0.558 MB of 0.558 MB uploadedwandb: / 0.558 MB of 0.558 MB uploadedwandb: - 0.558 MB of 0.558 MB uploadedwandb: \ 0.558 MB of 0.859 MB uploadedwandb: | 0.859 MB of 0.859 MB uploadedwandb: / 0.859 MB of 0.859 MB uploadedwandb: - 0.859 MB of 0.859 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–†â–‡â–‡â–†â–†â–ˆâ–…â–„â–‡â–…â–†â–„â–…â–…â–…â–‡â–…â–…â–†â–†â–…â–†
wandb:                 train/loss â–ˆâ–…â–„â–ƒâ–‚â–â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–‡â–„â–„â–ƒâ–‡â–ˆâ–†â–‚â–ƒâ–ƒâ–‡â–â–ƒâ–…â–†â–†â–‚â–…â–†â–„â–ƒâ–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 24
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.16832
wandb:                 train/loss 0.26919
wandb:   val/directional_accuracy 49.11015
wandb:                   val/loss 0.17331
wandb:                    val/mae 0.01644
wandb:                   val/mape 114.99966
wandb:                    val/mse 0.00047
wandb:                     val/r2 -0.01553
wandb:                   val/rmse 0.02165
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/oq5a1wpy
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_102945-oq5a1wpy/logs
Completed: ABSA H=10

Training: Informer on ABSA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_103312-s2ppi0uh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/s2ppi0uh
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ABSA_H22   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/s2ppi0uh
>>>>>>>start training : long_term_forecast_Informer_ABSA_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 22
============================================================
train 4221
Overriding target from 'OT' to 'close' for stock data
val 218
Overriding target from 'OT' to 'close' for stock data
test 219
Epoch: 1, Steps: 132 | Train Loss: 0.3722463 Vali Loss: 0.2054857 Test Loss: 0.1797229
Validation loss decreased (inf --> 0.205486).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3722462981487765, 'val/loss': 0.20548569304602488, 'test/loss': 0.1797229094164712, '_timestamp': 1762331607.8817034}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3106162208273555, 'val/loss': 0.19855701071875437, 'test/loss': 0.17661406844854355, '_timestamp': 1762331615.801429}).
Epoch: 2, Steps: 132 | Train Loss: 0.3106162 Vali Loss: 0.1985570 Test Loss: 0.1766141
Validation loss decreased (0.205486 --> 0.198557).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2933964 Vali Loss: 0.1873458 Test Loss: 0.1693016
Validation loss decreased (0.198557 --> 0.187346).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2844002 Vali Loss: 0.1821411 Test Loss: 0.1648176
Validation loss decreased (0.187346 --> 0.182141).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2793286 Vali Loss: 0.1805134 Test Loss: 0.1629852
Validation loss decreased (0.182141 --> 0.180513).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2759879 Vali Loss: 0.1806796 Test Loss: 0.1618368
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2741922 Vali Loss: 0.1781205 Test Loss: 0.1622793
Validation loss decreased (0.180513 --> 0.178120).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2735956 Vali Loss: 0.1795838 Test Loss: 0.1610172
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2731754 Vali Loss: 0.1790736 Test Loss: 0.1618768
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2730613 Vali Loss: 0.1789450 Test Loss: 0.1618765
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2728979 Vali Loss: 0.1789787 Test Loss: 0.1622018
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2729347 Vali Loss: 0.1788129 Test Loss: 0.1617814
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2729206 Vali Loss: 0.1786288 Test Loss: 0.1628775
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2725983 Vali Loss: 0.1779298 Test Loss: 0.1614916
Validation loss decreased (0.178120 --> 0.177930).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2729203 Vali Loss: 0.1787013 Test Loss: 0.1623597
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2726357 Vali Loss: 0.1784357 Test Loss: 0.1614198
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2730827 Vali Loss: 0.1784316 Test Loss: 0.1619335
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2725698 Vali Loss: 0.1780829 Test Loss: 0.1627121
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2727572 Vali Loss: 0.1791938 Test Loss: 0.1617452
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2725960 Vali Loss: 0.1788963 Test Loss: 0.1623898
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2731919 Vali Loss: 0.1786337 Test Loss: 0.1626836
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2728086 Vali Loss: 0.1786771 Test Loss: 0.1616508
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.2730628 Vali Loss: 0.1785173 Test Loss: 0.1621118
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 132 | Train Loss: 0.2726678 Vali Loss: 0.1782456 Test Loss: 0.1616956
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ABSA_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 219
test shape: (219, 22, 1) (219, 22, 1)
test shape: (219, 22, 1) (219, 22, 1)


	mse:0.0004756544076371938, mae:0.016578003764152527, rmse:0.021809503436088562, r2:-0.015224456787109375, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0166, RMSE: 0.0218, RÂ²: -0.0152, MAPE: 1.31%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.621 MB of 0.622 MB uploadedwandb: \ 0.622 MB of 0.622 MB uploadedwandb: | 0.622 MB of 0.622 MB uploadedwandb: / 0.622 MB of 0.923 MB uploadedwandb: - 0.923 MB of 0.923 MB uploadedwandb: \ 0.923 MB of 0.923 MB uploadedwandb: | 0.923 MB of 0.923 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–‚â–‚â–‚â–‚â–ƒâ–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–ƒâ–ƒâ–â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–‚â–‚â–‚â–‚â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 23
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.1617
wandb:                 train/loss 0.27267
wandb:   val/directional_accuracy 50.46749
wandb:                   val/loss 0.17825
wandb:                    val/mae 0.01658
wandb:                   val/mape 130.62208
wandb:                    val/mse 0.00048
wandb:                     val/r2 -0.01522
wandb:                   val/rmse 0.02181
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/s2ppi0uh
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_103312-s2ppi0uh/logs
Completed: ABSA H=22

Training: Informer on ABSA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_103627-cg5z3o7p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/cg5z3o7p
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ABSA_H50   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/cg5z3o7p
>>>>>>>start training : long_term_forecast_Informer_ABSA_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 50
============================================================
train 4193
Overriding target from 'OT' to 'close' for stock data
val 190
Overriding target from 'OT' to 'close' for stock data
test 191
Epoch: 1, Steps: 132 | Train Loss: 0.3892853 Vali Loss: 0.2178150 Test Loss: 0.1814929
Validation loss decreased (inf --> 0.217815).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.3192905 Vali Loss: 0.1847499 Test Loss: 0.1636354
Validation loss decreased (0.217815 --> 0.184750).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3892853476784446, 'val/loss': 0.21781504899263382, 'test/loss': 0.18149294455846152, '_timestamp': 1762331802.746853}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3192905302752148, 'val/loss': 0.1847498839100202, 'test/loss': 0.16363541161020598, '_timestamp': 1762331809.836709}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3192905302752148, 'val/loss': 0.1847498839100202, 'test/loss': 0.16363541161020598, '_timestamp': 1762331809.836709}).
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
Epoch: 3, Steps: 132 | Train Loss: 0.3062107 Vali Loss: 0.2033462 Test Loss: 0.1655724
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2977366 Vali Loss: 0.1883296 Test Loss: 0.1590386
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2901599 Vali Loss: 0.1837160 Test Loss: 0.1593033
Validation loss decreased (0.184750 --> 0.183716).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2882412 Vali Loss: 0.1861960 Test Loss: 0.1594871
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2869492 Vali Loss: 0.1895617 Test Loss: 0.1604515
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2852196 Vali Loss: 0.1871496 Test Loss: 0.1622874
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2864049 Vali Loss: 0.1919938 Test Loss: 0.1657690
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2838207 Vali Loss: 0.1904046 Test Loss: 0.1614456
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2841923 Vali Loss: 0.1859967 Test Loss: 0.1592443
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2841878 Vali Loss: 0.1858788 Test Loss: 0.1600524
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2861685 Vali Loss: 0.1820523 Test Loss: 0.1603129
Validation loss decreased (0.183716 --> 0.182052).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2856789 Vali Loss: 0.1848585 Test Loss: 0.1606616
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2859729 Vali Loss: 0.1848254 Test Loss: 0.1602993
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2839578 Vali Loss: 0.1877272 Test Loss: 0.1607799
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2853231 Vali Loss: 0.1841835 Test Loss: 0.1599482
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2859447 Vali Loss: 0.1879185 Test Loss: 0.1623799
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2851594 Vali Loss: 0.1899196 Test Loss: 0.1623120
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2867832 Vali Loss: 0.1869651 Test Loss: 0.1639900
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2843205 Vali Loss: 0.1855025 Test Loss: 0.1590940
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2837264 Vali Loss: 0.1882757 Test Loss: 0.1614542
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.2881065 Vali Loss: 0.1888405 Test Loss: 0.1612196
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ABSA_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 191
test shape: (191, 50, 1) (191, 50, 1)
test shape: (191, 50, 1) (191, 50, 1)


	mse:0.0004897164180874825, mae:0.016928670927882195, rmse:0.022129537537693977, r2:-0.006632089614868164, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0169, RMSE: 0.0221, RÂ²: -0.0066, MAPE: 1.09%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.707 MB of 0.709 MB uploadedwandb: \ 0.707 MB of 0.709 MB uploadedwandb: | 0.707 MB of 0.709 MB uploadedwandb: / 0.709 MB of 0.709 MB uploadedwandb: - 0.709 MB of 1.009 MB uploadedwandb: \ 0.709 MB of 1.009 MB uploadedwandb: | 1.009 MB of 1.009 MB uploadedwandb: / 1.009 MB of 1.009 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–â–â–‚â–„â–ˆâ–„â–â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–„â–„â–†â–â–„â–ƒ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–‚â–â–â–â–‚â–‚â–‚â–â–â–‚â–â–‚â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–‚â–‚â–ƒâ–ƒâ–„â–„â–‚â–‚â–â–‚â–‚â–ƒâ–‚â–ƒâ–„â–ƒâ–‚â–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.16122
wandb:                 train/loss 0.28811
wandb:   val/directional_accuracy 49.43904
wandb:                   val/loss 0.18884
wandb:                    val/mae 0.01693
wandb:                   val/mape 109.36202
wandb:                    val/mse 0.00049
wandb:                     val/r2 -0.00663
wandb:                   val/rmse 0.02213
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/cg5z3o7p
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_103627-cg5z3o7p/logs
Completed: ABSA H=50

Training: Informer on ABSA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_103939-p81ma2xo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/p81ma2xo
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ABSA_H100  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/p81ma2xo
>>>>>>>start training : long_term_forecast_Informer_ABSA_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 100
============================================================
train 4143
Overriding target from 'OT' to 'close' for stock data
val 140
Overriding target from 'OT' to 'close' for stock data
test 141
Epoch: 1, Steps: 130 | Train Loss: 0.4023068 Vali Loss: 0.2544544 Test Loss: 0.2264965
Validation loss decreased (inf --> 0.254454).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.3281881 Vali Loss: 0.2265791 Test Loss: 0.1855627
Validation loss decreased (0.254454 --> 0.226579).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.4023068395944742, 'val/loss': 0.25445438027381895, 'test/loss': 0.22649648785591125, '_timestamp': 1762331994.412934}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.32818810653228025, 'val/loss': 0.22657912373542785, 'test/loss': 0.18556270897388458, '_timestamp': 1762332001.3291495}).
Epoch: 3, Steps: 130 | Train Loss: 0.3101745 Vali Loss: 0.2368202 Test Loss: 0.2047639
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.3035419 Vali Loss: 0.2053586 Test Loss: 0.1753999
Validation loss decreased (0.226579 --> 0.205359).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2994942 Vali Loss: 0.2066755 Test Loss: 0.1777113
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2973428 Vali Loss: 0.2030276 Test Loss: 0.1769933
Validation loss decreased (0.205359 --> 0.203028).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2964553 Vali Loss: 0.2189331 Test Loss: 0.1867382
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2962559 Vali Loss: 0.2196159 Test Loss: 0.1891042
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2954723 Vali Loss: 0.2140784 Test Loss: 0.1865352
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2958702 Vali Loss: 0.2140026 Test Loss: 0.1838071
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2953463 Vali Loss: 0.2136339 Test Loss: 0.1857439
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2946593 Vali Loss: 0.2214954 Test Loss: 0.1872878
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2947259 Vali Loss: 0.2152547 Test Loss: 0.1849658
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.2950679 Vali Loss: 0.2122893 Test Loss: 0.1845132
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.2945564 Vali Loss: 0.2158203 Test Loss: 0.1842557
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 130 | Train Loss: 0.2953508 Vali Loss: 0.2159752 Test Loss: 0.1861176
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ABSA_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 141
test shape: (141, 100, 1) (141, 100, 1)
test shape: (141, 100, 1) (141, 100, 1)


	mse:0.0005178150604479015, mae:0.017345953732728958, rmse:0.02275555022060871, r2:-0.0034519433975219727, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0173, RMSE: 0.0228, RÂ²: -0.0035, MAPE: 1.02%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.796 MB of 0.801 MB uploadedwandb: \ 0.796 MB of 0.801 MB uploadedwandb: | 0.796 MB of 0.801 MB uploadedwandb: / 0.796 MB of 0.801 MB uploadedwandb: - 0.801 MB of 0.801 MB uploadedwandb: \ 0.801 MB of 1.100 MB uploadedwandb: | 1.100 MB of 1.100 MB uploadedwandb: / 1.100 MB of 1.100 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–‚â–â–„â–„â–„â–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–„
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–‚â–â–„â–„â–ƒâ–ƒâ–ƒâ–…â–„â–ƒâ–„â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.18612
wandb:                 train/loss 0.29535
wandb:   val/directional_accuracy 50.0394
wandb:                   val/loss 0.21598
wandb:                    val/mae 0.01735
wandb:                   val/mape 101.8885
wandb:                    val/mse 0.00052
wandb:                     val/r2 -0.00345
wandb:                   val/rmse 0.02276
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/p81ma2xo
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_103939-p81ma2xo/logs
Exception in thread ChkStopThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread IntMsgThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 300, in check_internal_messages
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 286, in check_stop_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    self._loop_check_status(    
local_handle = request()  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status

  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 856, in deliver_internal_messages
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 840, in deliver_stop_status
    return self._deliver_internal_messages(internal_message)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 516, in _deliver_internal_messages
    return self._deliver_stop_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 494, in _deliver_stop_status
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: ABSA H=100

Training: Informer on SASOL for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_104159-bjevmitm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/bjevmitm
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SASOL_H3   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/bjevmitm
>>>>>>>start training : long_term_forecast_Informer_SASOL_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 3
============================================================
train 3772
Overriding target from 'OT' to 'close' for stock data
val 211
Overriding target from 'OT' to 'close' for stock data
test 212
Epoch: 1, Steps: 118 | Train Loss: 0.2685171 Vali Loss: 0.1162650 Test Loss: 0.1566935
Validation loss decreased (inf --> 0.116265).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2169178 Vali Loss: 0.1115578 Test Loss: 0.1523891
Validation loss decreased (0.116265 --> 0.111558).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.26851713253279863, 'val/loss': 0.11626502977950233, 'test/loss': 0.1566935000675065, '_timestamp': 1762332131.9401517}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2169177710252293, 'val/loss': 0.1115578178848539, 'test/loss': 0.15238906655992782, '_timestamp': 1762332138.219561}).
Epoch: 3, Steps: 118 | Train Loss: 0.2049099 Vali Loss: 0.1166368 Test Loss: 0.1723981
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.1995534 Vali Loss: 0.1108172 Test Loss: 0.1533306
Validation loss decreased (0.111558 --> 0.110817).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.1973590 Vali Loss: 0.1022946 Test Loss: 0.1476605
Validation loss decreased (0.110817 --> 0.102295).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.1955117 Vali Loss: 0.1027513 Test Loss: 0.1489131
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.1951750 Vali Loss: 0.1033729 Test Loss: 0.1490261
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.1944145 Vali Loss: 0.1062933 Test Loss: 0.1501827
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.1943578 Vali Loss: 0.1021044 Test Loss: 0.1477116
Validation loss decreased (0.102295 --> 0.102104).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.1947098 Vali Loss: 0.1030483 Test Loss: 0.1480091
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.1944191 Vali Loss: 0.1027156 Test Loss: 0.1482696
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.1943556 Vali Loss: 0.1020221 Test Loss: 0.1479485
Validation loss decreased (0.102104 --> 0.102022).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.1946899 Vali Loss: 0.1012806 Test Loss: 0.1461083
Validation loss decreased (0.102022 --> 0.101281).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.1936612 Vali Loss: 0.1047212 Test Loss: 0.1472596
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.1944719 Vali Loss: 0.1015772 Test Loss: 0.1482545
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.1947227 Vali Loss: 0.1073301 Test Loss: 0.1495129
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.1937500 Vali Loss: 0.1034139 Test Loss: 0.1494259
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.1947249 Vali Loss: 0.1027212 Test Loss: 0.1473043
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.1942359 Vali Loss: 0.1038365 Test Loss: 0.1483261
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.1943292 Vali Loss: 0.1028868 Test Loss: 0.1476038
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 118 | Train Loss: 0.1945893 Vali Loss: 0.1022050 Test Loss: 0.1476414
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 118 | Train Loss: 0.1938944 Vali Loss: 0.1046246 Test Loss: 0.1482330
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 118 | Train Loss: 0.1943459 Vali Loss: 0.1030088 Test Loss: 0.1487532
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SASOL_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 212
test shape: (212, 3, 1) (212, 3, 1)
test shape: (212, 3, 1) (212, 3, 1)


	mse:0.002299347659572959, mae:0.03629216179251671, rmse:0.04795151203870773, r2:-0.04373300075531006, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0363, RMSE: 0.0480, RÂ²: -0.0437, MAPE: 20984678.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.460 MB of 0.461 MB uploadedwandb: \ 0.460 MB of 0.461 MB uploadedwandb: | 0.460 MB of 0.461 MB uploadedwandb: / 0.461 MB of 0.461 MB uploadedwandb: - 0.461 MB of 0.461 MB uploadedwandb: \ 0.461 MB of 0.461 MB uploadedwandb: | 0.461 MB of 0.461 MB uploadedwandb: / 0.642 MB of 0.942 MB uploaded (0.002 MB deduped)wandb: - 0.642 MB of 0.942 MB uploaded (0.002 MB deduped)wandb: \ 0.942 MB of 0.942 MB uploaded (0.002 MB deduped)wandb: | 0.942 MB of 0.942 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–‚â–‚â–‚â–â–‚â–â–â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–‚â–â–â–‚â–â–‚â–‚â–â–‚â–â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–â–‚â–‚â–ƒâ–â–‚â–‚â–â–â–ƒâ–â–„â–‚â–‚â–‚â–‚â–â–ƒâ–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.14875
wandb:                 train/loss 0.19435
wandb:   val/directional_accuracy 50.9434
wandb:                   val/loss 0.10301
wandb:                    val/mae 0.03629
wandb:                   val/mape 2098467800.0
wandb:                    val/mse 0.0023
wandb:                     val/r2 -0.04373
wandb:                   val/rmse 0.04795
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/bjevmitm
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_104159-bjevmitm/logs
Completed: SASOL H=3

Training: Informer on SASOL for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_104501-uzf06awc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/uzf06awc
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SASOL_H5   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/uzf06awc
>>>>>>>start training : long_term_forecast_Informer_SASOL_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 5
============================================================
train 3770
Overriding target from 'OT' to 'close' for stock data
val 209
Overriding target from 'OT' to 'close' for stock data
test 210
Epoch: 1, Steps: 118 | Train Loss: 0.2843590 Vali Loss: 0.1125318 Test Loss: 0.1577500
Validation loss decreased (inf --> 0.112532).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2201999 Vali Loss: 0.1158899 Test Loss: 0.1796478
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.28435897650355, 'val/loss': 0.11253180993454796, 'test/loss': 0.15774997430188314, '_timestamp': 1762332316.3409376}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.22019994164169845, 'val/loss': 0.11588993987866811, 'test/loss': 0.17964777243988855, '_timestamp': 1762332322.828807}).
Epoch: 3, Steps: 118 | Train Loss: 0.2087922 Vali Loss: 0.1060139 Test Loss: 0.1577098
Validation loss decreased (0.112532 --> 0.106014).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2034589 Vali Loss: 0.1090340 Test Loss: 0.1518346
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2012415 Vali Loss: 0.1063503 Test Loss: 0.1519108
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2000031 Vali Loss: 0.1097525 Test Loss: 0.1530218
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.1989332 Vali Loss: 0.1096405 Test Loss: 0.1530700
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.1988005 Vali Loss: 0.1059770 Test Loss: 0.1523683
Validation loss decreased (0.106014 --> 0.105977).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.1988691 Vali Loss: 0.1061712 Test Loss: 0.1507499
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.1987448 Vali Loss: 0.1047807 Test Loss: 0.1509123
Validation loss decreased (0.105977 --> 0.104781).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.1978888 Vali Loss: 0.1049948 Test Loss: 0.1501860
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.1986782 Vali Loss: 0.1054223 Test Loss: 0.1523575
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.1982109 Vali Loss: 0.1088775 Test Loss: 0.1515250
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.1983195 Vali Loss: 0.1079975 Test Loss: 0.1517277
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.1987907 Vali Loss: 0.1076120 Test Loss: 0.1512516
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.1984429 Vali Loss: 0.1080516 Test Loss: 0.1518972
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.1986691 Vali Loss: 0.1063830 Test Loss: 0.1532803
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.1983557 Vali Loss: 0.1054954 Test Loss: 0.1524352
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.1981356 Vali Loss: 0.1056392 Test Loss: 0.1527453
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.1981205 Vali Loss: 0.1049820 Test Loss: 0.1516576
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SASOL_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 210
test shape: (210, 5, 1) (210, 5, 1)
test shape: (210, 5, 1) (210, 5, 1)


	mse:0.0023078392259776592, mae:0.03626290708780289, rmse:0.04803997650742531, r2:-0.0398482084274292, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0363, RMSE: 0.0480, RÂ²: -0.0398, MAPE: 19244538.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.497 MB of 0.497 MB uploadedwandb: \ 0.497 MB of 0.497 MB uploadedwandb: | 0.497 MB of 0.497 MB uploadedwandb: / 0.497 MB of 0.797 MB uploadedwandb: - 0.797 MB of 0.797 MB uploadedwandb: \ 0.797 MB of 0.797 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–ƒâ–„â–„â–ƒâ–‚â–‚â–â–ƒâ–‚â–‚â–‚â–ƒâ–„â–ƒâ–ƒâ–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–‚â–â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–‡â–ƒâ–ˆâ–ˆâ–ƒâ–ƒâ–â–â–‚â–‡â–†â–…â–†â–ƒâ–‚â–‚â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.15166
wandb:                 train/loss 0.19812
wandb:   val/directional_accuracy 49.28571
wandb:                   val/loss 0.10498
wandb:                    val/mae 0.03626
wandb:                   val/mape 1924453800.0
wandb:                    val/mse 0.00231
wandb:                     val/r2 -0.03985
wandb:                   val/rmse 0.04804
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/uzf06awc
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_104501-uzf06awc/logs
Exception in thread ChkStopThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 268, in check_network_status
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 286, in check_stop_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 840, in deliver_stop_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 848, in deliver_network_status
    return self._deliver_stop_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 494, in _deliver_stop_status
    return self._deliver_network_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 510, in _deliver_network_status
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: SASOL H=5

Training: Informer on SASOL for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_104739-vaw3d8jt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/vaw3d8jt
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SASOL_H10  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/vaw3d8jt
>>>>>>>start training : long_term_forecast_Informer_SASOL_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 10
============================================================
train 3765
Overriding target from 'OT' to 'close' for stock data
val 204
Overriding target from 'OT' to 'close' for stock data
test 205
Epoch: 1, Steps: 118 | Train Loss: 0.2795335 Vali Loss: 0.1151186 Test Loss: 0.1557303
Validation loss decreased (inf --> 0.115119).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2235360 Vali Loss: 0.1114356 Test Loss: 0.1566787
Validation loss decreased (0.115119 --> 0.111436).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.27953354483943876, 'val/loss': 0.11511857275451932, 'test/loss': 0.15573030710220337, '_timestamp': 1762332473.031847}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.22353604827391899, 'val/loss': 0.11143561346190316, 'test/loss': 0.15667867021901266, '_timestamp': 1762332479.4097042}).
Epoch: 3, Steps: 118 | Train Loss: 0.2112338 Vali Loss: 0.1151824 Test Loss: 0.1650844
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2068786 Vali Loss: 0.1141646 Test Loss: 0.1556221
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2045894 Vali Loss: 0.1099148 Test Loss: 0.1547943
Validation loss decreased (0.111436 --> 0.109915).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2028954 Vali Loss: 0.1111513 Test Loss: 0.1557373
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2021504 Vali Loss: 0.1105035 Test Loss: 0.1532829
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2025988 Vali Loss: 0.1131832 Test Loss: 0.1539188
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2017551 Vali Loss: 0.1100841 Test Loss: 0.1545958
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2013228 Vali Loss: 0.1127735 Test Loss: 0.1542511
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2013624 Vali Loss: 0.1102771 Test Loss: 0.1550337
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2013206 Vali Loss: 0.1100715 Test Loss: 0.1545980
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2020949 Vali Loss: 0.1127183 Test Loss: 0.1551218
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2020743 Vali Loss: 0.1115892 Test Loss: 0.1547270
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.2020962 Vali Loss: 0.1089315 Test Loss: 0.1556267
Validation loss decreased (0.109915 --> 0.108932).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.2013538 Vali Loss: 0.1117674 Test Loss: 0.1541169
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.2014802 Vali Loss: 0.1126703 Test Loss: 0.1552345
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.2012564 Vali Loss: 0.1116602 Test Loss: 0.1551926
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.2015035 Vali Loss: 0.1100698 Test Loss: 0.1544008
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.2019859 Vali Loss: 0.1102513 Test Loss: 0.1546338
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 118 | Train Loss: 0.2017105 Vali Loss: 0.1107357 Test Loss: 0.1547493
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 118 | Train Loss: 0.2015893 Vali Loss: 0.1127642 Test Loss: 0.1537754
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 118 | Train Loss: 0.2011634 Vali Loss: 0.1122184 Test Loss: 0.1551917
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 118 | Train Loss: 0.2015839 Vali Loss: 0.1114804 Test Loss: 0.1545525
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 118 | Train Loss: 0.2013108 Vali Loss: 0.1097087 Test Loss: 0.1560479
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SASOL_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 205
test shape: (205, 10, 1) (205, 10, 1)
test shape: (205, 10, 1) (205, 10, 1)


	mse:0.0023291578982025385, mae:0.036450911313295364, rmse:0.04826134815812111, r2:-0.04931151866912842, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0365, RMSE: 0.0483, RÂ²: -0.0493, MAPE: 19361254.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.540 MB of 0.541 MB uploadedwandb: \ 0.541 MB of 0.541 MB uploadedwandb: | 0.541 MB of 0.541 MB uploadedwandb: / 0.541 MB of 0.841 MB uploadedwandb: - 0.841 MB of 0.841 MB uploadedwandb: \ 0.841 MB of 0.841 MB uploadedwandb: | 0.841 MB of 0.841 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–ƒ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–‚â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‡â–‚â–ƒâ–ƒâ–†â–‚â–…â–ƒâ–‚â–…â–„â–â–„â–…â–„â–‚â–‚â–ƒâ–…â–…â–„â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 24
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.15605
wandb:                 train/loss 0.20131
wandb:   val/directional_accuracy 49.8645
wandb:                   val/loss 0.10971
wandb:                    val/mae 0.03645
wandb:                   val/mape 1936125400.0
wandb:                    val/mse 0.00233
wandb:                     val/r2 -0.04931
wandb:                   val/rmse 0.04826
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/vaw3d8jt
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_104739-vaw3d8jt/logs
Completed: SASOL H=10

Training: Informer on SASOL for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_105044-aflbufz6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/aflbufz6
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SASOL_H22  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/aflbufz6
>>>>>>>start training : long_term_forecast_Informer_SASOL_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 22
============================================================
train 3753
Overriding target from 'OT' to 'close' for stock data
val 192
Overriding target from 'OT' to 'close' for stock data
test 193
Epoch: 1, Steps: 118 | Train Loss: 0.2894357 Vali Loss: 0.1353161 Test Loss: 0.1776740
Validation loss decreased (inf --> 0.135316).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2328280 Vali Loss: 0.1223815 Test Loss: 0.1689371
Validation loss decreased (0.135316 --> 0.122381).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.289435709179458, 'val/loss': 0.13531609748800597, 'test/loss': 0.17767401039600372, '_timestamp': 1762332659.241968}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2328280247116493, 'val/loss': 0.12238145743807156, 'test/loss': 0.16893712750502995, '_timestamp': 1762332665.6172245}).
Epoch: 3, Steps: 118 | Train Loss: 0.2192589 Vali Loss: 0.1264723 Test Loss: 0.1658268
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2147508 Vali Loss: 0.1201001 Test Loss: 0.1738241
Validation loss decreased (0.122381 --> 0.120100).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2121971 Vali Loss: 0.1191135 Test Loss: 0.1665076
Validation loss decreased (0.120100 --> 0.119114).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2094930 Vali Loss: 0.1227142 Test Loss: 0.1652436
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2095173 Vali Loss: 0.1184575 Test Loss: 0.1656983
Validation loss decreased (0.119114 --> 0.118458).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2086143 Vali Loss: 0.1196418 Test Loss: 0.1670979
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2087628 Vali Loss: 0.1196659 Test Loss: 0.1668551
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2076866 Vali Loss: 0.1182844 Test Loss: 0.1648390
Validation loss decreased (0.118458 --> 0.118284).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2085716 Vali Loss: 0.1187783 Test Loss: 0.1666249
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2102059 Vali Loss: 0.1183119 Test Loss: 0.1635907
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2087950 Vali Loss: 0.1178438 Test Loss: 0.1655839
Validation loss decreased (0.118284 --> 0.117844).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2085860 Vali Loss: 0.1201691 Test Loss: 0.1681925
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.2100601 Vali Loss: 0.1194883 Test Loss: 0.1661723
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.2081221 Vali Loss: 0.1182894 Test Loss: 0.1655492
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.2082065 Vali Loss: 0.1192003 Test Loss: 0.1654987
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.2078711 Vali Loss: 0.1204848 Test Loss: 0.1671503
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.2083400 Vali Loss: 0.1192809 Test Loss: 0.1678333
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.2091304 Vali Loss: 0.1186774 Test Loss: 0.1664052
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 118 | Train Loss: 0.2084680 Vali Loss: 0.1192633 Test Loss: 0.1647030
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 118 | Train Loss: 0.2083553 Vali Loss: 0.1209401 Test Loss: 0.1678813
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 118 | Train Loss: 0.2082416 Vali Loss: 0.1189209 Test Loss: 0.1680297
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SASOL_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 193
test shape: (193, 22, 1) (193, 22, 1)
test shape: (193, 22, 1) (193, 22, 1)


	mse:0.0023534256033599377, mae:0.03652108088135719, rmse:0.04851211979985237, r2:-0.048703908920288086, dtw:Not calculated


VAL - MSE: 0.0024, MAE: 0.0365, RMSE: 0.0485, RÂ²: -0.0487, MAPE: 19121942.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.594 MB of 0.595 MB uploadedwandb: \ 0.594 MB of 0.595 MB uploadedwandb: | 0.595 MB of 0.595 MB uploadedwandb: / 0.595 MB of 0.896 MB uploadedwandb: - 0.896 MB of 0.896 MB uploadedwandb: \ 0.896 MB of 0.896 MB uploadedwandb: | 0.896 MB of 0.896 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ƒâ–ˆâ–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–ƒâ–â–‚â–„â–ƒâ–‚â–‚â–ƒâ–„â–ƒâ–‚â–„â–„
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–â–‚â–ƒâ–‚â–‚â–‚â–â–â–â–â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–‚â–…â–â–‚â–‚â–â–‚â–â–â–ƒâ–‚â–â–‚â–ƒâ–‚â–‚â–‚â–„â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.16803
wandb:                 train/loss 0.20824
wandb:   val/directional_accuracy 50.06168
wandb:                   val/loss 0.11892
wandb:                    val/mae 0.03652
wandb:                   val/mape 1912194200.0
wandb:                    val/mse 0.00235
wandb:                     val/r2 -0.0487
wandb:                   val/rmse 0.04851
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/aflbufz6
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_105044-aflbufz6/logs
Completed: SASOL H=22

Training: Informer on SASOL for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_105342-yed0qlop
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/yed0qlop
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SASOL_H50  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/yed0qlop
>>>>>>>start training : long_term_forecast_Informer_SASOL_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 50
============================================================
train 3725
Overriding target from 'OT' to 'close' for stock data
val 164
Overriding target from 'OT' to 'close' for stock data
test 165
Epoch: 1, Steps: 117 | Train Loss: 0.3136791 Vali Loss: 0.1479640 Test Loss: 0.1887185
Validation loss decreased (inf --> 0.147964).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 117 | Train Loss: 0.2450129 Vali Loss: 0.1502286 Test Loss: 0.3469099
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3136791290126295, 'val/loss': 0.14796402553717294, 'test/loss': 0.18871846298376718, '_timestamp': 1762332836.5209806}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24501290522579455, 'val/loss': 0.15022864192724228, 'test/loss': 0.3469099203745524, '_timestamp': 1762332842.8007061}).
Epoch: 3, Steps: 117 | Train Loss: 0.2304719 Vali Loss: 0.1205539 Test Loss: 0.4102870
Validation loss decreased (0.147964 --> 0.120554).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 117 | Train Loss: 0.2224172 Vali Loss: 0.1450623 Test Loss: 0.3285781
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 117 | Train Loss: 0.2182716 Vali Loss: 0.1305091 Test Loss: 0.4472594
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 117 | Train Loss: 0.2164440 Vali Loss: 0.1295615 Test Loss: 0.3995494
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 117 | Train Loss: 0.2154223 Vali Loss: 0.1336101 Test Loss: 0.4125730
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 117 | Train Loss: 0.2154330 Vali Loss: 0.1321901 Test Loss: 0.4497451
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 117 | Train Loss: 0.2146145 Vali Loss: 0.1376096 Test Loss: 0.4459431
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 117 | Train Loss: 0.2141242 Vali Loss: 0.1284694 Test Loss: 0.4341790
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 117 | Train Loss: 0.2139898 Vali Loss: 0.1343362 Test Loss: 0.4381543
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 117 | Train Loss: 0.2150381 Vali Loss: 0.1352431 Test Loss: 0.4238490
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 117 | Train Loss: 0.2142092 Vali Loss: 0.1326943 Test Loss: 0.4345811
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SASOL_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 165
test shape: (165, 50, 1) (165, 50, 1)
test shape: (165, 50, 1) (165, 50, 1)


	mse:0.0024049635976552963, mae:0.03812910616397858, rmse:0.049040429294109344, r2:-0.17386162281036377, dtw:Not calculated


VAL - MSE: 0.0024, MAE: 0.0381, RMSE: 0.0490, RÂ²: -0.1739, MAPE: 34007452.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.648 MB of 0.651 MB uploadedwandb: \ 0.648 MB of 0.651 MB uploadedwandb: | 0.648 MB of 0.651 MB uploadedwandb: / 0.651 MB of 0.651 MB uploadedwandb: - 0.651 MB of 0.950 MB uploadedwandb: \ 0.651 MB of 0.950 MB uploadedwandb: | 0.950 MB of 0.950 MB uploadedwandb: / 0.950 MB of 0.950 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–†â–â–ˆâ–…â–†â–ˆâ–ˆâ–‡â–‡â–‡â–‡
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ˆâ–„â–„â–…â–„â–†â–ƒâ–…â–…â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.43458
wandb:                 train/loss 0.21421
wandb:   val/directional_accuracy 48.89301
wandb:                   val/loss 0.13269
wandb:                    val/mae 0.03813
wandb:                   val/mape 3400745200.0
wandb:                    val/mse 0.0024
wandb:                     val/r2 -0.17386
wandb:                   val/rmse 0.04904
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/yed0qlop
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_105342-yed0qlop/logs
Completed: SASOL H=50

Training: Informer on SASOL for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_105538-ua7a71wd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/ua7a71wd
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SASOL_H100 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/ua7a71wd
>>>>>>>start training : long_term_forecast_Informer_SASOL_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 100
============================================================
train 3675
Overriding target from 'OT' to 'close' for stock data
val 114
Overriding target from 'OT' to 'close' for stock data
test 115
Epoch: 1, Steps: 115 | Train Loss: 0.3474808 Vali Loss: 0.1603588 Test Loss: 0.4706765
Validation loss decreased (inf --> 0.160359).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 115 | Train Loss: 0.2768320 Vali Loss: 0.1757802 Test Loss: 0.4710779
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3474807550077853, 'val/loss': 0.16035880148410797, 'test/loss': 0.4706764593720436, '_timestamp': 1762332949.5182128}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.27683204490205515, 'val/loss': 0.17578023299574852, 'test/loss': 0.47107789292931557, '_timestamp': 1762332955.7865937}).
Epoch: 3, Steps: 115 | Train Loss: 0.2488319 Vali Loss: 0.1422803 Test Loss: 1.0694523
Validation loss decreased (0.160359 --> 0.142280).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 115 | Train Loss: 0.2341683 Vali Loss: 0.1465143 Test Loss: 1.2548440
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 115 | Train Loss: 0.2277054 Vali Loss: 0.1535468 Test Loss: 1.2444277
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 115 | Train Loss: 0.2260900 Vali Loss: 0.1544441 Test Loss: 1.1906301
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 115 | Train Loss: 0.2251168 Vali Loss: 0.1533407 Test Loss: 1.2446788
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 115 | Train Loss: 0.2248009 Vali Loss: 0.1540869 Test Loss: 1.3037087
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 115 | Train Loss: 0.2244587 Vali Loss: 0.1553618 Test Loss: 1.1752912
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 115 | Train Loss: 0.2234880 Vali Loss: 0.1546282 Test Loss: 1.2041713
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 115 | Train Loss: 0.2241411 Vali Loss: 0.1578246 Test Loss: 1.1882331
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 115 | Train Loss: 0.2231468 Vali Loss: 0.1560192 Test Loss: 1.2295829
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 115 | Train Loss: 0.2235147 Vali Loss: 0.1589976 Test Loss: 1.2204702
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SASOL_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 115
test shape: (115, 100, 1) (115, 100, 1)
test shape: (115, 100, 1) (115, 100, 1)


	mse:0.0023244130425155163, mae:0.03789474815130234, rmse:0.048212166875600815, r2:-0.171331524848938, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0379, RMSE: 0.0482, RÂ²: -0.1713, MAPE: 35830140.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.689 MB of 0.694 MB uploadedwandb: \ 0.689 MB of 0.694 MB uploadedwandb: | 0.689 MB of 0.694 MB uploadedwandb: / 0.694 MB of 0.694 MB uploadedwandb: - 0.694 MB of 0.694 MB uploadedwandb: \ 0.694 MB of 0.993 MB uploadedwandb: | 0.993 MB of 0.993 MB uploadedwandb: / 0.993 MB of 0.993 MB uploadedwandb: - 0.993 MB of 0.993 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–‡â–†â–…â–†â–ˆâ–„â–…â–…â–†â–†
wandb:                 train/loss â–ˆâ–„â–‚â–‚â–‚â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ƒâ–†â–†â–†â–†â–†â–†â–ˆâ–‡â–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 1.22047
wandb:                 train/loss 0.22351
wandb:   val/directional_accuracy 48.20378
wandb:                   val/loss 0.159
wandb:                    val/mae 0.03789
wandb:                   val/mape 3583014000.0
wandb:                    val/mse 0.00232
wandb:                     val/r2 -0.17133
wandb:                   val/rmse 0.04821
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/ua7a71wd
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_105538-ua7a71wd/logs
Completed: SASOL H=100

Informer training completed for all datasets!
