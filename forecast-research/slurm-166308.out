##############################################################################
# Training Autoformer Model on All Datasets
##############################################################################
Training: Autoformer on NVIDIA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_030309-yniczeoy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/yniczeoy
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NVIDIA_H3Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/yniczeoy
>>>>>>>start training : long_term_forecast_Autoformer_NVIDIA_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.3041071 Vali Loss: 0.1951896 Test Loss: 0.3391316
Validation loss decreased (inf --> 0.195190).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3041071155689713, 'val/loss': 0.19518960546702147, 'test/loss': 0.33913156390190125, '_timestamp': 1762304621.115214}).
Epoch: 2, Steps: 133 | Train Loss: 0.2598918 Vali Loss: 0.1839016 Test Loss: 0.3174035
Validation loss decreased (0.195190 --> 0.183902).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2463117 Vali Loss: 0.1819042 Test Loss: 0.3134404
Validation loss decreased (0.183902 --> 0.181904).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25989182024521934, 'val/loss': 0.18390158284455538, 'test/loss': 0.3174035158008337, '_timestamp': 1762304627.9310415}).
Epoch: 4, Steps: 133 | Train Loss: 0.2413227 Vali Loss: 0.1737165 Test Loss: 0.3022797
Validation loss decreased (0.181904 --> 0.173716).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2367596 Vali Loss: 0.1962830 Test Loss: 0.3026448
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2342285 Vali Loss: 0.2010931 Test Loss: 0.3022507
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2330703 Vali Loss: 0.1709814 Test Loss: 0.3029150
Validation loss decreased (0.173716 --> 0.170981).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2334865 Vali Loss: 0.1764461 Test Loss: 0.3023368
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2330682 Vali Loss: 0.1805787 Test Loss: 0.3019181
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2330712 Vali Loss: 0.1728638 Test Loss: 0.3019047
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2329950 Vali Loss: 0.1786087 Test Loss: 0.3018190
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2325706 Vali Loss: 0.1749596 Test Loss: 0.3018805
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2326175 Vali Loss: 0.1751953 Test Loss: 0.3018991
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2326047 Vali Loss: 0.1750332 Test Loss: 0.3019033
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2323925 Vali Loss: 0.1866329 Test Loss: 0.3019045
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2322076 Vali Loss: 0.1786345 Test Loss: 0.3019052
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2315506 Vali Loss: 0.1949016 Test Loss: 0.3019075
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NVIDIA_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.0011540327686816454, mae:0.025432204827666283, rmse:0.03397105634212494, r2:-0.028745412826538086, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0254, RMSE: 0.0340, RÂ²: -0.0287, MAPE: 765055.12%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.467 MB of 0.467 MB uploadedwandb: \ 0.467 MB of 0.467 MB uploadedwandb: | 0.467 MB of 0.467 MB uploadedwandb: / 0.467 MB of 0.467 MB uploadedwandb: - 0.467 MB of 0.467 MB uploadedwandb: \ 0.467 MB of 0.467 MB uploadedwandb: | 0.467 MB of 0.467 MB uploadedwandb: / 0.598 MB of 0.813 MB uploaded (0.002 MB deduped)wandb: - 0.598 MB of 0.813 MB uploaded (0.002 MB deduped)wandb: \ 0.813 MB of 0.813 MB uploaded (0.002 MB deduped)wandb: | 0.813 MB of 0.813 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–†â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–‚â–‡â–ˆâ–â–‚â–ƒâ–â–ƒâ–‚â–‚â–‚â–…â–ƒâ–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.30191
wandb:                 train/loss 0.23155
wandb:   val/directional_accuracy 49.78903
wandb:                   val/loss 0.1949
wandb:                    val/mae 0.02543
wandb:                   val/mape 76505512.5
wandb:                    val/mse 0.00115
wandb:                     val/r2 -0.02875
wandb:                   val/rmse 0.03397
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/yniczeoy
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_030309-yniczeoy/logs
Completed: NVIDIA H=3

Training: Autoformer on NVIDIA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_030602-rpsilkan
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/rpsilkan
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NVIDIA_H5Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/rpsilkan
>>>>>>>start training : long_term_forecast_Autoformer_NVIDIA_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.2974591 Vali Loss: 0.2025359 Test Loss: 0.3360861
Validation loss decreased (inf --> 0.202536).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2597452 Vali Loss: 0.1822914 Test Loss: 0.3361451
Validation loss decreased (0.202536 --> 0.182291).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29745913212908837, 'val/loss': 0.20253594405949116, 'test/loss': 0.33608606923371553, '_timestamp': 1762304775.3919752}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25974518441616146, 'val/loss': 0.1822914108633995, 'test/loss': 0.33614508900791407, '_timestamp': 1762304782.2457652}).
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29745913212908837, 'val/loss': 0.20253594405949116, 'test/loss': 0.33608606923371553, '_timestamp': 1762304775.3919752}).
Epoch: 3, Steps: 133 | Train Loss: 0.2446367 Vali Loss: 0.1813647 Test Loss: 0.3374875
Validation loss decreased (0.182291 --> 0.181365).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2399572 Vali Loss: 0.1811206 Test Loss: 0.3281510
Validation loss decreased (0.181365 --> 0.181121).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2363241 Vali Loss: 0.1787971 Test Loss: 0.3293608
Validation loss decreased (0.181121 --> 0.178797).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2356610 Vali Loss: 0.1788925 Test Loss: 0.3284750
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2357084 Vali Loss: 0.1802033 Test Loss: 0.3267760
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2350293 Vali Loss: 0.1794900 Test Loss: 0.3272763
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2343159 Vali Loss: 0.1803950 Test Loss: 0.3264976
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2347258 Vali Loss: 0.1932399 Test Loss: 0.3266611
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2355364 Vali Loss: 0.1763491 Test Loss: 0.3266465
Validation loss decreased (0.178797 --> 0.176349).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2347315 Vali Loss: 0.1809428 Test Loss: 0.3267347
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2341086 Vali Loss: 0.2010181 Test Loss: 0.3267124
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2343191 Vali Loss: 0.1951373 Test Loss: 0.3266807
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2347227 Vali Loss: 0.1942562 Test Loss: 0.3266738
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2346301 Vali Loss: 0.1930565 Test Loss: 0.3266734
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2341679 Vali Loss: 0.1865903 Test Loss: 0.3266736
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2349158 Vali Loss: 0.1950104 Test Loss: 0.3266741
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2339307 Vali Loss: 0.1860501 Test Loss: 0.3266740
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2347025 Vali Loss: 0.1798945 Test Loss: 0.3266739
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2347151 Vali Loss: 0.1791008 Test Loss: 0.3266735
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NVIDIA_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.0011718031018972397, mae:0.025821896269917488, rmse:0.03423161059617996, r2:-0.03783869743347168, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0258, RMSE: 0.0342, RÂ²: -0.0378, MAPE: 847171.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.524 MB of 0.524 MB uploadedwandb: \ 0.524 MB of 0.524 MB uploadedwandb: | 0.524 MB of 0.524 MB uploadedwandb: / 0.524 MB of 0.741 MB uploadedwandb: - 0.730 MB of 0.741 MB uploadedwandb: \ 0.741 MB of 0.741 MB uploadedwandb: | 0.741 MB of 0.741 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–‚â–â–â–‚â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–†â–â–‚â–ˆâ–†â–†â–†â–„â–†â–„â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.32667
wandb:                 train/loss 0.23472
wandb:   val/directional_accuracy 46.17021
wandb:                   val/loss 0.1791
wandb:                    val/mae 0.02582
wandb:                   val/mape 84717100.0
wandb:                    val/mse 0.00117
wandb:                     val/r2 -0.03784
wandb:                   val/rmse 0.03423
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/rpsilkan
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_030602-rpsilkan/logs
Completed: NVIDIA H=5

Training: Autoformer on NVIDIA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_030851-sahyb5u7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/sahyb5u7
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NVIDIA_H10Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/sahyb5u7
>>>>>>>start training : long_term_forecast_Autoformer_NVIDIA_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2896228 Vali Loss: 0.1963089 Test Loss: 0.3669261
Validation loss decreased (inf --> 0.196309).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2540279 Vali Loss: 0.1866587 Test Loss: 0.3626865
Validation loss decreased (0.196309 --> 0.186659).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.28962277794690955, 'val/loss': 0.19630887918174267, 'test/loss': 0.36692606657743454, '_timestamp': 1762304944.0943584}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2540278680118403, 'val/loss': 0.1866587344557047, 'test/loss': 0.36268647760152817, '_timestamp': 1762304950.9853458}).
Epoch: 3, Steps: 133 | Train Loss: 0.2445286 Vali Loss: 0.1820491 Test Loss: 0.3547021
Validation loss decreased (0.186659 --> 0.182049).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2400852 Vali Loss: 0.2060736 Test Loss: 0.3564139
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2384721 Vali Loss: 0.1925825 Test Loss: 0.3503092
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2368146 Vali Loss: 0.1746700 Test Loss: 0.3504969
Validation loss decreased (0.182049 --> 0.174670).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2366005 Vali Loss: 0.1827882 Test Loss: 0.3528080
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2368214 Vali Loss: 0.1830253 Test Loss: 0.3524328
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2372317 Vali Loss: 0.1807511 Test Loss: 0.3521083
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2367237 Vali Loss: 0.1938270 Test Loss: 0.3518615
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2363579 Vali Loss: 0.1885290 Test Loss: 0.3518619
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2363534 Vali Loss: 0.1739653 Test Loss: 0.3520883
Validation loss decreased (0.174670 --> 0.173965).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2367181 Vali Loss: 0.1998493 Test Loss: 0.3523931
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2366904 Vali Loss: 0.1842932 Test Loss: 0.3523274
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2355601 Vali Loss: 0.1912234 Test Loss: 0.3523223
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2364685 Vali Loss: 0.1752211 Test Loss: 0.3523250
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2378480 Vali Loss: 0.2128738 Test Loss: 0.3523248
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2363391 Vali Loss: 0.1731312 Test Loss: 0.3523248
Validation loss decreased (0.173965 --> 0.173131).  Saving model ...
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2357246 Vali Loss: 0.1802835 Test Loss: 0.3523252
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2367806 Vali Loss: 0.1772407 Test Loss: 0.3523251
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2365531 Vali Loss: 0.1780909 Test Loss: 0.3523254
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2365294 Vali Loss: 0.2090920 Test Loss: 0.3523252
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2374599 Vali Loss: 0.1794327 Test Loss: 0.3523250
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2368226 Vali Loss: 0.1752790 Test Loss: 0.3523252
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2359128 Vali Loss: 0.2001195 Test Loss: 0.3523253
EarlyStopping counter: 7 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2361740 Vali Loss: 0.1748557 Test Loss: 0.3523252
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.2355576 Vali Loss: 0.1745129 Test Loss: 0.3523250
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.2358712 Vali Loss: 0.1780961 Test Loss: 0.3523251
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NVIDIA_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.001174458651803434, mae:0.02590499259531498, rmse:0.03427037596702576, r2:-0.02382791042327881, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0259, RMSE: 0.0343, RÂ²: -0.0238, MAPE: 783513.06%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.561 MB of 0.562 MB uploadedwandb: \ 0.561 MB of 0.562 MB uploadedwandb: | 0.561 MB of 0.562 MB uploadedwandb: / 0.562 MB of 0.562 MB uploadedwandb: - 0.562 MB of 0.779 MB uploadedwandb: \ 0.663 MB of 0.779 MB uploadedwandb: | 0.779 MB of 0.779 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–†â–ˆâ–â–â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–ƒâ–‚â–â–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–‡â–„â–â–ƒâ–ƒâ–‚â–…â–„â–â–†â–ƒâ–„â–â–ˆâ–â–‚â–‚â–‚â–‡â–‚â–â–†â–â–â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 27
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.35233
wandb:                 train/loss 0.23587
wandb:   val/directional_accuracy 52.27053
wandb:                   val/loss 0.1781
wandb:                    val/mae 0.0259
wandb:                   val/mape 78351306.25
wandb:                    val/mse 0.00117
wandb:                     val/r2 -0.02383
wandb:                   val/rmse 0.03427
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/sahyb5u7
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_030851-sahyb5u7/logs
Completed: NVIDIA H=10

Training: Autoformer on NVIDIA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_031222-7ljs55gz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/7ljs55gz
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NVIDIA_H22Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/7ljs55gz
>>>>>>>start training : long_term_forecast_Autoformer_NVIDIA_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2827456 Vali Loss: 0.1948418 Test Loss: 0.4371982
Validation loss decreased (inf --> 0.194842).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2528017 Vali Loss: 0.1883602 Test Loss: 0.4283730
Validation loss decreased (0.194842 --> 0.188360).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2827455773949623, 'val/loss': 0.19484179147652217, 'test/loss': 0.4371981791087559, '_timestamp': 1762305156.705834}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.252801709328637, 'val/loss': 0.1883601929460253, 'test/loss': 0.4283729557480131, '_timestamp': 1762305163.6241617}).
Epoch: 3, Steps: 132 | Train Loss: 0.2457379 Vali Loss: 0.1887221 Test Loss: 0.4450546
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2425634 Vali Loss: 0.1882162 Test Loss: 0.4251661
Validation loss decreased (0.188360 --> 0.188216).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2405597 Vali Loss: 0.1881900 Test Loss: 0.4290532
Validation loss decreased (0.188216 --> 0.188190).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2396370 Vali Loss: 0.1907973 Test Loss: 0.4303394
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2393150 Vali Loss: 0.1887680 Test Loss: 0.4268830
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2388214 Vali Loss: 0.1893992 Test Loss: 0.4277163
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2386953 Vali Loss: 0.1898751 Test Loss: 0.4273261
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2386085 Vali Loss: 0.1896639 Test Loss: 0.4275250
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2384954 Vali Loss: 0.1893504 Test Loss: 0.4274857
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2383888 Vali Loss: 0.1882108 Test Loss: 0.4275845
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2383456 Vali Loss: 0.1891123 Test Loss: 0.4276091
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2384252 Vali Loss: 0.1881860 Test Loss: 0.4276144
Validation loss decreased (0.188190 --> 0.188186).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2385874 Vali Loss: 0.1902217 Test Loss: 0.4276184
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2383761 Vali Loss: 0.1892310 Test Loss: 0.4276198
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2384361 Vali Loss: 0.1885492 Test Loss: 0.4276214
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2385732 Vali Loss: 0.1895011 Test Loss: 0.4276213
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2383070 Vali Loss: 0.1879381 Test Loss: 0.4276218
Validation loss decreased (0.188186 --> 0.187938).  Saving model ...
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2384514 Vali Loss: 0.1896265 Test Loss: 0.4276218
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2384743 Vali Loss: 0.1891222 Test Loss: 0.4276216
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2385759 Vali Loss: 0.1886836 Test Loss: 0.4276215
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.2385887 Vali Loss: 0.1885593 Test Loss: 0.4276217
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 132 | Train Loss: 0.2385972 Vali Loss: 0.1907249 Test Loss: 0.4276216
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 132 | Train Loss: 0.2386831 Vali Loss: 0.1890935 Test Loss: 0.4276216
EarlyStopping counter: 6 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 132 | Train Loss: 0.2387211 Vali Loss: 0.1888613 Test Loss: 0.4276215
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 132 | Train Loss: 0.2385484 Vali Loss: 0.1880503 Test Loss: 0.4276216
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 132 | Train Loss: 0.2386695 Vali Loss: 0.1888666 Test Loss: 0.4276215
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 132 | Train Loss: 0.2381939 Vali Loss: 0.1877386 Test Loss: 0.4276215
Validation loss decreased (0.187938 --> 0.187739).  Saving model ...
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 132 | Train Loss: 0.2381466 Vali Loss: 0.1890071 Test Loss: 0.4276216
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 132 | Train Loss: 0.2385488 Vali Loss: 0.1881574 Test Loss: 0.4276215
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 132 | Train Loss: 0.2386043 Vali Loss: 0.1888369 Test Loss: 0.4276215
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 132 | Train Loss: 0.2385106 Vali Loss: 0.1877492 Test Loss: 0.4276215
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 132 | Train Loss: 0.2385468 Vali Loss: 0.1893972 Test Loss: 0.4276215
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.1641532182693482e-14
Epoch: 35, Steps: 132 | Train Loss: 0.2386006 Vali Loss: 0.1877196 Test Loss: 0.4276215
Validation loss decreased (0.187739 --> 0.187720).  Saving model ...
Updating learning rate to 5.820766091346741e-15
Epoch: 36, Steps: 132 | Train Loss: 0.2383909 Vali Loss: 0.1918933 Test Loss: 0.4276215
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.9103830456733705e-15
Epoch: 37, Steps: 132 | Train Loss: 0.2385302 Vali Loss: 0.1893129 Test Loss: 0.4276215
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.4551915228366853e-15
Epoch: 38, Steps: 132 | Train Loss: 0.2385902 Vali Loss: 0.1898540 Test Loss: 0.4276215
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.275957614183426e-16
Epoch: 39, Steps: 132 | Train Loss: 0.2384410 Vali Loss: 0.1880883 Test Loss: 0.4276215
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.637978807091713e-16
Epoch: 40, Steps: 132 | Train Loss: 0.2384940 Vali Loss: 0.1886796 Test Loss: 0.4276215
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.8189894035458566e-16
Epoch: 41, Steps: 132 | Train Loss: 0.2386704 Vali Loss: 0.1877268 Test Loss: 0.4276215
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.094947017729283e-17
Epoch: 42, Steps: 132 | Train Loss: 0.2385207 Vali Loss: 0.1892709 Test Loss: 0.4276215
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.5474735088646414e-17
Epoch: 43, Steps: 132 | Train Loss: 0.2386432 Vali Loss: 0.1886515 Test Loss: 0.4276215
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.2737367544323207e-17
Epoch: 44, Steps: 132 | Train Loss: 0.2385489 Vali Loss: 0.1879384 Test Loss: 0.4276215
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1368683772161604e-17
Epoch: 45, Steps: 132 | Train Loss: 0.2382818 Vali Loss: 0.1885735 Test Loss: 0.4276215
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NVIDIA_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.0012114342534914613, mae:0.02622671239078045, rmse:0.03480566293001175, r2:-0.026764869689941406, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0262, RMSE: 0.0348, RÂ²: -0.0268, MAPE: 400061.41%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.585 MB of 0.586 MB uploadedwandb: \ 0.585 MB of 0.586 MB uploadedwandb: | 0.585 MB of 0.586 MB uploadedwandb: / 0.586 MB of 0.586 MB uploadedwandb: - 0.586 MB of 0.807 MB uploadedwandb: \ 0.586 MB of 0.807 MB uploadedwandb: | 0.807 MB of 0.807 MB uploadedwandb: / 0.807 MB of 0.807 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–‚â–‚â–†â–ƒâ–„â–…â–„â–„â–‚â–ƒâ–‚â–…â–‚â–„â–â–„â–ƒâ–ƒâ–‚â–†â–ƒâ–ƒâ–‚â–ƒâ–â–‚â–ƒâ–â–„â–â–ˆâ–„â–…â–‚â–ƒâ–â–„â–ƒâ–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 44
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.42762
wandb:                 train/loss 0.23828
wandb:   val/directional_accuracy 50.43687
wandb:                   val/loss 0.18857
wandb:                    val/mae 0.02623
wandb:                   val/mape 40006140.625
wandb:                    val/mse 0.00121
wandb:                     val/r2 -0.02676
wandb:                   val/rmse 0.03481
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/7ljs55gz
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_031222-7ljs55gz/logs
Completed: NVIDIA H=22

Training: Autoformer on NVIDIA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_031744-lm5zpldh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/lm5zpldh
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NVIDIA_H50Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/lm5zpldh
>>>>>>>start training : long_term_forecast_Autoformer_NVIDIA_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.2879562 Vali Loss: 0.2083400 Test Loss: 0.5586436
Validation loss decreased (inf --> 0.208340).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2601156 Vali Loss: 0.2106600 Test Loss: 0.5675657
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2879562284232992, 'val/loss': 0.20834004630645117, 'test/loss': 0.5586436241865158, '_timestamp': 1762305477.7656703}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2601156083471847, 'val/loss': 0.21065996338923773, 'test/loss': 0.5675656944513321, '_timestamp': 1762305484.6288035}).
Epoch: 3, Steps: 132 | Train Loss: 0.2536865 Vali Loss: 0.2035299 Test Loss: 0.5438865
Validation loss decreased (0.208340 --> 0.203530).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2513566 Vali Loss: 0.2055419 Test Loss: 0.5532441
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2498255 Vali Loss: 0.2061172 Test Loss: 0.5373503
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2483722 Vali Loss: 0.2064393 Test Loss: 0.5350762
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2477931 Vali Loss: 0.2064942 Test Loss: 0.5357829
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2475574 Vali Loss: 0.2075875 Test Loss: 0.5359164
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2477707 Vali Loss: 0.2069636 Test Loss: 0.5365542
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2477018 Vali Loss: 0.2077251 Test Loss: 0.5368447
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2474495 Vali Loss: 0.2076623 Test Loss: 0.5369729
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2471662 Vali Loss: 0.2076524 Test Loss: 0.5370584
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2469056 Vali Loss: 0.2077404 Test Loss: 0.5370321
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NVIDIA_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.0012080897577106953, mae:0.026512056589126587, rmse:0.0347575843334198, r2:-0.015208244323730469, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0265, RMSE: 0.0348, RÂ²: -0.0152, MAPE: 334672.09%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.631 MB of 0.634 MB uploadedwandb: \ 0.631 MB of 0.634 MB uploadedwandb: | 0.631 MB of 0.634 MB uploadedwandb: / 0.634 MB of 0.634 MB uploadedwandb: - 0.634 MB of 0.849 MB uploadedwandb: \ 0.849 MB of 0.849 MB uploadedwandb: | 0.849 MB of 0.849 MB uploadedwandb: / 0.849 MB of 0.849 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–„â–ˆâ–‚â–â–â–â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–†â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–„â–…â–†â–†â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.53703
wandb:                 train/loss 0.24691
wandb:   val/directional_accuracy 51.28894
wandb:                   val/loss 0.20774
wandb:                    val/mae 0.02651
wandb:                   val/mape 33467209.375
wandb:                    val/mse 0.00121
wandb:                     val/r2 -0.01521
wandb:                   val/rmse 0.03476
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/lm5zpldh
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_031744-lm5zpldh/logs
Completed: NVIDIA H=50

Training: Autoformer on NVIDIA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_031943-axe42s9i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/axe42s9i
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NVIDIA_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/axe42s9i
>>>>>>>start training : long_term_forecast_Autoformer_NVIDIA_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.3054019 Vali Loss: 0.2303091 Test Loss: 0.8291395
Validation loss decreased (inf --> 0.230309).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2802191 Vali Loss: 0.2287480 Test Loss: 0.8282070
Validation loss decreased (0.230309 --> 0.228748).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.30540187725654017, 'val/loss': 0.2303090661764145, 'test/loss': 0.8291395366191864, '_timestamp': 1762305595.8300343}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.28021914339982545, 'val/loss': 0.22874801754951476, 'test/loss': 0.8282069802284241, '_timestamp': 1762305602.9973667}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.28021914339982545, 'val/loss': 0.22874801754951476, 'test/loss': 0.8282069802284241, '_timestamp': 1762305602.9973667}).
Epoch: 3, Steps: 130 | Train Loss: 0.2755358 Vali Loss: 0.2373757 Test Loss: 0.8075909
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2734031 Vali Loss: 0.2284191 Test Loss: 0.8197070
Validation loss decreased (0.228748 --> 0.228419).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2721373 Vali Loss: 0.2356133 Test Loss: 0.8149086
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2715898 Vali Loss: 0.2368629 Test Loss: 0.8119760
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2718637 Vali Loss: 0.2423611 Test Loss: 0.8200042
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2715518 Vali Loss: 0.2385793 Test Loss: 0.8175332
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2715472 Vali Loss: 0.2333574 Test Loss: 0.8176839
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2712464 Vali Loss: 0.2371191 Test Loss: 0.8177166
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2713411 Vali Loss: 0.2408910 Test Loss: 0.8176183
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2715080 Vali Loss: 0.2374094 Test Loss: 0.8175796
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2714388 Vali Loss: 0.2391464 Test Loss: 0.8175748
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.2712569 Vali Loss: 0.2397065 Test Loss: 0.8175775
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NVIDIA_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0013063044752925634, mae:0.027484353631734848, rmse:0.03614283353090286, r2:-0.014672160148620605, dtw:Not calculated


VAL - MSE: 0.0013, MAE: 0.0275, RMSE: 0.0361, RÂ²: -0.0147, MAPE: 237263.31%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.682 MB of 0.687 MB uploadedwandb: \ 0.682 MB of 0.687 MB uploadedwandb: | 0.687 MB of 0.687 MB uploadedwandb: / 0.687 MB of 0.687 MB uploadedwandb: - 0.687 MB of 0.902 MB uploadedwandb: \ 0.902 MB of 0.902 MB uploadedwandb: | 0.902 MB of 0.902 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–…â–ƒâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:                 train/loss â–ˆâ–…â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–â–…â–…â–ˆâ–†â–ƒâ–…â–‡â–†â–†â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 13
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.81758
wandb:                 train/loss 0.27126
wandb:   val/directional_accuracy 49.75469
wandb:                   val/loss 0.23971
wandb:                    val/mae 0.02748
wandb:                   val/mape 23726331.25
wandb:                    val/mse 0.00131
wandb:                     val/r2 -0.01467
wandb:                   val/rmse 0.03614
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/axe42s9i
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_031943-axe42s9i/logs
Completed: NVIDIA H=100

Training: Autoformer on APPLE for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_032147-cr9nqde4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/cr9nqde4
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_APPLE_H3 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/cr9nqde4
>>>>>>>start training : long_term_forecast_Autoformer_APPLE_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.2977459 Vali Loss: 0.0966506 Test Loss: 0.1430472
Validation loss decreased (inf --> 0.096651).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2530756 Vali Loss: 0.0969880 Test Loss: 0.1429442
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2977459075531565, 'val/loss': 0.09665060136467218, 'test/loss': 0.1430472396314144, '_timestamp': 1762305720.597705}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2530756069529325, 'val/loss': 0.09698801580816507, 'test/loss': 0.14294417388737202, '_timestamp': 1762305727.616508}).
Epoch: 3, Steps: 133 | Train Loss: 0.2357228 Vali Loss: 0.0885444 Test Loss: 0.1392749
Validation loss decreased (0.096651 --> 0.088544).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2294803 Vali Loss: 0.0892474 Test Loss: 0.1349460
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2246543 Vali Loss: 0.0861016 Test Loss: 0.1343906
Validation loss decreased (0.088544 --> 0.086102).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2238960 Vali Loss: 0.0826414 Test Loss: 0.1332904
Validation loss decreased (0.086102 --> 0.082641).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2224907 Vali Loss: 0.0850544 Test Loss: 0.1331430
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2223560 Vali Loss: 0.0851160 Test Loss: 0.1333678
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2219864 Vali Loss: 0.0857963 Test Loss: 0.1328316
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2213822 Vali Loss: 0.0835007 Test Loss: 0.1327585
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2221330 Vali Loss: 0.0848264 Test Loss: 0.1327086
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2225805 Vali Loss: 0.0893141 Test Loss: 0.1326832
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2219944 Vali Loss: 0.0866423 Test Loss: 0.1326898
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2219076 Vali Loss: 0.0879858 Test Loss: 0.1326845
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2217183 Vali Loss: 0.0892901 Test Loss: 0.1326812
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2213878 Vali Loss: 0.0829562 Test Loss: 0.1326812
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_APPLE_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.00020874376059509814, mae:0.010620839893817902, rmse:0.014447967521846294, r2:-0.044017672538757324, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0106, RMSE: 0.0144, RÂ²: -0.0440, MAPE: 1025444.81%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.469 MB of 0.469 MB uploadedwandb: \ 0.469 MB of 0.469 MB uploadedwandb: | 0.469 MB of 0.469 MB uploadedwandb: / 0.469 MB of 0.469 MB uploadedwandb: - 0.469 MB of 0.469 MB uploadedwandb: \ 0.469 MB of 0.469 MB uploadedwandb: | 0.469 MB of 0.469 MB uploadedwandb: / 0.599 MB of 0.815 MB uploaded (0.002 MB deduped)wandb: - 0.761 MB of 0.815 MB uploaded (0.002 MB deduped)wandb: \ 0.815 MB of 0.815 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–ƒâ–‚â–â–‚â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–ˆâ–…â–â–„â–„â–„â–‚â–ƒâ–ˆâ–…â–‡â–ˆâ–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.13268
wandb:                 train/loss 0.22139
wandb:   val/directional_accuracy 48.52321
wandb:                   val/loss 0.08296
wandb:                    val/mae 0.01062
wandb:                   val/mape 102544481.25
wandb:                    val/mse 0.00021
wandb:                     val/r2 -0.04402
wandb:                   val/rmse 0.01445
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/cr9nqde4
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_032147-cr9nqde4/logs
Completed: APPLE H=3

Training: Autoformer on APPLE for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_032407-45mqc3yt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/45mqc3yt
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_APPLE_H5 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/45mqc3yt
>>>>>>>start training : long_term_forecast_Autoformer_APPLE_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.2882580 Vali Loss: 0.0955478 Test Loss: 0.1452196
Validation loss decreased (inf --> 0.095548).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2494177 Vali Loss: 0.0899837 Test Loss: 0.1440144
Validation loss decreased (0.095548 --> 0.089984).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.28825801610946655, 'val/loss': 0.09554780647158623, 'test/loss': 0.14521960075944662, '_timestamp': 1762305860.7943656}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24941767404850265, 'val/loss': 0.08998373430222273, 'test/loss': 0.14401442930102348, '_timestamp': 1762305867.826523}).
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.28825801610946655, 'val/loss': 0.09554780647158623, 'test/loss': 0.14521960075944662, '_timestamp': 1762305860.7943656}).
Epoch: 3, Steps: 133 | Train Loss: 0.2364687 Vali Loss: 0.0912505 Test Loss: 0.1436067
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2313922 Vali Loss: 0.0893447 Test Loss: 0.1392387
Validation loss decreased (0.089984 --> 0.089345).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2297294 Vali Loss: 0.0900216 Test Loss: 0.1367061
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2262313 Vali Loss: 0.0857646 Test Loss: 0.1371126
Validation loss decreased (0.089345 --> 0.085765).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2272258 Vali Loss: 0.0851610 Test Loss: 0.1368926
Validation loss decreased (0.085765 --> 0.085161).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2270113 Vali Loss: 0.0871651 Test Loss: 0.1366272
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2273913 Vali Loss: 0.0855856 Test Loss: 0.1364755
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2265541 Vali Loss: 0.0845608 Test Loss: 0.1363287
Validation loss decreased (0.085161 --> 0.084561).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2261083 Vali Loss: 0.0845788 Test Loss: 0.1362367
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2269718 Vali Loss: 0.0850356 Test Loss: 0.1362621
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2257165 Vali Loss: 0.0851924 Test Loss: 0.1362993
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2261621 Vali Loss: 0.0846052 Test Loss: 0.1362990
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2265711 Vali Loss: 0.0851961 Test Loss: 0.1362967
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2264699 Vali Loss: 0.0841106 Test Loss: 0.1362949
Validation loss decreased (0.084561 --> 0.084111).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2258957 Vali Loss: 0.0856529 Test Loss: 0.1362951
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2251396 Vali Loss: 0.0846213 Test Loss: 0.1362954
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2263736 Vali Loss: 0.0854292 Test Loss: 0.1362951
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2260946 Vali Loss: 0.0882000 Test Loss: 0.1362952
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2274391 Vali Loss: 0.0846416 Test Loss: 0.1362956
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2260866 Vali Loss: 0.0883487 Test Loss: 0.1362952
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2262210 Vali Loss: 0.0867402 Test Loss: 0.1362950
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2259765 Vali Loss: 0.0840258 Test Loss: 0.1362953
Validation loss decreased (0.084111 --> 0.084026).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2260640 Vali Loss: 0.0862711 Test Loss: 0.1362952
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2260494 Vali Loss: 0.0846210 Test Loss: 0.1362951
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.2261980 Vali Loss: 0.0841334 Test Loss: 0.1362953
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.2267103 Vali Loss: 0.0850171 Test Loss: 0.1362951
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.2264491 Vali Loss: 0.0858405 Test Loss: 0.1362953
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 133 | Train Loss: 0.2254865 Vali Loss: 0.0871772 Test Loss: 0.1362952
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 133 | Train Loss: 0.2256935 Vali Loss: 0.0861238 Test Loss: 0.1362953
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 133 | Train Loss: 0.2261248 Vali Loss: 0.0850602 Test Loss: 0.1362953
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 133 | Train Loss: 0.2258820 Vali Loss: 0.0877206 Test Loss: 0.1362953
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 133 | Train Loss: 0.2265096 Vali Loss: 0.0849079 Test Loss: 0.1362953
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_APPLE_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.00021334884513635188, mae:0.010591539554297924, rmse:0.014606465585529804, r2:-0.0621790885925293, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0106, RMSE: 0.0146, RÂ²: -0.0622, MAPE: 284651.44%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.488 MB of 0.522 MB uploadedwandb: \ 0.488 MB of 0.522 MB uploadedwandb: | 0.522 MB of 0.522 MB uploadedwandb: / 0.522 MB of 0.522 MB uploadedwandb: - 0.522 MB of 0.740 MB uploadedwandb: \ 0.740 MB of 0.740 MB uploadedwandb: | 0.740 MB of 0.740 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–‡â–ƒâ–‚â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–ƒâ–‚â–‚â–…â–‚â–…â–„â–â–ƒâ–‚â–â–‚â–ƒâ–„â–ƒâ–‚â–…â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 33
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.1363
wandb:                 train/loss 0.22651
wandb:   val/directional_accuracy 44.68085
wandb:                   val/loss 0.08491
wandb:                    val/mae 0.01059
wandb:                   val/mape 28465143.75
wandb:                    val/mse 0.00021
wandb:                     val/r2 -0.06218
wandb:                   val/rmse 0.01461
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/45mqc3yt
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_032407-45mqc3yt/logs
Completed: APPLE H=5

Training: Autoformer on APPLE for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_032823-c2mpbcjq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/c2mpbcjq
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_APPLE_H10Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/c2mpbcjq
>>>>>>>start training : long_term_forecast_Autoformer_APPLE_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2774969 Vali Loss: 0.1027227 Test Loss: 0.1470551
Validation loss decreased (inf --> 0.102723).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2446196 Vali Loss: 0.0970829 Test Loss: 0.1401247
Validation loss decreased (0.102723 --> 0.097083).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.27749692262115333, 'val/loss': 0.10272268299013376, 'test/loss': 0.14705510716885328, '_timestamp': 1762306117.9597363}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24461955829222398, 'val/loss': 0.09708293061703444, 'test/loss': 0.14012472331523895, '_timestamp': 1762306124.8667674}).
Epoch: 3, Steps: 133 | Train Loss: 0.2350076 Vali Loss: 0.0925856 Test Loss: 0.1435915
Validation loss decreased (0.097083 --> 0.092586).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2294169 Vali Loss: 0.0953050 Test Loss: 0.1393720
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2277939 Vali Loss: 0.0910665 Test Loss: 0.1403388
Validation loss decreased (0.092586 --> 0.091067).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2255514 Vali Loss: 0.0911514 Test Loss: 0.1392741
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2250117 Vali Loss: 0.0904690 Test Loss: 0.1387602
Validation loss decreased (0.091067 --> 0.090469).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2252416 Vali Loss: 0.0892013 Test Loss: 0.1391634
Validation loss decreased (0.090469 --> 0.089201).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2256406 Vali Loss: 0.0892750 Test Loss: 0.1386702
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2242806 Vali Loss: 0.0909825 Test Loss: 0.1383149
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2243103 Vali Loss: 0.0885372 Test Loss: 0.1382818
Validation loss decreased (0.089201 --> 0.088537).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2237666 Vali Loss: 0.0933207 Test Loss: 0.1383021
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2247477 Vali Loss: 0.0900591 Test Loss: 0.1382957
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2251468 Vali Loss: 0.0917079 Test Loss: 0.1383052
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2238496 Vali Loss: 0.0897896 Test Loss: 0.1383053
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2244497 Vali Loss: 0.0907528 Test Loss: 0.1383738
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2252091 Vali Loss: 0.0906143 Test Loss: 0.1383741
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2238669 Vali Loss: 0.0916843 Test Loss: 0.1383738
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2243029 Vali Loss: 0.0910609 Test Loss: 0.1383739
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2249003 Vali Loss: 0.0906281 Test Loss: 0.1383736
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2257333 Vali Loss: 0.0908578 Test Loss: 0.1383738
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_APPLE_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.00021889241179451346, mae:0.010740987956523895, rmse:0.014795012772083282, r2:-0.08112776279449463, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0107, RMSE: 0.0148, RÂ²: -0.0811, MAPE: 626384.44%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.404 MB of 0.583 MB uploadedwandb: \ 0.404 MB of 0.583 MB uploadedwandb: | 0.404 MB of 0.583 MB uploadedwandb: / 0.404 MB of 0.583 MB uploadedwandb: - 0.404 MB of 0.583 MB uploadedwandb: \ 0.404 MB of 0.800 MB uploadedwandb: | 0.621 MB of 0.800 MB uploadedwandb: / 0.621 MB of 0.800 MB uploadedwandb: - 0.621 MB of 0.800 MB uploadedwandb: \ 0.621 MB of 0.800 MB uploadedwandb: | 0.621 MB of 0.800 MB uploadedwandb: / 0.621 MB of 0.800 MB uploadedwandb: - 0.621 MB of 0.800 MB uploadedwandb: \ 0.621 MB of 0.800 MB uploadedwandb: | 0.621 MB of 0.800 MB uploadedwandb: / 0.621 MB of 0.800 MB uploadedwandb: - 0.621 MB of 0.800 MB uploadedwandb: \ 0.621 MB of 0.800 MB uploadedwandb: | 0.621 MB of 0.800 MB uploadedwandb: / 0.800 MB of 0.800 MB uploadedwandb: - 0.800 MB of 0.800 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–â–â–â–‚â–‚â–â–â–‚â–â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–ˆâ–„â–„â–ƒâ–‚â–‚â–„â–â–†â–ƒâ–„â–‚â–ƒâ–ƒâ–„â–„â–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.13837
wandb:                 train/loss 0.22573
wandb:   val/directional_accuracy 50.14493
wandb:                   val/loss 0.09086
wandb:                    val/mae 0.01074
wandb:                   val/mape 62638443.75
wandb:                    val/mse 0.00022
wandb:                     val/r2 -0.08113
wandb:                   val/rmse 0.0148
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/c2mpbcjq
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_032823-c2mpbcjq/logs
Completed: APPLE H=10

Training: Autoformer on APPLE for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_033130-zv01bp7o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/zv01bp7o
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_APPLE_H22Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/zv01bp7o
>>>>>>>start training : long_term_forecast_Autoformer_APPLE_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2778098 Vali Loss: 0.0946688 Test Loss: 0.1386285
Validation loss decreased (inf --> 0.094669).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2467058 Vali Loss: 0.0918039 Test Loss: 0.1357589
Validation loss decreased (0.094669 --> 0.091804).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2778098354059638, 'val/loss': 0.0946687821831022, 'test/loss': 0.1386285200715065, '_timestamp': 1762306303.3477082}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24670575773625664, 'val/loss': 0.09180391260555812, 'test/loss': 0.13575890873159682, '_timestamp': 1762306310.3093739}).
Epoch: 3, Steps: 132 | Train Loss: 0.2389133 Vali Loss: 0.0920411 Test Loss: 0.1407657
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2352495 Vali Loss: 0.0922461 Test Loss: 0.1418669
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2334501 Vali Loss: 0.0902620 Test Loss: 0.1395311
Validation loss decreased (0.091804 --> 0.090262).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2319935 Vali Loss: 0.0901175 Test Loss: 0.1409302
Validation loss decreased (0.090262 --> 0.090117).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2316977 Vali Loss: 0.0898691 Test Loss: 0.1405742
Validation loss decreased (0.090117 --> 0.089869).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2311760 Vali Loss: 0.0904338 Test Loss: 0.1406200
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2312152 Vali Loss: 0.0902834 Test Loss: 0.1406125
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2308099 Vali Loss: 0.0906016 Test Loss: 0.1405908
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2311513 Vali Loss: 0.0902826 Test Loss: 0.1406797
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2307744 Vali Loss: 0.0905059 Test Loss: 0.1406794
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2304723 Vali Loss: 0.0904976 Test Loss: 0.1406289
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2306059 Vali Loss: 0.0902317 Test Loss: 0.1406282
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2309456 Vali Loss: 0.0901596 Test Loss: 0.1406326
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2308133 Vali Loss: 0.0902618 Test Loss: 0.1406419
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2311489 Vali Loss: 0.0904623 Test Loss: 0.1406425
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_APPLE_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.0002226932701887563, mae:0.010784906335175037, rmse:0.014922911301255226, r2:-0.0731973648071289, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0108, RMSE: 0.0149, RÂ²: -0.0732, MAPE: 341995.81%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.657 MB of 0.658 MB uploadedwandb: \ 0.657 MB of 0.658 MB uploadedwandb: | 0.658 MB of 0.658 MB uploadedwandb: / 0.658 MB of 0.658 MB uploadedwandb: - 0.658 MB of 0.874 MB uploadedwandb: \ 0.874 MB of 0.874 MB uploadedwandb: | 0.874 MB of 0.874 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–ˆâ–â–…â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–ˆâ–‚â–‚â–â–ƒâ–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.14064
wandb:                 train/loss 0.23115
wandb:   val/directional_accuracy 49.27916
wandb:                   val/loss 0.09046
wandb:                    val/mae 0.01078
wandb:                   val/mape 34199581.25
wandb:                    val/mse 0.00022
wandb:                     val/r2 -0.0732
wandb:                   val/rmse 0.01492
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/zv01bp7o
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_033130-zv01bp7o/logs
Completed: APPLE H=22

Training: Autoformer on APPLE for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_033355-unrfv6o0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/unrfv6o0
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_APPLE_H50Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/unrfv6o0
>>>>>>>start training : long_term_forecast_Autoformer_APPLE_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.2839284 Vali Loss: 0.1002721 Test Loss: 0.1611214
Validation loss decreased (inf --> 0.100272).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2548239 Vali Loss: 0.0961139 Test Loss: 0.1555233
Validation loss decreased (0.100272 --> 0.096114).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2839284295385534, 'val/loss': 0.10027208800117175, 'test/loss': 0.16112137585878372, '_timestamp': 1762306447.7613888}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25482386339342955, 'val/loss': 0.09611387178301811, 'test/loss': 0.1555233101050059, '_timestamp': 1762306454.8443005}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25482386339342955, 'val/loss': 0.09611387178301811, 'test/loss': 0.1555233101050059, '_timestamp': 1762306454.8443005}).
Epoch: 3, Steps: 132 | Train Loss: 0.2450044 Vali Loss: 0.0942673 Test Loss: 0.1545592
Validation loss decreased (0.096114 --> 0.094267).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2419354 Vali Loss: 0.0930889 Test Loss: 0.1561792
Validation loss decreased (0.094267 --> 0.093089).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2406017 Vali Loss: 0.0909053 Test Loss: 0.1534958
Validation loss decreased (0.093089 --> 0.090905).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2394408 Vali Loss: 0.0929471 Test Loss: 0.1557990
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2383848 Vali Loss: 0.0934847 Test Loss: 0.1565549
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2378540 Vali Loss: 0.0934857 Test Loss: 0.1563623
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2384113 Vali Loss: 0.0937693 Test Loss: 0.1567837
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2385875 Vali Loss: 0.0938236 Test Loss: 0.1568964
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2386374 Vali Loss: 0.0937533 Test Loss: 0.1567288
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2380865 Vali Loss: 0.0936249 Test Loss: 0.1566582
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2380385 Vali Loss: 0.0936498 Test Loss: 0.1566124
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2374199 Vali Loss: 0.0937258 Test Loss: 0.1566114
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2377335 Vali Loss: 0.0936547 Test Loss: 0.1566047
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_APPLE_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.0002359730569878593, mae:0.011133214458823204, rmse:0.015361414290964603, r2:-0.06354045867919922, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0111, RMSE: 0.0154, RÂ²: -0.0635, MAPE: 368584.66%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.730 MB of 0.732 MB uploadedwandb: \ 0.730 MB of 0.732 MB uploadedwandb: | 0.732 MB of 0.732 MB uploadedwandb: / 0.732 MB of 0.948 MB uploadedwandb: - 0.732 MB of 0.948 MB uploadedwandb: \ 0.948 MB of 0.948 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ƒâ–‡â–â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡
wandb:                 train/loss â–ˆâ–…â–„â–ƒâ–‚â–â–‚â–‚â–‚â–‚â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.1566
wandb:                 train/loss 0.23773
wandb:   val/directional_accuracy 48.24919
wandb:                   val/loss 0.09365
wandb:                    val/mae 0.01113
wandb:                   val/mape 36858465.625
wandb:                    val/mse 0.00024
wandb:                     val/r2 -0.06354
wandb:                   val/rmse 0.01536
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/unrfv6o0
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_033355-unrfv6o0/logs
Completed: APPLE H=50

Training: Autoformer on APPLE for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_033607-duv4v91o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/duv4v91o
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_APPLE_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/duv4v91o
>>>>>>>start training : long_term_forecast_Autoformer_APPLE_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.2972880 Vali Loss: 0.0995324 Test Loss: 0.1655734
Validation loss decreased (inf --> 0.099532).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2709301 Vali Loss: 0.0952674 Test Loss: 0.1651530
Validation loss decreased (0.099532 --> 0.095267).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29728796768646976, 'val/loss': 0.09953241348266602, 'test/loss': 0.1655734211206436, '_timestamp': 1762306579.5821173}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2709301193173115, 'val/loss': 0.09526742100715638, 'test/loss': 0.16515299081802368, '_timestamp': 1762306586.4856846}).
Epoch: 3, Steps: 130 | Train Loss: 0.2642930 Vali Loss: 0.0998392 Test Loss: 0.1680554
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2614498 Vali Loss: 0.0970257 Test Loss: 0.1680686
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2599127 Vali Loss: 0.0972784 Test Loss: 0.1692739
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2595719 Vali Loss: 0.0977064 Test Loss: 0.1675165
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2590939 Vali Loss: 0.0982818 Test Loss: 0.1684128
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2583011 Vali Loss: 0.0984717 Test Loss: 0.1676188
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2582373 Vali Loss: 0.0976655 Test Loss: 0.1679335
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2580839 Vali Loss: 0.0979038 Test Loss: 0.1678759
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2583388 Vali Loss: 0.0979041 Test Loss: 0.1677444
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2585419 Vali Loss: 0.0981608 Test Loss: 0.1677744
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_APPLE_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.00024839333491399884, mae:0.011314782314002514, rmse:0.015760499984025955, r2:-0.05201590061187744, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0113, RMSE: 0.0158, RÂ²: -0.0520, MAPE: 977528.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.729 MB of 0.734 MB uploadedwandb: \ 0.729 MB of 0.734 MB uploadedwandb: | 0.729 MB of 0.734 MB uploadedwandb: / 0.734 MB of 0.734 MB uploadedwandb: - 0.734 MB of 0.949 MB uploadedwandb: \ 0.734 MB of 0.949 MB uploadedwandb: | 0.949 MB of 0.949 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ƒâ–ƒâ–ˆâ–â–…â–â–ƒâ–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–‚â–ƒâ–„â–…â–ƒâ–ƒâ–ƒâ–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.16777
wandb:                 train/loss 0.25854
wandb:   val/directional_accuracy 48.99711
wandb:                   val/loss 0.09816
wandb:                    val/mae 0.01131
wandb:                   val/mape 97752875.0
wandb:                    val/mse 0.00025
wandb:                     val/r2 -0.05202
wandb:                   val/rmse 0.01576
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/duv4v91o
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_033607-duv4v91o/logs
Completed: APPLE H=100

Training: Autoformer on SP500 for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_033758-4vocbmkh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/4vocbmkh
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SP500_H3 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/4vocbmkh
>>>>>>>start training : long_term_forecast_Autoformer_SP500_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 3
============================================================
train 4241
val 237
test 238
Epoch: 1, Steps: 133 | Train Loss: 0.2429843 Vali Loss: 0.0952922 Test Loss: 0.1066584
Validation loss decreased (inf --> 0.095292).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.24298425293282458, 'val/loss': 0.09529215656220913, 'test/loss': 0.10665836278349161, '_timestamp': 1762306691.6340654}).
Epoch: 2, Steps: 133 | Train Loss: 0.2058276 Vali Loss: 0.0754469 Test Loss: 0.0785746
Validation loss decreased (0.095292 --> 0.075447).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.20582759145059085, 'val/loss': 0.07544690649956465, 'test/loss': 0.0785746150650084, '_timestamp': 1762306698.7434366}).
Epoch: 3, Steps: 133 | Train Loss: 0.1935405 Vali Loss: 0.0737432 Test Loss: 0.0790757
Validation loss decreased (0.075447 --> 0.073743).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1884929 Vali Loss: 0.0682752 Test Loss: 0.0747063
Validation loss decreased (0.073743 --> 0.068275).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1861127 Vali Loss: 0.0695079 Test Loss: 0.0744310
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1848808 Vali Loss: 0.0670731 Test Loss: 0.0753209
Validation loss decreased (0.068275 --> 0.067073).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1851637 Vali Loss: 0.0686886 Test Loss: 0.0741931
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1836783 Vali Loss: 0.0690364 Test Loss: 0.0740071
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1830544 Vali Loss: 0.0698589 Test Loss: 0.0737824
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1847197 Vali Loss: 0.0694053 Test Loss: 0.0738349
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1830441 Vali Loss: 0.0682477 Test Loss: 0.0737988
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1831012 Vali Loss: 0.0684983 Test Loss: 0.0737916
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1835681 Vali Loss: 0.0663511 Test Loss: 0.0737693
Validation loss decreased (0.067073 --> 0.066351).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1828281 Vali Loss: 0.0682932 Test Loss: 0.0737807
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1825822 Vali Loss: 0.0716302 Test Loss: 0.0737834
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1844565 Vali Loss: 0.0693553 Test Loss: 0.0737993
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1821313 Vali Loss: 0.0680434 Test Loss: 0.0737992
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1830571 Vali Loss: 0.0678284 Test Loss: 0.0737997
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1829800 Vali Loss: 0.0700148 Test Loss: 0.0737999
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1828784 Vali Loss: 0.0688358 Test Loss: 0.0737996
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1834797 Vali Loss: 0.0681648 Test Loss: 0.0738001
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1838643 Vali Loss: 0.0691249 Test Loss: 0.0738000
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1847221 Vali Loss: 0.0689960 Test Loss: 0.0737998
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SP500_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 238
test shape: (238, 3, 1) (238, 3, 1)
test shape: (238, 3, 1) (238, 3, 1)


	mse:6.638374179601669e-05, mae:0.006026967894285917, rmse:0.008147621527314186, r2:-0.021111488342285156, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0060, RMSE: 0.0081, RÂ²: -0.0211, MAPE: 2.19%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.490 MB of 0.490 MB uploadedwandb: \ 0.490 MB of 0.490 MB uploadedwandb: | 0.490 MB of 0.490 MB uploadedwandb: / 0.490 MB of 0.490 MB uploadedwandb: - 0.490 MB of 0.490 MB uploadedwandb: \ 0.490 MB of 0.490 MB uploadedwandb: | 0.490 MB of 0.490 MB uploadedwandb: / 0.490 MB of 0.490 MB uploadedwandb: - 0.621 MB of 0.837 MB uploaded (0.002 MB deduped)wandb: \ 0.837 MB of 0.837 MB uploaded (0.002 MB deduped)wandb: | 0.837 MB of 0.837 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–‚â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–â–â–‚â–â–‚â–‚â–â–‚â–‚â–ƒ
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–„â–‚â–ƒâ–„â–„â–„â–ƒâ–ƒâ–â–ƒâ–†â–„â–ƒâ–‚â–„â–ƒâ–ƒâ–„â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.0738
wandb:                 train/loss 0.18472
wandb:   val/directional_accuracy 51.89076
wandb:                   val/loss 0.069
wandb:                    val/mae 0.00603
wandb:                   val/mape 218.75553
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.02111
wandb:                   val/rmse 0.00815
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/4vocbmkh
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_033758-4vocbmkh/logs
Completed: SP500 H=3

Training: Autoformer on SP500 for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_034116-8tp54o4m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/8tp54o4m
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SP500_H5 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/8tp54o4m
>>>>>>>start training : long_term_forecast_Autoformer_SP500_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 5
============================================================
train 4239
val 235
test 236
Epoch: 1, Steps: 133 | Train Loss: 0.2376391 Vali Loss: 0.0925028 Test Loss: 0.1064454
Validation loss decreased (inf --> 0.092503).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23763913790086158, 'val/loss': 0.092502829618752, 'test/loss': 0.10644535068422556, '_timestamp': 1762306889.6922379}).
Epoch: 2, Steps: 133 | Train Loss: 0.1999260 Vali Loss: 0.0736941 Test Loss: 0.0838923
Validation loss decreased (0.092503 --> 0.073694).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.1873395 Vali Loss: 0.0748921 Test Loss: 0.0801365
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19992598835238837, 'val/loss': 0.07369413692504168, 'test/loss': 0.08389230165630579, '_timestamp': 1762306899.7508295}).
Epoch: 4, Steps: 133 | Train Loss: 0.1836883 Vali Loss: 0.0727723 Test Loss: 0.0792148
Validation loss decreased (0.073694 --> 0.072772).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1810970 Vali Loss: 0.0716268 Test Loss: 0.0797504
Validation loss decreased (0.072772 --> 0.071627).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1800660 Vali Loss: 0.0696897 Test Loss: 0.0794828
Validation loss decreased (0.071627 --> 0.069690).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1785557 Vali Loss: 0.0691234 Test Loss: 0.0784747
Validation loss decreased (0.069690 --> 0.069123).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1786429 Vali Loss: 0.0704514 Test Loss: 0.0783717
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1795459 Vali Loss: 0.0685140 Test Loss: 0.0784568
Validation loss decreased (0.069123 --> 0.068514).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1783119 Vali Loss: 0.0702636 Test Loss: 0.0783704
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1782923 Vali Loss: 0.0706658 Test Loss: 0.0783274
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1785907 Vali Loss: 0.0699898 Test Loss: 0.0783163
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1790048 Vali Loss: 0.0706161 Test Loss: 0.0783261
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1779803 Vali Loss: 0.0696744 Test Loss: 0.0783244
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1783883 Vali Loss: 0.0686805 Test Loss: 0.0783263
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1783948 Vali Loss: 0.0694443 Test Loss: 0.0783287
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1776619 Vali Loss: 0.0721019 Test Loss: 0.0783295
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1782540 Vali Loss: 0.0705197 Test Loss: 0.0783295
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1784687 Vali Loss: 0.0724527 Test Loss: 0.0783296
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SP500_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
test shape: (236, 5, 1) (236, 5, 1)
test shape: (236, 5, 1) (236, 5, 1)


	mse:6.935843703104183e-05, mae:0.006229704711586237, rmse:0.008328171446919441, r2:-0.06768882274627686, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0062, RMSE: 0.0083, RÂ²: -0.0677, MAPE: 2.74%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.526 MB of 0.527 MB uploadedwandb: \ 0.526 MB of 0.527 MB uploadedwandb: | 0.527 MB of 0.527 MB uploadedwandb: / 0.527 MB of 0.743 MB uploadedwandb: - 0.743 MB of 0.743 MB uploadedwandb: \ 0.743 MB of 0.743 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‡â–…â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–‚â–‚â–â–‚â–‚â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–„â–‚â–‚â–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–â–‚â–…â–ƒâ–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 18
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.07833
wandb:                 train/loss 0.17847
wandb:   val/directional_accuracy 47.77542
wandb:                   val/loss 0.07245
wandb:                    val/mae 0.00623
wandb:                   val/mape 273.63961
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.06769
wandb:                   val/rmse 0.00833
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/8tp54o4m
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_034116-8tp54o4m/logs
Completed: SP500 H=5

Training: Autoformer on SP500 for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_034404-wgypf5ac
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/wgypf5ac
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SP500_H10Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/wgypf5ac
>>>>>>>start training : long_term_forecast_Autoformer_SP500_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 10
============================================================
train 4234
val 230
test 231
Epoch: 1, Steps: 133 | Train Loss: 0.2237856 Vali Loss: 0.0761753 Test Loss: 0.0863861
Validation loss decreased (inf --> 0.076175).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
Epoch: 2, Steps: 133 | Train Loss: 0.1929026 Vali Loss: 0.0759234 Test Loss: 0.0826288
Validation loss decreased (0.076175 --> 0.075923).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.22378563589619516, 'val/loss': 0.07617525476962328, 'test/loss': 0.08638611901551485, '_timestamp': 1762307058.9047866}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19290261799679662, 'val/loss': 0.07592340186238289, 'test/loss': 0.08262875536456704, '_timestamp': 1762307066.049811}).
Epoch: 3, Steps: 133 | Train Loss: 0.1853814 Vali Loss: 0.0738630 Test Loss: 0.0806484
Validation loss decreased (0.075923 --> 0.073863).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1813984 Vali Loss: 0.0738741 Test Loss: 0.0804201
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1809569 Vali Loss: 0.0731011 Test Loss: 0.0799140
Validation loss decreased (0.073863 --> 0.073101).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1794683 Vali Loss: 0.0716134 Test Loss: 0.0798785
Validation loss decreased (0.073101 --> 0.071613).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1794421 Vali Loss: 0.0728505 Test Loss: 0.0795529
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1795299 Vali Loss: 0.0698377 Test Loss: 0.0794098
Validation loss decreased (0.071613 --> 0.069838).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1790117 Vali Loss: 0.0701236 Test Loss: 0.0793751
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1789922 Vali Loss: 0.0701395 Test Loss: 0.0794530
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1786513 Vali Loss: 0.0715189 Test Loss: 0.0794365
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1798424 Vali Loss: 0.0702327 Test Loss: 0.0794275
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1795557 Vali Loss: 0.0718826 Test Loss: 0.0794217
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1788134 Vali Loss: 0.0708923 Test Loss: 0.0793933
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1785732 Vali Loss: 0.0696325 Test Loss: 0.0793900
Validation loss decreased (0.069838 --> 0.069633).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1790077 Vali Loss: 0.0696501 Test Loss: 0.0793882
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1821857 Vali Loss: 0.0709535 Test Loss: 0.0793874
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1793717 Vali Loss: 0.0708820 Test Loss: 0.0793876
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1789770 Vali Loss: 0.0731072 Test Loss: 0.0793877
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1797197 Vali Loss: 0.0718501 Test Loss: 0.0793875
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1788703 Vali Loss: 0.0745189 Test Loss: 0.0793874
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1795501 Vali Loss: 0.0720059 Test Loss: 0.0793875
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1788130 Vali Loss: 0.0695039 Test Loss: 0.0793874
Validation loss decreased (0.069633 --> 0.069504).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.1789878 Vali Loss: 0.0704054 Test Loss: 0.0793875
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.1806837 Vali Loss: 0.0706041 Test Loss: 0.0793872
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.1793993 Vali Loss: 0.0697114 Test Loss: 0.0793875
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.1789678 Vali Loss: 0.0717218 Test Loss: 0.0793874
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.1787453 Vali Loss: 0.0728503 Test Loss: 0.0793874
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.1787314 Vali Loss: 0.0718615 Test Loss: 0.0793874
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 133 | Train Loss: 0.1788526 Vali Loss: 0.0702573 Test Loss: 0.0793874
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 133 | Train Loss: 0.1787585 Vali Loss: 0.0717013 Test Loss: 0.0793873
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 133 | Train Loss: 0.1801687 Vali Loss: 0.0703607 Test Loss: 0.0793873
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 133 | Train Loss: 0.1783847 Vali Loss: 0.0711041 Test Loss: 0.0793873
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SP500_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 231
test shape: (231, 10, 1) (231, 10, 1)
test shape: (231, 10, 1) (231, 10, 1)


	mse:6.712463073199615e-05, mae:0.006095127202570438, rmse:0.008192962035536766, r2:-0.03264212608337402, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0082, RÂ²: -0.0326, MAPE: 2.45%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.558 MB of 0.559 MB uploadedwandb: \ 0.558 MB of 0.559 MB uploadedwandb: | 0.559 MB of 0.559 MB uploadedwandb: / 0.559 MB of 0.559 MB uploadedwandb: - 0.559 MB of 0.777 MB uploadedwandb: \ 0.559 MB of 0.777 MB uploadedwandb: | 0.777 MB of 0.777 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‡â–„â–„â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–„â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–‚â–…â–‚â–‚â–‚â–â–‚â–â–‚â–ƒâ–‚â–‚â–â–â–â–â–ƒâ–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–‡â–†â–„â–†â–â–‚â–‚â–„â–‚â–„â–ƒâ–â–â–ƒâ–ƒâ–†â–„â–ˆâ–„â–â–‚â–ƒâ–â–„â–†â–„â–‚â–„â–‚â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 32
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.07939
wandb:                 train/loss 0.17838
wandb:   val/directional_accuracy 51.37085
wandb:                   val/loss 0.0711
wandb:                    val/mae 0.0061
wandb:                   val/mape 244.61133
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.03264
wandb:                   val/rmse 0.00819
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/wgypf5ac
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_034404-wgypf5ac/logs
Completed: SP500 H=10

Training: Autoformer on SP500 for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_034816-g1z1zulq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/g1z1zulq
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SP500_H22Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/g1z1zulq
>>>>>>>start training : long_term_forecast_Autoformer_SP500_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 22
============================================================
train 4222
val 218
test 219
Epoch: 1, Steps: 132 | Train Loss: 0.2191521 Vali Loss: 0.0728380 Test Loss: 0.0731487
Validation loss decreased (inf --> 0.072838).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.1902702 Vali Loss: 0.0738821 Test Loss: 0.0734931
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.21915211556761555, 'val/loss': 0.07283797115087509, 'test/loss': 0.07314871677330562, '_timestamp': 1762307309.443737}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.1902702416885983, 'val/loss': 0.07388212531805038, 'test/loss': 0.0734930554670947, '_timestamp': 1762307316.6881154}).
Epoch: 3, Steps: 132 | Train Loss: 0.1844031 Vali Loss: 0.0720774 Test Loss: 0.0744461
Validation loss decreased (0.072838 --> 0.072077).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.1821239 Vali Loss: 0.0720602 Test Loss: 0.0725543
Validation loss decreased (0.072077 --> 0.072060).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.1810196 Vali Loss: 0.0719924 Test Loss: 0.0721705
Validation loss decreased (0.072060 --> 0.071992).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.1804481 Vali Loss: 0.0719771 Test Loss: 0.0731203
Validation loss decreased (0.071992 --> 0.071977).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.1803473 Vali Loss: 0.0718843 Test Loss: 0.0727223
Validation loss decreased (0.071977 --> 0.071884).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.1800379 Vali Loss: 0.0717979 Test Loss: 0.0726807
Validation loss decreased (0.071884 --> 0.071798).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.1797958 Vali Loss: 0.0718508 Test Loss: 0.0725451
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.1797824 Vali Loss: 0.0717238 Test Loss: 0.0726168
Validation loss decreased (0.071798 --> 0.071724).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.1796853 Vali Loss: 0.0717917 Test Loss: 0.0725882
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.1798326 Vali Loss: 0.0720927 Test Loss: 0.0725743
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.1797354 Vali Loss: 0.0716876 Test Loss: 0.0725590
Validation loss decreased (0.071724 --> 0.071688).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.1797472 Vali Loss: 0.0718182 Test Loss: 0.0725584
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.1797050 Vali Loss: 0.0718407 Test Loss: 0.0725601
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.1797524 Vali Loss: 0.0717725 Test Loss: 0.0725615
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.1798352 Vali Loss: 0.0717319 Test Loss: 0.0725621
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.1799316 Vali Loss: 0.0716276 Test Loss: 0.0725619
Validation loss decreased (0.071688 --> 0.071628).  Saving model ...
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.1798726 Vali Loss: 0.0716634 Test Loss: 0.0725621
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.1799121 Vali Loss: 0.0716908 Test Loss: 0.0725620
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.1797682 Vali Loss: 0.0716397 Test Loss: 0.0725620
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.1797242 Vali Loss: 0.0718353 Test Loss: 0.0725620
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.1799354 Vali Loss: 0.0717639 Test Loss: 0.0725621
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 132 | Train Loss: 0.1799621 Vali Loss: 0.0718013 Test Loss: 0.0725620
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 132 | Train Loss: 0.1798434 Vali Loss: 0.0717119 Test Loss: 0.0725620
EarlyStopping counter: 7 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 132 | Train Loss: 0.1798515 Vali Loss: 0.0716978 Test Loss: 0.0725620
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 132 | Train Loss: 0.1798469 Vali Loss: 0.0719117 Test Loss: 0.0725620
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 132 | Train Loss: 0.1799474 Vali Loss: 0.0719834 Test Loss: 0.0725620
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SP500_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 219
test shape: (219, 22, 1) (219, 22, 1)
test shape: (219, 22, 1) (219, 22, 1)


	mse:6.682013190584257e-05, mae:0.006075476296246052, rmse:0.008174357935786247, r2:-0.046592116355895996, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0082, RÂ²: -0.0466, MAPE: 3.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.649 MB of 0.650 MB uploadedwandb: \ 0.649 MB of 0.650 MB uploadedwandb: | 0.650 MB of 0.650 MB uploadedwandb: / 0.650 MB of 0.868 MB uploadedwandb: - 0.868 MB of 0.868 MB uploadedwandb: \ 0.868 MB of 0.868 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ˆâ–†â–†â–…â–„â–„â–‚â–ƒâ–ˆâ–‚â–„â–„â–ƒâ–ƒâ–â–‚â–‚â–â–„â–ƒâ–„â–‚â–‚â–…â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 27
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.07256
wandb:                 train/loss 0.17995
wandb:   val/directional_accuracy 47.55382
wandb:                   val/loss 0.07198
wandb:                    val/mae 0.00608
wandb:                   val/mape 324.70901
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.04659
wandb:                   val/rmse 0.00817
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/g1z1zulq
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_034816-g1z1zulq/logs
Completed: SP500 H=22

Training: Autoformer on SP500 for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_035157-subbehfw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/subbehfw
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SP500_H50Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/subbehfw
>>>>>>>start training : long_term_forecast_Autoformer_SP500_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 50
============================================================
train 4194
val 190
test 191
Epoch: 1, Steps: 132 | Train Loss: 0.2199751 Vali Loss: 0.0776507 Test Loss: 0.0816604
Validation loss decreased (inf --> 0.077651).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.1929164 Vali Loss: 0.0744830 Test Loss: 0.0803296
Validation loss decreased (0.077651 --> 0.074483).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2199751308018511, 'val/loss': 0.07765065630276997, 'test/loss': 0.0816603774825732, '_timestamp': 1762307532.4389453}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19291639356224827, 'val/loss': 0.07448299850026767, 'test/loss': 0.08032963114480178, '_timestamp': 1762307539.4721653}).
Epoch: 3, Steps: 132 | Train Loss: 0.1882269 Vali Loss: 0.0749896 Test Loss: 0.0790528
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.1855838 Vali Loss: 0.0727047 Test Loss: 0.0744897
Validation loss decreased (0.074483 --> 0.072705).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.1909326 Vali Loss: 0.0731563 Test Loss: 0.0762280
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.1845207 Vali Loss: 0.0731051 Test Loss: 0.0765367
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.1843032 Vali Loss: 0.0726948 Test Loss: 0.0756730
Validation loss decreased (0.072705 --> 0.072695).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.1893304 Vali Loss: 0.0728606 Test Loss: 0.0761376
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.1853148 Vali Loss: 0.0729964 Test Loss: 0.0759688
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.1838997 Vali Loss: 0.0729694 Test Loss: 0.0759398
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.1838018 Vali Loss: 0.0729400 Test Loss: 0.0759351
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.1837126 Vali Loss: 0.0729662 Test Loss: 0.0759273
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.1841523 Vali Loss: 0.0729546 Test Loss: 0.0759107
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.1841637 Vali Loss: 0.0729355 Test Loss: 0.0759045
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.1858663 Vali Loss: 0.0729136 Test Loss: 0.0759024
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.1872873 Vali Loss: 0.0728913 Test Loss: 0.0759039
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.1840714 Vali Loss: 0.0729270 Test Loss: 0.0759048
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SP500_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 191
test shape: (191, 50, 1) (191, 50, 1)
test shape: (191, 50, 1) (191, 50, 1)


	mse:6.615004531340674e-05, mae:0.006012385245412588, rmse:0.008133267983794212, r2:-0.017183899879455566, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0060, RMSE: 0.0081, RÂ²: -0.0172, MAPE: 2.96%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.711 MB of 0.713 MB uploadedwandb: \ 0.713 MB of 0.713 MB uploadedwandb: | 0.713 MB of 0.713 MB uploadedwandb: / 0.713 MB of 0.930 MB uploadedwandb: - 0.930 MB of 0.930 MB uploadedwandb: \ 0.930 MB of 0.930 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–„â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–…â–ƒâ–ˆâ–‚â–‚â–†â–ƒâ–â–â–â–â–â–ƒâ–„â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.0759
wandb:                 train/loss 0.18407
wandb:   val/directional_accuracy 51.08452
wandb:                   val/loss 0.07293
wandb:                    val/mae 0.00601
wandb:                   val/mape 296.0103
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.01718
wandb:                   val/rmse 0.00813
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/subbehfw
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_035157-subbehfw/logs
Completed: SP500 H=50

Training: Autoformer on SP500 for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_035429-kp85d60m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/kp85d60m
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SP500_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/kp85d60m
>>>>>>>start training : long_term_forecast_Autoformer_SP500_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 100
============================================================
train 4144
val 140
test 141
Epoch: 1, Steps: 130 | Train Loss: 0.2300285 Vali Loss: 0.0692015 Test Loss: 0.0836321
Validation loss decreased (inf --> 0.069202).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2033488 Vali Loss: 0.0678985 Test Loss: 0.0841580
Validation loss decreased (0.069202 --> 0.067898).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2300284810937368, 'val/loss': 0.06920152604579925, 'test/loss': 0.08363205492496491, '_timestamp': 1762307682.2679648}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.20334880930873064, 'val/loss': 0.06789848953485489, 'test/loss': 0.08415797203779221, '_timestamp': 1762307689.110059}).
Epoch: 3, Steps: 130 | Train Loss: 0.1982727 Vali Loss: 0.0702651 Test Loss: 0.0854445
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.1969074 Vali Loss: 0.0688117 Test Loss: 0.0843313
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.1958064 Vali Loss: 0.0696050 Test Loss: 0.0851286
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.1957394 Vali Loss: 0.0691776 Test Loss: 0.0848058
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.1964132 Vali Loss: 0.0703228 Test Loss: 0.0861758
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.1951461 Vali Loss: 0.0695150 Test Loss: 0.0852612
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.1955921 Vali Loss: 0.0694253 Test Loss: 0.0853345
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.1953356 Vali Loss: 0.0696057 Test Loss: 0.0854071
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.1955266 Vali Loss: 0.0697528 Test Loss: 0.0854552
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.1952957 Vali Loss: 0.0693281 Test Loss: 0.0854230
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SP500_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 141
test shape: (141, 100, 1) (141, 100, 1)
test shape: (141, 100, 1) (141, 100, 1)


	mse:6.958399171708152e-05, mae:0.006157101597636938, rmse:0.008341701701283455, r2:-0.015700936317443848, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0062, RMSE: 0.0083, RÂ²: -0.0157, MAPE: 3.48%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.793 MB of 0.798 MB uploadedwandb: \ 0.793 MB of 0.798 MB uploadedwandb: | 0.798 MB of 0.798 MB uploadedwandb: / 0.798 MB of 1.013 MB uploadedwandb: - 1.013 MB of 1.013 MB uploadedwandb: \ 1.013 MB of 1.013 MB uploadedwandb: | 1.013 MB of 1.013 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–â–„â–ƒâ–ˆâ–…â–…â–…â–…â–…
wandb:                 train/loss â–ˆâ–…â–‚â–‚â–„â–â–‚â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–…â–ƒâ–ˆâ–„â–„â–…â–…â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.08542
wandb:                 train/loss 0.1953
wandb:   val/directional_accuracy 51.9808
wandb:                   val/loss 0.06933
wandb:                    val/mae 0.00616
wandb:                   val/mape 348.16766
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.0157
wandb:                   val/rmse 0.00834
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/kp85d60m
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_035429-kp85d60m/logs
Completed: SP500 H=100

Training: Autoformer on NASDAQ for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_035627-4ia2r45s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/4ia2r45s
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NASDAQ_H3Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/4ia2r45s
>>>>>>>start training : long_term_forecast_Autoformer_NASDAQ_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.3219786 Vali Loss: 0.1676329 Test Loss: 0.1560249
Validation loss decreased (inf --> 0.167633).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2746887 Vali Loss: 0.1444798 Test Loss: 0.1388970
Validation loss decreased (0.167633 --> 0.144480).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3219785967043468, 'val/loss': 0.16763288620859385, 'test/loss': 0.1560249188914895, '_timestamp': 1762307800.5954163}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2746886609864414, 'val/loss': 0.14447977021336555, 'test/loss': 0.13889696169644594, '_timestamp': 1762307807.6203787}).
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3219785967043468, 'val/loss': 0.16763288620859385, 'test/loss': 0.1560249188914895, '_timestamp': 1762307800.5954163}).
Epoch: 3, Steps: 133 | Train Loss: 0.2586130 Vali Loss: 0.1432993 Test Loss: 0.1308648
Validation loss decreased (0.144480 --> 0.143299).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2504345 Vali Loss: 0.1387538 Test Loss: 0.1293149
Validation loss decreased (0.143299 --> 0.138754).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2457062 Vali Loss: 0.1365356 Test Loss: 0.1272162
Validation loss decreased (0.138754 --> 0.136536).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2432570 Vali Loss: 0.1390202 Test Loss: 0.1277678
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2410964 Vali Loss: 0.1369643 Test Loss: 0.1268573
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2402369 Vali Loss: 0.1339657 Test Loss: 0.1270348
Validation loss decreased (0.136536 --> 0.133966).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2403177 Vali Loss: 0.1323713 Test Loss: 0.1272077
Validation loss decreased (0.133966 --> 0.132371).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2407485 Vali Loss: 0.1353125 Test Loss: 0.1273997
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2391239 Vali Loss: 0.1330216 Test Loss: 0.1270820
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2406510 Vali Loss: 0.1354471 Test Loss: 0.1271104
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2399269 Vali Loss: 0.1382517 Test Loss: 0.1272865
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2398918 Vali Loss: 0.1355885 Test Loss: 0.1272909
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2394150 Vali Loss: 0.1368358 Test Loss: 0.1272590
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2401219 Vali Loss: 0.1355029 Test Loss: 0.1272570
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2405799 Vali Loss: 0.1353412 Test Loss: 0.1272573
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2395578 Vali Loss: 0.1354709 Test Loss: 0.1272568
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2394412 Vali Loss: 0.1376308 Test Loss: 0.1272570
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NASDAQ_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.00014087653835304081, mae:0.008719079196453094, rmse:0.011869141831994057, r2:-0.035028696060180664, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0087, RMSE: 0.0119, RÂ²: -0.0350, MAPE: 5240364.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.497 MB of 0.497 MB uploadedwandb: \ 0.497 MB of 0.497 MB uploadedwandb: | 0.497 MB of 0.497 MB uploadedwandb: / 0.497 MB of 0.497 MB uploadedwandb: - 0.497 MB of 0.497 MB uploadedwandb: \ 0.497 MB of 0.497 MB uploadedwandb: | 0.497 MB of 0.497 MB uploadedwandb: / 0.628 MB of 0.844 MB uploaded (0.002 MB deduped)wandb: - 0.844 MB of 0.844 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–…â–‚â–ƒâ–â–â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–‚â–â–‚â–â–â–â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–„â–…â–„â–‚â–â–ƒâ–â–ƒâ–…â–ƒâ–„â–ƒâ–ƒâ–ƒâ–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 18
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.12726
wandb:                 train/loss 0.23944
wandb:   val/directional_accuracy 51.05485
wandb:                   val/loss 0.13763
wandb:                    val/mae 0.00872
wandb:                   val/mape 524036450.0
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.03503
wandb:                   val/rmse 0.01187
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/4ia2r45s
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_035627-4ia2r45s/logs
Completed: NASDAQ H=3

Training: Autoformer on NASDAQ for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_035910-msbrpmr0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/msbrpmr0
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NASDAQ_H5Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/msbrpmr0
>>>>>>>start training : long_term_forecast_Autoformer_NASDAQ_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.3078917 Vali Loss: 0.1532934 Test Loss: 0.1435582
Validation loss decreased (inf --> 0.153293).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2698381 Vali Loss: 0.1565865 Test Loss: 0.1367858
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.30789168335889516, 'val/loss': 0.15329340565949678, 'test/loss': 0.14355820417404175, '_timestamp': 1762307964.3506775}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2698381399749813, 'val/loss': 0.156586529687047, 'test/loss': 0.13678580150008202, '_timestamp': 1762307971.3316085}).
Epoch: 3, Steps: 133 | Train Loss: 0.2538851 Vali Loss: 0.1497560 Test Loss: 0.1327822
Validation loss decreased (0.153293 --> 0.149756).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2472334 Vali Loss: 0.1444524 Test Loss: 0.1330186
Validation loss decreased (0.149756 --> 0.144452).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2442860 Vali Loss: 0.1355967 Test Loss: 0.1317726
Validation loss decreased (0.144452 --> 0.135597).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2419973 Vali Loss: 0.1349670 Test Loss: 0.1312173
Validation loss decreased (0.135597 --> 0.134967).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2414869 Vali Loss: 0.1450474 Test Loss: 0.1322408
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2416777 Vali Loss: 0.1451979 Test Loss: 0.1321211
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2412451 Vali Loss: 0.1364201 Test Loss: 0.1316596
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2406381 Vali Loss: 0.1446577 Test Loss: 0.1318618
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2404919 Vali Loss: 0.1362580 Test Loss: 0.1318096
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2415069 Vali Loss: 0.1399096 Test Loss: 0.1318203
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2406292 Vali Loss: 0.1459958 Test Loss: 0.1318208
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2406308 Vali Loss: 0.1374942 Test Loss: 0.1318221
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2408851 Vali Loss: 0.1450081 Test Loss: 0.1318252
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2406241 Vali Loss: 0.1363785 Test Loss: 0.1318270
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NASDAQ_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.00014578853733837605, mae:0.008798754774034023, rmse:0.01207429263740778, r2:-0.06568300724029541, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0088, RMSE: 0.0121, RÂ²: -0.0657, MAPE: 6551135.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.529 MB of 0.529 MB uploadedwandb: \ 0.529 MB of 0.529 MB uploadedwandb: | 0.529 MB of 0.529 MB uploadedwandb: / 0.529 MB of 0.529 MB uploadedwandb: - 0.529 MB of 0.745 MB uploadedwandb: \ 0.629 MB of 0.745 MB uploadedwandb: | 0.745 MB of 0.745 MB uploadedwandb: / 0.745 MB of 0.745 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‡â–ˆâ–ƒâ–â–…â–…â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–â–â–†â–†â–‚â–†â–‚â–ƒâ–†â–‚â–†â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.13183
wandb:                 train/loss 0.24062
wandb:   val/directional_accuracy 50.10638
wandb:                   val/loss 0.13638
wandb:                    val/mae 0.0088
wandb:                   val/mape 655113500.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.06568
wandb:                   val/rmse 0.01207
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/msbrpmr0
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_035910-msbrpmr0/logs
Completed: NASDAQ H=5

Training: Autoformer on NASDAQ for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_040131-97zneyh5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/97zneyh5
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NASDAQ_H10Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/97zneyh5
>>>>>>>start training : long_term_forecast_Autoformer_NASDAQ_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2956628 Vali Loss: 0.1606157 Test Loss: 0.1466446
Validation loss decreased (inf --> 0.160616).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2619469 Vali Loss: 0.1530750 Test Loss: 0.1410014
Validation loss decreased (0.160616 --> 0.153075).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29566279356192826, 'val/loss': 0.1606157384812832, 'test/loss': 0.14664460066705942, '_timestamp': 1762308104.2041132}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2619468780388509, 'val/loss': 0.15307499840855598, 'test/loss': 0.14100138656795025, '_timestamp': 1762308111.157508}).
Epoch: 3, Steps: 133 | Train Loss: 0.2513856 Vali Loss: 0.1487255 Test Loss: 0.1411956
Validation loss decreased (0.153075 --> 0.148725).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2461041 Vali Loss: 0.1457665 Test Loss: 0.1388410
Validation loss decreased (0.148725 --> 0.145766).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2447351 Vali Loss: 0.1436619 Test Loss: 0.1375771
Validation loss decreased (0.145766 --> 0.143662).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2425174 Vali Loss: 0.1441380 Test Loss: 0.1372842
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2411918 Vali Loss: 0.1592301 Test Loss: 0.1381857
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2425749 Vali Loss: 0.1466767 Test Loss: 0.1384171
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2423235 Vali Loss: 0.1483965 Test Loss: 0.1384638
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2405839 Vali Loss: 0.1432123 Test Loss: 0.1381857
Validation loss decreased (0.143662 --> 0.143212).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2401220 Vali Loss: 0.1541326 Test Loss: 0.1381461
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2413348 Vali Loss: 0.1469236 Test Loss: 0.1381079
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2410030 Vali Loss: 0.1483804 Test Loss: 0.1379750
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2421230 Vali Loss: 0.1471346 Test Loss: 0.1379789
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2412703 Vali Loss: 0.1464075 Test Loss: 0.1379759
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2409362 Vali Loss: 0.1456304 Test Loss: 0.1379770
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2422302 Vali Loss: 0.1478065 Test Loss: 0.1379776
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2401229 Vali Loss: 0.1484772 Test Loss: 0.1379778
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2411350 Vali Loss: 0.1582062 Test Loss: 0.1379780
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2414967 Vali Loss: 0.1590572 Test Loss: 0.1379781
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NASDAQ_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.00015043905295897275, mae:0.009070763364434242, rmse:0.012265359982848167, r2:-0.08678257465362549, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0091, RMSE: 0.0123, RÂ²: -0.0868, MAPE: 7536190.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.613 MB of 0.614 MB uploadedwandb: \ 0.613 MB of 0.614 MB uploadedwandb: | 0.613 MB of 0.614 MB uploadedwandb: / 0.614 MB of 0.614 MB uploadedwandb: - 0.614 MB of 0.830 MB uploadedwandb: \ 0.714 MB of 0.830 MB uploadedwandb: | 0.830 MB of 0.830 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–ƒâ–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–‚â–â–â–ˆâ–ƒâ–ƒâ–â–†â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–ˆâ–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.13798
wandb:                 train/loss 0.2415
wandb:   val/directional_accuracy 48.98551
wandb:                   val/loss 0.15906
wandb:                    val/mae 0.00907
wandb:                   val/mape 753619050.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.08678
wandb:                   val/rmse 0.01227
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/97zneyh5
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_040131-97zneyh5/logs
Completed: NASDAQ H=10

Training: Autoformer on NASDAQ for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_040418-m1l90rsu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/m1l90rsu
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NASDAQ_H22Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/m1l90rsu
>>>>>>>start training : long_term_forecast_Autoformer_NASDAQ_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2921134 Vali Loss: 0.1570683 Test Loss: 0.1333240
Validation loss decreased (inf --> 0.157068).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2602128 Vali Loss: 0.1527678 Test Loss: 0.1370055
Validation loss decreased (0.157068 --> 0.152768).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2921133561793602, 'val/loss': 0.15706833558423178, 'test/loss': 0.13332402173961913, '_timestamp': 1762308274.294326}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26021278807611176, 'val/loss': 0.1527677880866187, 'test/loss': 0.1370054855942726, '_timestamp': 1762308281.2559717}).
Epoch: 3, Steps: 132 | Train Loss: 0.2506662 Vali Loss: 0.1582457 Test Loss: 0.1388818
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2458404 Vali Loss: 0.1566034 Test Loss: 0.1378490
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2432507 Vali Loss: 0.1572627 Test Loss: 0.1370618
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2429304 Vali Loss: 0.1574534 Test Loss: 0.1376303
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2412812 Vali Loss: 0.1575311 Test Loss: 0.1373550
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2411649 Vali Loss: 0.1577243 Test Loss: 0.1383036
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2411051 Vali Loss: 0.1570224 Test Loss: 0.1381167
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2408345 Vali Loss: 0.1568596 Test Loss: 0.1381646
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2409295 Vali Loss: 0.1574815 Test Loss: 0.1382399
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2408146 Vali Loss: 0.1565077 Test Loss: 0.1382697
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NASDAQ_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.00015202151553239673, mae:0.009027136489748955, rmse:0.012329700402915478, r2:-0.08644616603851318, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0090, RMSE: 0.0123, RÂ²: -0.0864, MAPE: 4006795.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.650 MB of 0.651 MB uploadedwandb: \ 0.650 MB of 0.651 MB uploadedwandb: | 0.650 MB of 0.651 MB uploadedwandb: / 0.651 MB of 0.651 MB uploadedwandb: - 0.651 MB of 0.651 MB uploadedwandb: \ 0.651 MB of 0.866 MB uploadedwandb: | 0.866 MB of 0.866 MB uploadedwandb: / 0.866 MB of 0.866 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–â–ƒâ–‚â–†â–…â–…â–†â–†
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–„â–…â–…â–†â–ƒâ–‚â–…â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.13827
wandb:                 train/loss 0.24081
wandb:   val/directional_accuracy 48.51464
wandb:                   val/loss 0.15651
wandb:                    val/mae 0.00903
wandb:                   val/mape 400679500.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.08645
wandb:                   val/rmse 0.01233
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/m1l90rsu
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_040418-m1l90rsu/logs
Completed: NASDAQ H=22

Training: Autoformer on NASDAQ for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_040619-2c3l0cj1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/2c3l0cj1
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NASDAQ_H50Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/2c3l0cj1
>>>>>>>start training : long_term_forecast_Autoformer_NASDAQ_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.2907473 Vali Loss: 0.1777456 Test Loss: 0.1392462
Validation loss decreased (inf --> 0.177746).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2621662 Vali Loss: 0.1706589 Test Loss: 0.1452479
Validation loss decreased (0.177746 --> 0.170659).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29074729775840585, 'val/loss': 0.17774558812379837, 'test/loss': 0.1392462154229482, '_timestamp': 1762308393.400236}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2621662027456544, 'val/loss': 0.17065894355376562, 'test/loss': 0.14524786174297333, '_timestamp': 1762308400.518154}).
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2621662027456544, 'val/loss': 0.17065894355376562, 'test/loss': 0.14524786174297333, '_timestamp': 1762308400.518154}).
Epoch: 3, Steps: 132 | Train Loss: 0.2552009 Vali Loss: 0.1709432 Test Loss: 0.1393520
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2514692 Vali Loss: 0.1705279 Test Loss: 0.1395675
Validation loss decreased (0.170659 --> 0.170528).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2506544 Vali Loss: 0.1698436 Test Loss: 0.1396569
Validation loss decreased (0.170528 --> 0.169844).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2484009 Vali Loss: 0.1695109 Test Loss: 0.1388717
Validation loss decreased (0.169844 --> 0.169511).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2472004 Vali Loss: 0.1702438 Test Loss: 0.1383573
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2468922 Vali Loss: 0.1703771 Test Loss: 0.1387373
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2474462 Vali Loss: 0.1705155 Test Loss: 0.1383468
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2477926 Vali Loss: 0.1707344 Test Loss: 0.1384330
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2474475 Vali Loss: 0.1705573 Test Loss: 0.1384800
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2474917 Vali Loss: 0.1704949 Test Loss: 0.1384816
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2469719 Vali Loss: 0.1704363 Test Loss: 0.1384761
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2465558 Vali Loss: 0.1704511 Test Loss: 0.1384874
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2467543 Vali Loss: 0.1704917 Test Loss: 0.1384920
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2475140 Vali Loss: 0.1702433 Test Loss: 0.1384906
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NASDAQ_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.00015295269258785993, mae:0.008985588327050209, rmse:0.01236740406602621, r2:-0.05597031116485596, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0090, RMSE: 0.0124, RÂ²: -0.0560, MAPE: 6326818.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.721 MB of 0.723 MB uploadedwandb: \ 0.721 MB of 0.723 MB uploadedwandb: | 0.723 MB of 0.723 MB uploadedwandb: / 0.723 MB of 0.939 MB uploadedwandb: - 0.723 MB of 0.939 MB uploadedwandb: \ 0.939 MB of 0.939 MB uploadedwandb: | 0.939 MB of 0.939 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–†â–ˆâ–ˆâ–„â–â–ƒâ–â–â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–ƒâ–â–…â–…â–†â–‡â–†â–†â–†â–†â–†â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.13849
wandb:                 train/loss 0.24751
wandb:   val/directional_accuracy 53.12567
wandb:                   val/loss 0.17024
wandb:                    val/mae 0.00899
wandb:                   val/mape 632681800.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.05597
wandb:                   val/rmse 0.01237
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/2c3l0cj1
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_040619-2c3l0cj1/logs
Completed: NASDAQ H=50

Training: Autoformer on NASDAQ for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_040850-ov2wpzya
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/ov2wpzya
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NASDAQ_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/ov2wpzya
>>>>>>>start training : long_term_forecast_Autoformer_NASDAQ_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.2994044 Vali Loss: 0.1910581 Test Loss: 0.1444534
Validation loss decreased (inf --> 0.191058).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2723751 Vali Loss: 0.1798798 Test Loss: 0.1481728
Validation loss decreased (0.191058 --> 0.179880).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2994044458636871, 'val/loss': 0.19105809628963472, 'test/loss': 0.14445340186357497, '_timestamp': 1762308544.4342318}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2723750786139415, 'val/loss': 0.17987980246543883, 'test/loss': 0.14817282557487488, '_timestamp': 1762308551.3900335}).
Epoch: 3, Steps: 130 | Train Loss: 0.2659621 Vali Loss: 0.1863925 Test Loss: 0.1468452
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2632401 Vali Loss: 0.1878681 Test Loss: 0.1461041
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2617587 Vali Loss: 0.1866745 Test Loss: 0.1457034
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2605303 Vali Loss: 0.1846563 Test Loss: 0.1463887
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2605516 Vali Loss: 0.1871405 Test Loss: 0.1464022
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2599904 Vali Loss: 0.1859097 Test Loss: 0.1464733
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2598406 Vali Loss: 0.1819893 Test Loss: 0.1464361
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2592839 Vali Loss: 0.1849512 Test Loss: 0.1464605
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2593259 Vali Loss: 0.1858661 Test Loss: 0.1464930
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2595639 Vali Loss: 0.1852275 Test Loss: 0.1464711
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NASDAQ_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.00015787068696226925, mae:0.008816290646791458, rmse:0.012564660049974918, r2:-0.0408555269241333, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0088, RMSE: 0.0126, RÂ²: -0.0409, MAPE: 2920688.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.731 MB of 0.736 MB uploadedwandb: \ 0.731 MB of 0.736 MB uploadedwandb: | 0.731 MB of 0.736 MB uploadedwandb: / 0.736 MB of 0.736 MB uploadedwandb: - 0.736 MB of 0.736 MB uploadedwandb: \ 0.736 MB of 0.951 MB uploadedwandb: | 0.951 MB of 0.951 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–…â–…â–†â–…â–†â–†â–†
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–ˆâ–‡â–„â–‡â–†â–â–…â–†â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.14647
wandb:                 train/loss 0.25956
wandb:   val/directional_accuracy 51.77489
wandb:                   val/loss 0.18523
wandb:                    val/mae 0.00882
wandb:                   val/mape 292068850.0
wandb:                    val/mse 0.00016
wandb:                     val/r2 -0.04086
wandb:                   val/rmse 0.01256
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/ov2wpzya
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_040850-ov2wpzya/logs
Completed: NASDAQ H=100

Training: Autoformer on ABSA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_041042-pcs6jlok
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/pcs6jlok
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ABSA_H3  Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/pcs6jlok
>>>>>>>start training : long_term_forecast_Autoformer_ABSA_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 238
Epoch: 1, Steps: 133 | Train Loss: 0.3692652 Vali Loss: 0.1924904 Test Loss: 0.1777160
Validation loss decreased (inf --> 0.192490).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.3182573 Vali Loss: 0.1814452 Test Loss: 0.1772967
Validation loss decreased (0.192490 --> 0.181445).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3692651658802104, 'val/loss': 0.19249042682349682, 'test/loss': 0.17771601490676403, '_timestamp': 1762308653.774043}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3182572768370908, 'val/loss': 0.18144517578184605, 'test/loss': 0.17729671206325293, '_timestamp': 1762308660.7420714}).
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3692651658802104, 'val/loss': 0.19249042682349682, 'test/loss': 0.17771601490676403, '_timestamp': 1762308653.774043}).
Epoch: 3, Steps: 133 | Train Loss: 0.3013058 Vali Loss: 0.1708524 Test Loss: 0.1643259
Validation loss decreased (0.181445 --> 0.170852).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2869462 Vali Loss: 0.1689742 Test Loss: 0.1579385
Validation loss decreased (0.170852 --> 0.168974).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2831655 Vali Loss: 0.1691406 Test Loss: 0.1593022
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2829192 Vali Loss: 0.1659602 Test Loss: 0.1570797
Validation loss decreased (0.168974 --> 0.165960).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2807105 Vali Loss: 0.1656809 Test Loss: 0.1580043
Validation loss decreased (0.165960 --> 0.165681).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2793892 Vali Loss: 0.1651293 Test Loss: 0.1570895
Validation loss decreased (0.165681 --> 0.165129).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2793098 Vali Loss: 0.1649034 Test Loss: 0.1575810
Validation loss decreased (0.165129 --> 0.164903).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2804337 Vali Loss: 0.1647774 Test Loss: 0.1574134
Validation loss decreased (0.164903 --> 0.164777).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2783090 Vali Loss: 0.1654194 Test Loss: 0.1573398
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2794922 Vali Loss: 0.1658458 Test Loss: 0.1573367
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2790336 Vali Loss: 0.1597354 Test Loss: 0.1574149
Validation loss decreased (0.164777 --> 0.159735).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2785074 Vali Loss: 0.1627609 Test Loss: 0.1574128
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2781140 Vali Loss: 0.1636371 Test Loss: 0.1574101
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2793166 Vali Loss: 0.1651698 Test Loss: 0.1574084
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2796231 Vali Loss: 0.1667777 Test Loss: 0.1574080
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2775796 Vali Loss: 0.1674087 Test Loss: 0.1574078
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2785526 Vali Loss: 0.1663601 Test Loss: 0.1574078
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2786780 Vali Loss: 0.1700495 Test Loss: 0.1574082
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2778171 Vali Loss: 0.1666459 Test Loss: 0.1574079
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2796205 Vali Loss: 0.1629703 Test Loss: 0.1574080
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2789698 Vali Loss: 0.1641390 Test Loss: 0.1574078
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ABSA_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 238
test shape: (238, 3, 1) (238, 3, 1)
test shape: (238, 3, 1) (238, 3, 1)


	mse:0.00047834881115704775, mae:0.016764940693974495, rmse:0.021871186792850494, r2:-0.05048644542694092, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0168, RMSE: 0.0219, RÂ²: -0.0505, MAPE: 1.93%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.497 MB of 0.497 MB uploadedwandb: \ 0.497 MB of 0.497 MB uploadedwandb: | 0.497 MB of 0.497 MB uploadedwandb: / 0.497 MB of 0.497 MB uploadedwandb: - 0.497 MB of 0.497 MB uploadedwandb: \ 0.497 MB of 0.497 MB uploadedwandb: | 0.497 MB of 0.497 MB uploadedwandb: / 0.497 MB of 0.497 MB uploadedwandb: - 0.497 MB of 0.497 MB uploadedwandb: \ 0.627 MB of 0.844 MB uploaded (0.002 MB deduped)wandb: | 0.739 MB of 0.844 MB uploaded (0.002 MB deduped)wandb: / 0.844 MB of 0.844 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–ƒâ–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–â–â–‚â–‚â–â–â–â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‡â–‡â–…â–…â–„â–„â–„â–…â–…â–â–ƒâ–ƒâ–„â–…â–†â–…â–‡â–…â–ƒâ–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.15741
wandb:                 train/loss 0.27897
wandb:   val/directional_accuracy 49.57983
wandb:                   val/loss 0.16414
wandb:                    val/mae 0.01676
wandb:                   val/mape 192.67027
wandb:                    val/mse 0.00048
wandb:                     val/r2 -0.05049
wandb:                   val/rmse 0.02187
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/pcs6jlok
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_041042-pcs6jlok/logs
Completed: ABSA H=3

Training: Autoformer on ABSA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_041401-hxu3a4nd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/hxu3a4nd
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ABSA_H5  Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/hxu3a4nd
>>>>>>>start training : long_term_forecast_Autoformer_ABSA_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 236
Epoch: 1, Steps: 133 | Train Loss: 0.3658023 Vali Loss: 0.1916416 Test Loss: 0.1815165
Validation loss decreased (inf --> 0.191642).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.3231285 Vali Loss: 0.1932487 Test Loss: 0.1767264
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3658022805488199, 'val/loss': 0.19164163805544376, 'test/loss': 0.18151652440428734, '_timestamp': 1762308856.0469985}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3231285411612432, 'val/loss': 0.1932487152516842, 'test/loss': 0.17672637104988098, '_timestamp': 1762308863.021138}).
Epoch: 3, Steps: 133 | Train Loss: 0.3037112 Vali Loss: 0.1827119 Test Loss: 0.1676354
Validation loss decreased (0.191642 --> 0.182712).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2964829 Vali Loss: 0.1787691 Test Loss: 0.1657370
Validation loss decreased (0.182712 --> 0.178769).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2925865 Vali Loss: 0.1779244 Test Loss: 0.1680984
Validation loss decreased (0.178769 --> 0.177924).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2907753 Vali Loss: 0.1696766 Test Loss: 0.1651106
Validation loss decreased (0.177924 --> 0.169677).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2889506 Vali Loss: 0.1739500 Test Loss: 0.1652680
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2874183 Vali Loss: 0.1767759 Test Loss: 0.1652108
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2903012 Vali Loss: 0.1733971 Test Loss: 0.1650322
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2876688 Vali Loss: 0.1693425 Test Loss: 0.1650721
Validation loss decreased (0.169677 --> 0.169343).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2876559 Vali Loss: 0.1738799 Test Loss: 0.1650739
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2880517 Vali Loss: 0.1747586 Test Loss: 0.1650408
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2873572 Vali Loss: 0.1730120 Test Loss: 0.1650585
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2869627 Vali Loss: 0.1739627 Test Loss: 0.1650664
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2873093 Vali Loss: 0.1735322 Test Loss: 0.1650642
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2877750 Vali Loss: 0.1742423 Test Loss: 0.1650634
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2867623 Vali Loss: 0.1717491 Test Loss: 0.1650638
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2880086 Vali Loss: 0.1759542 Test Loss: 0.1650639
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2878993 Vali Loss: 0.1782459 Test Loss: 0.1650639
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2885822 Vali Loss: 0.1748270 Test Loss: 0.1650637
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ABSA_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
test shape: (236, 5, 1) (236, 5, 1)
test shape: (236, 5, 1) (236, 5, 1)


	mse:0.0004879684711340815, mae:0.016794873401522636, rmse:0.022090008482336998, r2:-0.06538128852844238, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0168, RMSE: 0.0221, RÂ²: -0.0654, MAPE: 1.58%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.530 MB of 0.530 MB uploadedwandb: \ 0.530 MB of 0.530 MB uploadedwandb: | 0.530 MB of 0.530 MB uploadedwandb: / 0.530 MB of 0.530 MB uploadedwandb: - 0.530 MB of 0.747 MB uploadedwandb: \ 0.747 MB of 0.747 MB uploadedwandb: | 0.747 MB of 0.747 MB uploadedwandb: / 0.747 MB of 0.747 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‡â–ƒâ–ˆâ–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–â–‚â–â–â–‚â–â–â–â–â–â–‚â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–…â–â–ƒâ–…â–ƒâ–â–ƒâ–„â–ƒâ–ƒâ–ƒâ–„â–‚â–„â–†â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.16506
wandb:                 train/loss 0.28858
wandb:   val/directional_accuracy 49.15254
wandb:                   val/loss 0.17483
wandb:                    val/mae 0.01679
wandb:                   val/mape 157.92239
wandb:                    val/mse 0.00049
wandb:                     val/r2 -0.06538
wandb:                   val/rmse 0.02209
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/hxu3a4nd
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_041401-hxu3a4nd/logs
Completed: ABSA H=5

Training: Autoformer on ABSA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_041649-jbap8vec
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/jbap8vec
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ABSA_H10 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/jbap8vec
>>>>>>>start training : long_term_forecast_Autoformer_ABSA_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 231
Epoch: 1, Steps: 133 | Train Loss: 0.3583407 Vali Loss: 0.1839340 Test Loss: 0.1734656
Validation loss decreased (inf --> 0.183934).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.3208033 Vali Loss: 0.1968349 Test Loss: 0.1705733
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3583406608803828, 'val/loss': 0.18393396958708763, 'test/loss': 0.17346558161079884, '_timestamp': 1762309022.314439}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3208033131029373, 'val/loss': 0.19683489203453064, 'test/loss': 0.17057326436042786, '_timestamp': 1762309029.421535}).
Epoch: 3, Steps: 133 | Train Loss: 0.3069437 Vali Loss: 0.1824606 Test Loss: 0.1694456
Validation loss decreased (0.183934 --> 0.182461).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.3005196 Vali Loss: 0.1809074 Test Loss: 0.1651689
Validation loss decreased (0.182461 --> 0.180907).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2985628 Vali Loss: 0.1792733 Test Loss: 0.1654363
Validation loss decreased (0.180907 --> 0.179273).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2943736 Vali Loss: 0.1818509 Test Loss: 0.1657559
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2940198 Vali Loss: 0.1838245 Test Loss: 0.1651457
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2942012 Vali Loss: 0.1775840 Test Loss: 0.1651630
Validation loss decreased (0.179273 --> 0.177584).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2943860 Vali Loss: 0.1819144 Test Loss: 0.1651481
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2927834 Vali Loss: 0.1803094 Test Loss: 0.1651746
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2926728 Vali Loss: 0.1782588 Test Loss: 0.1650560
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2921699 Vali Loss: 0.1764148 Test Loss: 0.1652007
Validation loss decreased (0.177584 --> 0.176415).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2936182 Vali Loss: 0.1825248 Test Loss: 0.1652045
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2945223 Vali Loss: 0.1809097 Test Loss: 0.1651847
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2925740 Vali Loss: 0.1767294 Test Loss: 0.1651730
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2915013 Vali Loss: 0.1753929 Test Loss: 0.1651765
Validation loss decreased (0.176415 --> 0.175393).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2954236 Vali Loss: 0.1817965 Test Loss: 0.1651767
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2931446 Vali Loss: 0.1800881 Test Loss: 0.1651767
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2933490 Vali Loss: 0.1809905 Test Loss: 0.1651771
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2917091 Vali Loss: 0.1845923 Test Loss: 0.1651769
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2933853 Vali Loss: 0.1887218 Test Loss: 0.1651766
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2928087 Vali Loss: 0.1841443 Test Loss: 0.1651766
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2954787 Vali Loss: 0.1787115 Test Loss: 0.1651765
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2963366 Vali Loss: 0.1783901 Test Loss: 0.1651764
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2937316 Vali Loss: 0.1775086 Test Loss: 0.1651768
EarlyStopping counter: 9 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2927817 Vali Loss: 0.1814128 Test Loss: 0.1651765
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ABSA_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 231
test shape: (231, 10, 1) (231, 10, 1)
test shape: (231, 10, 1) (231, 10, 1)


	mse:0.0004876858147326857, mae:0.01683664321899414, rmse:0.02208361029624939, r2:-0.056337952613830566, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0168, RMSE: 0.0221, RÂ²: -0.0563, MAPE: 1.64%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.574 MB of 0.574 MB uploadedwandb: \ 0.574 MB of 0.574 MB uploadedwandb: | 0.574 MB of 0.574 MB uploadedwandb: / 0.574 MB of 0.574 MB uploadedwandb: - 0.574 MB of 0.574 MB uploadedwandb: \ 0.574 MB of 0.791 MB uploadedwandb: | 0.754 MB of 0.791 MB uploadedwandb: / 0.791 MB of 0.791 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–ƒâ–‚â–‚â–â–‚â–‚â–ƒâ–ƒâ–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–„â–ƒâ–„â–…â–‚â–„â–„â–ƒâ–‚â–…â–„â–‚â–â–„â–ƒâ–„â–†â–ˆâ–†â–ƒâ–ƒâ–‚â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 25
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.16518
wandb:                 train/loss 0.29278
wandb:   val/directional_accuracy 51.99615
wandb:                   val/loss 0.18141
wandb:                    val/mae 0.01684
wandb:                   val/mape 163.74465
wandb:                    val/mse 0.00049
wandb:                     val/r2 -0.05634
wandb:                   val/rmse 0.02208
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/jbap8vec
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_041649-jbap8vec/logs
Exception in thread IntMsgThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 300, in check_internal_messages
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 268, in check_network_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 848, in deliver_network_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 856, in deliver_internal_messages
    return self._deliver_internal_messages(internal_message)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 516, in _deliver_internal_messages
    return self._deliver_network_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 510, in _deliver_network_status
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: ABSA H=10

Training: Autoformer on ABSA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_042021-dqlk0r57
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/dqlk0r57
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ABSA_H22 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/dqlk0r57
>>>>>>>start training : long_term_forecast_Autoformer_ABSA_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 219
Epoch: 1, Steps: 132 | Train Loss: 0.3592384 Vali Loss: 0.1831329 Test Loss: 0.1657673
Validation loss decreased (inf --> 0.183133).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.35923838728305063, 'val/loss': 0.18313289753028325, 'test/loss': 0.16576729608433588, '_timestamp': 1762309234.4676766}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.32579140173214854, 'val/loss': 0.18308352359703609, 'test/loss': 0.16326671093702316, '_timestamp': 1762309242.0446017}).
Epoch: 2, Steps: 132 | Train Loss: 0.3257914 Vali Loss: 0.1830835 Test Loss: 0.1632667
Validation loss decreased (0.183133 --> 0.183084).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.3116817 Vali Loss: 0.1918897 Test Loss: 0.1673307
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.3044949 Vali Loss: 0.1887406 Test Loss: 0.1682591
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2994149 Vali Loss: 0.1893693 Test Loss: 0.1703300
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2972117 Vali Loss: 0.1890052 Test Loss: 0.1686175
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2958284 Vali Loss: 0.1885261 Test Loss: 0.1678530
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2963827 Vali Loss: 0.1888108 Test Loss: 0.1675459
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2951497 Vali Loss: 0.1879973 Test Loss: 0.1679313
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2958122 Vali Loss: 0.1878742 Test Loss: 0.1679254
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2948705 Vali Loss: 0.1880438 Test Loss: 0.1678355
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2951406 Vali Loss: 0.1884636 Test Loss: 0.1678065
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ABSA_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 219
test shape: (219, 22, 1) (219, 22, 1)
test shape: (219, 22, 1) (219, 22, 1)


	mse:0.00048664724454283714, mae:0.01692899689078331, rmse:0.022060083225369453, r2:-0.03868734836578369, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0169, RMSE: 0.0221, RÂ²: -0.0387, MAPE: 1.71%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.657 MB of 0.658 MB uploadedwandb: \ 0.657 MB of 0.658 MB uploadedwandb: | 0.658 MB of 0.658 MB uploadedwandb: / 0.658 MB of 0.872 MB uploadedwandb: - 0.658 MB of 0.872 MB uploadedwandb: \ 0.872 MB of 0.872 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ƒâ–ˆâ–„â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–„â–ƒâ–‚â–ƒâ–â–â–â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.16781
wandb:                 train/loss 0.29514
wandb:   val/directional_accuracy 51.18504
wandb:                   val/loss 0.18846
wandb:                    val/mae 0.01693
wandb:                   val/mape 171.1396
wandb:                    val/mse 0.00049
wandb:                     val/r2 -0.03869
wandb:                   val/rmse 0.02206
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/dqlk0r57
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_042021-dqlk0r57/logs
Completed: ABSA H=22

Training: Autoformer on ABSA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_042213-w8sk7n1p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/w8sk7n1p
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ABSA_H50 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/w8sk7n1p
>>>>>>>start training : long_term_forecast_Autoformer_ABSA_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 191
Epoch: 1, Steps: 132 | Train Loss: 0.3697436 Vali Loss: 0.1875181 Test Loss: 0.1687955
Validation loss decreased (inf --> 0.187518).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.3386014 Vali Loss: 0.1946717 Test Loss: 0.1704345
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.36974356605699565, 'val/loss': 0.18751811981201172, 'test/loss': 0.16879554962118468, '_timestamp': 1762309346.1717255}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3386014108404969, 'val/loss': 0.19467168301343918, 'test/loss': 0.1704345258573691, '_timestamp': 1762309353.219659}).
Epoch: 3, Steps: 132 | Train Loss: 0.3270176 Vali Loss: 0.1992594 Test Loss: 0.1726355
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.3218632 Vali Loss: 0.1993169 Test Loss: 0.1739710
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.3178157 Vali Loss: 0.1991527 Test Loss: 0.1760627
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.3161776 Vali Loss: 0.2033595 Test Loss: 0.1731220
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.3148446 Vali Loss: 0.2023605 Test Loss: 0.1733374
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.3140758 Vali Loss: 0.2018905 Test Loss: 0.1740208
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.3144771 Vali Loss: 0.2017580 Test Loss: 0.1737733
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.3151879 Vali Loss: 0.2019245 Test Loss: 0.1737544
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.3151734 Vali Loss: 0.2020338 Test Loss: 0.1736936
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ABSA_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 191
test shape: (191, 50, 1) (191, 50, 1)
test shape: (191, 50, 1) (191, 50, 1)


	mse:0.000520047964528203, mae:0.017735440284013748, rmse:0.022804560139775276, r2:-0.06897997856140137, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0177, RMSE: 0.0228, RÂ²: -0.0690, MAPE: 1.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.770 MB of 0.773 MB uploadedwandb: \ 0.770 MB of 0.773 MB uploadedwandb: | 0.773 MB of 0.773 MB uploadedwandb: / 0.773 MB of 0.987 MB uploadedwandb: - 0.773 MB of 0.987 MB uploadedwandb: \ 0.987 MB of 0.987 MB uploadedwandb: | 0.987 MB of 0.987 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–„â–ˆâ–‚â–‚â–„â–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–â–â–ˆâ–†â–†â–…â–†â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.17369
wandb:                 train/loss 0.31517
wandb:   val/directional_accuracy 50.79603
wandb:                   val/loss 0.20203
wandb:                    val/mae 0.01774
wandb:                   val/mape 149.86994
wandb:                    val/mse 0.00052
wandb:                     val/r2 -0.06898
wandb:                   val/rmse 0.0228
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/w8sk7n1p
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_042213-w8sk7n1p/logs
Completed: ABSA H=50

Training: Autoformer on ABSA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_042356-etl0u8dx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/etl0u8dx
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ABSA_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/etl0u8dx
>>>>>>>start training : long_term_forecast_Autoformer_ABSA_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 141
Epoch: 1, Steps: 130 | Train Loss: 0.4089620 Vali Loss: 0.1917019 Test Loss: 0.1701127
Validation loss decreased (inf --> 0.191702).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.3807721 Vali Loss: 0.2040048 Test Loss: 0.1674307
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.40896195838084587, 'val/loss': 0.1917018860578537, 'test/loss': 0.17011269479990004, '_timestamp': 1762309448.6186943}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.38077212010438627, 'val/loss': 0.20400480329990386, 'test/loss': 0.16743073910474776, '_timestamp': 1762309455.6716077}).
Epoch: 3, Steps: 130 | Train Loss: 0.3726568 Vali Loss: 0.2034975 Test Loss: 0.1682429
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.3685942 Vali Loss: 0.2034671 Test Loss: 0.1677654
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.3661372 Vali Loss: 0.2037277 Test Loss: 0.1686954
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.3647803 Vali Loss: 0.2044348 Test Loss: 0.1691926
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.3651793 Vali Loss: 0.2047860 Test Loss: 0.1692238
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.3639503 Vali Loss: 0.2040879 Test Loss: 0.1693953
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.3640682 Vali Loss: 0.2042469 Test Loss: 0.1695198
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.3633687 Vali Loss: 0.2032825 Test Loss: 0.1694663
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.3635955 Vali Loss: 0.2038127 Test Loss: 0.1696089
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ABSA_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 141
test shape: (141, 100, 1) (141, 100, 1)
test shape: (141, 100, 1) (141, 100, 1)


	mse:0.000537899206392467, mae:0.017689086496829987, rmse:0.02319265343248844, r2:-0.04237210750579834, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0177, RMSE: 0.0232, RÂ²: -0.0424, MAPE: 1.24%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.805 MB of 0.810 MB uploadedwandb: \ 0.805 MB of 0.810 MB uploadedwandb: | 0.805 MB of 0.810 MB uploadedwandb: / 0.810 MB of 0.810 MB uploadedwandb: - 0.810 MB of 1.025 MB uploadedwandb: \ 1.025 MB of 1.025 MB uploadedwandb: | 1.025 MB of 1.025 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ƒâ–â–…â–†â–‡â–‡â–ˆâ–‡â–ˆ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–‚â–ƒâ–†â–ˆâ–…â–…â–â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.16961
wandb:                 train/loss 0.3636
wandb:   val/directional_accuracy 48.98632
wandb:                   val/loss 0.20381
wandb:                    val/mae 0.01769
wandb:                   val/mape 123.80092
wandb:                    val/mse 0.00054
wandb:                     val/r2 -0.04237
wandb:                   val/rmse 0.02319
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/etl0u8dx
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_042356-etl0u8dx/logs
Completed: ABSA H=100

Training: Autoformer on SASOL for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_042541-ut2bik8k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/ut2bik8k
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SASOL_H3 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/ut2bik8k
>>>>>>>start training : long_term_forecast_Autoformer_SASOL_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 3
============================================================
train 3772
val 211
test 212
Epoch: 1, Steps: 118 | Train Loss: 0.2944105 Vali Loss: 0.1201977 Test Loss: 0.1622130
Validation loss decreased (inf --> 0.120198).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2368346 Vali Loss: 0.1210791 Test Loss: 0.1679773
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29441054367412955, 'val/loss': 0.12019771976130349, 'test/loss': 0.16221299767494202, '_timestamp': 1762309553.3980818}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23683461622666505, 'val/loss': 0.12107912770339421, 'test/loss': 0.16797725962741034, '_timestamp': 1762309559.8518713}).
Epoch: 3, Steps: 118 | Train Loss: 0.2197720 Vali Loss: 0.1157863 Test Loss: 0.1550698
Validation loss decreased (0.120198 --> 0.115786).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2125458 Vali Loss: 0.1131033 Test Loss: 0.1502718
Validation loss decreased (0.115786 --> 0.113103).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2100780 Vali Loss: 0.1091803 Test Loss: 0.1470791
Validation loss decreased (0.113103 --> 0.109180).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2066590 Vali Loss: 0.1098056 Test Loss: 0.1477043
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2063520 Vali Loss: 0.1119070 Test Loss: 0.1467056
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2053398 Vali Loss: 0.1127445 Test Loss: 0.1466859
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2055863 Vali Loss: 0.1088094 Test Loss: 0.1467928
Validation loss decreased (0.109180 --> 0.108809).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2040348 Vali Loss: 0.1094765 Test Loss: 0.1467380
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2053217 Vali Loss: 0.1093764 Test Loss: 0.1468278
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2058822 Vali Loss: 0.1071341 Test Loss: 0.1468304
Validation loss decreased (0.108809 --> 0.107134).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2047024 Vali Loss: 0.1091390 Test Loss: 0.1467646
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2038559 Vali Loss: 0.1084710 Test Loss: 0.1468319
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.2041999 Vali Loss: 0.1093796 Test Loss: 0.1468093
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.2049789 Vali Loss: 0.1071317 Test Loss: 0.1468090
Validation loss decreased (0.107134 --> 0.107132).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.2034189 Vali Loss: 0.1080522 Test Loss: 0.1468102
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.2045189 Vali Loss: 0.1093557 Test Loss: 0.1468098
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.2046126 Vali Loss: 0.1101230 Test Loss: 0.1468088
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.2042166 Vali Loss: 0.1100802 Test Loss: 0.1468272
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 118 | Train Loss: 0.2039787 Vali Loss: 0.1129622 Test Loss: 0.1468095
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 118 | Train Loss: 0.2044652 Vali Loss: 0.1100829 Test Loss: 0.1468270
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 118 | Train Loss: 0.2051767 Vali Loss: 0.1084654 Test Loss: 0.1468091
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 118 | Train Loss: 0.2056824 Vali Loss: 0.1084140 Test Loss: 0.1468269
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 118 | Train Loss: 0.2049944 Vali Loss: 0.1075026 Test Loss: 0.1468270
EarlyStopping counter: 9 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 118 | Train Loss: 0.2047850 Vali Loss: 0.1082895 Test Loss: 0.1468268
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SASOL_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 212
test shape: (212, 3, 1) (212, 3, 1)
test shape: (212, 3, 1) (212, 3, 1)


	mse:0.002275313250720501, mae:0.03515957295894623, rmse:0.047700244933366776, r2:-0.03282308578491211, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0352, RMSE: 0.0477, RÂ²: -0.0328, MAPE: 9093613.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.461 MB of 0.461 MB uploadedwandb: \ 0.461 MB of 0.461 MB uploadedwandb: | 0.461 MB of 0.461 MB uploadedwandb: / 0.461 MB of 0.461 MB uploadedwandb: - 0.461 MB of 0.461 MB uploadedwandb: \ 0.461 MB of 0.461 MB uploadedwandb: | 0.591 MB of 0.808 MB uploaded (0.002 MB deduped)wandb: / 0.591 MB of 0.808 MB uploaded (0.002 MB deduped)wandb: - 0.808 MB of 0.808 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–‚â–â–â–‚â–â–â–â–‚â–‚â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–ƒâ–ƒâ–…â–†â–‚â–ƒâ–ƒâ–â–ƒâ–‚â–ƒâ–â–‚â–ƒâ–ƒâ–ƒâ–†â–ƒâ–‚â–‚â–â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 25
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.14683
wandb:                 train/loss 0.20478
wandb:   val/directional_accuracy 50.4717
wandb:                   val/loss 0.10829
wandb:                    val/mae 0.03516
wandb:                   val/mape 909361300.0
wandb:                    val/mse 0.00228
wandb:                     val/r2 -0.03282
wandb:                   val/rmse 0.0477
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/ut2bik8k
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_042541-ut2bik8k/logs
Completed: SASOL H=3

Training: Autoformer on SASOL for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_042855-oueuidd9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/oueuidd9
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SASOL_H5 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/oueuidd9
>>>>>>>start training : long_term_forecast_Autoformer_SASOL_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 5
============================================================
train 3770
val 209
test 210
Epoch: 1, Steps: 118 | Train Loss: 0.2916863 Vali Loss: 0.1317959 Test Loss: 0.1683561
Validation loss decreased (inf --> 0.131796).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2411894 Vali Loss: 0.1188349 Test Loss: 0.1704240
Validation loss decreased (0.131796 --> 0.118835).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2916863436921168, 'val/loss': 0.1317958618913378, 'test/loss': 0.16835610355649674, '_timestamp': 1762309748.7108948}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24118935367313482, 'val/loss': 0.1188349404505321, 'test/loss': 0.17042396004710877, '_timestamp': 1762309755.1572487}).
Epoch: 3, Steps: 118 | Train Loss: 0.2214939 Vali Loss: 0.1097670 Test Loss: 0.1650231
Validation loss decreased (0.118835 --> 0.109767).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2151922 Vali Loss: 0.1136200 Test Loss: 0.1581912
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2111941 Vali Loss: 0.1124589 Test Loss: 0.1564524
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2090733 Vali Loss: 0.1121205 Test Loss: 0.1551948
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2070484 Vali Loss: 0.1112327 Test Loss: 0.1548119
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2078922 Vali Loss: 0.1104070 Test Loss: 0.1542812
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2076399 Vali Loss: 0.1106444 Test Loss: 0.1544910
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2076976 Vali Loss: 0.1106891 Test Loss: 0.1544497
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2071568 Vali Loss: 0.1130796 Test Loss: 0.1543554
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2066661 Vali Loss: 0.1114816 Test Loss: 0.1544903
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2079810 Vali Loss: 0.1123725 Test Loss: 0.1543714
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SASOL_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 210
test shape: (210, 5, 1) (210, 5, 1)
test shape: (210, 5, 1) (210, 5, 1)


	mse:0.002292431890964508, mae:0.035176921635866165, rmse:0.04787934571504593, r2:-0.032906174659729004, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0352, RMSE: 0.0479, RÂ²: -0.0329, MAPE: 16369777.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.505 MB of 0.505 MB uploadedwandb: \ 0.505 MB of 0.505 MB uploadedwandb: | 0.505 MB of 0.505 MB uploadedwandb: / 0.505 MB of 0.505 MB uploadedwandb: - 0.505 MB of 0.720 MB uploadedwandb: \ 0.720 MB of 0.720 MB uploadedwandb: | 0.720 MB of 0.720 MB uploadedwandb: / 0.720 MB of 0.720 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–‚â–â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ˆâ–†â–…â–„â–‚â–ƒâ–ƒâ–‡â–„â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.15437
wandb:                 train/loss 0.20798
wandb:   val/directional_accuracy 55.71429
wandb:                   val/loss 0.11237
wandb:                    val/mae 0.03518
wandb:                   val/mape 1636977700.0
wandb:                    val/mse 0.00229
wandb:                     val/r2 -0.03291
wandb:                   val/rmse 0.04788
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/oueuidd9
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_042855-oueuidd9/logs
Completed: SASOL H=5

Training: Autoformer on SASOL for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_043049-eia75ghp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/eia75ghp
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SASOL_H10Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/eia75ghp
>>>>>>>start training : long_term_forecast_Autoformer_SASOL_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 10
============================================================
train 3765
val 204
test 205
Epoch: 1, Steps: 118 | Train Loss: 0.2812424 Vali Loss: 0.1220050 Test Loss: 0.1640196
Validation loss decreased (inf --> 0.122005).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2374641 Vali Loss: 0.1227997 Test Loss: 0.1716450
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.28124238891621767, 'val/loss': 0.1220049506851605, 'test/loss': 0.16401963574545725, '_timestamp': 1762309861.5130198}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2374641482355231, 'val/loss': 0.1227997403059687, 'test/loss': 0.17164497077465057, '_timestamp': 1762309867.9659324}).
Epoch: 3, Steps: 118 | Train Loss: 0.2234784 Vali Loss: 0.1118729 Test Loss: 0.1631045
Validation loss decreased (0.122005 --> 0.111873).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2181043 Vali Loss: 0.1117927 Test Loss: 0.1613675
Validation loss decreased (0.111873 --> 0.111793).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2148525 Vali Loss: 0.1107743 Test Loss: 0.1625784
Validation loss decreased (0.111793 --> 0.110774).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2129758 Vali Loss: 0.1110334 Test Loss: 0.1621540
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2127130 Vali Loss: 0.1170082 Test Loss: 0.1617413
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2121700 Vali Loss: 0.1123012 Test Loss: 0.1612158
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2126836 Vali Loss: 0.1115181 Test Loss: 0.1610250
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2119987 Vali Loss: 0.1116357 Test Loss: 0.1614250
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2121436 Vali Loss: 0.1144736 Test Loss: 0.1613988
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2123813 Vali Loss: 0.1122518 Test Loss: 0.1614228
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2131436 Vali Loss: 0.1146924 Test Loss: 0.1613908
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2124331 Vali Loss: 0.1137643 Test Loss: 0.1613858
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.2123801 Vali Loss: 0.1161166 Test Loss: 0.1613898
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SASOL_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 205
test shape: (205, 10, 1) (205, 10, 1)
test shape: (205, 10, 1) (205, 10, 1)


	mse:0.002293173223733902, mae:0.035255685448646545, rmse:0.04788708686828613, r2:-0.033100008964538574, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0353, RMSE: 0.0479, RÂ²: -0.0331, MAPE: 11285855.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.549 MB of 0.550 MB uploadedwandb: \ 0.549 MB of 0.550 MB uploadedwandb: | 0.549 MB of 0.550 MB uploadedwandb: / 0.550 MB of 0.550 MB uploadedwandb: - 0.550 MB of 0.550 MB uploadedwandb: \ 0.550 MB of 0.765 MB uploadedwandb: | 0.727 MB of 0.765 MB uploadedwandb: / 0.765 MB of 0.765 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–†â–…â–ƒâ–‚â–â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–‚â–â–â–ˆâ–ƒâ–‚â–‚â–…â–ƒâ–…â–„â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.16139
wandb:                 train/loss 0.21238
wandb:   val/directional_accuracy 51.81572
wandb:                   val/loss 0.11612
wandb:                    val/mae 0.03526
wandb:                   val/mape 1128585500.0
wandb:                    val/mse 0.00229
wandb:                     val/r2 -0.0331
wandb:                   val/rmse 0.04789
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/eia75ghp
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_043049-eia75ghp/logs
Completed: SASOL H=10

Training: Autoformer on SASOL for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_043257-95fbcwrg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/95fbcwrg
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SASOL_H22Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/95fbcwrg
>>>>>>>start training : long_term_forecast_Autoformer_SASOL_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 22
============================================================
train 3753
val 192
test 193
Epoch: 1, Steps: 118 | Train Loss: 0.2886764 Vali Loss: 0.1130944 Test Loss: 0.1686458
Validation loss decreased (inf --> 0.113094).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2529186 Vali Loss: 0.1223120 Test Loss: 0.1721591
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.288676423162727, 'val/loss': 0.11309442420800526, 'test/loss': 0.16864584173474992, '_timestamp': 1762309989.3254204}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25291856075242414, 'val/loss': 0.12231200933456421, 'test/loss': 0.17215909170252935, '_timestamp': 1762309995.7762358}).
Epoch: 3, Steps: 118 | Train Loss: 0.2382303 Vali Loss: 0.1154748 Test Loss: 0.1674601
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2316386 Vali Loss: 0.1166466 Test Loss: 0.1742646
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2273770 Vali Loss: 0.1159587 Test Loss: 0.1752156
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2262501 Vali Loss: 0.1167707 Test Loss: 0.1737631
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2241385 Vali Loss: 0.1172869 Test Loss: 0.1742391
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2228782 Vali Loss: 0.1174526 Test Loss: 0.1756091
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2234713 Vali Loss: 0.1172132 Test Loss: 0.1749934
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2240778 Vali Loss: 0.1174075 Test Loss: 0.1747835
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2232249 Vali Loss: 0.1171479 Test Loss: 0.1746929
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SASOL_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 193
test shape: (193, 22, 1) (193, 22, 1)
test shape: (193, 22, 1) (193, 22, 1)


	mse:0.0023865660186856985, mae:0.03577885404229164, rmse:0.04885249212384224, r2:-0.06347143650054932, dtw:Not calculated


VAL - MSE: 0.0024, MAE: 0.0358, RMSE: 0.0489, RÂ²: -0.0635, MAPE: 13318066.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.609 MB of 0.610 MB uploadedwandb: \ 0.609 MB of 0.610 MB uploadedwandb: | 0.610 MB of 0.610 MB uploadedwandb: / 0.610 MB of 0.610 MB uploadedwandb: - 0.610 MB of 0.825 MB uploadedwandb: \ 0.825 MB of 0.825 MB uploadedwandb: | 0.825 MB of 0.825 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–‡â–ˆâ–†â–‡â–ˆâ–‡â–‡â–‡
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–…â–ƒâ–†â–‡â–ˆâ–‡â–ˆâ–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.17469
wandb:                 train/loss 0.22322
wandb:   val/directional_accuracy 47.9398
wandb:                   val/loss 0.11715
wandb:                    val/mae 0.03578
wandb:                   val/mape 1331806600.0
wandb:                    val/mse 0.00239
wandb:                     val/r2 -0.06347
wandb:                   val/rmse 0.04885
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/95fbcwrg
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_043257-95fbcwrg/logs
Completed: SASOL H=22

Training: Autoformer on SASOL for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_043435-hmmh0xpn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/hmmh0xpn
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SASOL_H50Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/hmmh0xpn
>>>>>>>start training : long_term_forecast_Autoformer_SASOL_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 50
============================================================
train 3725
val 164
test 165
Epoch: 1, Steps: 117 | Train Loss: 0.3166531 Vali Loss: 0.1080589 Test Loss: 0.1893601
Validation loss decreased (inf --> 0.108059).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 117 | Train Loss: 0.2692836 Vali Loss: 0.1186277 Test Loss: 0.2217647
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3166530900276624, 'val/loss': 0.1080588698387146, 'test/loss': 0.18936005483071008, '_timestamp': 1762310087.3070254}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2692835786913195, 'val/loss': 0.11862766121824582, 'test/loss': 0.2217646837234497, '_timestamp': 1762310093.6855118}).
Epoch: 3, Steps: 117 | Train Loss: 0.2513077 Vali Loss: 0.1260456 Test Loss: 0.2179907
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 117 | Train Loss: 0.2425828 Vali Loss: 0.1156716 Test Loss: 0.2204852
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 117 | Train Loss: 0.2387359 Vali Loss: 0.1235875 Test Loss: 0.2191159
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 117 | Train Loss: 0.2373890 Vali Loss: 0.1232404 Test Loss: 0.2204300
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 117 | Train Loss: 0.2355038 Vali Loss: 0.1236277 Test Loss: 0.2180883
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 117 | Train Loss: 0.2354692 Vali Loss: 0.1196044 Test Loss: 0.2208703
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 117 | Train Loss: 0.2350933 Vali Loss: 0.1218438 Test Loss: 0.2210089
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 117 | Train Loss: 0.2350116 Vali Loss: 0.1219903 Test Loss: 0.2218353
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 117 | Train Loss: 0.2345175 Vali Loss: 0.1181798 Test Loss: 0.2215616
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SASOL_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 165
test shape: (165, 50, 1) (165, 50, 1)
test shape: (165, 50, 1) (165, 50, 1)


	mse:0.002190675586462021, mae:0.03470447659492493, rmse:0.046804651618003845, r2:-0.06926774978637695, dtw:Not calculated


VAL - MSE: 0.0022, MAE: 0.0347, RMSE: 0.0468, RÂ²: -0.0693, MAPE: 12551695.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.650 MB of 0.652 MB uploadedwandb: \ 0.650 MB of 0.652 MB uploadedwandb: | 0.650 MB of 0.652 MB uploadedwandb: / 0.652 MB of 0.652 MB uploadedwandb: - 0.652 MB of 0.652 MB uploadedwandb: \ 0.652 MB of 0.867 MB uploadedwandb: | 0.863 MB of 0.867 MB uploadedwandb: / 0.867 MB of 0.867 MB uploadedwandb: - 0.867 MB of 0.867 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–†â–ƒâ–…â–â–†â–†â–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–†â–†â–†â–„â–…â–…â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.22156
wandb:                 train/loss 0.23452
wandb:   val/directional_accuracy 47.92826
wandb:                   val/loss 0.11818
wandb:                    val/mae 0.0347
wandb:                   val/mape 1255169500.0
wandb:                    val/mse 0.00219
wandb:                     val/r2 -0.06927
wandb:                   val/rmse 0.0468
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/hmmh0xpn
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_043435-hmmh0xpn/logs
Completed: SASOL H=50

Training: Autoformer on SASOL for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_043614-ht6i5ijo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/ht6i5ijo
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SASOL_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/ht6i5ijo
>>>>>>>start training : long_term_forecast_Autoformer_SASOL_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 100
============================================================
train 3675
val 114
test 115
Epoch: 1, Steps: 115 | Train Loss: 0.3811456 Vali Loss: 0.1240216 Test Loss: 0.1873607
Validation loss decreased (inf --> 0.124022).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 115 | Train Loss: 0.3454182 Vali Loss: 0.1189029 Test Loss: 0.2084669
Validation loss decreased (0.124022 --> 0.118903).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.38114562138267183, 'val/loss': 0.12402156181633472, 'test/loss': 0.1873607411980629, '_timestamp': 1762310186.1878374}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.34541819911936056, 'val/loss': 0.11890288069844246, 'test/loss': 0.20846687257289886, '_timestamp': 1762310192.381953}).
Epoch: 3, Steps: 115 | Train Loss: 0.3303631 Vali Loss: 0.1258290 Test Loss: 0.2289675
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 115 | Train Loss: 0.3249524 Vali Loss: 0.1370940 Test Loss: 0.2375934
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 115 | Train Loss: 0.3223952 Vali Loss: 0.1355912 Test Loss: 0.2354829
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 115 | Train Loss: 0.3206145 Vali Loss: 0.1319996 Test Loss: 0.2304470
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 115 | Train Loss: 0.3195397 Vali Loss: 0.1349918 Test Loss: 0.2363396
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 115 | Train Loss: 0.3189979 Vali Loss: 0.1310370 Test Loss: 0.2308082
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 115 | Train Loss: 0.3189947 Vali Loss: 0.1315259 Test Loss: 0.2348730
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 115 | Train Loss: 0.3191936 Vali Loss: 0.1325800 Test Loss: 0.2325510
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 115 | Train Loss: 0.3186770 Vali Loss: 0.1313906 Test Loss: 0.2321507
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 115 | Train Loss: 0.3190968 Vali Loss: 0.1336974 Test Loss: 0.2324192
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SASOL_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 115
test shape: (115, 100, 1) (115, 100, 1)
test shape: (115, 100, 1) (115, 100, 1)


	mse:0.0020187904592603445, mae:0.03301677107810974, rmse:0.04493095353245735, r2:-0.01732051372528076, dtw:Not calculated


VAL - MSE: 0.0020, MAE: 0.0330, RMSE: 0.0449, RÂ²: -0.0173, MAPE: 7447414.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.652 MB of 0.656 MB uploadedwandb: \ 0.652 MB of 0.656 MB uploadedwandb: | 0.652 MB of 0.656 MB uploadedwandb: / 0.656 MB of 0.656 MB uploadedwandb: - 0.656 MB of 0.656 MB uploadedwandb: \ 0.656 MB of 0.871 MB uploadedwandb: | 0.871 MB of 0.871 MB uploadedwandb: / 0.871 MB of 0.871 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–†â–‚â–‡â–‚â–†â–„â–„â–„
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ˆâ–‡â–…â–‡â–„â–…â–…â–„â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.23242
wandb:                 train/loss 0.3191
wandb:   val/directional_accuracy 50.50505
wandb:                   val/loss 0.1337
wandb:                    val/mae 0.03302
wandb:                   val/mape 744741400.0
wandb:                    val/mse 0.00202
wandb:                     val/r2 -0.01732
wandb:                   val/rmse 0.04493
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/ht6i5ijo
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_043614-ht6i5ijo/logs
Completed: SASOL H=100

Autoformer training completed for all datasets!
