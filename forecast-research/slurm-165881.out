##############################################################################
# Training Autoformer Model on All Datasets
##############################################################################
Training: Autoformer on NVIDIA for H=3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NVIDIA_H3Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_NVIDIA_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 3
============================================================
train 2201
val 123
test 125
Validation loss decreased (inf --> 0.130777).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Autoformer_NVIDIA_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 125
test shape: (125, 3, 6) (125, 3, 6)
test shape: (125, 3, 6) (125, 3, 6)


	mse:0.35301473736763, mae:0.4224323630332947, rmse:0.5941504240036011, r2:0.7416477501392365, dtw:Not calculated


VAL - MSE: 0.3530, MAE: 0.4224, RMSE: 0.5942, RÂ²: 0.7416, MAPE: 1.06%
Completed: NVIDIA H=3

Training: Autoformer on NVIDIA for H=5
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NVIDIA_H5Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_NVIDIA_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 5
============================================================
train 2199
val 121
test 123
Validation loss decreased (inf --> 0.140739).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
Validation loss decreased (0.140739 --> 0.126537).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.126537 --> 0.122413).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (0.122413 --> 0.120261).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (0.120261 --> 0.118085).  Saving model ...
Updating learning rate to 3.125e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
Validation loss decreased (0.118085 --> 0.117595).  Saving model ...
Updating learning rate to 3.90625e-07
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.953125e-07
EarlyStopping counter: 2 out of 5
Updating learning rate to 9.765625e-08
EarlyStopping counter: 3 out of 5
Updating learning rate to 4.8828125e-08
Validation loss decreased (0.117595 --> 0.117226).  Saving model ...
Updating learning rate to 2.44140625e-08
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.220703125e-08
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.103515625e-09
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.0517578125e-09
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.52587890625e-09
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Autoformer_NVIDIA_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 123
test shape: (123, 5, 6) (123, 5, 6)
test shape: (123, 5, 6) (123, 5, 6)


	mse:0.28966253995895386, mae:0.36650389432907104, rmse:0.5382030606269836, r2:0.7862006425857544, dtw:Not calculated


VAL - MSE: 0.2897, MAE: 0.3665, RMSE: 0.5382, RÂ²: 0.7862, MAPE: 0.78%
Completed: NVIDIA H=5

Training: Autoformer on NVIDIA for H=10
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NVIDIA_H10Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_NVIDIA_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 10
============================================================
train 2194
val 116
test 118
Validation loss decreased (inf --> 0.129845).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
Validation loss decreased (0.129845 --> 0.127302).  Saving model ...
Updating learning rate to 1.25e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Autoformer_NVIDIA_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 118
test shape: (118, 10, 6) (118, 10, 6)
test shape: (118, 10, 6) (118, 10, 6)


	mse:0.3355998992919922, mae:0.41149163246154785, rmse:0.5793098211288452, r2:0.7475222647190094, dtw:Not calculated


VAL - MSE: 0.3356, MAE: 0.4115, RMSE: 0.5793, RÂ²: 0.7475, MAPE: 0.89%
Completed: NVIDIA H=10

Training: Autoformer on NVIDIA for H=22
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NVIDIA_H22Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_NVIDIA_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 22
============================================================
train 2182
val 104
test 106
Validation loss decreased (inf --> 0.160176).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.160176 --> 0.155344).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.155344 --> 0.153336).  Saving model ...
Updating learning rate to 2.5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
Validation loss decreased (0.153336 --> 0.151313).  Saving model ...
Updating learning rate to 3.125e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
Validation loss decreased (0.151313 --> 0.150054).  Saving model ...
Updating learning rate to 7.8125e-07
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.90625e-07
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.953125e-07
EarlyStopping counter: 3 out of 5
Updating learning rate to 9.765625e-08
EarlyStopping counter: 4 out of 5
Updating learning rate to 4.8828125e-08
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Autoformer_NVIDIA_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 106
test shape: (106, 22, 6) (106, 22, 6)
test shape: (106, 22, 6) (106, 22, 6)


	mse:0.31828686594963074, mae:0.4100913405418396, rmse:0.56416916847229, r2:0.7478142976760864, dtw:Not calculated


VAL - MSE: 0.3183, MAE: 0.4101, RMSE: 0.5642, RÂ²: 0.7478, MAPE: 0.81%
Completed: NVIDIA H=22

Training: Autoformer on NVIDIA for H=50
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NVIDIA_H50Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_NVIDIA_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 50
============================================================
train 2154
val 76
test 78
Validation loss decreased (inf --> 0.176895).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Autoformer_NVIDIA_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 78
test shape: (78, 50, 6) (78, 50, 6)
test shape: (78, 50, 6) (78, 50, 6)


	mse:0.497711181640625, mae:0.5249426364898682, rmse:0.7054864764213562, r2:0.5630657076835632, dtw:Not calculated


VAL - MSE: 0.4977, MAE: 0.5249, RMSE: 0.7055, RÂ²: 0.5631, MAPE: 0.84%
Completed: NVIDIA H=50

Training: Autoformer on NVIDIA for H=100
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NVIDIA_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_NVIDIA_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 100
============================================================
train 2104
val 26
test 28
Validation loss decreased (inf --> 0.168316).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Autoformer_NVIDIA_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 28
test shape: (28, 100, 6) (28, 100, 6)
test shape: (28, 100, 6) (28, 100, 6)


	mse:0.7882751822471619, mae:0.6465346217155457, rmse:0.8878486156463623, r2:0.3585852384567261, dtw:Not calculated


VAL - MSE: 0.7883, MAE: 0.6465, RMSE: 0.8878, RÂ²: 0.3586, MAPE: 0.83%
Completed: NVIDIA H=100

Training: Autoformer on APPLE for H=3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_APPLE_H3 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_APPLE_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 3
============================================================
train 2201
val 123
test 125
Validation loss decreased (inf --> 0.362035).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.362035 --> 0.354986).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.354986 --> 0.338433).  Saving model ...
Updating learning rate to 2.5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
Validation loss decreased (0.338433 --> 0.334592).  Saving model ...
Updating learning rate to 6.25e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 3 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.90625e-07
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Autoformer_APPLE_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 125
test shape: (125, 3, 6) (125, 3, 6)
test shape: (125, 3, 6) (125, 3, 6)


	mse:0.483011394739151, mae:0.4107993543148041, rmse:0.6949902176856995, r2:0.6445348858833313, dtw:Not calculated


VAL - MSE: 0.4830, MAE: 0.4108, RMSE: 0.6950, RÂ²: 0.6445, MAPE: 0.69%
Completed: APPLE H=3

Training: Autoformer on APPLE for H=5
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_APPLE_H5 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_APPLE_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 5
============================================================
train 2199
val 121
test 123
Validation loss decreased (inf --> 0.383033).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.383033 --> 0.345880).  Saving model ...
Updating learning rate to 5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
Validation loss decreased (0.345880 --> 0.341030).  Saving model ...
Updating learning rate to 1.25e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
Validation loss decreased (0.341030 --> 0.340101).  Saving model ...
Updating learning rate to 3.125e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.90625e-07
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.953125e-07
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Autoformer_APPLE_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 123
test shape: (123, 5, 6) (123, 5, 6)
test shape: (123, 5, 6) (123, 5, 6)


	mse:0.5169369578361511, mae:0.42962396144866943, rmse:0.7189832925796509, r2:0.618799090385437, dtw:Not calculated


VAL - MSE: 0.5169, MAE: 0.4296, RMSE: 0.7190, RÂ²: 0.6188, MAPE: 0.76%
Completed: APPLE H=5

Training: Autoformer on APPLE for H=10
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_APPLE_H10Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_APPLE_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 10
============================================================
train 2194
val 116
test 118
Validation loss decreased (inf --> 0.380954).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.380954 --> 0.370210).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.370210 --> 0.368968).  Saving model ...
Updating learning rate to 2.5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
Validation loss decreased (0.368968 --> 0.364070).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (0.364070 --> 0.363556).  Saving model ...
Updating learning rate to 3.125e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
Validation loss decreased (0.363556 --> 0.358192).  Saving model ...
Updating learning rate to 7.8125e-07
Validation loss decreased (0.358192 --> 0.353589).  Saving model ...
Updating learning rate to 3.90625e-07
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.953125e-07
EarlyStopping counter: 2 out of 5
Updating learning rate to 9.765625e-08
EarlyStopping counter: 3 out of 5
Updating learning rate to 4.8828125e-08
EarlyStopping counter: 4 out of 5
Updating learning rate to 2.44140625e-08
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Autoformer_APPLE_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 118
test shape: (118, 10, 6) (118, 10, 6)
test shape: (118, 10, 6) (118, 10, 6)


	mse:0.5491179823875427, mae:0.4517698884010315, rmse:0.7410249710083008, r2:0.5944615602493286, dtw:Not calculated


VAL - MSE: 0.5491, MAE: 0.4518, RMSE: 0.7410, RÂ²: 0.5945, MAPE: 0.63%
Completed: APPLE H=10

Training: Autoformer on APPLE for H=22
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_APPLE_H22Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_APPLE_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 22
============================================================
train 2182
val 104
test 106
Validation loss decreased (inf --> 0.370437).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Autoformer_APPLE_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 106
test shape: (106, 22, 6) (106, 22, 6)
test shape: (106, 22, 6) (106, 22, 6)


	mse:0.6435604691505432, mae:0.48983103036880493, rmse:0.8022221922874451, r2:0.5223646759986877, dtw:Not calculated


VAL - MSE: 0.6436, MAE: 0.4898, RMSE: 0.8022, RÂ²: 0.5224, MAPE: 0.80%
Completed: APPLE H=22

Training: Autoformer on APPLE for H=50
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_APPLE_H50Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_APPLE_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 50
============================================================
train 2154
val 76
test 78
Validation loss decreased (inf --> 0.408057).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Autoformer_APPLE_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 78
test shape: (78, 50, 6) (78, 50, 6)
test shape: (78, 50, 6) (78, 50, 6)


	mse:0.6925850510597229, mae:0.5653182864189148, rmse:0.8322169780731201, r2:0.45511674880981445, dtw:Not calculated


VAL - MSE: 0.6926, MAE: 0.5653, RMSE: 0.8322, RÂ²: 0.4551, MAPE: 0.63%
Completed: APPLE H=50

Training: Autoformer on APPLE for H=100
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_APPLE_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_APPLE_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 100
============================================================
train 2104
val 26
test 28
Validation loss decreased (inf --> 0.440693).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.440693 --> 0.430676).  Saving model ...
Updating learning rate to 5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Autoformer_APPLE_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 28
test shape: (28, 100, 6) (28, 100, 6)
test shape: (28, 100, 6) (28, 100, 6)


	mse:0.7677888870239258, mae:0.667697548866272, rmse:0.8762356638908386, r2:0.4250997304916382, dtw:Not calculated


VAL - MSE: 0.7678, MAE: 0.6677, RMSE: 0.8762, RÂ²: 0.4251, MAPE: 0.88%
Completed: APPLE H=100

Training: Autoformer on SP500 for H=3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SP500_H3 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_SP500_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 3
============================================================
train 2201
val 123
test 125
Validation loss decreased (inf --> nan).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.125e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.5625e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.8125e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.90625e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.953125e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.765625e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.8828125e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.44140625e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.220703125e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 6.103515625e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.0517578125e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.52587890625e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.62939453125e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.814697265625e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.960464477539063e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.9802322387695314e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4901161193847657e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.450580596923828e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.725290298461914e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.862645149230957e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.313225746154786e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.656612873077393e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.3283064365386964e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1641532182693482e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.820766091346741e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.9103830456733705e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4551915228366853e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.275957614183426e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.637978807091713e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.8189894035458566e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.094947017729283e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.5474735088646414e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.2737367544323207e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1368683772161604e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.684341886080802e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.842170943040401e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4210854715202004e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.105427357601002e-19
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.552713678800501e-19
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.7763568394002505e-19
>>>>>>>testing : long_term_forecast_Autoformer_SP500_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 125
test shape: (125, 3, 6) (125, 3, 6)
test shape: (125, 3, 6) (125, 3, 6)


	mse:nan, mae:nan, rmse:nan, r2:nan, dtw:Not calculated


VAL - MSE: nan, MAE: nan, RMSE: nan, RÂ²: nan, MAPE: nan%
Completed: SP500 H=3

Training: Autoformer on SP500 for H=5
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SP500_H5 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_SP500_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 5
============================================================
train 2199
val 121
test 123
Validation loss decreased (inf --> nan).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.125e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.5625e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.8125e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.90625e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.953125e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.765625e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.8828125e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.44140625e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.220703125e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 6.103515625e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.0517578125e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.52587890625e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.62939453125e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.814697265625e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.960464477539063e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.9802322387695314e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4901161193847657e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.450580596923828e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.725290298461914e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.862645149230957e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.313225746154786e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.656612873077393e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.3283064365386964e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1641532182693482e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.820766091346741e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.9103830456733705e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4551915228366853e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.275957614183426e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.637978807091713e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.8189894035458566e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.094947017729283e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.5474735088646414e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.2737367544323207e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1368683772161604e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.684341886080802e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.842170943040401e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4210854715202004e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.105427357601002e-19
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.552713678800501e-19
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.7763568394002505e-19
>>>>>>>testing : long_term_forecast_Autoformer_SP500_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 123
test shape: (123, 5, 6) (123, 5, 6)
test shape: (123, 5, 6) (123, 5, 6)


	mse:nan, mae:nan, rmse:nan, r2:nan, dtw:Not calculated


VAL - MSE: nan, MAE: nan, RMSE: nan, RÂ²: nan, MAPE: nan%
Completed: SP500 H=5

Training: Autoformer on SP500 for H=10
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SP500_H10Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_SP500_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 10
============================================================
train 2194
val 116
test 118
Validation loss decreased (inf --> nan).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.125e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.5625e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.8125e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.90625e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.953125e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.765625e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.8828125e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.44140625e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.220703125e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 6.103515625e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.0517578125e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.52587890625e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.62939453125e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.814697265625e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.960464477539063e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.9802322387695314e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4901161193847657e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.450580596923828e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.725290298461914e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.862645149230957e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.313225746154786e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.656612873077393e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.3283064365386964e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1641532182693482e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.820766091346741e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.9103830456733705e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4551915228366853e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.275957614183426e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.637978807091713e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.8189894035458566e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.094947017729283e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.5474735088646414e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.2737367544323207e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1368683772161604e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.684341886080802e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.842170943040401e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4210854715202004e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.105427357601002e-19
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.552713678800501e-19
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.7763568394002505e-19
>>>>>>>testing : long_term_forecast_Autoformer_SP500_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 118
test shape: (118, 10, 6) (118, 10, 6)
test shape: (118, 10, 6) (118, 10, 6)


	mse:nan, mae:nan, rmse:nan, r2:nan, dtw:Not calculated


VAL - MSE: nan, MAE: nan, RMSE: nan, RÂ²: nan, MAPE: nan%
Completed: SP500 H=10

Training: Autoformer on SP500 for H=22
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SP500_H22Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_SP500_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 22
============================================================
train 2182
val 104
test 106
Validation loss decreased (inf --> nan).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.125e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.5625e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.8125e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.90625e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.953125e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.765625e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.8828125e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.44140625e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.220703125e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 6.103515625e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.0517578125e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.52587890625e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.62939453125e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.814697265625e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.960464477539063e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.9802322387695314e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4901161193847657e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.450580596923828e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.725290298461914e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.862645149230957e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.313225746154786e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.656612873077393e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.3283064365386964e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1641532182693482e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.820766091346741e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.9103830456733705e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4551915228366853e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.275957614183426e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.637978807091713e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.8189894035458566e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.094947017729283e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.5474735088646414e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.2737367544323207e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1368683772161604e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.684341886080802e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.842170943040401e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4210854715202004e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.105427357601002e-19
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.552713678800501e-19
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.7763568394002505e-19
>>>>>>>testing : long_term_forecast_Autoformer_SP500_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 106
test shape: (106, 22, 6) (106, 22, 6)
test shape: (106, 22, 6) (106, 22, 6)


	mse:nan, mae:nan, rmse:nan, r2:nan, dtw:Not calculated


VAL - MSE: nan, MAE: nan, RMSE: nan, RÂ²: nan, MAPE: nan%
Completed: SP500 H=22

Training: Autoformer on SP500 for H=50
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SP500_H50Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_SP500_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 50
============================================================
train 2154
val 76
test 78
Validation loss decreased (inf --> nan).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.125e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.5625e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.8125e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.90625e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.953125e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.765625e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.8828125e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.44140625e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.220703125e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 6.103515625e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.0517578125e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.52587890625e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.62939453125e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.814697265625e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.960464477539063e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.9802322387695314e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4901161193847657e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.450580596923828e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.725290298461914e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.862645149230957e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.313225746154786e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.656612873077393e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.3283064365386964e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1641532182693482e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.820766091346741e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.9103830456733705e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4551915228366853e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.275957614183426e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.637978807091713e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.8189894035458566e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.094947017729283e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.5474735088646414e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.2737367544323207e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1368683772161604e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.684341886080802e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.842170943040401e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4210854715202004e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.105427357601002e-19
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.552713678800501e-19
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.7763568394002505e-19
>>>>>>>testing : long_term_forecast_Autoformer_SP500_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 78
test shape: (78, 50, 6) (78, 50, 6)
test shape: (78, 50, 6) (78, 50, 6)


	mse:nan, mae:nan, rmse:nan, r2:nan, dtw:Not calculated


VAL - MSE: nan, MAE: nan, RMSE: nan, RÂ²: nan, MAPE: nan%
Completed: SP500 H=50

Training: Autoformer on SP500 for H=100
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SP500_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_SP500_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 100
============================================================
train 2104
val 26
test 28
Validation loss decreased (inf --> nan).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.125e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.5625e-06
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.8125e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.90625e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.953125e-07
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.765625e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.8828125e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.44140625e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.220703125e-08
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 6.103515625e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.0517578125e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.52587890625e-09
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.62939453125e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.814697265625e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.384185791015625e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.960464477539063e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.9802322387695314e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4901161193847657e-12
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.450580596923828e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.725290298461914e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.862645149230957e-13
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.313225746154786e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.656612873077393e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.3283064365386964e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1641532182693482e-14
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.820766091346741e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.9103830456733705e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4551915228366853e-15
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.275957614183426e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.637978807091713e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.8189894035458566e-16
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 9.094947017729283e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 4.5474735088646414e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.2737367544323207e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.1368683772161604e-17
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 5.684341886080802e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 2.842170943040401e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.4210854715202004e-18
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 7.105427357601002e-19
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 3.552713678800501e-19
Validation loss decreased (nan --> nan).  Saving model ...
Updating learning rate to 1.7763568394002505e-19
>>>>>>>testing : long_term_forecast_Autoformer_SP500_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 28
test shape: (28, 100, 6) (28, 100, 6)
test shape: (28, 100, 6) (28, 100, 6)


	mse:nan, mae:nan, rmse:nan, r2:nan, dtw:Not calculated


VAL - MSE: nan, MAE: nan, RMSE: nan, RÂ²: nan, MAPE: nan%
Completed: SP500 H=100

Training: Autoformer on NASDAQ for H=3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NASDAQ_H3Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_NASDAQ_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 3
============================================================
train 2201
val 123
test 125
Validation loss decreased (inf --> 0.108612).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.108612 --> 0.101199).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.101199 --> 0.079461).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.079461 --> 0.074001).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (0.074001 --> 0.072272).  Saving model ...
Updating learning rate to 6.25e-06
Validation loss decreased (0.072272 --> 0.071708).  Saving model ...
Updating learning rate to 3.125e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.90625e-07
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.953125e-07
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Autoformer_NASDAQ_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 125
test shape: (125, 3, 6) (125, 3, 6)
test shape: (125, 3, 6) (125, 3, 6)


	mse:0.13909347355365753, mae:0.25500860810279846, rmse:0.3729523718357086, r2:0.9525816142559052, dtw:Not calculated


VAL - MSE: 0.1391, MAE: 0.2550, RMSE: 0.3730, RÂ²: 0.9526, MAPE: 0.60%
Completed: NASDAQ H=3

Training: Autoformer on NASDAQ for H=5
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NASDAQ_H5Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_NASDAQ_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 5
============================================================
train 2199
val 121
test 123
Validation loss decreased (inf --> 0.124930).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
Validation loss decreased (0.124930 --> 0.120294).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.120294 --> 0.113593).  Saving model ...
Updating learning rate to 1.25e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
Validation loss decreased (0.113593 --> 0.111539).  Saving model ...
Updating learning rate to 3.125e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
Validation loss decreased (0.111539 --> 0.111538).  Saving model ...
Updating learning rate to 7.8125e-07
Validation loss decreased (0.111538 --> 0.111467).  Saving model ...
Updating learning rate to 3.90625e-07
Validation loss decreased (0.111467 --> 0.111299).  Saving model ...
Updating learning rate to 1.953125e-07
Validation loss decreased (0.111299 --> 0.111243).  Saving model ...
Updating learning rate to 9.765625e-08
Validation loss decreased (0.111243 --> 0.111055).  Saving model ...
Updating learning rate to 4.8828125e-08
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.44140625e-08
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.220703125e-08
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.103515625e-09
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.0517578125e-09
Validation loss decreased (0.111055 --> 0.109744).  Saving model ...
Updating learning rate to 1.52587890625e-09
EarlyStopping counter: 1 out of 5
Updating learning rate to 7.62939453125e-10
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.814697265625e-10
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.9073486328125e-10
EarlyStopping counter: 4 out of 5
Updating learning rate to 9.5367431640625e-11
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Autoformer_NASDAQ_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 123
test shape: (123, 5, 6) (123, 5, 6)
test shape: (123, 5, 6) (123, 5, 6)


	mse:0.17427614331245422, mae:0.30348169803619385, rmse:0.41746392846107483, r2:0.940576758235693, dtw:Not calculated


VAL - MSE: 0.1743, MAE: 0.3035, RMSE: 0.4175, RÂ²: 0.9406, MAPE: 0.51%
Completed: NASDAQ H=5

Training: Autoformer on NASDAQ for H=10
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NASDAQ_H10Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_NASDAQ_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 10
============================================================
train 2194
val 116
test 118
Validation loss decreased (inf --> 0.125704).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.125704 --> 0.123703).  Saving model ...
Updating learning rate to 5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Autoformer_NASDAQ_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 118
test shape: (118, 10, 6) (118, 10, 6)
test shape: (118, 10, 6) (118, 10, 6)


	mse:0.16309666633605957, mae:0.2883314788341522, rmse:0.40385228395462036, r2:0.9445141665637493, dtw:Not calculated


VAL - MSE: 0.1631, MAE: 0.2883, RMSE: 0.4039, RÂ²: 0.9445, MAPE: 0.63%
Completed: NASDAQ H=10

Training: Autoformer on NASDAQ for H=22
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NASDAQ_H22Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_NASDAQ_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 22
============================================================
train 2182
val 104
test 106
Validation loss decreased (inf --> 0.115354).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.115354 --> 0.095973).  Saving model ...
Updating learning rate to 5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Autoformer_NASDAQ_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 106
test shape: (106, 22, 6) (106, 22, 6)
test shape: (106, 22, 6) (106, 22, 6)


	mse:0.2140253484249115, mae:0.35203084349632263, rmse:0.4626287519931793, r2:0.9273119941353798, dtw:Not calculated


VAL - MSE: 0.2140, MAE: 0.3520, RMSE: 0.4626, RÂ²: 0.9273, MAPE: 0.51%
Completed: NASDAQ H=22

Training: Autoformer on NASDAQ for H=50
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NASDAQ_H50Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_NASDAQ_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 50
============================================================
train 2154
val 76
test 78
Validation loss decreased (inf --> 0.158240).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Autoformer_NASDAQ_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 78
test shape: (78, 50, 6) (78, 50, 6)
test shape: (78, 50, 6) (78, 50, 6)


	mse:0.20147515833377838, mae:0.32812249660491943, rmse:0.448859840631485, r2:0.930385060608387, dtw:Not calculated


VAL - MSE: 0.2015, MAE: 0.3281, RMSE: 0.4489, RÂ²: 0.9304, MAPE: 0.42%
Completed: NASDAQ H=50

Training: Autoformer on NASDAQ for H=100
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NASDAQ_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_NASDAQ_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 100
============================================================
train 2104
val 26
test 28
Validation loss decreased (inf --> 0.258377).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.258377 --> 0.244376).  Saving model ...
Updating learning rate to 5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Autoformer_NASDAQ_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 28
test shape: (28, 100, 6) (28, 100, 6)
test shape: (28, 100, 6) (28, 100, 6)


	mse:0.37323254346847534, mae:0.5146076083183289, rmse:0.6109275817871094, r2:0.8729213923215866, dtw:Not calculated


VAL - MSE: 0.3732, MAE: 0.5146, RMSE: 0.6109, RÂ²: 0.8729, MAPE: 0.55%
Completed: NASDAQ H=100

Training: Autoformer on ABSA for H=3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ABSA_H3  Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_ABSA_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 3
============================================================
train 2201
val 123
test 125
Validation loss decreased (inf --> 0.110244).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
Validation loss decreased (0.110244 --> 0.098037).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.098037 --> 0.077405).  Saving model ...
Updating learning rate to 1.25e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Autoformer_ABSA_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 125
test shape: (125, 3, 6) (125, 3, 6)
test shape: (125, 3, 6) (125, 3, 6)


	mse:0.2900107502937317, mae:0.35523152351379395, rmse:0.5385264754295349, r2:0.8903825283050537, dtw:Not calculated


VAL - MSE: 0.2900, MAE: 0.3552, RMSE: 0.5385, RÂ²: 0.8904, MAPE: 0.96%
Completed: ABSA H=3

Training: Autoformer on ABSA for H=5
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ABSA_H5  Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_ABSA_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 5
============================================================
train 2199
val 121
test 123
Validation loss decreased (inf --> 0.125350).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.125350 --> 0.119938).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.119938 --> 0.118692).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.118692 --> 0.106037).  Saving model ...
Updating learning rate to 1.25e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Autoformer_ABSA_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 123
test shape: (123, 5, 6) (123, 5, 6)
test shape: (123, 5, 6) (123, 5, 6)


	mse:0.29820016026496887, mae:0.35662853717803955, rmse:0.5460770726203918, r2:0.8870360627770424, dtw:Not calculated


VAL - MSE: 0.2982, MAE: 0.3566, RMSE: 0.5461, RÂ²: 0.8870, MAPE: 0.96%
Completed: ABSA H=5

Training: Autoformer on ABSA for H=10
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ABSA_H10 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_ABSA_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 10
============================================================
train 2194
val 116
test 118
Validation loss decreased (inf --> 0.110743).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
Validation loss decreased (0.110743 --> 0.106209).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.106209 --> 0.099840).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (0.099840 --> 0.097015).  Saving model ...
Updating learning rate to 6.25e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 3 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.90625e-07
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Autoformer_ABSA_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 118
test shape: (118, 10, 6) (118, 10, 6)
test shape: (118, 10, 6) (118, 10, 6)


	mse:0.31573471426963806, mae:0.3673536479473114, rmse:0.5619027614593506, r2:0.8800823241472244, dtw:Not calculated


VAL - MSE: 0.3157, MAE: 0.3674, RMSE: 0.5619, RÂ²: 0.8801, MAPE: 1.00%
Completed: ABSA H=10

Training: Autoformer on ABSA for H=22
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ABSA_H22 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_ABSA_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 22
============================================================
train 2182
val 104
test 106
Validation loss decreased (inf --> 0.113374).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
Validation loss decreased (0.113374 --> 0.106067).  Saving model ...
Updating learning rate to 2.5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Autoformer_ABSA_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 106
test shape: (106, 22, 6) (106, 22, 6)
test shape: (106, 22, 6) (106, 22, 6)


	mse:0.3413873314857483, mae:0.40150824189186096, rmse:0.5842835903167725, r2:0.8687533140182495, dtw:Not calculated


VAL - MSE: 0.3414, MAE: 0.4015, RMSE: 0.5843, RÂ²: 0.8688, MAPE: 0.99%
Completed: ABSA H=22

Training: Autoformer on ABSA for H=50
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ABSA_H50 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_ABSA_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 50
============================================================
train 2154
val 76
test 78
Validation loss decreased (inf --> 0.144761).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
Validation loss decreased (0.144761 --> 0.143372).  Saving model ...
Updating learning rate to 2.5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Autoformer_ABSA_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 78
test shape: (78, 50, 6) (78, 50, 6)
test shape: (78, 50, 6) (78, 50, 6)


	mse:0.3807763159275055, mae:0.4360659122467041, rmse:0.617070734500885, r2:0.8552990108728409, dtw:Not calculated


VAL - MSE: 0.3808, MAE: 0.4361, RMSE: 0.6171, RÂ²: 0.8553, MAPE: 1.88%
Completed: ABSA H=50

Training: Autoformer on ABSA for H=100
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ABSA_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_ABSA_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 100
============================================================
train 2104
val 26
test 28
Validation loss decreased (inf --> 0.190736).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Autoformer_ABSA_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 28
test shape: (28, 100, 6) (28, 100, 6)
test shape: (28, 100, 6) (28, 100, 6)


	mse:0.505074679851532, mae:0.5449374318122864, rmse:0.7106860876083374, r2:0.8053619563579559, dtw:Not calculated


VAL - MSE: 0.5051, MAE: 0.5449, RMSE: 0.7107, RÂ²: 0.8054, MAPE: 1.78%
Completed: ABSA H=100

Training: Autoformer on SASOL for H=3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SASOL_H3 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_SASOL_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 3
============================================================
train 2201
val 123
test 125
Validation loss decreased (inf --> 0.093151).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.093151 --> 0.072664).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.072664 --> 0.065031).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.065031 --> 0.061063).  Saving model ...
Updating learning rate to 1.25e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
Validation loss decreased (0.061063 --> 0.060334).  Saving model ...
Updating learning rate to 1.5625e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.90625e-07
Validation loss decreased (0.060334 --> 0.060077).  Saving model ...
Updating learning rate to 1.953125e-07
Validation loss decreased (0.060077 --> 0.058756).  Saving model ...
Updating learning rate to 9.765625e-08
EarlyStopping counter: 1 out of 5
Updating learning rate to 4.8828125e-08
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.44140625e-08
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.220703125e-08
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.103515625e-09
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Autoformer_SASOL_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 125
test shape: (125, 3, 6) (125, 3, 6)
test shape: (125, 3, 6) (125, 3, 6)


	mse:0.08402735739946365, mae:0.1298997849225998, rmse:0.28987473249435425, r2:0.5815951526165009, dtw:Not calculated


VAL - MSE: 0.0840, MAE: 0.1299, RMSE: 0.2899, RÂ²: 0.5816, MAPE: 0.35%
Completed: SASOL H=3

Training: Autoformer on SASOL for H=5
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SASOL_H5 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_SASOL_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 5
============================================================
train 2199
val 121
test 123
Validation loss decreased (inf --> 0.086357).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.086357 --> 0.077576).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.077576 --> 0.070701).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.070701 --> 0.064448).  Saving model ...
Updating learning rate to 1.25e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
Validation loss decreased (0.064448 --> 0.063991).  Saving model ...
Updating learning rate to 3.125e-06
Validation loss decreased (0.063991 --> 0.063349).  Saving model ...
Updating learning rate to 1.5625e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 7.8125e-07
Validation loss decreased (0.063349 --> 0.062757).  Saving model ...
Updating learning rate to 3.90625e-07
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.953125e-07
Validation loss decreased (0.062757 --> 0.062693).  Saving model ...
Updating learning rate to 9.765625e-08
EarlyStopping counter: 1 out of 5
Updating learning rate to 4.8828125e-08
Validation loss decreased (0.062693 --> 0.062013).  Saving model ...
Updating learning rate to 2.44140625e-08
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.220703125e-08
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.103515625e-09
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.0517578125e-09
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.52587890625e-09
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Autoformer_SASOL_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 123
test shape: (123, 5, 6) (123, 5, 6)
test shape: (123, 5, 6) (123, 5, 6)


	mse:0.08676689118146896, mae:0.1415753960609436, rmse:0.2945621907711029, r2:0.5685025751590729, dtw:Not calculated


VAL - MSE: 0.0868, MAE: 0.1416, RMSE: 0.2946, RÂ²: 0.5685, MAPE: 0.44%
Completed: SASOL H=5

Training: Autoformer on SASOL for H=10
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SASOL_H10Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_SASOL_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 10
============================================================
train 2194
val 116
test 118
Validation loss decreased (inf --> 0.068816).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.068816 --> 0.068719).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.068719 --> 0.068479).  Saving model ...
Updating learning rate to 2.5e-05
Validation loss decreased (0.068479 --> 0.061806).  Saving model ...
Updating learning rate to 1.25e-05
Validation loss decreased (0.061806 --> 0.059284).  Saving model ...
Updating learning rate to 6.25e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.5625e-06
Validation loss decreased (0.059284 --> 0.058503).  Saving model ...
Updating learning rate to 7.8125e-07
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.90625e-07
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.953125e-07
Validation loss decreased (0.058503 --> 0.057506).  Saving model ...
Updating learning rate to 9.765625e-08
EarlyStopping counter: 1 out of 5
Updating learning rate to 4.8828125e-08
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.44140625e-08
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.220703125e-08
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.103515625e-09
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Autoformer_SASOL_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 118
test shape: (118, 10, 6) (118, 10, 6)
test shape: (118, 10, 6) (118, 10, 6)


	mse:0.08616922795772552, mae:0.13762636482715607, rmse:0.2935459613800049, r2:0.5767276883125305, dtw:Not calculated


VAL - MSE: 0.0862, MAE: 0.1376, RMSE: 0.2935, RÂ²: 0.5767, MAPE: 0.36%
Completed: SASOL H=10

Training: Autoformer on SASOL for H=22
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SASOL_H22Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_SASOL_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 22
============================================================
train 2182
val 104
test 106
Validation loss decreased (inf --> 0.078106).  Saving model ...
Updating learning rate to 0.0001
Validation loss decreased (0.078106 --> 0.078004).  Saving model ...
Updating learning rate to 5e-05
Validation loss decreased (0.078004 --> 0.075981).  Saving model ...
Updating learning rate to 2.5e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
Validation loss decreased (0.075981 --> 0.069313).  Saving model ...
Updating learning rate to 6.25e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
Validation loss decreased (0.069313 --> 0.067028).  Saving model ...
Updating learning rate to 1.5625e-06
EarlyStopping counter: 1 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.90625e-07
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.953125e-07
EarlyStopping counter: 4 out of 5
Updating learning rate to 9.765625e-08
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Autoformer_SASOL_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 106
test shape: (106, 22, 6) (106, 22, 6)
test shape: (106, 22, 6) (106, 22, 6)


	mse:0.09461122006177902, mae:0.15539255738258362, rmse:0.3075893819332123, r2:0.5385848879814148, dtw:Not calculated


VAL - MSE: 0.0946, MAE: 0.1554, RMSE: 0.3076, RÂ²: 0.5386, MAPE: 0.42%
Completed: SASOL H=22

Training: Autoformer on SASOL for H=50
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SASOL_H50Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_SASOL_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 50
============================================================
train 2154
val 76
test 78
Validation loss decreased (inf --> 0.063332).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Autoformer_SASOL_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 78
test shape: (78, 50, 6) (78, 50, 6)
test shape: (78, 50, 6) (78, 50, 6)


	mse:0.1205608993768692, mae:0.23395264148712158, rmse:0.3472188115119934, r2:0.4481964707374573, dtw:Not calculated


VAL - MSE: 0.1206, MAE: 0.2340, RMSE: 0.3472, RÂ²: 0.4482, MAPE: 0.43%
Completed: SASOL H=50

Training: Autoformer on SASOL for H=100
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SASOL_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       50                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_Autoformer_SASOL_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 2515
Train: 2263 samples (90%) - rows 0 to 2262
Val: 125 samples (5%) - rows 2263 to 2387
Test: 127 samples (5%) - rows 2388 to 2514
Sequence length: 60, Prediction length: 100
============================================================
train 2104
val 26
test 28
Validation loss decreased (inf --> 0.064689).  Saving model ...
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
Validation loss decreased (0.064689 --> 0.063570).  Saving model ...
Updating learning rate to 1.25e-05
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
EarlyStopping counter: 5 out of 5
>>>>>>>testing : long_term_forecast_Autoformer_SASOL_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 28
test shape: (28, 100, 6) (28, 100, 6)
test shape: (28, 100, 6) (28, 100, 6)


	mse:0.09932985156774521, mae:0.18451747298240662, rmse:0.3151663839817047, r2:0.521434485912323, dtw:Not calculated


VAL - MSE: 0.0993, MAE: 0.1845, RMSE: 0.3152, RÂ²: 0.5214, MAPE: 0.34%
Completed: SASOL H=100

Autoformer training completed for all datasets!
