##############################################################################
# Training Informer Model on All Datasets
##############################################################################
Training: Informer on NVIDIA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_143147-1ndrenjt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/1ndrenjt
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NVIDIA_H3  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/1ndrenjt
>>>>>>>start training : long_term_forecast_Informer_NVIDIA_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
Overriding target from 'OT' to 'close' for stock data
val 237
Overriding target from 'OT' to 'close' for stock data
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.3090763 Vali Loss: 0.2373640 Test Loss: 1.0199698
Validation loss decreased (inf --> 0.237364).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.309076313013421, 'val/loss': 0.2373640462756157, 'test/loss': 1.0199698321521282, '_timestamp': 1762777924.1636698}).
Epoch: 2, Steps: 133 | Train Loss: 0.2480693 Vali Loss: 0.2605158 Test Loss: 1.2876806
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2335113 Vali Loss: 0.1901600 Test Loss: 1.0602882
Validation loss decreased (0.237364 --> 0.190160).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24806934090933405, 'val/loss': 0.260515782982111, 'test/loss': 1.2876806259155273, '_timestamp': 1762777931.079425}).
Epoch: 4, Steps: 133 | Train Loss: 0.2290878 Vali Loss: 0.1874332 Test Loss: 0.9959977
Validation loss decreased (0.190160 --> 0.187433).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2262103 Vali Loss: 0.1903338 Test Loss: 0.9918285
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2242947 Vali Loss: 0.1853470 Test Loss: 0.9860193
Validation loss decreased (0.187433 --> 0.185347).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2240262 Vali Loss: 0.1853440 Test Loss: 1.0041339
Validation loss decreased (0.185347 --> 0.185344).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2229576 Vali Loss: 0.1900484 Test Loss: 1.0059371
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2234326 Vali Loss: 0.1861396 Test Loss: 0.9881106
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2228886 Vali Loss: 0.1861330 Test Loss: 1.0028772
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2234630 Vali Loss: 0.1886702 Test Loss: 0.9972471
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2231476 Vali Loss: 0.1883033 Test Loss: 1.0110121
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2231024 Vali Loss: 0.1887777 Test Loss: 1.0083842
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2225665 Vali Loss: 0.1847958 Test Loss: 0.9884580
Validation loss decreased (0.185344 --> 0.184796).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2232512 Vali Loss: 0.2041983 Test Loss: 0.9918901
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2227235 Vali Loss: 0.1826041 Test Loss: 1.0032211
Validation loss decreased (0.184796 --> 0.182604).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2227413 Vali Loss: 0.1969003 Test Loss: 0.9975009
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2226888 Vali Loss: 0.1887321 Test Loss: 1.0096300
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2227660 Vali Loss: 0.1873049 Test Loss: 0.9891770
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2226509 Vali Loss: 0.1869496 Test Loss: 1.0033843
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2222661 Vali Loss: 0.1878533 Test Loss: 0.9883107
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2227053 Vali Loss: 0.1824452 Test Loss: 0.9969026
Validation loss decreased (0.182604 --> 0.182445).  Saving model ...
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2228673 Vali Loss: 0.2056017 Test Loss: 1.0038955
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2230112 Vali Loss: 0.1858577 Test Loss: 1.0025808
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2233890 Vali Loss: 0.2069770 Test Loss: 0.9991104
EarlyStopping counter: 3 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2228615 Vali Loss: 0.1872162 Test Loss: 1.0123036
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.2228918 Vali Loss: 0.1875194 Test Loss: 0.9983141
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.2234653 Vali Loss: 0.2041703 Test Loss: 0.9974608
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.2236856 Vali Loss: 0.2051333 Test Loss: 0.9980594
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 133 | Train Loss: 0.2229363 Vali Loss: 0.1875253 Test Loss: 1.0106900
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 133 | Train Loss: 0.2229877 Vali Loss: 0.1890040 Test Loss: 0.9924434
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 133 | Train Loss: 0.2230874 Vali Loss: 0.1877765 Test Loss: 1.0033437
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NVIDIA_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.0011495904764160514, mae:0.025464488193392754, rmse:0.03390561044216156, r2:-0.024785280227661133, dtw:Not calculated


VAL - MSE: 0.0011, MAE: 0.0255, RMSE: 0.0339, RÂ²: -0.0248, MAPE: 84775.98%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.479 MB of 0.480 MB uploadedwandb: \ 0.479 MB of 0.480 MB uploadedwandb: | 0.480 MB of 0.480 MB uploadedwandb: / 0.480 MB of 0.480 MB uploadedwandb: - 0.480 MB of 0.480 MB uploadedwandb: \ 0.480 MB of 0.480 MB uploadedwandb: | 0.480 MB of 0.480 MB uploadedwandb: / 0.480 MB of 0.480 MB uploadedwandb: - 0.661 MB of 0.963 MB uploaded (0.002 MB deduped)wandb: \ 0.963 MB of 0.963 MB uploaded (0.002 MB deduped)wandb: | 0.963 MB of 0.963 MB uploaded (0.002 MB deduped)wandb: / 0.963 MB of 0.963 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–‚â–â–ƒâ–ƒâ–â–ƒâ–‚â–ƒâ–ƒâ–â–‚â–ƒâ–‚â–ƒâ–â–ƒâ–â–‚â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–ƒ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–‚â–â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–â–‚â–‚â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–‡â–â–…â–ƒâ–‚â–‚â–ƒâ–â–ˆâ–‚â–ˆâ–‚â–‚â–‡â–‡â–‚â–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 31
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 1.00334
wandb:                 train/loss 0.22309
wandb:   val/directional_accuracy 51.47679
wandb:                   val/loss 0.18778
wandb:                    val/mae 0.02546
wandb:                   val/mape 8477597.65625
wandb:                    val/mse 0.00115
wandb:                     val/r2 -0.02479
wandb:                   val/rmse 0.03391
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/1ndrenjt
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_143147-1ndrenjt/logs
Completed: NVIDIA H=3

Training: Informer on NVIDIA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_143554-qk0eoggt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/qk0eoggt
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NVIDIA_H5  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/qk0eoggt
>>>>>>>start training : long_term_forecast_Informer_NVIDIA_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
Overriding target from 'OT' to 'close' for stock data
val 235
Overriding target from 'OT' to 'close' for stock data
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.3085442 Vali Loss: 0.2136047 Test Loss: 1.1673648
Validation loss decreased (inf --> 0.213605).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2499992 Vali Loss: 0.2235355 Test Loss: 1.2111189
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.30854420576776775, 'val/loss': 0.21360473707318306, 'test/loss': 1.1673647537827492, '_timestamp': 1762778167.701054}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24999916161361493, 'val/loss': 0.22353548556566238, 'test/loss': 1.211118869483471, '_timestamp': 1762778174.6245568}).
Epoch: 3, Steps: 133 | Train Loss: 0.2373743 Vali Loss: 0.2215291 Test Loss: 1.0690877
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2320671 Vali Loss: 0.2313912 Test Loss: 1.1400828
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2292430 Vali Loss: 0.2037974 Test Loss: 1.0355464
Validation loss decreased (0.213605 --> 0.203797).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2275617 Vali Loss: 0.1987931 Test Loss: 1.0363445
Validation loss decreased (0.203797 --> 0.198793).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2271854 Vali Loss: 0.2011003 Test Loss: 0.9905163
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2266913 Vali Loss: 0.1953582 Test Loss: 1.0061917
Validation loss decreased (0.198793 --> 0.195358).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2262928 Vali Loss: 0.1936961 Test Loss: 1.0116787
Validation loss decreased (0.195358 --> 0.193696).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2273486 Vali Loss: 0.1955028 Test Loss: 1.0051557
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2261646 Vali Loss: 0.1906162 Test Loss: 1.0164621
Validation loss decreased (0.193696 --> 0.190616).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2262284 Vali Loss: 0.2102058 Test Loss: 1.0012081
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2257350 Vali Loss: 0.2248300 Test Loss: 1.0098110
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2266933 Vali Loss: 0.2116527 Test Loss: 1.0065741
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2261546 Vali Loss: 0.1914713 Test Loss: 1.0132853
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2261187 Vali Loss: 0.1921242 Test Loss: 1.0000756
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2262306 Vali Loss: 0.1903011 Test Loss: 0.9981998
Validation loss decreased (0.190616 --> 0.190301).  Saving model ...
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2272713 Vali Loss: 0.2008758 Test Loss: 1.0075805
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2260355 Vali Loss: 0.1884650 Test Loss: 1.0002016
Validation loss decreased (0.190301 --> 0.188465).  Saving model ...
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2266002 Vali Loss: 0.1936429 Test Loss: 1.0058501
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2263420 Vali Loss: 0.1955487 Test Loss: 1.0074458
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2259336 Vali Loss: 0.1900399 Test Loss: 1.0032975
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2261385 Vali Loss: 0.2099644 Test Loss: 0.9887090
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2255180 Vali Loss: 0.1904843 Test Loss: 1.0005754
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2264599 Vali Loss: 0.1906163 Test Loss: 0.9992335
EarlyStopping counter: 6 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2258775 Vali Loss: 0.2002336 Test Loss: 1.0084530
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.2257116 Vali Loss: 0.1910148 Test Loss: 1.0082626
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.2255650 Vali Loss: 0.1908750 Test Loss: 0.9968485
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.2264481 Vali Loss: 0.1988367 Test Loss: 1.0026529
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NVIDIA_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.0011496527586132288, mae:0.02538921870291233, rmse:0.03390653058886528, r2:-0.01822066307067871, dtw:Not calculated


VAL - MSE: 0.0011, MAE: 0.0254, RMSE: 0.0339, RÂ²: -0.0182, MAPE: 63150.66%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.512 MB of 0.512 MB uploadedwandb: \ 0.512 MB of 0.512 MB uploadedwandb: | 0.512 MB of 0.512 MB uploadedwandb: / 0.512 MB of 0.512 MB uploadedwandb: - 0.512 MB of 0.512 MB uploadedwandb: \ 0.512 MB of 0.813 MB uploadedwandb: | 0.813 MB of 0.813 MB uploadedwandb: / 0.813 MB of 0.813 MB uploadedwandb: - 0.813 MB of 0.813 MB uploadedwandb: \ 0.813 MB of 0.813 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–ˆâ–ƒâ–ƒâ–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–‚â–â–â–â–‚â–â–‚â–â–â–â–â–‚â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–…â–‡â–…â–â–‚â–â–ƒâ–â–‚â–‚â–â–…â–â–â–ƒâ–â–â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 28
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 1.00265
wandb:                 train/loss 0.22645
wandb:   val/directional_accuracy 49.78723
wandb:                   val/loss 0.19884
wandb:                    val/mae 0.02539
wandb:                   val/mape 6315066.01562
wandb:                    val/mse 0.00115
wandb:                     val/r2 -0.01822
wandb:                   val/rmse 0.03391
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/qk0eoggt
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_143554-qk0eoggt/logs
Completed: NVIDIA H=5

Training: Informer on NVIDIA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_143938-a9e0mgjw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/a9e0mgjw
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NVIDIA_H10 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/a9e0mgjw
>>>>>>>start training : long_term_forecast_Informer_NVIDIA_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
Overriding target from 'OT' to 'close' for stock data
val 230
Overriding target from 'OT' to 'close' for stock data
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.3094449 Vali Loss: 0.2175583 Test Loss: 1.3073564
Validation loss decreased (inf --> 0.217558).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2516882 Vali Loss: 0.2123657 Test Loss: 1.2289282
Validation loss decreased (0.217558 --> 0.212366).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.30944489894952987, 'val/loss': 0.21755827963352203, 'test/loss': 1.3073563948273659, '_timestamp': 1762778391.938756}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2516881821298958, 'val/loss': 0.2123656515032053, 'test/loss': 1.2289282195270061, '_timestamp': 1762778398.978118}).
Epoch: 3, Steps: 133 | Train Loss: 0.2413126 Vali Loss: 0.2120994 Test Loss: 1.1618040
Validation loss decreased (0.212366 --> 0.212099).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2348120 Vali Loss: 0.2214528 Test Loss: 1.0712850
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2327994 Vali Loss: 0.2118336 Test Loss: 1.0253846
Validation loss decreased (0.212099 --> 0.211834).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2323361 Vali Loss: 0.2110203 Test Loss: 1.1345587
Validation loss decreased (0.211834 --> 0.211020).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2299630 Vali Loss: 0.2042730 Test Loss: 1.1082107
Validation loss decreased (0.211020 --> 0.204273).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2302172 Vali Loss: 0.2274493 Test Loss: 1.1079609
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2296332 Vali Loss: 0.2099768 Test Loss: 1.0959363
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2288827 Vali Loss: 0.2220034 Test Loss: 1.1122719
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2293812 Vali Loss: 0.2169177 Test Loss: 1.1000315
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2288693 Vali Loss: 0.2086153 Test Loss: 1.0927934
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2291363 Vali Loss: 0.2031280 Test Loss: 1.1128932
Validation loss decreased (0.204273 --> 0.203128).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2282832 Vali Loss: 0.2095715 Test Loss: 1.1074268
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2295783 Vali Loss: 0.2064359 Test Loss: 1.1078000
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2291645 Vali Loss: 0.2215758 Test Loss: 1.1106265
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2285081 Vali Loss: 0.2048851 Test Loss: 1.1057492
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2287721 Vali Loss: 0.2085218 Test Loss: 1.0933670
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2291332 Vali Loss: 0.2073350 Test Loss: 1.1046021
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2292338 Vali Loss: 0.2090436 Test Loss: 1.1115176
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2282989 Vali Loss: 0.2045270 Test Loss: 1.1045144
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2283063 Vali Loss: 0.2042556 Test Loss: 1.1090919
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2289881 Vali Loss: 0.2065232 Test Loss: 1.1060315
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NVIDIA_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.0011641104938462377, mae:0.02560419775545597, rmse:0.03411906212568283, r2:-0.014806985855102539, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0256, RMSE: 0.0341, RÂ²: -0.0148, MAPE: 80092.49%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.545 MB of 0.545 MB uploadedwandb: \ 0.545 MB of 0.545 MB uploadedwandb: | 0.545 MB of 0.545 MB uploadedwandb: / 0.545 MB of 0.545 MB uploadedwandb: - 0.545 MB of 0.845 MB uploadedwandb: \ 0.845 MB of 0.845 MB uploadedwandb: | 0.845 MB of 0.845 MB uploadedwandb: / 0.845 MB of 0.845 MB uploadedwandb: - 0.845 MB of 0.845 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–‡â–…â–…â–…â–…â–…â–„â–…â–…â–…â–…â–…â–„â–…â–…â–…â–…â–…
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–‚â–â–â–â–â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–†â–„â–ƒâ–â–ˆâ–ƒâ–†â–…â–ƒâ–â–ƒâ–‚â–†â–‚â–ƒâ–‚â–ƒâ–â–â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 1.10603
wandb:                 train/loss 0.22899
wandb:   val/directional_accuracy 53.1401
wandb:                   val/loss 0.20652
wandb:                    val/mae 0.0256
wandb:                   val/mape 8009249.21875
wandb:                    val/mse 0.00116
wandb:                     val/r2 -0.01481
wandb:                   val/rmse 0.03412
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/a9e0mgjw
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_143938-a9e0mgjw/logs
Completed: NVIDIA H=10

Training: Informer on NVIDIA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_144243-9dsz749f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/9dsz749f
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NVIDIA_H22 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/9dsz749f
>>>>>>>start training : long_term_forecast_Informer_NVIDIA_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
Overriding target from 'OT' to 'close' for stock data
val 218
Overriding target from 'OT' to 'close' for stock data
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.3124623 Vali Loss: 0.2080259 Test Loss: 1.3519837
Validation loss decreased (inf --> 0.208026).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2559628 Vali Loss: 0.2616136 Test Loss: 1.3931831
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.31246229046673485, 'val/loss': 0.20802588760852814, 'test/loss': 1.3519836579050337, '_timestamp': 1762778576.237666}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25596284437360184, 'val/loss': 0.26161364146641325, 'test/loss': 1.3931831206594194, '_timestamp': 1762778583.1042328}).
Epoch: 3, Steps: 132 | Train Loss: 0.2445853 Vali Loss: 0.2561157 Test Loss: 1.4277326
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2396067 Vali Loss: 0.2252631 Test Loss: 1.2320970
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2364620 Vali Loss: 0.2272189 Test Loss: 1.2311600
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2353287 Vali Loss: 0.2266663 Test Loss: 1.2662384
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2341404 Vali Loss: 0.2253339 Test Loss: 1.2566904
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2339382 Vali Loss: 0.2264735 Test Loss: 1.2433811
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2335999 Vali Loss: 0.2254803 Test Loss: 1.2455738
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2336343 Vali Loss: 0.2247475 Test Loss: 1.2370272
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2334772 Vali Loss: 0.2223802 Test Loss: 1.2401277
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NVIDIA_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.0012057498097419739, mae:0.026020580902695656, rmse:0.034723907709121704, r2:-0.021947026252746582, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0260, RMSE: 0.0347, RÂ²: -0.0219, MAPE: 180222.66%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.583 MB of 0.584 MB uploadedwandb: \ 0.583 MB of 0.584 MB uploadedwandb: | 0.583 MB of 0.584 MB uploadedwandb: / 0.584 MB of 0.584 MB uploadedwandb: - 0.584 MB of 0.584 MB uploadedwandb: \ 0.584 MB of 0.883 MB uploadedwandb: | 0.883 MB of 0.883 MB uploadedwandb: / 0.883 MB of 0.883 MB uploadedwandb: - 0.883 MB of 0.883 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–â–‚â–‚â–â–‚â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 1.24013
wandb:                 train/loss 0.23348
wandb:   val/directional_accuracy 50.3495
wandb:                   val/loss 0.22238
wandb:                    val/mae 0.02602
wandb:                   val/mape 18022265.625
wandb:                    val/mse 0.00121
wandb:                     val/r2 -0.02195
wandb:                   val/rmse 0.03472
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/9dsz749f
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_144243-9dsz749f/logs
Completed: NVIDIA H=22

Training: Informer on NVIDIA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_144426-aiwl2axk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/aiwl2axk
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NVIDIA_H50 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/aiwl2axk
>>>>>>>start training : long_term_forecast_Informer_NVIDIA_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
Overriding target from 'OT' to 'close' for stock data
val 190
Overriding target from 'OT' to 'close' for stock data
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.3227664 Vali Loss: 0.2321387 Test Loss: 1.4080281
Validation loss decreased (inf --> 0.232139).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2645141 Vali Loss: 0.2676066 Test Loss: 1.4351379
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.32276637287754, 'val/loss': 0.23213870326677957, 'test/loss': 1.4080281058947246, '_timestamp': 1762778679.706175}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26451405295819946, 'val/loss': 0.2676066483060519, 'test/loss': 1.4351378778616588, '_timestamp': 1762778686.5637782}).
Epoch: 3, Steps: 132 | Train Loss: 0.2530232 Vali Loss: 0.2724388 Test Loss: 1.5512122
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2470966 Vali Loss: 0.2795667 Test Loss: 1.4678739
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2425822 Vali Loss: 0.2747502 Test Loss: 1.4580490
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2430364 Vali Loss: 0.2745016 Test Loss: 1.4467620
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2441975 Vali Loss: 0.2615860 Test Loss: 1.4476226
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2410081 Vali Loss: 0.2883373 Test Loss: 1.4522173
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2459671 Vali Loss: 0.2684328 Test Loss: 1.4538450
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2393643 Vali Loss: 0.2816233 Test Loss: 1.4381814
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2439135 Vali Loss: 0.2824738 Test Loss: 1.4432942
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NVIDIA_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.0012211560970172286, mae:0.0265627671033144, rmse:0.0349450446665287, r2:-0.02618849277496338, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0266, RMSE: 0.0349, RÂ²: -0.0262, MAPE: 232630.92%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.625 MB of 0.628 MB uploadedwandb: \ 0.625 MB of 0.628 MB uploadedwandb: | 0.628 MB of 0.628 MB uploadedwandb: / 0.628 MB of 0.926 MB uploadedwandb: - 0.628 MB of 0.926 MB uploadedwandb: \ 0.926 MB of 0.926 MB uploadedwandb: | 0.926 MB of 0.926 MB uploadedwandb: / 0.926 MB of 0.926 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–ƒâ–‚â–„â–â–ƒ
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–†â–„â–„â–â–ˆâ–ƒâ–†â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 1.44329
wandb:                 train/loss 0.24391
wandb:   val/directional_accuracy 50.22556
wandb:                   val/loss 0.28247
wandb:                    val/mae 0.02656
wandb:                   val/mape 23263092.1875
wandb:                    val/mse 0.00122
wandb:                     val/r2 -0.02619
wandb:                   val/rmse 0.03495
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/aiwl2axk
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_144426-aiwl2axk/logs
Completed: NVIDIA H=50

Training: Informer on NVIDIA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_144607-khvfhhis
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/khvfhhis
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NVIDIA_H100Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/khvfhhis
>>>>>>>start training : long_term_forecast_Informer_NVIDIA_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
Overriding target from 'OT' to 'close' for stock data
val 140
Overriding target from 'OT' to 'close' for stock data
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.3292139 Vali Loss: 0.3006699 Test Loss: 1.4712469
Validation loss decreased (inf --> 0.300670).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2681432 Vali Loss: 0.3279834 Test Loss: 1.7705722
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.32921387977325, 'val/loss': 0.3006698787212372, 'test/loss': 1.4712469339370728, '_timestamp': 1762778780.548424}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26814319067276443, 'val/loss': 0.32798336148262025, 'test/loss': 1.7705722093582152, '_timestamp': 1762778787.4585934}).
Epoch: 3, Steps: 130 | Train Loss: 0.2528259 Vali Loss: 0.2469334 Test Loss: 1.6237518
Validation loss decreased (0.300670 --> 0.246933).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2474724 Vali Loss: 0.2722769 Test Loss: 1.6127859
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2442161 Vali Loss: 0.2639703 Test Loss: 1.5957743
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2428503 Vali Loss: 0.2609400 Test Loss: 1.6283062
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2421390 Vali Loss: 0.2603632 Test Loss: 1.5895895
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2417374 Vali Loss: 0.2622629 Test Loss: 1.6004428
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2411075 Vali Loss: 0.2629152 Test Loss: 1.5897757
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2414230 Vali Loss: 0.2567731 Test Loss: 1.5878516
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2416971 Vali Loss: 0.2617441 Test Loss: 1.5947982
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2409088 Vali Loss: 0.2601937 Test Loss: 1.5939151
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2411919 Vali Loss: 0.2560516 Test Loss: 1.5911222
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NVIDIA_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0012927777133882046, mae:0.02747027575969696, rmse:0.035955216735601425, r2:-0.004165172576904297, dtw:Not calculated


VAL - MSE: 0.0013, MAE: 0.0275, RMSE: 0.0360, RÂ²: -0.0042, MAPE: 68488.80%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.648 MB of 0.653 MB uploadedwandb: \ 0.648 MB of 0.653 MB uploadedwandb: | 0.653 MB of 0.653 MB uploadedwandb: / 0.653 MB of 0.653 MB uploadedwandb: - 0.653 MB of 0.951 MB uploadedwandb: \ 0.877 MB of 0.951 MB uploadedwandb: | 0.951 MB of 0.951 MB uploadedwandb: / 0.951 MB of 0.951 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‡â–…â–‚â–ˆâ–â–ƒâ–â–â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ˆâ–†â–…â–…â–…â–…â–„â–…â–…â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 1.59112
wandb:                 train/loss 0.24119
wandb:   val/directional_accuracy 49.87734
wandb:                   val/loss 0.25605
wandb:                    val/mae 0.02747
wandb:                   val/mape 6848879.6875
wandb:                    val/mse 0.00129
wandb:                     val/r2 -0.00417
wandb:                   val/rmse 0.03596
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/khvfhhis
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_144607-khvfhhis/logs
Completed: NVIDIA H=100

Training: Informer on APPLE for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_144804-uu8cq2s2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/uu8cq2s2
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_APPLE_H3   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/uu8cq2s2
>>>>>>>start training : long_term_forecast_Informer_APPLE_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
Overriding target from 'OT' to 'close' for stock data
val 237
Overriding target from 'OT' to 'close' for stock data
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.2920972 Vali Loss: 0.1164460 Test Loss: 0.1721313
Validation loss decreased (inf --> 0.116446).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2377366 Vali Loss: 0.1051841 Test Loss: 0.1638768
Validation loss decreased (0.116446 --> 0.105184).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2920971766002196, 'val/loss': 0.11644602194428444, 'test/loss': 0.17213134467601776, '_timestamp': 1762778895.7970326}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23773655553061263, 'val/loss': 0.10518409498035908, 'test/loss': 0.16387677006423473, '_timestamp': 1762778902.639078}).
Epoch: 3, Steps: 133 | Train Loss: 0.2224151 Vali Loss: 0.0895789 Test Loss: 0.1391965
Validation loss decreased (0.105184 --> 0.089579).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2171143 Vali Loss: 0.0880569 Test Loss: 0.1430734
Validation loss decreased (0.089579 --> 0.088057).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2159970 Vali Loss: 0.0886673 Test Loss: 0.1425292
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2135212 Vali Loss: 0.0859188 Test Loss: 0.1411883
Validation loss decreased (0.088057 --> 0.085919).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2124524 Vali Loss: 0.0912709 Test Loss: 0.1459934
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2121958 Vali Loss: 0.0932839 Test Loss: 0.1445307
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2121124 Vali Loss: 0.0891976 Test Loss: 0.1430150
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2114628 Vali Loss: 0.0875510 Test Loss: 0.1437780
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2118925 Vali Loss: 0.0890078 Test Loss: 0.1441213
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2115967 Vali Loss: 0.0871500 Test Loss: 0.1421174
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2115005 Vali Loss: 0.0876319 Test Loss: 0.1437314
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2111597 Vali Loss: 0.0904354 Test Loss: 0.1462603
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2119115 Vali Loss: 0.0869522 Test Loss: 0.1425772
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2116246 Vali Loss: 0.0888117 Test Loss: 0.1442543
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_APPLE_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.00020289731037337333, mae:0.010390602052211761, rmse:0.0142442025244236, r2:-0.014777064323425293, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0104, RMSE: 0.0142, RÂ²: -0.0148, MAPE: 455705.84%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.468 MB of 0.469 MB uploadedwandb: \ 0.468 MB of 0.469 MB uploadedwandb: | 0.468 MB of 0.469 MB uploadedwandb: / 0.469 MB of 0.469 MB uploadedwandb: - 0.469 MB of 0.469 MB uploadedwandb: \ 0.469 MB of 0.469 MB uploadedwandb: | 0.469 MB of 0.469 MB uploadedwandb: / 0.469 MB of 0.469 MB uploadedwandb: - 0.650 MB of 0.949 MB uploaded (0.002 MB deduped)wandb: \ 0.949 MB of 0.949 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–…â–„â–ƒâ–ˆâ–†â–…â–†â–†â–„â–…â–ˆâ–„â–†
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–ƒâ–„â–â–†â–ˆâ–„â–ƒâ–„â–‚â–ƒâ–…â–‚â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.14425
wandb:                 train/loss 0.21162
wandb:   val/directional_accuracy 50.42194
wandb:                   val/loss 0.08881
wandb:                    val/mae 0.01039
wandb:                   val/mape 45570584.375
wandb:                    val/mse 0.0002
wandb:                     val/r2 -0.01478
wandb:                   val/rmse 0.01424
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/uu8cq2s2
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_144804-uu8cq2s2/logs
Completed: APPLE H=3

Training: Informer on APPLE for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_145022-eqbsqu1u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/eqbsqu1u
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_APPLE_H5   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/eqbsqu1u
>>>>>>>start training : long_term_forecast_Informer_APPLE_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
Overriding target from 'OT' to 'close' for stock data
val 235
Overriding target from 'OT' to 'close' for stock data
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.2904990 Vali Loss: 0.1086548 Test Loss: 0.1672770
Validation loss decreased (inf --> 0.108655).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2383448 Vali Loss: 0.0912267 Test Loss: 0.1553419
Validation loss decreased (0.108655 --> 0.091227).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2904990057748063, 'val/loss': 0.10865475237369537, 'test/loss': 0.16727703530341387, '_timestamp': 1762779036.3760915}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2383448475957813, 'val/loss': 0.09122665785253048, 'test/loss': 0.15534191858023405, '_timestamp': 1762779043.2826235}).
Epoch: 3, Steps: 133 | Train Loss: 0.2249937 Vali Loss: 0.0915772 Test Loss: 0.1536464
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2188879 Vali Loss: 0.0925543 Test Loss: 0.1631419
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2163772 Vali Loss: 0.1001922 Test Loss: 0.1671541
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2145792 Vali Loss: 0.0876342 Test Loss: 0.1503627
Validation loss decreased (0.091227 --> 0.087634).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2142406 Vali Loss: 0.0951293 Test Loss: 0.1598824
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2136848 Vali Loss: 0.0936192 Test Loss: 0.1575532
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2130312 Vali Loss: 0.0950406 Test Loss: 0.1580035
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2139188 Vali Loss: 0.0925532 Test Loss: 0.1575889
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2129478 Vali Loss: 0.0942425 Test Loss: 0.1591065
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2127158 Vali Loss: 0.0920742 Test Loss: 0.1570321
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2135505 Vali Loss: 0.0913648 Test Loss: 0.1554503
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2132871 Vali Loss: 0.0900621 Test Loss: 0.1549386
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2131645 Vali Loss: 0.0948465 Test Loss: 0.1582728
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2136893 Vali Loss: 0.0946731 Test Loss: 0.1574351
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_APPLE_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.0002028476883424446, mae:0.010308667086064816, rmse:0.014242460951209068, r2:-0.009898066520690918, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0103, RMSE: 0.0142, RÂ²: -0.0099, MAPE: 662735.06%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.501 MB of 0.501 MB uploadedwandb: \ 0.501 MB of 0.501 MB uploadedwandb: | 0.501 MB of 0.800 MB uploadedwandb: / 0.501 MB of 0.800 MB uploadedwandb: - 0.800 MB of 0.800 MB uploadedwandb: \ 0.800 MB of 0.800 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‚â–†â–ˆâ–â–…â–„â–„â–„â–…â–„â–ƒâ–ƒâ–„â–„
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–„â–ˆâ–â–…â–„â–…â–„â–…â–ƒâ–ƒâ–‚â–…â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.15744
wandb:                 train/loss 0.21369
wandb:   val/directional_accuracy 48.93617
wandb:                   val/loss 0.09467
wandb:                    val/mae 0.01031
wandb:                   val/mape 66273506.25
wandb:                    val/mse 0.0002
wandb:                     val/r2 -0.0099
wandb:                   val/rmse 0.01424
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/eqbsqu1u
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_145022-eqbsqu1u/logs
Completed: APPLE H=5

Training: Informer on APPLE for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_145238-y06qexw9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/y06qexw9
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_APPLE_H10  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/y06qexw9
>>>>>>>start training : long_term_forecast_Informer_APPLE_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
Overriding target from 'OT' to 'close' for stock data
val 230
Overriding target from 'OT' to 'close' for stock data
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2938860 Vali Loss: 0.1561513 Test Loss: 0.2011111
Validation loss decreased (inf --> 0.156151).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2390296 Vali Loss: 0.0966006 Test Loss: 0.1617506
Validation loss decreased (0.156151 --> 0.096601).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29388602189999774, 'val/loss': 0.15615126676857471, 'test/loss': 0.20111109502613544, '_timestamp': 1762779170.9663978}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23902963964562668, 'val/loss': 0.0966005502268672, 'test/loss': 0.1617506267502904, '_timestamp': 1762779177.7261508}).
Epoch: 3, Steps: 133 | Train Loss: 0.2273482 Vali Loss: 0.1009601 Test Loss: 0.1695030
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2207876 Vali Loss: 0.1254095 Test Loss: 0.1964676
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2180946 Vali Loss: 0.1090403 Test Loss: 0.1750231
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2188084 Vali Loss: 0.1157811 Test Loss: 0.1866013
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2161775 Vali Loss: 0.1126523 Test Loss: 0.1810155
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2154762 Vali Loss: 0.1115838 Test Loss: 0.1813293
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2153454 Vali Loss: 0.1094274 Test Loss: 0.1802650
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2155136 Vali Loss: 0.1074190 Test Loss: 0.1779247
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2152498 Vali Loss: 0.1117279 Test Loss: 0.1790702
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2150919 Vali Loss: 0.1103104 Test Loss: 0.1771010
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_APPLE_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.00021237721375655383, mae:0.010737105272710323, rmse:0.014573168009519577, r2:-0.04894876480102539, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0107, RMSE: 0.0146, RÂ²: -0.0489, MAPE: 475097.31%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.573 MB of 0.573 MB uploadedwandb: \ 0.573 MB of 0.573 MB uploadedwandb: | 0.573 MB of 0.573 MB uploadedwandb: / 0.573 MB of 0.573 MB uploadedwandb: - 0.573 MB of 0.573 MB uploadedwandb: \ 0.573 MB of 0.872 MB uploadedwandb: | 0.669 MB of 0.872 MB uploadedwandb: / 0.872 MB of 0.872 MB uploadedwandb: - 0.872 MB of 0.872 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–‚â–…â–„â–„â–„â–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–„â–ƒâ–ƒâ–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ˆâ–ƒâ–…â–„â–„â–ƒâ–ƒâ–„â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.1771
wandb:                 train/loss 0.21509
wandb:   val/directional_accuracy 49.27536
wandb:                   val/loss 0.11031
wandb:                    val/mae 0.01074
wandb:                   val/mape 47509731.25
wandb:                    val/mse 0.00021
wandb:                     val/r2 -0.04895
wandb:                   val/rmse 0.01457
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/y06qexw9
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_145238-y06qexw9/logs
Completed: APPLE H=10

Training: Informer on APPLE for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_145428-x72u4urz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/x72u4urz
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_APPLE_H22  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/x72u4urz
>>>>>>>start training : long_term_forecast_Informer_APPLE_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
Overriding target from 'OT' to 'close' for stock data
val 218
Overriding target from 'OT' to 'close' for stock data
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2957365 Vali Loss: 0.1369042 Test Loss: 0.2065163
Validation loss decreased (inf --> 0.136904).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2416652 Vali Loss: 0.1431584 Test Loss: 0.2222051
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29573649461522244, 'val/loss': 0.13690417792115891, 'test/loss': 0.20651633611747197, '_timestamp': 1762779281.7664921}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24166522920131683, 'val/loss': 0.14315839111804962, 'test/loss': 0.22220513224601746, '_timestamp': 1762779288.5050929}).
Epoch: 3, Steps: 132 | Train Loss: 0.2302731 Vali Loss: 0.1119592 Test Loss: 0.1977351
Validation loss decreased (0.136904 --> 0.111959).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2243622 Vali Loss: 0.1063027 Test Loss: 0.1893507
Validation loss decreased (0.111959 --> 0.106303).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2209161 Vali Loss: 0.1138992 Test Loss: 0.1963406
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2196486 Vali Loss: 0.1179492 Test Loss: 0.2046347
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2185082 Vali Loss: 0.1150391 Test Loss: 0.2015650
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2181005 Vali Loss: 0.1197256 Test Loss: 0.2061287
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2180079 Vali Loss: 0.1191791 Test Loss: 0.2057746
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2177434 Vali Loss: 0.1174569 Test Loss: 0.2038334
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2180377 Vali Loss: 0.1177744 Test Loss: 0.2042722
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2176711 Vali Loss: 0.1195782 Test Loss: 0.2057713
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2178413 Vali Loss: 0.1175975 Test Loss: 0.2045093
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2174159 Vali Loss: 0.1181532 Test Loss: 0.2040713
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_APPLE_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.00020869601576123387, mae:0.010363031178712845, rmse:0.014446315355598927, r2:-0.005742192268371582, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0104, RMSE: 0.0144, RÂ²: -0.0057, MAPE: 667653.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.620 MB of 0.621 MB uploadedwandb: \ 0.620 MB of 0.621 MB uploadedwandb: | 0.620 MB of 0.621 MB uploadedwandb: / 0.621 MB of 0.621 MB uploadedwandb: - 0.621 MB of 0.920 MB uploadedwandb: \ 0.621 MB of 0.920 MB uploadedwandb: | 0.920 MB of 0.920 MB uploadedwandb: / 0.920 MB of 0.920 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–„â–â–„â–‡â–†â–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–‡
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–â–…â–‡â–†â–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 13
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.20407
wandb:                 train/loss 0.21742
wandb:   val/directional_accuracy 49.6505
wandb:                   val/loss 0.11815
wandb:                    val/mae 0.01036
wandb:                   val/mape 66765375.0
wandb:                    val/mse 0.00021
wandb:                     val/r2 -0.00574
wandb:                   val/rmse 0.01445
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/x72u4urz
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_145428-x72u4urz/logs
Exception in thread IntMsgThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 268, in check_network_status
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 300, in check_internal_messages
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 848, in deliver_network_status
    return self._deliver_network_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 510, in _deliver_network_status
        self._loop_check_status(return self._deliver_record(record)

  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 856, in deliver_internal_messages
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    return self._deliver_internal_messages(internal_message)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 516, in _deliver_internal_messages
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
        handle = mailbox._deliver_record(record, interface=self)interface._publish(record)

  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: APPLE H=22

Training: Informer on APPLE for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_145631-08wch7dz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/08wch7dz
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_APPLE_H50  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/08wch7dz
>>>>>>>start training : long_term_forecast_Informer_APPLE_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
Overriding target from 'OT' to 'close' for stock data
val 190
Overriding target from 'OT' to 'close' for stock data
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.3044744 Vali Loss: 0.1637732 Test Loss: 0.2483220
Validation loss decreased (inf --> 0.163773).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2525732 Vali Loss: 0.1593793 Test Loss: 0.2547373
Validation loss decreased (0.163773 --> 0.159379).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.30447444218126213, 'val/loss': 0.16377317905426025, 'test/loss': 0.2483220472931862, '_timestamp': 1762779404.3566232}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25257322959827655, 'val/loss': 0.15937931587298712, 'test/loss': 0.25473734736442566, '_timestamp': 1762779411.0715852}).
Epoch: 3, Steps: 132 | Train Loss: 0.2388267 Vali Loss: 0.2132619 Test Loss: 0.3316585
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2314679 Vali Loss: 0.1494369 Test Loss: 0.2642646
Validation loss decreased (0.159379 --> 0.149437).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2265311 Vali Loss: 0.1611341 Test Loss: 0.2773257
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2254110 Vali Loss: 0.1678032 Test Loss: 0.2891948
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2245553 Vali Loss: 0.1637075 Test Loss: 0.2812140
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2255578 Vali Loss: 0.1592068 Test Loss: 0.2804963
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2248920 Vali Loss: 0.1589237 Test Loss: 0.2805145
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2254403 Vali Loss: 0.1698953 Test Loss: 0.2901101
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2232862 Vali Loss: 0.1588960 Test Loss: 0.2777242
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2251695 Vali Loss: 0.1461479 Test Loss: 0.2619295
Validation loss decreased (0.149437 --> 0.146148).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2245383 Vali Loss: 0.1542044 Test Loss: 0.2734044
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2259900 Vali Loss: 0.1491819 Test Loss: 0.2656547
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2239670 Vali Loss: 0.1565655 Test Loss: 0.2734685
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2233233 Vali Loss: 0.1612275 Test Loss: 0.2794307
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2264935 Vali Loss: 0.1513885 Test Loss: 0.2688224
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2242074 Vali Loss: 0.1541614 Test Loss: 0.2745784
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2240401 Vali Loss: 0.1647944 Test Loss: 0.2847158
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2257802 Vali Loss: 0.1537990 Test Loss: 0.2748648
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2241115 Vali Loss: 0.1585026 Test Loss: 0.2766710
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2234393 Vali Loss: 0.1636413 Test Loss: 0.2831674
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_APPLE_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.00022367583005689085, mae:0.010754202492535114, rmse:0.014955796301364899, r2:-0.008116364479064941, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0108, RMSE: 0.0150, RÂ²: -0.0081, MAPE: 188488.22%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.678 MB of 0.681 MB uploadedwandb: \ 0.678 MB of 0.681 MB uploadedwandb: | 0.678 MB of 0.681 MB uploadedwandb: / 0.681 MB of 0.681 MB uploadedwandb: - 0.681 MB of 0.681 MB uploadedwandb: \ 0.681 MB of 0.981 MB uploadedwandb: | 0.981 MB of 0.981 MB uploadedwandb: / 0.981 MB of 0.981 MB uploadedwandb: - 0.981 MB of 0.981 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–ƒâ–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–â–‚â–â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–ƒ
wandb:                 train/loss â–ˆâ–…â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–‚â–â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–â–‚â–â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.28317
wandb:                 train/loss 0.22344
wandb:   val/directional_accuracy 49.25886
wandb:                   val/loss 0.16364
wandb:                    val/mae 0.01075
wandb:                   val/mape 18848821.875
wandb:                    val/mse 0.00022
wandb:                     val/r2 -0.00812
wandb:                   val/rmse 0.01496
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/08wch7dz
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_145631-08wch7dz/logs
Completed: APPLE H=50

Training: Informer on APPLE for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_145929-l5lo3nn3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/l5lo3nn3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_APPLE_H100 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/l5lo3nn3
>>>>>>>start training : long_term_forecast_Informer_APPLE_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
Overriding target from 'OT' to 'close' for stock data
val 140
Overriding target from 'OT' to 'close' for stock data
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.3123698 Vali Loss: 0.1475606 Test Loss: 0.2410340
Validation loss decreased (inf --> 0.147561).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2573032 Vali Loss: 0.1596651 Test Loss: 0.2828030
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3123698176099704, 'val/loss': 0.1475606232881546, 'test/loss': 0.24103395640850067, '_timestamp': 1762779582.5426214}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2573031866779694, 'val/loss': 0.15966513454914094, 'test/loss': 0.28280297517776487, '_timestamp': 1762779589.4280272}).
Epoch: 3, Steps: 130 | Train Loss: 0.2420640 Vali Loss: 0.1668204 Test Loss: 0.3248538
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2342938 Vali Loss: 0.1846694 Test Loss: 0.3386358
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2304585 Vali Loss: 0.1727618 Test Loss: 0.3211490
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2286879 Vali Loss: 0.1592867 Test Loss: 0.3093177
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2283025 Vali Loss: 0.1678659 Test Loss: 0.3195415
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2272245 Vali Loss: 0.1656410 Test Loss: 0.3144249
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2269911 Vali Loss: 0.1680858 Test Loss: 0.3172432
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2274884 Vali Loss: 0.1658302 Test Loss: 0.3144420
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2270629 Vali Loss: 0.1659902 Test Loss: 0.3200983
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_APPLE_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0002402615500614047, mae:0.011114588007330894, rmse:0.015500372275710106, r2:-0.017575383186340332, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0111, RMSE: 0.0155, RÂ²: -0.0176, MAPE: 193722.39%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.700 MB of 0.705 MB uploadedwandb: \ 0.700 MB of 0.705 MB uploadedwandb: | 0.700 MB of 0.705 MB uploadedwandb: / 0.705 MB of 0.705 MB uploadedwandb: - 0.705 MB of 1.003 MB uploadedwandb: \ 1.003 MB of 1.003 MB uploadedwandb: | 1.003 MB of 1.003 MB uploadedwandb: / 1.003 MB of 1.003 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–ˆâ–„â–â–ƒâ–‚â–ƒâ–‚â–„
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–ˆâ–…â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.3201
wandb:                 train/loss 0.22706
wandb:   val/directional_accuracy 50.25253
wandb:                   val/loss 0.16599
wandb:                    val/mae 0.01111
wandb:                   val/mape 19372239.0625
wandb:                    val/mse 0.00024
wandb:                     val/r2 -0.01758
wandb:                   val/rmse 0.0155
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/l5lo3nn3
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_145929-l5lo3nn3/logs
Completed: APPLE H=100

Training: Informer on SP500 for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_150111-9p1il8pn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/9p1il8pn
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SP500_H3   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/9p1il8pn
>>>>>>>start training : long_term_forecast_Informer_SP500_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 3
============================================================
train 4241
Overriding target from 'OT' to 'close' for stock data
val 237
Overriding target from 'OT' to 'close' for stock data
test 238
Epoch: 1, Steps: 133 | Train Loss: 0.2329134 Vali Loss: 0.0767337 Test Loss: 0.1080998
Validation loss decreased (inf --> 0.076734).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.1928986 Vali Loss: 0.0865119 Test Loss: 0.0874949
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2329134208367283, 'val/loss': 0.07673373352736235, 'test/loss': 0.10809975955635309, '_timestamp': 1762779683.2052958}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.1928985685222131, 'val/loss': 0.08651192858815193, 'test/loss': 0.0874948725104332, '_timestamp': 1762779690.199037}).
Epoch: 3, Steps: 133 | Train Loss: 0.1806004 Vali Loss: 0.0788205 Test Loss: 0.0807137
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1775933 Vali Loss: 0.0693407 Test Loss: 0.0930000
Validation loss decreased (0.076734 --> 0.069341).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1740223 Vali Loss: 0.0680652 Test Loss: 0.0944544
Validation loss decreased (0.069341 --> 0.068065).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1719530 Vali Loss: 0.0691787 Test Loss: 0.1036461
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1717379 Vali Loss: 0.0679694 Test Loss: 0.0998501
Validation loss decreased (0.068065 --> 0.067969).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1713727 Vali Loss: 0.0673563 Test Loss: 0.0931459
Validation loss decreased (0.067969 --> 0.067356).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1711640 Vali Loss: 0.0677858 Test Loss: 0.0982316
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1702002 Vali Loss: 0.0661417 Test Loss: 0.0968035
Validation loss decreased (0.067356 --> 0.066142).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1704349 Vali Loss: 0.0669647 Test Loss: 0.0971897
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1702585 Vali Loss: 0.0667453 Test Loss: 0.0939409
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1711612 Vali Loss: 0.0677765 Test Loss: 0.0930692
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1708401 Vali Loss: 0.0667803 Test Loss: 0.0959166
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1706755 Vali Loss: 0.0698496 Test Loss: 0.0920403
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1697729 Vali Loss: 0.0655860 Test Loss: 0.0944899
Validation loss decreased (0.066142 --> 0.065586).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1701185 Vali Loss: 0.0681928 Test Loss: 0.0950526
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1706684 Vali Loss: 0.0679996 Test Loss: 0.0927012
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1707741 Vali Loss: 0.0669288 Test Loss: 0.0976936
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1698535 Vali Loss: 0.0664727 Test Loss: 0.0960157
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1705217 Vali Loss: 0.0674529 Test Loss: 0.0953123
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1725718 Vali Loss: 0.0678704 Test Loss: 0.0950294
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1701518 Vali Loss: 0.0684573 Test Loss: 0.0948567
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.1695173 Vali Loss: 0.0662907 Test Loss: 0.0925450
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.1710647 Vali Loss: 0.0669127 Test Loss: 0.0926788
EarlyStopping counter: 9 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.1699678 Vali Loss: 0.0666939 Test Loss: 0.0928019
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SP500_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 238
test shape: (238, 3, 1) (238, 3, 1)
test shape: (238, 3, 1) (238, 3, 1)


	mse:6.797599780838937e-05, mae:0.006225700955837965, rmse:0.008244755677878857, r2:-0.04560351371765137, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0062, RMSE: 0.0082, RÂ²: -0.0456, MAPE: 2.33%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.488 MB of 0.488 MB uploadedwandb: \ 0.488 MB of 0.488 MB uploadedwandb: | 0.488 MB of 0.488 MB uploadedwandb: / 0.488 MB of 0.488 MB uploadedwandb: - 0.488 MB of 0.488 MB uploadedwandb: \ 0.488 MB of 0.488 MB uploadedwandb: | 0.488 MB of 0.488 MB uploadedwandb: / 0.670 MB of 0.970 MB uploaded (0.002 MB deduped)wandb: - 0.970 MB of 0.970 MB uploaded (0.002 MB deduped)wandb: \ 0.970 MB of 0.970 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–…â–…â–ˆâ–‡â–…â–†â–†â–†â–…â–…â–†â–„â–…â–…â–…â–†â–†â–…â–…â–…â–…â–…â–…
wandb:                 train/loss â–ˆâ–†â–„â–ƒâ–‚â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–â–â–‚â–‚â–â–‚â–ƒâ–â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–ƒâ–â–‚â–‚â–‚â–â–‚â–‚â–ƒâ–â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 25
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.0928
wandb:                 train/loss 0.16997
wandb:   val/directional_accuracy 49.57983
wandb:                   val/loss 0.06669
wandb:                    val/mae 0.00623
wandb:                   val/mape 233.24795
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.0456
wandb:                   val/rmse 0.00824
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/9p1il8pn
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_150111-9p1il8pn/logs
Completed: SP500 H=3

Training: Informer on SP500 for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_150432-33oylh93
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/33oylh93
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SP500_H5   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/33oylh93
>>>>>>>start training : long_term_forecast_Informer_SP500_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 5
============================================================
train 4239
Overriding target from 'OT' to 'close' for stock data
val 235
Overriding target from 'OT' to 'close' for stock data
test 236
Epoch: 1, Steps: 133 | Train Loss: 0.2364108 Vali Loss: 0.0944321 Test Loss: 0.1282908
Validation loss decreased (inf --> 0.094432).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.1899400 Vali Loss: 0.1161122 Test Loss: 0.0824679
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23641077143357211, 'val/loss': 0.09443205315619707, 'test/loss': 0.1282907808199525, '_timestamp': 1762779885.7929995}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.1899400245874448, 'val/loss': 0.11611217726022005, 'test/loss': 0.08246785681694746, '_timestamp': 1762779892.7673852}).
Epoch: 3, Steps: 133 | Train Loss: 0.1832170 Vali Loss: 0.0687441 Test Loss: 0.1045214
Validation loss decreased (0.094432 --> 0.068744).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1814660 Vali Loss: 0.0717870 Test Loss: 0.1118360
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1782338 Vali Loss: 0.0678638 Test Loss: 0.0859833
Validation loss decreased (0.068744 --> 0.067864).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1772963 Vali Loss: 0.0683512 Test Loss: 0.0855306
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1766314 Vali Loss: 0.0700898 Test Loss: 0.0888309
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1766207 Vali Loss: 0.0680354 Test Loss: 0.0882410
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1767560 Vali Loss: 0.0689642 Test Loss: 0.0901707
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1771970 Vali Loss: 0.0665053 Test Loss: 0.0891896
Validation loss decreased (0.067864 --> 0.066505).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1769055 Vali Loss: 0.0697464 Test Loss: 0.0906031
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1762931 Vali Loss: 0.0674646 Test Loss: 0.0875639
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1765211 Vali Loss: 0.0684528 Test Loss: 0.0899919
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1774146 Vali Loss: 0.0676775 Test Loss: 0.0873671
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1770416 Vali Loss: 0.0681855 Test Loss: 0.0890346
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1767284 Vali Loss: 0.0680166 Test Loss: 0.0889590
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1768097 Vali Loss: 0.0663167 Test Loss: 0.0869367
Validation loss decreased (0.066505 --> 0.066317).  Saving model ...
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1793504 Vali Loss: 0.0688480 Test Loss: 0.0891810
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1763435 Vali Loss: 0.0682694 Test Loss: 0.0873263
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1766238 Vali Loss: 0.0670137 Test Loss: 0.0886378
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1767317 Vali Loss: 0.0674618 Test Loss: 0.0882423
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1769731 Vali Loss: 0.0690322 Test Loss: 0.0879241
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1765006 Vali Loss: 0.0676552 Test Loss: 0.0872445
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.1771812 Vali Loss: 0.0664166 Test Loss: 0.0920554
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.1768052 Vali Loss: 0.0685932 Test Loss: 0.0902798
EarlyStopping counter: 8 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.1767519 Vali Loss: 0.0673563 Test Loss: 0.0898909
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.1769236 Vali Loss: 0.0668467 Test Loss: 0.0897744
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SP500_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 236
test shape: (236, 5, 1) (236, 5, 1)
test shape: (236, 5, 1) (236, 5, 1)


	mse:6.702035898342729e-05, mae:0.006108399014919996, rmse:0.00818659644573927, r2:-0.03169691562652588, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0082, RÂ²: -0.0317, MAPE: 1.54%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.518 MB of 0.519 MB uploadedwandb: \ 0.518 MB of 0.519 MB uploadedwandb: | 0.519 MB of 0.519 MB uploadedwandb: / 0.519 MB of 0.519 MB uploadedwandb: - 0.519 MB of 0.820 MB uploadedwandb: \ 0.620 MB of 0.820 MB uploadedwandb: | 0.820 MB of 0.820 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–†â–ˆâ–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–â–ƒâ–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–†â–ƒâ–‚â–â–â–â–‚â–‚â–â–â–‚â–‚â–â–‚â–„â–â–â–â–‚â–â–‚â–‚â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–ˆâ–ƒâ–„â–†â–ƒâ–„â–â–…â–‚â–„â–ƒâ–ƒâ–ƒâ–â–„â–ƒâ–‚â–‚â–„â–ƒâ–â–„â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 26
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.08977
wandb:                 train/loss 0.17692
wandb:   val/directional_accuracy 48.62288
wandb:                   val/loss 0.06685
wandb:                    val/mae 0.00611
wandb:                   val/mape 154.10632
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.0317
wandb:                   val/rmse 0.00819
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/33oylh93
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_150432-33oylh93/logs
Completed: SP500 H=5

Training: Informer on SP500 for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_150800-z1noztrv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/z1noztrv
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SP500_H10  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/z1noztrv
>>>>>>>start training : long_term_forecast_Informer_SP500_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 10
============================================================
train 4234
Overriding target from 'OT' to 'close' for stock data
val 230
Overriding target from 'OT' to 'close' for stock data
test 231
Epoch: 1, Steps: 133 | Train Loss: 0.2346289 Vali Loss: 0.0886496 Test Loss: 0.0980177
Validation loss decreased (inf --> 0.088650).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.1891868 Vali Loss: 0.0727169 Test Loss: 0.0991437
Validation loss decreased (0.088650 --> 0.072717).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23462892878324465, 'val/loss': 0.08864958304911852, 'test/loss': 0.09801767440512776, '_timestamp': 1762780093.7532775}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.18918676663162118, 'val/loss': 0.07271691737696528, 'test/loss': 0.09914370160549879, '_timestamp': 1762780100.7558713}).
Epoch: 3, Steps: 133 | Train Loss: 0.1824167 Vali Loss: 0.0695003 Test Loss: 0.1043266
Validation loss decreased (0.072717 --> 0.069500).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1792260 Vali Loss: 0.0673903 Test Loss: 0.1082703
Validation loss decreased (0.069500 --> 0.067390).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1776429 Vali Loss: 0.0671760 Test Loss: 0.0999687
Validation loss decreased (0.067390 --> 0.067176).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1769303 Vali Loss: 0.0671069 Test Loss: 0.1037922
Validation loss decreased (0.067176 --> 0.067107).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1769362 Vali Loss: 0.0648793 Test Loss: 0.1056657
Validation loss decreased (0.067107 --> 0.064879).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1764662 Vali Loss: 0.0685649 Test Loss: 0.0987737
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1767671 Vali Loss: 0.0684196 Test Loss: 0.0985668
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1763810 Vali Loss: 0.0675553 Test Loss: 0.0987644
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1765024 Vali Loss: 0.0665865 Test Loss: 0.1012862
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1763535 Vali Loss: 0.0666339 Test Loss: 0.0969018
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1767071 Vali Loss: 0.0683399 Test Loss: 0.0988833
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1755058 Vali Loss: 0.0680507 Test Loss: 0.1013953
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1781790 Vali Loss: 0.0669316 Test Loss: 0.0980171
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1765254 Vali Loss: 0.0691270 Test Loss: 0.0980591
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1769211 Vali Loss: 0.0688552 Test Loss: 0.1018872
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SP500_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 231
test shape: (231, 10, 1) (231, 10, 1)
test shape: (231, 10, 1) (231, 10, 1)


	mse:6.645327084697783e-05, mae:0.006071287672966719, rmse:0.0081518879160285, r2:-0.022313952445983887, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0082, RÂ²: -0.0223, MAPE: 1.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.540 MB of 0.540 MB uploadedwandb: \ 0.540 MB of 0.540 MB uploadedwandb: | 0.540 MB of 0.540 MB uploadedwandb: / 0.540 MB of 0.839 MB uploadedwandb: - 0.540 MB of 0.839 MB uploadedwandb: \ 0.839 MB of 0.839 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–†â–ˆâ–ƒâ–…â–†â–‚â–‚â–‚â–„â–â–‚â–„â–‚â–‚â–„
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–„â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–„â–„â–â–‡â–†â–…â–„â–„â–†â–†â–„â–‡â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.10189
wandb:                 train/loss 0.17692
wandb:   val/directional_accuracy 48.24435
wandb:                   val/loss 0.06886
wandb:                    val/mae 0.00607
wandb:                   val/mape 149.53561
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.02231
wandb:                   val/rmse 0.00815
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/z1noztrv
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_150800-z1noztrv/logs
Completed: SP500 H=10

Training: Informer on SP500 for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_151025-08oorpas
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/08oorpas
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SP500_H22  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/08oorpas
>>>>>>>start training : long_term_forecast_Informer_SP500_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 22
============================================================
train 4222
Overriding target from 'OT' to 'close' for stock data
val 218
Overriding target from 'OT' to 'close' for stock data
test 219
Epoch: 1, Steps: 132 | Train Loss: 0.2366669 Vali Loss: 0.0748574 Test Loss: 0.1368040
Validation loss decreased (inf --> 0.074857).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.1903814 Vali Loss: 0.0774931 Test Loss: 0.0823491
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23666692084886812, 'val/loss': 0.074857411640031, 'test/loss': 0.13680397612707956, '_timestamp': 1762780239.0778081}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19038143793516088, 'val/loss': 0.07749309816530772, 'test/loss': 0.08234908059239388, '_timestamp': 1762780245.9728236}).
Epoch: 3, Steps: 132 | Train Loss: 0.1837964 Vali Loss: 0.0733547 Test Loss: 0.0867824
Validation loss decreased (0.074857 --> 0.073355).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.1813338 Vali Loss: 0.0728370 Test Loss: 0.0897542
Validation loss decreased (0.073355 --> 0.072837).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.1799067 Vali Loss: 0.0716632 Test Loss: 0.0880330
Validation loss decreased (0.072837 --> 0.071663).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.1791492 Vali Loss: 0.0696782 Test Loss: 0.0909918
Validation loss decreased (0.071663 --> 0.069678).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.1790094 Vali Loss: 0.0709520 Test Loss: 0.0891641
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.1788039 Vali Loss: 0.0692814 Test Loss: 0.0956896
Validation loss decreased (0.069678 --> 0.069281).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.1787272 Vali Loss: 0.0688848 Test Loss: 0.0969056
Validation loss decreased (0.069281 --> 0.068885).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.1785744 Vali Loss: 0.0690552 Test Loss: 0.0951646
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.1784037 Vali Loss: 0.0689765 Test Loss: 0.0973096
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.1786291 Vali Loss: 0.0687743 Test Loss: 0.0965454
Validation loss decreased (0.068885 --> 0.068774).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.1785228 Vali Loss: 0.0689387 Test Loss: 0.0969803
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.1786731 Vali Loss: 0.0688821 Test Loss: 0.0972810
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.1784218 Vali Loss: 0.0688407 Test Loss: 0.0981739
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.1782467 Vali Loss: 0.0689353 Test Loss: 0.0968161
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.1785053 Vali Loss: 0.0693262 Test Loss: 0.0942266
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.1785946 Vali Loss: 0.0690812 Test Loss: 0.0963252
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.1785191 Vali Loss: 0.0690119 Test Loss: 0.0970460
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.1785944 Vali Loss: 0.0690158 Test Loss: 0.0964185
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.1785192 Vali Loss: 0.0689201 Test Loss: 0.0971030
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.1786901 Vali Loss: 0.0688643 Test Loss: 0.0987486
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SP500_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 219
test shape: (219, 22, 1) (219, 22, 1)
test shape: (219, 22, 1) (219, 22, 1)


	mse:6.561739428434521e-05, mae:0.006042588967829943, rmse:0.008100456558167934, r2:-0.027753829956054688, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0060, RMSE: 0.0081, RÂ²: -0.0278, MAPE: 1.51%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.616 MB of 0.617 MB uploadedwandb: \ 0.616 MB of 0.617 MB uploadedwandb: | 0.616 MB of 0.617 MB uploadedwandb: / 0.617 MB of 0.617 MB uploadedwandb: - 0.617 MB of 0.617 MB uploadedwandb: \ 0.617 MB of 0.917 MB uploadedwandb: | 0.917 MB of 0.917 MB uploadedwandb: / 0.917 MB of 0.917 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ƒâ–‚â–ƒâ–‚â–†â–‡â–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–…â–‡â–‡â–‡â–‡â–ˆ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‡â–…â–‚â–„â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.09875
wandb:                 train/loss 0.17869
wandb:   val/directional_accuracy 51.03283
wandb:                   val/loss 0.06886
wandb:                    val/mae 0.00604
wandb:                   val/mape 151.13113
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.02775
wandb:                   val/rmse 0.0081
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/08oorpas
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_151025-08oorpas/logs
Completed: SP500 H=22

Training: Informer on SP500 for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_151324-srcjfvjt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/srcjfvjt
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SP500_H50  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/srcjfvjt
>>>>>>>start training : long_term_forecast_Informer_SP500_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 50
============================================================
train 4194
Overriding target from 'OT' to 'close' for stock data
val 190
Overriding target from 'OT' to 'close' for stock data
test 191
Epoch: 1, Steps: 132 | Train Loss: 0.2439934 Vali Loss: 0.0846346 Test Loss: 0.1063455
Validation loss decreased (inf --> 0.084635).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.1944459 Vali Loss: 0.0807868 Test Loss: 0.1808572
Validation loss decreased (0.084635 --> 0.080787).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.24399335546927017, 'val/loss': 0.08463459089398384, 'test/loss': 0.10634550576408704, '_timestamp': 1762780417.2063804}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19444594329053705, 'val/loss': 0.08078679318229358, 'test/loss': 0.18085724487900734, '_timestamp': 1762780424.1886475}).
Epoch: 3, Steps: 132 | Train Loss: 0.1889395 Vali Loss: 0.0719656 Test Loss: 0.1093272
Validation loss decreased (0.080787 --> 0.071966).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.1841815 Vali Loss: 0.0701995 Test Loss: 0.1126931
Validation loss decreased (0.071966 --> 0.070200).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.1819025 Vali Loss: 0.0709899 Test Loss: 0.1130549
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.1826700 Vali Loss: 0.0695628 Test Loss: 0.1122310
Validation loss decreased (0.070200 --> 0.069563).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.1836844 Vali Loss: 0.0697103 Test Loss: 0.1135624
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.1816037 Vali Loss: 0.0693991 Test Loss: 0.1217315
Validation loss decreased (0.069563 --> 0.069399).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.1824173 Vali Loss: 0.0697621 Test Loss: 0.1182557
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.1815681 Vali Loss: 0.0697868 Test Loss: 0.1144120
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.1844028 Vali Loss: 0.0692690 Test Loss: 0.1167596
Validation loss decreased (0.069399 --> 0.069269).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.1807744 Vali Loss: 0.0698990 Test Loss: 0.1139444
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.1815809 Vali Loss: 0.0698375 Test Loss: 0.1144665
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.1805583 Vali Loss: 0.0698070 Test Loss: 0.1139657
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.1810149 Vali Loss: 0.0694125 Test Loss: 0.1166614
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.1810639 Vali Loss: 0.0696996 Test Loss: 0.1142119
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.1808247 Vali Loss: 0.0700827 Test Loss: 0.1134310
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.1816575 Vali Loss: 0.0698124 Test Loss: 0.1165478
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.1877356 Vali Loss: 0.0696341 Test Loss: 0.1141083
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.1893613 Vali Loss: 0.0693198 Test Loss: 0.1159592
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.1814327 Vali Loss: 0.0696371 Test Loss: 0.1149048
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SP500_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 191
test shape: (191, 50, 1) (191, 50, 1)
test shape: (191, 50, 1) (191, 50, 1)


	mse:6.621277134399861e-05, mae:0.006064991001039743, rmse:0.008137122727930546, r2:-0.018148422241210938, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0081, RÂ²: -0.0181, MAPE: 1.34%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.673 MB of 0.675 MB uploadedwandb: \ 0.673 MB of 0.675 MB uploadedwandb: | 0.675 MB of 0.675 MB uploadedwandb: / 0.675 MB of 0.675 MB uploadedwandb: - 0.675 MB of 0.975 MB uploadedwandb: \ 0.975 MB of 0.975 MB uploadedwandb: | 0.975 MB of 0.975 MB uploadedwandb: / 0.975 MB of 0.975 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–†â–„â–…â–„â–„â–„â–…â–„â–ƒâ–…â–„â–…â–„
wandb:                 train/loss â–ˆâ–„â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–„â–â–‚â–â–â–â–â–‚â–‡â–ˆâ–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–…â–‚â–‚â–â–‚â–‚â–â–ƒâ–‚â–‚â–â–‚â–ƒâ–‚â–‚â–â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.1149
wandb:                 train/loss 0.18143
wandb:   val/directional_accuracy 49.41767
wandb:                   val/loss 0.06964
wandb:                    val/mae 0.00606
wandb:                   val/mape 133.77098
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.01815
wandb:                   val/rmse 0.00814
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/srcjfvjt
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_151324-srcjfvjt/logs
Completed: SP500 H=50

Training: Informer on SP500 for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_151616-jo27bz5y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/jo27bz5y
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SP500_H100 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/jo27bz5y
>>>>>>>start training : long_term_forecast_Informer_SP500_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 100
============================================================
train 4144
Overriding target from 'OT' to 'close' for stock data
val 140
Overriding target from 'OT' to 'close' for stock data
test 141
Epoch: 1, Steps: 130 | Train Loss: 0.2515482 Vali Loss: 0.0760379 Test Loss: 0.2091517
Validation loss decreased (inf --> 0.076038).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2016727 Vali Loss: 0.0702210 Test Loss: 0.1760226
Validation loss decreased (0.076038 --> 0.070221).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2515481568299807, 'val/loss': 0.07603787630796432, 'test/loss': 0.2091516971588135, '_timestamp': 1762780590.096972}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2016726520199042, 'val/loss': 0.07022098898887634, 'test/loss': 0.1760226383805275, '_timestamp': 1762780597.2702212}).
Epoch: 3, Steps: 130 | Train Loss: 0.1899332 Vali Loss: 0.0669819 Test Loss: 0.1648132
Validation loss decreased (0.070221 --> 0.066982).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.1857637 Vali Loss: 0.0704874 Test Loss: 0.1891877
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.1839745 Vali Loss: 0.0653618 Test Loss: 0.1753114
Validation loss decreased (0.066982 --> 0.065362).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.1830727 Vali Loss: 0.0667659 Test Loss: 0.1661794
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.1830024 Vali Loss: 0.0660568 Test Loss: 0.1632208
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.1832223 Vali Loss: 0.0655978 Test Loss: 0.1655262
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.1822948 Vali Loss: 0.0661499 Test Loss: 0.1690659
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.1824082 Vali Loss: 0.0663568 Test Loss: 0.1648520
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.1826910 Vali Loss: 0.0662051 Test Loss: 0.1625260
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.1822525 Vali Loss: 0.0656361 Test Loss: 0.1645387
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.1824187 Vali Loss: 0.0662969 Test Loss: 0.1653826
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.1821082 Vali Loss: 0.0663869 Test Loss: 0.1666281
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.1828083 Vali Loss: 0.0655252 Test Loss: 0.1630797
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SP500_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 141
test shape: (141, 100, 1) (141, 100, 1)
test shape: (141, 100, 1) (141, 100, 1)


	mse:6.994536670390517e-05, mae:0.006222406402230263, rmse:0.008363334462046623, r2:-0.020975947380065918, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0062, RMSE: 0.0084, RÂ²: -0.0210, MAPE: 1.52%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.764 MB of 0.769 MB uploadedwandb: \ 0.764 MB of 0.769 MB uploadedwandb: | 0.769 MB of 0.769 MB uploadedwandb: / 0.769 MB of 0.769 MB uploadedwandb: - 0.769 MB of 1.068 MB uploadedwandb: \ 1.068 MB of 1.068 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‚â–ˆâ–„â–‚â–â–‚â–ƒâ–‚â–â–‚â–‚â–‚â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–‚â–â–â–‚â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–ˆâ–â–ƒâ–‚â–â–‚â–‚â–‚â–â–‚â–‚â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.16308
wandb:                 train/loss 0.18281
wandb:   val/directional_accuracy 49.14392
wandb:                   val/loss 0.06553
wandb:                    val/mae 0.00622
wandb:                   val/mape 151.50903
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.02098
wandb:                   val/rmse 0.00836
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/jo27bz5y
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_151616-jo27bz5y/logs
Completed: SP500 H=100

Training: Informer on NASDAQ for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_151827-zscz4xdg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/zscz4xdg
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NASDAQ_H3  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/zscz4xdg
>>>>>>>start training : long_term_forecast_Informer_NASDAQ_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
Overriding target from 'OT' to 'close' for stock data
val 237
Overriding target from 'OT' to 'close' for stock data
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.3080075 Vali Loss: 0.1963592 Test Loss: 0.1628178
Validation loss decreased (inf --> 0.196359).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2523662 Vali Loss: 0.1590251 Test Loss: 0.1520693
Validation loss decreased (0.196359 --> 0.159025).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.30800749944116834, 'val/loss': 0.19635923206806183, 'test/loss': 0.16281780507415533, '_timestamp': 1762780720.977901}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25236623011585463, 'val/loss': 0.15902507677674294, 'test/loss': 0.15206927992403507, '_timestamp': 1762780728.0308614}).
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
Epoch: 3, Steps: 133 | Train Loss: 0.2373717 Vali Loss: 0.1387923 Test Loss: 0.1334508
Validation loss decreased (0.159025 --> 0.138792).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2323035 Vali Loss: 0.1389563 Test Loss: 0.1264679
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2292118 Vali Loss: 0.1448573 Test Loss: 0.1417942
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2281862 Vali Loss: 0.1443895 Test Loss: 0.1246166
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2269145 Vali Loss: 0.1374298 Test Loss: 0.1241501
Validation loss decreased (0.138792 --> 0.137430).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2258599 Vali Loss: 0.1339859 Test Loss: 0.1274851
Validation loss decreased (0.137430 --> 0.133986).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2258721 Vali Loss: 0.1348602 Test Loss: 0.1260128
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2258780 Vali Loss: 0.1363857 Test Loss: 0.1247983
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2258762 Vali Loss: 0.1356871 Test Loss: 0.1222134
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2252786 Vali Loss: 0.1315258 Test Loss: 0.1241719
Validation loss decreased (0.133986 --> 0.131526).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2259865 Vali Loss: 0.1369581 Test Loss: 0.1241965
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2253574 Vali Loss: 0.1372120 Test Loss: 0.1242118
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2253982 Vali Loss: 0.1325906 Test Loss: 0.1226930
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2258074 Vali Loss: 0.1342971 Test Loss: 0.1237839
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2258901 Vali Loss: 0.1344018 Test Loss: 0.1267807
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2253812 Vali Loss: 0.1327360 Test Loss: 0.1229386
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2250952 Vali Loss: 0.1349310 Test Loss: 0.1269274
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2255445 Vali Loss: 0.1317307 Test Loss: 0.1228450
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2247829 Vali Loss: 0.1375994 Test Loss: 0.1257649
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2251657 Vali Loss: 0.1316562 Test Loss: 0.1234407
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NASDAQ_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.00014310328697320074, mae:0.00877311546355486, rmse:0.011962578631937504, r2:-0.05138885974884033, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0088, RMSE: 0.0120, RÂ²: -0.0514, MAPE: 2728300.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.486 MB of 0.486 MB uploadedwandb: \ 0.486 MB of 0.486 MB uploadedwandb: | 0.486 MB of 0.486 MB uploadedwandb: / 0.486 MB of 0.486 MB uploadedwandb: - 0.486 MB of 0.486 MB uploadedwandb: \ 0.486 MB of 0.486 MB uploadedwandb: | 0.486 MB of 0.486 MB uploadedwandb: / 0.486 MB of 0.486 MB uploadedwandb: - 0.668 MB of 0.968 MB uploaded (0.002 MB deduped)wandb: \ 0.894 MB of 0.968 MB uploaded (0.002 MB deduped)wandb: | 0.968 MB of 0.968 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–ƒâ–ˆâ–‚â–‚â–ƒâ–‚â–‚â–â–‚â–‚â–‚â–â–‚â–ƒâ–â–ƒâ–â–‚â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–‚â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–…â–ˆâ–ˆâ–„â–‚â–ƒâ–„â–ƒâ–â–„â–„â–‚â–‚â–ƒâ–‚â–ƒâ–â–„â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.12344
wandb:                 train/loss 0.22517
wandb:   val/directional_accuracy 45.14768
wandb:                   val/loss 0.13166
wandb:                    val/mae 0.00877
wandb:                   val/mape 272830050.0
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.05139
wandb:                   val/rmse 0.01196
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/zscz4xdg
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_151827-zscz4xdg/logs
Completed: NASDAQ H=3

Training: Informer on NASDAQ for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_152133-xo2tmnw1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/xo2tmnw1
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NASDAQ_H5  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/xo2tmnw1
>>>>>>>start training : long_term_forecast_Informer_NASDAQ_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
Overriding target from 'OT' to 'close' for stock data
val 235
Overriding target from 'OT' to 'close' for stock data
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.3106751 Vali Loss: 0.1649842 Test Loss: 0.1467131
Validation loss decreased (inf --> 0.164984).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2550929 Vali Loss: 0.1563869 Test Loss: 0.1561670
Validation loss decreased (0.164984 --> 0.156387).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.31067510318935365, 'val/loss': 0.1649842020124197, 'test/loss': 0.14671307522803545, '_timestamp': 1762780907.392119}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.255092895344684, 'val/loss': 0.15638687275350094, 'test/loss': 0.1561669921502471, '_timestamp': 1762780914.8071673}).
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.255092895344684, 'val/loss': 0.15638687275350094, 'test/loss': 0.1561669921502471, '_timestamp': 1762780914.8071673}).
Epoch: 3, Steps: 133 | Train Loss: 0.2430567 Vali Loss: 0.1588349 Test Loss: 0.1381660
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2369821 Vali Loss: 0.1525545 Test Loss: 0.1636380
Validation loss decreased (0.156387 --> 0.152555).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2340842 Vali Loss: 0.1396554 Test Loss: 0.1383736
Validation loss decreased (0.152555 --> 0.139655).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2322008 Vali Loss: 0.1426279 Test Loss: 0.1306741
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2314782 Vali Loss: 0.1411528 Test Loss: 0.1335103
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2310393 Vali Loss: 0.1426083 Test Loss: 0.1367187
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2303894 Vali Loss: 0.1456864 Test Loss: 0.1357528
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2319505 Vali Loss: 0.1489820 Test Loss: 0.1328209
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2317641 Vali Loss: 0.1395828 Test Loss: 0.1322428
Validation loss decreased (0.139655 --> 0.139583).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2305851 Vali Loss: 0.1436425 Test Loss: 0.1377173
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2311864 Vali Loss: 0.1429161 Test Loss: 0.1300112
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2307737 Vali Loss: 0.1547236 Test Loss: 0.1414071
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2308382 Vali Loss: 0.1398754 Test Loss: 0.1305156
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2295953 Vali Loss: 0.1424454 Test Loss: 0.1365682
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2305307 Vali Loss: 0.1428808 Test Loss: 0.1363974
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2306453 Vali Loss: 0.1449265 Test Loss: 0.1418782
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2303308 Vali Loss: 0.1411557 Test Loss: 0.1351332
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2315208 Vali Loss: 0.1387210 Test Loss: 0.1320728
Validation loss decreased (0.139583 --> 0.138721).  Saving model ...
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2313966 Vali Loss: 0.1403066 Test Loss: 0.1352722
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2310722 Vali Loss: 0.1494879 Test Loss: 0.1323786
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2306510 Vali Loss: 0.1548071 Test Loss: 0.1428737
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2302346 Vali Loss: 0.1387995 Test Loss: 0.1327609
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2312490 Vali Loss: 0.1403757 Test Loss: 0.1341664
EarlyStopping counter: 5 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2306837 Vali Loss: 0.1476286 Test Loss: 0.1306868
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.2299086 Vali Loss: 0.1516107 Test Loss: 0.1371280
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.2305274 Vali Loss: 0.1497855 Test Loss: 0.1349042
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.2311298 Vali Loss: 0.1415885 Test Loss: 0.1352971
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 133 | Train Loss: 0.2298792 Vali Loss: 0.1396802 Test Loss: 0.1319959
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NASDAQ_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.00014436524361371994, mae:0.008803042583167553, rmse:0.012015208601951599, r2:-0.05527901649475098, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0088, RMSE: 0.0120, RÂ²: -0.0553, MAPE: 1737523.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.523 MB of 0.523 MB uploadedwandb: \ 0.523 MB of 0.523 MB uploadedwandb: | 0.523 MB of 0.523 MB uploadedwandb: / 0.523 MB of 0.523 MB uploadedwandb: - 0.523 MB of 0.523 MB uploadedwandb: \ 0.523 MB of 0.824 MB uploadedwandb: | 0.824 MB of 0.824 MB uploadedwandb: / 0.824 MB of 0.824 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ƒâ–ˆâ–ƒâ–â–‚â–‚â–‚â–‚â–â–ƒâ–â–ƒâ–â–‚â–‚â–ƒâ–‚â–â–‚â–â–„â–‚â–‚â–â–‚â–‚â–‚â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–â–‚â–‚â–‚â–ƒâ–…â–â–ƒâ–‚â–‡â–â–‚â–‚â–ƒâ–‚â–â–‚â–…â–‡â–â–‚â–„â–…â–…â–‚â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 29
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.132
wandb:                 train/loss 0.22988
wandb:   val/directional_accuracy 48.7234
wandb:                   val/loss 0.13968
wandb:                    val/mae 0.0088
wandb:                   val/mape 173752300.0
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.05528
wandb:                   val/rmse 0.01202
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/xo2tmnw1
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_152133-xo2tmnw1/logs
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread ChkStopThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 268, in check_network_status
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 286, in check_stop_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 840, in deliver_stop_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 848, in deliver_network_status
    return self._deliver_network_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 510, in _deliver_network_status
    return self._deliver_stop_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 494, in _deliver_stop_status
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: NASDAQ H=5

Training: Informer on NASDAQ for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_152522-i69qcg6b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/i69qcg6b
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NASDAQ_H10 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/i69qcg6b
>>>>>>>start training : long_term_forecast_Informer_NASDAQ_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
Overriding target from 'OT' to 'close' for stock data
val 230
Overriding target from 'OT' to 'close' for stock data
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.3080988 Vali Loss: 0.1753168 Test Loss: 0.1605917
Validation loss decreased (inf --> 0.175317).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2574779 Vali Loss: 0.1539199 Test Loss: 0.1472887
Validation loss decreased (0.175317 --> 0.153920).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.30809876658862695, 'val/loss': 0.17531676217913628, 'test/loss': 0.16059167217463255, '_timestamp': 1762781135.8249114}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25747792646849066, 'val/loss': 0.15391993336379528, 'test/loss': 0.1472887247800827, '_timestamp': 1762781142.8250883}).
Epoch: 3, Steps: 133 | Train Loss: 0.2461538 Vali Loss: 0.1623358 Test Loss: 0.1641660
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2387245 Vali Loss: 0.1701344 Test Loss: 0.1791124
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2380689 Vali Loss: 0.1564173 Test Loss: 0.1594793
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2375725 Vali Loss: 0.1501761 Test Loss: 0.1520121
Validation loss decreased (0.153920 --> 0.150176).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2348030 Vali Loss: 0.1505143 Test Loss: 0.1529265
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2345359 Vali Loss: 0.1605587 Test Loss: 0.1660911
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2351178 Vali Loss: 0.1586004 Test Loss: 0.1615676
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2341284 Vali Loss: 0.1540983 Test Loss: 0.1515765
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2340797 Vali Loss: 0.1520801 Test Loss: 0.1550870
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2343960 Vali Loss: 0.1598369 Test Loss: 0.1668440
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2340923 Vali Loss: 0.1515952 Test Loss: 0.1477664
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2339006 Vali Loss: 0.1559794 Test Loss: 0.1594704
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2343697 Vali Loss: 0.1513620 Test Loss: 0.1503420
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2345512 Vali Loss: 0.1510499 Test Loss: 0.1490207
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NASDAQ_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.00014421128435060382, mae:0.008764130994677544, rmse:0.01200880017131567, r2:-0.04179275035858154, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0088, RMSE: 0.0120, RÂ²: -0.0418, MAPE: 1543032.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.588 MB of 0.588 MB uploadedwandb: \ 0.588 MB of 0.588 MB uploadedwandb: | 0.588 MB of 0.588 MB uploadedwandb: / 0.588 MB of 0.588 MB uploadedwandb: - 0.588 MB of 0.887 MB uploadedwandb: \ 0.588 MB of 0.887 MB uploadedwandb: | 0.887 MB of 0.887 MB uploadedwandb: / 0.887 MB of 0.887 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–ˆâ–„â–‚â–‚â–…â–„â–‚â–ƒâ–…â–â–„â–‚â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–ƒâ–‚â–â–‚â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–ˆâ–ƒâ–â–â–…â–„â–‚â–‚â–„â–â–ƒâ–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.14902
wandb:                 train/loss 0.23455
wandb:   val/directional_accuracy 49.13043
wandb:                   val/loss 0.15105
wandb:                    val/mae 0.00876
wandb:                   val/mape 154303200.0
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.04179
wandb:                   val/rmse 0.01201
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/i69qcg6b
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_152522-i69qcg6b/logs
Completed: NASDAQ H=10

Training: Informer on NASDAQ for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_152742-1b7as21j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/1b7as21j
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NASDAQ_H22 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/1b7as21j
>>>>>>>start training : long_term_forecast_Informer_NASDAQ_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
Overriding target from 'OT' to 'close' for stock data
val 218
Overriding target from 'OT' to 'close' for stock data
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.3129668 Vali Loss: 0.2203817 Test Loss: 0.1784298
Validation loss decreased (inf --> 0.220382).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2619109 Vali Loss: 0.1775655 Test Loss: 0.1566375
Validation loss decreased (0.220382 --> 0.177565).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.31296683497952693, 'val/loss': 0.2203817367553711, 'test/loss': 0.17842984412397658, '_timestamp': 1762781276.157977}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26191086615576886, 'val/loss': 0.17756549375397818, 'test/loss': 0.15663745467151916, '_timestamp': 1762781283.0702953}).
Epoch: 3, Steps: 132 | Train Loss: 0.2489234 Vali Loss: 0.1601367 Test Loss: 0.1481347
Validation loss decreased (0.177565 --> 0.160137).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2451677 Vali Loss: 0.1607901 Test Loss: 0.1386172
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2419197 Vali Loss: 0.1612258 Test Loss: 0.1483812
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2402779 Vali Loss: 0.1593149 Test Loss: 0.1461162
Validation loss decreased (0.160137 --> 0.159315).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2394133 Vali Loss: 0.1585332 Test Loss: 0.1417043
Validation loss decreased (0.159315 --> 0.158533).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2391535 Vali Loss: 0.1593245 Test Loss: 0.1444345
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2387503 Vali Loss: 0.1589842 Test Loss: 0.1470448
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2386091 Vali Loss: 0.1620126 Test Loss: 0.1450558
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2387825 Vali Loss: 0.1581233 Test Loss: 0.1434208
Validation loss decreased (0.158533 --> 0.158123).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2386506 Vali Loss: 0.1597646 Test Loss: 0.1444959
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2385790 Vali Loss: 0.1592992 Test Loss: 0.1446774
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2387130 Vali Loss: 0.1597876 Test Loss: 0.1434590
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2391281 Vali Loss: 0.1585563 Test Loss: 0.1455524
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2385086 Vali Loss: 0.1600824 Test Loss: 0.1416665
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2383959 Vali Loss: 0.1606222 Test Loss: 0.1400761
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2382001 Vali Loss: 0.1588133 Test Loss: 0.1442809
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2385657 Vali Loss: 0.1593232 Test Loss: 0.1455669
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2386559 Vali Loss: 0.1596208 Test Loss: 0.1452489
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2389528 Vali Loss: 0.1587138 Test Loss: 0.1467312
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NASDAQ_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.00015360974066425115, mae:0.009129257872700691, rmse:0.012393939308822155, r2:-0.09779679775238037, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0091, RMSE: 0.0124, RÂ²: -0.0978, MAPE: 2349538.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.640 MB of 0.641 MB uploadedwandb: \ 0.640 MB of 0.641 MB uploadedwandb: | 0.641 MB of 0.641 MB uploadedwandb: / 0.641 MB of 0.641 MB uploadedwandb: - 0.641 MB of 0.941 MB uploadedwandb: \ 0.941 MB of 0.941 MB uploadedwandb: | 0.941 MB of 0.941 MB uploadedwandb: / 0.941 MB of 0.941 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–ˆâ–†â–ƒâ–…â–‡â–†â–„â–…â–…â–„â–†â–ƒâ–‚â–…â–†â–†â–‡
wandb:                 train/loss â–ˆâ–†â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–†â–‡â–ƒâ–‚â–ƒâ–ƒâ–ˆâ–â–„â–ƒâ–„â–‚â–…â–…â–‚â–ƒâ–„â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.14673
wandb:                 train/loss 0.23895
wandb:   val/directional_accuracy 50.67715
wandb:                   val/loss 0.15871
wandb:                    val/mae 0.00913
wandb:                   val/mape 234953800.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.0978
wandb:                   val/rmse 0.01239
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/1b7as21j
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_152742-1b7as21j/logs
Completed: NASDAQ H=22

Training: Informer on NASDAQ for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_153039-lzvxk2ea
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/lzvxk2ea
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NASDAQ_H50 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/lzvxk2ea
>>>>>>>start training : long_term_forecast_Informer_NASDAQ_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
Overriding target from 'OT' to 'close' for stock data
val 190
Overriding target from 'OT' to 'close' for stock data
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.3185147 Vali Loss: 0.2277727 Test Loss: 0.2151300
Validation loss decreased (inf --> 0.227773).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2662162 Vali Loss: 0.1879663 Test Loss: 0.1664014
Validation loss decreased (0.227773 --> 0.187966).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3185147393607732, 'val/loss': 0.22777273505926132, 'test/loss': 0.2151300013065338, '_timestamp': 1762781453.7227132}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2662161878789916, 'val/loss': 0.18796634177366892, 'test/loss': 0.16640139619509378, '_timestamp': 1762781460.6115482}).
Epoch: 3, Steps: 132 | Train Loss: 0.2549077 Vali Loss: 0.2103734 Test Loss: 0.1535040
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2496055 Vali Loss: 0.2051245 Test Loss: 0.1601037
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2443396 Vali Loss: 0.1888517 Test Loss: 0.1604013
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2439714 Vali Loss: 0.1882852 Test Loss: 0.1610084
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2431320 Vali Loss: 0.1944494 Test Loss: 0.1581538
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2420550 Vali Loss: 0.1804538 Test Loss: 0.1713900
Validation loss decreased (0.187966 --> 0.180454).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2439278 Vali Loss: 0.1822860 Test Loss: 0.1680438
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2408648 Vali Loss: 0.1835259 Test Loss: 0.1661361
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2413978 Vali Loss: 0.1799384 Test Loss: 0.1680954
Validation loss decreased (0.180454 --> 0.179938).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2431413 Vali Loss: 0.2037295 Test Loss: 0.1588841
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2416627 Vali Loss: 0.1906656 Test Loss: 0.1617574
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2431909 Vali Loss: 0.1986724 Test Loss: 0.1594904
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2437347 Vali Loss: 0.1970080 Test Loss: 0.1588586
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2409547 Vali Loss: 0.1829415 Test Loss: 0.1649072
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2430632 Vali Loss: 0.1937099 Test Loss: 0.1614097
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2453618 Vali Loss: 0.1927885 Test Loss: 0.1631230
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2413910 Vali Loss: 0.1829264 Test Loss: 0.1660807
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2417135 Vali Loss: 0.1814002 Test Loss: 0.1694756
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2410770 Vali Loss: 0.1824603 Test Loss: 0.1656759
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NASDAQ_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.0001614942739252001, mae:0.009312930516898632, rmse:0.012708039954304695, r2:-0.11494052410125732, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0093, RMSE: 0.0127, RÂ²: -0.1149, MAPE: 3271469.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.702 MB of 0.705 MB uploadedwandb: \ 0.702 MB of 0.705 MB uploadedwandb: | 0.705 MB of 0.705 MB uploadedwandb: / 0.705 MB of 1.004 MB uploadedwandb: - 0.805 MB of 1.004 MB uploadedwandb: \ 1.004 MB of 1.004 MB uploadedwandb: | 1.004 MB of 1.004 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–„â–„â–„â–ƒâ–ˆâ–‡â–†â–‡â–ƒâ–„â–ƒâ–ƒâ–…â–„â–…â–†â–‡â–†
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–ƒâ–â–â–‚â–â–‚â–‚â–â–‚â–ƒâ–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‡â–ƒâ–ƒâ–„â–â–‚â–‚â–â–†â–ƒâ–…â–…â–‚â–„â–„â–‚â–â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.16568
wandb:                 train/loss 0.24108
wandb:   val/directional_accuracy 50.67669
wandb:                   val/loss 0.18246
wandb:                    val/mae 0.00931
wandb:                   val/mape 327146950.0
wandb:                    val/mse 0.00016
wandb:                     val/r2 -0.11494
wandb:                   val/rmse 0.01271
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/lzvxk2ea
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_153039-lzvxk2ea/logs
Completed: NASDAQ H=50

Training: Informer on NASDAQ for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_153330-l91wzfpa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/l91wzfpa
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_NASDAQ_H100Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/l91wzfpa
>>>>>>>start training : long_term_forecast_Informer_NASDAQ_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
Overriding target from 'OT' to 'close' for stock data
val 140
Overriding target from 'OT' to 'close' for stock data
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.3241334 Vali Loss: 0.3727944 Test Loss: 0.2885767
Validation loss decreased (inf --> 0.372794).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2672844 Vali Loss: 0.3418271 Test Loss: 0.3169418
Validation loss decreased (0.372794 --> 0.341827).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3241333899589685, 'val/loss': 0.37279443740844725, 'test/loss': 0.2885767489671707, '_timestamp': 1762781623.5219774}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2672843639667217, 'val/loss': 0.34182708263397216, 'test/loss': 0.31694183945655824, '_timestamp': 1762781630.499612}).
Epoch: 3, Steps: 130 | Train Loss: 0.2540666 Vali Loss: 0.3605050 Test Loss: 0.3426241
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2485850 Vali Loss: 0.2974716 Test Loss: 0.3366668
Validation loss decreased (0.341827 --> 0.297472).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2454129 Vali Loss: 0.3029272 Test Loss: 0.4129882
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2441732 Vali Loss: 0.2944544 Test Loss: 0.3716329
Validation loss decreased (0.297472 --> 0.294454).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2438897 Vali Loss: 0.2960980 Test Loss: 0.3853419
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2432748 Vali Loss: 0.2964593 Test Loss: 0.3891966
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2430464 Vali Loss: 0.2962771 Test Loss: 0.3904569
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2434184 Vali Loss: 0.2939849 Test Loss: 0.3845550
Validation loss decreased (0.294454 --> 0.293985).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2432762 Vali Loss: 0.2999730 Test Loss: 0.3956310
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2424474 Vali Loss: 0.2960974 Test Loss: 0.4039418
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2428024 Vali Loss: 0.2935546 Test Loss: 0.3855005
Validation loss decreased (0.293985 --> 0.293555).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.2431925 Vali Loss: 0.2965681 Test Loss: 0.3854840
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.2426188 Vali Loss: 0.2954316 Test Loss: 0.3870541
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 130 | Train Loss: 0.2427702 Vali Loss: 0.2995947 Test Loss: 0.3856824
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 130 | Train Loss: 0.2428764 Vali Loss: 0.3008300 Test Loss: 0.3908827
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 130 | Train Loss: 0.2430118 Vali Loss: 0.2979272 Test Loss: 0.3902735
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 130 | Train Loss: 0.2427448 Vali Loss: 0.3012521 Test Loss: 0.4105243
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 130 | Train Loss: 0.2433844 Vali Loss: 0.2958804 Test Loss: 0.3934403
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 130 | Train Loss: 0.2422099 Vali Loss: 0.2928804 Test Loss: 0.3947551
Validation loss decreased (0.293555 --> 0.292880).  Saving model ...
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 130 | Train Loss: 0.2426890 Vali Loss: 0.2981900 Test Loss: 0.3944493
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 130 | Train Loss: 0.2437315 Vali Loss: 0.2965741 Test Loss: 0.3875681
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 130 | Train Loss: 0.2428733 Vali Loss: 0.2950738 Test Loss: 0.3869093
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 130 | Train Loss: 0.2428099 Vali Loss: 0.2965591 Test Loss: 0.3985684
EarlyStopping counter: 4 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 130 | Train Loss: 0.2429268 Vali Loss: 0.2984425 Test Loss: 0.3864893
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 130 | Train Loss: 0.2431225 Vali Loss: 0.2954469 Test Loss: 0.3923647
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 130 | Train Loss: 0.2430311 Vali Loss: 0.2988097 Test Loss: 0.3905339
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 130 | Train Loss: 0.2427034 Vali Loss: 0.2949006 Test Loss: 0.3865880
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 130 | Train Loss: 0.2425516 Vali Loss: 0.3021570 Test Loss: 0.4264378
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 130 | Train Loss: 0.2433493 Vali Loss: 0.2980957 Test Loss: 0.3903303
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_NASDAQ_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.00016072549624368548, mae:0.008940203115344048, rmse:0.012677756138145924, r2:-0.05967748165130615, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0089, RMSE: 0.0127, RÂ²: -0.0597, MAPE: 1997027.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.749 MB of 0.754 MB uploadedwandb: \ 0.749 MB of 0.754 MB uploadedwandb: | 0.749 MB of 0.754 MB uploadedwandb: / 0.749 MB of 0.754 MB uploadedwandb: - 0.754 MB of 0.754 MB uploadedwandb: \ 0.754 MB of 1.056 MB uploadedwandb: | 0.754 MB of 1.056 MB uploadedwandb: / 1.056 MB of 1.056 MB uploadedwandb: - 1.056 MB of 1.056 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–â–‡â–„â–…â–…â–…â–…â–†â–†â–…â–…â–…â–…â–…â–…â–‡â–…â–†â–†â–…â–…â–†â–…â–…â–…â–…â–ˆâ–…
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–â–â–‚â–â–â–â–â–â–‚â–â–â–‚â–â–â–â–‚â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–‚â–â–â–â–â–â–‚â–â–â–â–â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–‚â–â–‚â–â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 30
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.39033
wandb:                 train/loss 0.24335
wandb:   val/directional_accuracy 50.03608
wandb:                   val/loss 0.2981
wandb:                    val/mae 0.00894
wandb:                   val/mape 199702700.0
wandb:                    val/mse 0.00016
wandb:                     val/r2 -0.05968
wandb:                   val/rmse 0.01268
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/l91wzfpa
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_153330-l91wzfpa/logs
Completed: NASDAQ H=100

Training: Informer on ABSA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_153724-oy5njajg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/oy5njajg
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ABSA_H3    Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/oy5njajg
>>>>>>>start training : long_term_forecast_Informer_ABSA_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 3
============================================================
train 4240
Overriding target from 'OT' to 'close' for stock data
val 237
Overriding target from 'OT' to 'close' for stock data
test 238
Epoch: 1, Steps: 133 | Train Loss: 0.3543700 Vali Loss: 0.1902760 Test Loss: 0.1825247
Validation loss decreased (inf --> 0.190276).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2905647 Vali Loss: 0.2056322 Test Loss: 0.1990296
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3543700304694642, 'val/loss': 0.19027601554989815, 'test/loss': 0.18252469040453434, '_timestamp': 1762781857.295742}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.29056473983857867, 'val/loss': 0.20563220232725143, 'test/loss': 0.19902964495122433, '_timestamp': 1762781864.338112}).
Epoch: 3, Steps: 133 | Train Loss: 0.2770275 Vali Loss: 0.1733781 Test Loss: 0.1624781
Validation loss decreased (0.190276 --> 0.173378).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2705075 Vali Loss: 0.1644081 Test Loss: 0.1538339
Validation loss decreased (0.173378 --> 0.164408).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2657795 Vali Loss: 0.1632201 Test Loss: 0.1557451
Validation loss decreased (0.164408 --> 0.163220).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2652437 Vali Loss: 0.1618384 Test Loss: 0.1540003
Validation loss decreased (0.163220 --> 0.161838).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2625650 Vali Loss: 0.1654234 Test Loss: 0.1557144
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2611301 Vali Loss: 0.1595208 Test Loss: 0.1548123
Validation loss decreased (0.161838 --> 0.159521).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2623734 Vali Loss: 0.1659046 Test Loss: 0.1553680
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2633482 Vali Loss: 0.1616482 Test Loss: 0.1548404
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2611639 Vali Loss: 0.1611551 Test Loss: 0.1544249
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2606522 Vali Loss: 0.1595122 Test Loss: 0.1539063
Validation loss decreased (0.159521 --> 0.159512).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2635261 Vali Loss: 0.1637275 Test Loss: 0.1545517
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2605575 Vali Loss: 0.1638598 Test Loss: 0.1547131
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2632123 Vali Loss: 0.1685824 Test Loss: 0.1540134
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2617927 Vali Loss: 0.1664420 Test Loss: 0.1539158
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2616742 Vali Loss: 0.1620374 Test Loss: 0.1552681
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2617971 Vali Loss: 0.1607125 Test Loss: 0.1542637
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2610878 Vali Loss: 0.1620819 Test Loss: 0.1553014
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2612302 Vali Loss: 0.1595668 Test Loss: 0.1532476
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2622195 Vali Loss: 0.1635891 Test Loss: 0.1543754
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2616438 Vali Loss: 0.1626112 Test Loss: 0.1542485
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ABSA_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 238
test shape: (238, 3, 1) (238, 3, 1)
test shape: (238, 3, 1) (238, 3, 1)


	mse:0.00046215756447054446, mae:0.016341013833880424, rmse:0.021497849375009537, r2:-0.014929413795471191, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0163, RMSE: 0.0215, RÂ²: -0.0149, MAPE: 1.26%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.491 MB of 0.492 MB uploadedwandb: \ 0.492 MB of 0.492 MB uploadedwandb: | 0.492 MB of 0.492 MB uploadedwandb: / 0.492 MB of 0.492 MB uploadedwandb: - 0.492 MB of 0.492 MB uploadedwandb: \ 0.492 MB of 0.492 MB uploadedwandb: | 0.492 MB of 0.492 MB uploadedwandb: / 0.673 MB of 0.972 MB uploaded (0.002 MB deduped)wandb: - 0.972 MB of 0.972 MB uploaded (0.002 MB deduped)wandb: \ 0.972 MB of 0.972 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–‚â–â–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–â–‚â–‚â–â–â–‚â–â–‚â–‚â–â–‚â–â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–ƒâ–‚â–„â–â–„â–‚â–‚â–â–ƒâ–ƒâ–†â–„â–‚â–‚â–‚â–â–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.15425
wandb:                 train/loss 0.26164
wandb:   val/directional_accuracy 48.10924
wandb:                   val/loss 0.16261
wandb:                    val/mae 0.01634
wandb:                   val/mape 126.15389
wandb:                    val/mse 0.00046
wandb:                     val/r2 -0.01493
wandb:                   val/rmse 0.0215
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/oy5njajg
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_153724-oy5njajg/logs
Completed: ABSA H=3

Training: Informer on ABSA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_154025-scd18d0y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/scd18d0y
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ABSA_H5    Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/scd18d0y
>>>>>>>start training : long_term_forecast_Informer_ABSA_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 5
============================================================
train 4238
Overriding target from 'OT' to 'close' for stock data
val 235
Overriding target from 'OT' to 'close' for stock data
test 236
Epoch: 1, Steps: 133 | Train Loss: 0.3627857 Vali Loss: 0.1771803 Test Loss: 0.1654505
Validation loss decreased (inf --> 0.177180).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2963785 Vali Loss: 0.1765928 Test Loss: 0.1669739
Validation loss decreased (0.177180 --> 0.176593).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3627856969833374, 'val/loss': 0.1771803293377161, 'test/loss': 0.16545054223388433, '_timestamp': 1762782038.6315491}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.29637847583096727, 'val/loss': 0.1765927691012621, 'test/loss': 0.16697391401976347, '_timestamp': 1762782045.4679754}).
Epoch: 3, Steps: 133 | Train Loss: 0.2838315 Vali Loss: 0.1703052 Test Loss: 0.1608180
Validation loss decreased (0.176593 --> 0.170305).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2765684 Vali Loss: 0.1692809 Test Loss: 0.1668279
Validation loss decreased (0.170305 --> 0.169281).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2720605 Vali Loss: 0.1672768 Test Loss: 0.1607489
Validation loss decreased (0.169281 --> 0.167277).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2698159 Vali Loss: 0.1648023 Test Loss: 0.1603685
Validation loss decreased (0.167277 --> 0.164802).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2689035 Vali Loss: 0.1669565 Test Loss: 0.1604227
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2688846 Vali Loss: 0.1733052 Test Loss: 0.1606853
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2678227 Vali Loss: 0.1710137 Test Loss: 0.1603280
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2686207 Vali Loss: 0.1658188 Test Loss: 0.1600237
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2674791 Vali Loss: 0.1646724 Test Loss: 0.1613013
Validation loss decreased (0.164802 --> 0.164672).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2672488 Vali Loss: 0.1670783 Test Loss: 0.1618710
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2698123 Vali Loss: 0.1601262 Test Loss: 0.1597769
Validation loss decreased (0.164672 --> 0.160126).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2683110 Vali Loss: 0.1678773 Test Loss: 0.1613382
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2667725 Vali Loss: 0.1662934 Test Loss: 0.1601644
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2668078 Vali Loss: 0.1639391 Test Loss: 0.1616431
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2673975 Vali Loss: 0.1648846 Test Loss: 0.1606286
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2676377 Vali Loss: 0.1641896 Test Loss: 0.1603260
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2678109 Vali Loss: 0.1648557 Test Loss: 0.1608804
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2683948 Vali Loss: 0.1648935 Test Loss: 0.1609550
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2692144 Vali Loss: 0.1637194 Test Loss: 0.1607915
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2679075 Vali Loss: 0.1652986 Test Loss: 0.1612067
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2675669 Vali Loss: 0.1634808 Test Loss: 0.1616336
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ABSA_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 236
test shape: (236, 5, 1) (236, 5, 1)
test shape: (236, 5, 1) (236, 5, 1)


	mse:0.0004625672590918839, mae:0.016387322917580605, rmse:0.021507376804947853, r2:-0.00992274284362793, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0164, RMSE: 0.0215, RÂ²: -0.0099, MAPE: 1.32%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.525 MB of 0.525 MB uploadedwandb: \ 0.525 MB of 0.525 MB uploadedwandb: | 0.525 MB of 0.525 MB uploadedwandb: / 0.525 MB of 0.525 MB uploadedwandb: - 0.525 MB of 0.825 MB uploadedwandb: \ 0.825 MB of 0.825 MB uploadedwandb: | 0.825 MB of 0.825 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‚â–ˆâ–‚â–‚â–‚â–‚â–‚â–â–ƒâ–ƒâ–â–ƒâ–â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–ƒ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–‚â–‚â–â–â–â–â–â–‚â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–†â–…â–ƒâ–…â–ˆâ–‡â–„â–ƒâ–…â–â–…â–„â–ƒâ–„â–ƒâ–„â–„â–ƒâ–„â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.16163
wandb:                 train/loss 0.26757
wandb:   val/directional_accuracy 51.16525
wandb:                   val/loss 0.16348
wandb:                    val/mae 0.01639
wandb:                   val/mape 132.40232
wandb:                    val/mse 0.00046
wandb:                     val/r2 -0.00992
wandb:                   val/rmse 0.02151
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/scd18d0y
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_154025-scd18d0y/logs
Completed: ABSA H=5

Training: Informer on ABSA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_154331-wjmeywxd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/wjmeywxd
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ABSA_H10   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/wjmeywxd
>>>>>>>start training : long_term_forecast_Informer_ABSA_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 10
============================================================
train 4233
Overriding target from 'OT' to 'close' for stock data
val 230
Overriding target from 'OT' to 'close' for stock data
test 231
Epoch: 1, Steps: 133 | Train Loss: 0.3642672 Vali Loss: 0.1822682 Test Loss: 0.1712420
Validation loss decreased (inf --> 0.182268).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.3005945 Vali Loss: 0.1809118 Test Loss: 0.1810011
Validation loss decreased (0.182268 --> 0.180912).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3642671659476775, 'val/loss': 0.1822681613266468, 'test/loss': 0.17124200519174337, '_timestamp': 1762782225.243902}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3005945378900471, 'val/loss': 0.18091176822781563, 'test/loss': 0.18100113235414028, '_timestamp': 1762782232.2436185}).
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
Epoch: 3, Steps: 133 | Train Loss: 0.2880202 Vali Loss: 0.1694848 Test Loss: 0.1648938
Validation loss decreased (0.180912 --> 0.169485).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2800314 Vali Loss: 0.1786282 Test Loss: 0.1688342
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2768617 Vali Loss: 0.1737332 Test Loss: 0.1699253
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2752178 Vali Loss: 0.1728614 Test Loss: 0.1698092
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2725313 Vali Loss: 0.1702251 Test Loss: 0.1685788
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2714309 Vali Loss: 0.1754547 Test Loss: 0.1687984
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2719923 Vali Loss: 0.1778210 Test Loss: 0.1710126
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2713393 Vali Loss: 0.1748917 Test Loss: 0.1689193
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2705271 Vali Loss: 0.1682357 Test Loss: 0.1678654
Validation loss decreased (0.169485 --> 0.168236).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2717494 Vali Loss: 0.1701823 Test Loss: 0.1695506
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2719388 Vali Loss: 0.1702529 Test Loss: 0.1678611
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2710864 Vali Loss: 0.1758105 Test Loss: 0.1684955
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2713546 Vali Loss: 0.1669402 Test Loss: 0.1674889
Validation loss decreased (0.168236 --> 0.166940).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2710578 Vali Loss: 0.1706614 Test Loss: 0.1679495
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2708649 Vali Loss: 0.1735009 Test Loss: 0.1681995
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2704684 Vali Loss: 0.1751666 Test Loss: 0.1680780
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2705747 Vali Loss: 0.1754410 Test Loss: 0.1694610
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2716717 Vali Loss: 0.1685921 Test Loss: 0.1680742
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2705110 Vali Loss: 0.1726289 Test Loss: 0.1682430
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2704939 Vali Loss: 0.1756839 Test Loss: 0.1684775
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2704607 Vali Loss: 0.1714734 Test Loss: 0.1686202
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2717886 Vali Loss: 0.1699440 Test Loss: 0.1682052
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2701108 Vali Loss: 0.1732373 Test Loss: 0.1690735
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ABSA_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 231
test shape: (231, 10, 1) (231, 10, 1)
test shape: (231, 10, 1) (231, 10, 1)


	mse:0.00046725149150006473, mae:0.01642748713493347, rmse:0.021616000682115555, r2:-0.012076735496520996, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0164, RMSE: 0.0216, RÂ²: -0.0121, MAPE: 1.20%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.562 MB of 0.563 MB uploadedwandb: \ 0.562 MB of 0.563 MB uploadedwandb: | 0.563 MB of 0.563 MB uploadedwandb: / 0.563 MB of 0.563 MB uploadedwandb: - 0.563 MB of 0.863 MB uploadedwandb: \ 0.863 MB of 0.863 MB uploadedwandb: | 0.863 MB of 0.863 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–†â–‡â–‡â–…â–…â–ˆâ–†â–„â–†â–„â–…â–„â–„â–…â–…â–†â–…â–…â–…â–…â–…â–†
wandb:                 train/loss â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–â–â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–ˆâ–…â–…â–ƒâ–†â–ˆâ–†â–‚â–ƒâ–ƒâ–†â–â–ƒâ–…â–†â–†â–‚â–„â–†â–„â–ƒâ–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 24
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.16907
wandb:                 train/loss 0.27011
wandb:   val/directional_accuracy 49.25445
wandb:                   val/loss 0.17324
wandb:                    val/mae 0.01643
wandb:                   val/mape 119.63388
wandb:                    val/mse 0.00047
wandb:                     val/r2 -0.01208
wandb:                   val/rmse 0.02162
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/wjmeywxd
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_154331-wjmeywxd/logs
Completed: ABSA H=10

Training: Informer on ABSA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_154648-ywmrtllx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/ywmrtllx
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ABSA_H22   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/ywmrtllx
>>>>>>>start training : long_term_forecast_Informer_ABSA_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 22
============================================================
train 4221
Overriding target from 'OT' to 'close' for stock data
val 218
Overriding target from 'OT' to 'close' for stock data
test 219
Epoch: 1, Steps: 132 | Train Loss: 0.3722615 Vali Loss: 0.2059384 Test Loss: 0.1797485
Validation loss decreased (inf --> 0.205938).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.3106759 Vali Loss: 0.2010915 Test Loss: 0.1796821
Validation loss decreased (0.205938 --> 0.201091).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3722614817547076, 'val/loss': 0.2059383967093059, 'test/loss': 0.17974852664130075, '_timestamp': 1762782420.8062782}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.31067585279092647, 'val/loss': 0.20109145769051143, 'test/loss': 0.1796821036509105, '_timestamp': 1762782427.7742546}).
Epoch: 3, Steps: 132 | Train Loss: 0.2935294 Vali Loss: 0.1882522 Test Loss: 0.1696007
Validation loss decreased (0.201091 --> 0.188252).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2847290 Vali Loss: 0.1825715 Test Loss: 0.1654901
Validation loss decreased (0.188252 --> 0.182572).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2796722 Vali Loss: 0.1809311 Test Loss: 0.1638631
Validation loss decreased (0.182572 --> 0.180931).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2763369 Vali Loss: 0.1799670 Test Loss: 0.1628460
Validation loss decreased (0.180931 --> 0.179967).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2743983 Vali Loss: 0.1782245 Test Loss: 0.1636124
Validation loss decreased (0.179967 --> 0.178224).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2737981 Vali Loss: 0.1797160 Test Loss: 0.1627403
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2733921 Vali Loss: 0.1791164 Test Loss: 0.1633903
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2733277 Vali Loss: 0.1791427 Test Loss: 0.1631916
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2730470 Vali Loss: 0.1790712 Test Loss: 0.1636917
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2731414 Vali Loss: 0.1789609 Test Loss: 0.1633647
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2732357 Vali Loss: 0.1786951 Test Loss: 0.1641674
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2728045 Vali Loss: 0.1780726 Test Loss: 0.1627841
Validation loss decreased (0.178224 --> 0.178073).  Saving model ...
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2730758 Vali Loss: 0.1785927 Test Loss: 0.1638482
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2729297 Vali Loss: 0.1786105 Test Loss: 0.1627955
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2732729 Vali Loss: 0.1783470 Test Loss: 0.1629209
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2726768 Vali Loss: 0.1782289 Test Loss: 0.1638577
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2730288 Vali Loss: 0.1793549 Test Loss: 0.1631407
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2727544 Vali Loss: 0.1788797 Test Loss: 0.1634539
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2734976 Vali Loss: 0.1786973 Test Loss: 0.1639574
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2731593 Vali Loss: 0.1789615 Test Loss: 0.1629311
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.2731915 Vali Loss: 0.1788999 Test Loss: 0.1636729
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 132 | Train Loss: 0.2728396 Vali Loss: 0.1780015 Test Loss: 0.1629921
Validation loss decreased (0.178073 --> 0.178002).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 132 | Train Loss: 0.2733060 Vali Loss: 0.1786376 Test Loss: 0.1641618
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 132 | Train Loss: 0.2731090 Vali Loss: 0.1788638 Test Loss: 0.1633741
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 132 | Train Loss: 0.2729557 Vali Loss: 0.1786358 Test Loss: 0.1631630
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 132 | Train Loss: 0.2730698 Vali Loss: 0.1789470 Test Loss: 0.1628761
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 132 | Train Loss: 0.2728109 Vali Loss: 0.1795590 Test Loss: 0.1639953
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 132 | Train Loss: 0.2728826 Vali Loss: 0.1795009 Test Loss: 0.1640374
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 132 | Train Loss: 0.2728618 Vali Loss: 0.1784878 Test Loss: 0.1627263
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 132 | Train Loss: 0.2729718 Vali Loss: 0.1780991 Test Loss: 0.1627541
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 132 | Train Loss: 0.2730031 Vali Loss: 0.1792992 Test Loss: 0.1631614
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 132 | Train Loss: 0.2729461 Vali Loss: 0.1796305 Test Loss: 0.1639220
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ABSA_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 219
test shape: (219, 22, 1) (219, 22, 1)
test shape: (219, 22, 1) (219, 22, 1)


	mse:0.0004779828304890543, mae:0.016613075509667397, rmse:0.021862817928195, r2:-0.020194172859191895, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0166, RMSE: 0.0219, RÂ²: -0.0202, MAPE: 1.33%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.618 MB of 0.619 MB uploadedwandb: \ 0.618 MB of 0.619 MB uploadedwandb: | 0.618 MB of 0.619 MB uploadedwandb: / 0.619 MB of 0.619 MB uploadedwandb: - 0.619 MB of 0.619 MB uploadedwandb: \ 0.619 MB of 0.921 MB uploadedwandb: | 0.921 MB of 0.921 MB uploadedwandb: / 0.921 MB of 0.921 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–â–‚â–â–‚â–â–‚â–‚â–‚â–â–‚â–â–â–‚â–â–‚â–‚â–â–‚â–â–‚â–‚â–â–â–‚â–‚â–â–â–â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–ƒâ–‚â–â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–‚â–â–‚â–‚â–â–â–‚â–â–‚â–‚â–‚â–â–â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 33
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.16392
wandb:                 train/loss 0.27295
wandb:   val/directional_accuracy 48.61927
wandb:                   val/loss 0.17963
wandb:                    val/mae 0.01661
wandb:                   val/mape 133.12937
wandb:                    val/mse 0.00048
wandb:                     val/r2 -0.02019
wandb:                   val/rmse 0.02186
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/ywmrtllx
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_154648-ywmrtllx/logs
Completed: ABSA H=22

Training: Informer on ABSA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_155101-k5vmkdqy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/k5vmkdqy
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ABSA_H50   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/k5vmkdqy
>>>>>>>start training : long_term_forecast_Informer_ABSA_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 50
============================================================
train 4193
Overriding target from 'OT' to 'close' for stock data
val 190
Overriding target from 'OT' to 'close' for stock data
test 191
Epoch: 1, Steps: 132 | Train Loss: 0.3892298 Vali Loss: 0.2178087 Test Loss: 0.1812749
Validation loss decreased (inf --> 0.217809).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.3192162 Vali Loss: 0.1841953 Test Loss: 0.1635458
Validation loss decreased (0.217809 --> 0.184195).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3892297525749062, 'val/loss': 0.21780872096618017, 'test/loss': 0.1812749058008194, '_timestamp': 1762782674.7407348}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.31921618273763946, 'val/loss': 0.18419534216324487, 'test/loss': 0.1635457513233026, '_timestamp': 1762782681.6911056}).
Epoch: 3, Steps: 132 | Train Loss: 0.3063169 Vali Loss: 0.2025959 Test Loss: 0.1651461
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2978449 Vali Loss: 0.1874068 Test Loss: 0.1593396
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2903514 Vali Loss: 0.1828197 Test Loss: 0.1594680
Validation loss decreased (0.184195 --> 0.182820).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2884595 Vali Loss: 0.1856283 Test Loss: 0.1599361
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2871212 Vali Loss: 0.1889314 Test Loss: 0.1610345
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2853799 Vali Loss: 0.1868353 Test Loss: 0.1629049
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2865160 Vali Loss: 0.1912228 Test Loss: 0.1664978
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2840722 Vali Loss: 0.1899837 Test Loss: 0.1618736
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2844434 Vali Loss: 0.1856762 Test Loss: 0.1595776
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2844033 Vali Loss: 0.1851052 Test Loss: 0.1604415
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2864000 Vali Loss: 0.1812345 Test Loss: 0.1605612
Validation loss decreased (0.182820 --> 0.181235).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2858069 Vali Loss: 0.1841418 Test Loss: 0.1610055
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2861360 Vali Loss: 0.1839109 Test Loss: 0.1605010
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2840947 Vali Loss: 0.1870308 Test Loss: 0.1609798
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2854590 Vali Loss: 0.1833929 Test Loss: 0.1601674
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.2861423 Vali Loss: 0.1872735 Test Loss: 0.1629312
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.2853119 Vali Loss: 0.1893262 Test Loss: 0.1628337
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.2869306 Vali Loss: 0.1862366 Test Loss: 0.1645611
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.2844995 Vali Loss: 0.1851222 Test Loss: 0.1594484
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.2839476 Vali Loss: 0.1874939 Test Loss: 0.1619385
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.2881263 Vali Loss: 0.1883288 Test Loss: 0.1615968
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ABSA_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 191
test shape: (191, 50, 1) (191, 50, 1)
test shape: (191, 50, 1) (191, 50, 1)


	mse:0.0004896881873719394, mae:0.01693040505051613, rmse:0.02212889865040779, r2:-0.006574153900146484, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0169, RMSE: 0.0221, RÂ²: -0.0066, MAPE: 1.10%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.703 MB of 0.706 MB uploadedwandb: \ 0.703 MB of 0.706 MB uploadedwandb: | 0.706 MB of 0.706 MB uploadedwandb: / 0.706 MB of 0.706 MB uploadedwandb: - 0.706 MB of 1.006 MB uploadedwandb: \ 1.006 MB of 1.006 MB uploadedwandb: | 1.006 MB of 1.006 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‡â–â–â–‚â–ƒâ–„â–ˆâ–ƒâ–â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–…â–„â–†â–â–„â–ƒ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–‚â–â–â–â–‚â–‚â–‚â–â–â–‚â–â–‚â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–‚â–‚â–„â–ƒâ–„â–„â–‚â–‚â–â–‚â–‚â–ƒâ–‚â–ƒâ–„â–ƒâ–‚â–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.1616
wandb:                 train/loss 0.28813
wandb:   val/directional_accuracy 49.71685
wandb:                   val/loss 0.18833
wandb:                    val/mae 0.01693
wandb:                   val/mape 110.0643
wandb:                    val/mse 0.00049
wandb:                     val/r2 -0.00657
wandb:                   val/rmse 0.02213
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/k5vmkdqy
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_155101-k5vmkdqy/logs
Completed: ABSA H=50

Training: Informer on ABSA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_155405-kej8umsr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/kej8umsr
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ABSA_H100  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/kej8umsr
>>>>>>>start training : long_term_forecast_Informer_ABSA_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 100
============================================================
train 4143
Overriding target from 'OT' to 'close' for stock data
val 140
Overriding target from 'OT' to 'close' for stock data
test 141
Epoch: 1, Steps: 130 | Train Loss: 0.4023853 Vali Loss: 0.2576958 Test Loss: 0.2275243
Validation loss decreased (inf --> 0.257696).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.3287844 Vali Loss: 0.2284101 Test Loss: 0.1841186
Validation loss decreased (0.257696 --> 0.228410).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.40238532905395213, 'val/loss': 0.25769581496715543, 'test/loss': 0.22752425074577332, '_timestamp': 1762782858.770871}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.32878441432347666, 'val/loss': 0.22841013371944427, 'test/loss': 0.18411857783794403, '_timestamp': 1762782865.6914425}).
Epoch: 3, Steps: 130 | Train Loss: 0.3104835 Vali Loss: 0.2541662 Test Loss: 0.2056295
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.3034639 Vali Loss: 0.2228157 Test Loss: 0.1791713
Validation loss decreased (0.228410 --> 0.222816).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2992106 Vali Loss: 0.2230455 Test Loss: 0.1793372
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2969302 Vali Loss: 0.2219226 Test Loss: 0.1800997
Validation loss decreased (0.222816 --> 0.221923).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2959408 Vali Loss: 0.2444840 Test Loss: 0.1933540
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2958302 Vali Loss: 0.2438413 Test Loss: 0.1939256
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2948810 Vali Loss: 0.2382176 Test Loss: 0.1916247
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2953591 Vali Loss: 0.2364171 Test Loss: 0.1877113
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2948958 Vali Loss: 0.2354590 Test Loss: 0.1903261
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2941748 Vali Loss: 0.2461596 Test Loss: 0.1921142
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2941688 Vali Loss: 0.2392445 Test Loss: 0.1896172
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.2945398 Vali Loss: 0.2341130 Test Loss: 0.1887830
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 130 | Train Loss: 0.2939811 Vali Loss: 0.2386753 Test Loss: 0.1888169
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 130 | Train Loss: 0.2947981 Vali Loss: 0.2396211 Test Loss: 0.1913013
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ABSA_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 141
test shape: (141, 100, 1) (141, 100, 1)
test shape: (141, 100, 1) (141, 100, 1)


	mse:0.0005177446873858571, mae:0.017345886677503586, rmse:0.02275400422513485, r2:-0.003315567970275879, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0173, RMSE: 0.0228, RÂ²: -0.0033, MAPE: 1.02%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.799 MB of 0.804 MB uploadedwandb: \ 0.804 MB of 0.804 MB uploadedwandb: | 0.804 MB of 0.804 MB uploadedwandb: / 0.804 MB of 1.103 MB uploadedwandb: - 0.804 MB of 1.103 MB uploadedwandb: \ 1.103 MB of 1.103 MB uploadedwandb: | 1.103 MB of 1.103 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–â–â–…â–…â–„â–ƒâ–„â–„â–„â–„â–„â–„
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–â–â–†â–†â–…â–„â–„â–†â–…â–„â–…â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.1913
wandb:                 train/loss 0.2948
wandb:   val/directional_accuracy 50.41908
wandb:                   val/loss 0.23962
wandb:                    val/mae 0.01735
wandb:                   val/mape 102.45379
wandb:                    val/mse 0.00052
wandb:                     val/r2 -0.00332
wandb:                   val/rmse 0.02275
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/kej8umsr
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_155405-kej8umsr/logs
Exception in thread IntMsgThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 300, in check_internal_messages
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 268, in check_network_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 856, in deliver_internal_messages
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 848, in deliver_network_status
    return self._deliver_internal_messages(internal_message)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 516, in _deliver_internal_messages
    return self._deliver_network_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 510, in _deliver_network_status
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: ABSA H=100

Training: Informer on SASOL for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_155623-x6g0uite
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/x6g0uite
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SASOL_H3   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/x6g0uite
>>>>>>>start training : long_term_forecast_Informer_SASOL_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 3
============================================================
train 3772
Overriding target from 'OT' to 'close' for stock data
val 211
Overriding target from 'OT' to 'close' for stock data
test 212
Epoch: 1, Steps: 118 | Train Loss: 0.2685320 Vali Loss: 0.1177542 Test Loss: 0.1581979
Validation loss decreased (inf --> 0.117754).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2168604 Vali Loss: 0.1117312 Test Loss: 0.1517675
Validation loss decreased (0.117754 --> 0.111731).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.26853197661496825, 'val/loss': 0.11775423160621099, 'test/loss': 0.15819787553378514, '_timestamp': 1762782996.8326626}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2168603941040524, 'val/loss': 0.1117311886378697, 'test/loss': 0.1517674614276205, '_timestamp': 1762783003.0906065}).
Epoch: 3, Steps: 118 | Train Loss: 0.2051535 Vali Loss: 0.1175388 Test Loss: 0.1738920
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.1998270 Vali Loss: 0.1115774 Test Loss: 0.1540619
Validation loss decreased (0.111731 --> 0.111577).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.1974124 Vali Loss: 0.1022331 Test Loss: 0.1493469
Validation loss decreased (0.111577 --> 0.102233).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.1956938 Vali Loss: 0.1036442 Test Loss: 0.1499996
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.1954282 Vali Loss: 0.1038336 Test Loss: 0.1493342
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.1944037 Vali Loss: 0.1060894 Test Loss: 0.1500577
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.1945728 Vali Loss: 0.1024938 Test Loss: 0.1483764
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.1949721 Vali Loss: 0.1032097 Test Loss: 0.1482792
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.1942458 Vali Loss: 0.1034327 Test Loss: 0.1490759
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.1944944 Vali Loss: 0.1023043 Test Loss: 0.1480776
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.1947483 Vali Loss: 0.1017067 Test Loss: 0.1462725
Validation loss decreased (0.102233 --> 0.101707).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.1936958 Vali Loss: 0.1056422 Test Loss: 0.1480635
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.1944326 Vali Loss: 0.1020908 Test Loss: 0.1486323
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.1947201 Vali Loss: 0.1067007 Test Loss: 0.1499781
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.1939379 Vali Loss: 0.1036180 Test Loss: 0.1493722
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.1949411 Vali Loss: 0.1028341 Test Loss: 0.1478797
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.1942952 Vali Loss: 0.1038169 Test Loss: 0.1479520
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.1942624 Vali Loss: 0.1028875 Test Loss: 0.1481182
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 118 | Train Loss: 0.1949527 Vali Loss: 0.1021125 Test Loss: 0.1474487
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 118 | Train Loss: 0.1938728 Vali Loss: 0.1043241 Test Loss: 0.1478922
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 118 | Train Loss: 0.1942683 Vali Loss: 0.1030046 Test Loss: 0.1486124
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SASOL_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 212
test shape: (212, 3, 1) (212, 3, 1)
test shape: (212, 3, 1) (212, 3, 1)


	mse:0.002315605292096734, mae:0.03645557537674904, rmse:0.048120737075805664, r2:-0.05111265182495117, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0365, RMSE: 0.0481, RÂ²: -0.0511, MAPE: 20641812.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.465 MB of 0.466 MB uploadedwandb: \ 0.465 MB of 0.466 MB uploadedwandb: | 0.465 MB of 0.466 MB uploadedwandb: / 0.466 MB of 0.466 MB uploadedwandb: - 0.466 MB of 0.466 MB uploadedwandb: \ 0.466 MB of 0.466 MB uploadedwandb: | 0.466 MB of 0.466 MB uploadedwandb: / 0.466 MB of 0.466 MB uploadedwandb: - 0.647 MB of 0.947 MB uploaded (0.002 MB deduped)wandb: \ 0.947 MB of 0.947 MB uploaded (0.002 MB deduped)wandb: | 0.947 MB of 0.947 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–‚â–‚â–â–â–â–â–â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–‚â–‚â–â–â–‚â–â–â–‚â–â–‚â–â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–â–‚â–‚â–ƒâ–â–‚â–‚â–â–â–ƒâ–â–ƒâ–‚â–â–‚â–‚â–â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.14861
wandb:                 train/loss 0.19427
wandb:   val/directional_accuracy 51.41509
wandb:                   val/loss 0.103
wandb:                    val/mae 0.03646
wandb:                   val/mape 2064181200.0
wandb:                    val/mse 0.00232
wandb:                     val/r2 -0.05111
wandb:                   val/rmse 0.04812
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/x6g0uite
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_155623-x6g0uite/logs
Completed: SASOL H=3

Training: Informer on SASOL for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_155917-iv2kc35y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/iv2kc35y
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SASOL_H5   Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/iv2kc35y
>>>>>>>start training : long_term_forecast_Informer_SASOL_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 5
============================================================
train 3770
Overriding target from 'OT' to 'close' for stock data
val 209
Overriding target from 'OT' to 'close' for stock data
test 210
Epoch: 1, Steps: 118 | Train Loss: 0.2844564 Vali Loss: 0.1127404 Test Loss: 0.1564700
Validation loss decreased (inf --> 0.112740).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2203806 Vali Loss: 0.1167947 Test Loss: 0.1809002
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.28445636859889756, 'val/loss': 0.11274042193378721, 'test/loss': 0.1564699677484376, '_timestamp': 1762783171.7779582}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.22038064480333006, 'val/loss': 0.11679466175181526, 'test/loss': 0.18090019162212098, '_timestamp': 1762783178.2398913}).
Epoch: 3, Steps: 118 | Train Loss: 0.2084889 Vali Loss: 0.1064708 Test Loss: 0.1560091
Validation loss decreased (0.112740 --> 0.106471).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2034155 Vali Loss: 0.1098603 Test Loss: 0.1518185
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2012513 Vali Loss: 0.1060524 Test Loss: 0.1510906
Validation loss decreased (0.106471 --> 0.106052).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2001620 Vali Loss: 0.1102730 Test Loss: 0.1527042
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.1987542 Vali Loss: 0.1104192 Test Loss: 0.1526033
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.1988347 Vali Loss: 0.1068464 Test Loss: 0.1521973
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.1988736 Vali Loss: 0.1064208 Test Loss: 0.1507854
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.1989317 Vali Loss: 0.1047695 Test Loss: 0.1506067
Validation loss decreased (0.106052 --> 0.104769).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.1980388 Vali Loss: 0.1055522 Test Loss: 0.1500091
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.1987805 Vali Loss: 0.1056019 Test Loss: 0.1526936
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.1982649 Vali Loss: 0.1098968 Test Loss: 0.1516843
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.1983704 Vali Loss: 0.1079162 Test Loss: 0.1519461
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.1989553 Vali Loss: 0.1078150 Test Loss: 0.1510270
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.1984899 Vali Loss: 0.1085949 Test Loss: 0.1516818
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.1989239 Vali Loss: 0.1063460 Test Loss: 0.1526833
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.1984416 Vali Loss: 0.1062879 Test Loss: 0.1526280
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.1981892 Vali Loss: 0.1058382 Test Loss: 0.1524954
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.1982151 Vali Loss: 0.1049591 Test Loss: 0.1519757
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SASOL_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 210
test shape: (210, 5, 1) (210, 5, 1)
test shape: (210, 5, 1) (210, 5, 1)


	mse:0.0023194162640720606, mae:0.03637721762061119, rmse:0.04816031828522682, r2:-0.045064568519592285, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0364, RMSE: 0.0482, RÂ²: -0.0451, MAPE: 19166042.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.504 MB of 0.504 MB uploadedwandb: \ 0.504 MB of 0.504 MB uploadedwandb: | 0.504 MB of 0.504 MB uploadedwandb: / 0.504 MB of 0.803 MB uploadedwandb: - 0.794 MB of 0.803 MB uploadedwandb: \ 0.803 MB of 0.803 MB uploadedwandb: | 0.803 MB of 0.803 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–‚â–„â–„â–„â–‚â–‚â–â–„â–ƒâ–ƒâ–‚â–ƒâ–„â–„â–„â–ƒ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–‚â–‚â–‚â–â–â–â–â–‚â–â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–‡â–ƒâ–ˆâ–ˆâ–„â–ƒâ–â–‚â–‚â–‡â–…â–…â–†â–ƒâ–ƒâ–‚â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.15198
wandb:                 train/loss 0.19822
wandb:   val/directional_accuracy 50.83333
wandb:                   val/loss 0.10496
wandb:                    val/mae 0.03638
wandb:                   val/mape 1916604200.0
wandb:                    val/mse 0.00232
wandb:                     val/r2 -0.04506
wandb:                   val/rmse 0.04816
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/iv2kc35y
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_155917-iv2kc35y/logs
Exception in thread Exception in thread IntMsgThrChkStopThr:
:
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 300, in check_internal_messages
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 286, in check_stop_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 856, in deliver_internal_messages
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 840, in deliver_stop_status
    return self._deliver_internal_messages(internal_message)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 516, in _deliver_internal_messages
    return self._deliver_stop_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 494, in _deliver_stop_status
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: SASOL H=5

Training: Informer on SASOL for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_160152-7xdj5rmb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/7xdj5rmb
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SASOL_H10  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/7xdj5rmb
>>>>>>>start training : long_term_forecast_Informer_SASOL_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 10
============================================================
train 3765
Overriding target from 'OT' to 'close' for stock data
val 204
Overriding target from 'OT' to 'close' for stock data
test 205
Epoch: 1, Steps: 118 | Train Loss: 0.2794931 Vali Loss: 0.1157040 Test Loss: 0.1551265
Validation loss decreased (inf --> 0.115704).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2233842 Vali Loss: 0.1110838 Test Loss: 0.1553392
Validation loss decreased (0.115704 --> 0.111084).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2794930986681227, 'val/loss': 0.11570402021918978, 'test/loss': 0.1551264716046197, '_timestamp': 1762783325.1650004}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.22338415707572032, 'val/loss': 0.11108382897717613, 'test/loss': 0.15533917929444993, '_timestamp': 1762783331.6536155}).
Epoch: 3, Steps: 118 | Train Loss: 0.2111872 Vali Loss: 0.1142261 Test Loss: 0.1627017
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2067724 Vali Loss: 0.1134772 Test Loss: 0.1536648
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2044928 Vali Loss: 0.1091269 Test Loss: 0.1536609
Validation loss decreased (0.111084 --> 0.109127).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2027834 Vali Loss: 0.1106561 Test Loss: 0.1542728
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2019381 Vali Loss: 0.1098406 Test Loss: 0.1524175
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2024975 Vali Loss: 0.1129815 Test Loss: 0.1529306
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2017042 Vali Loss: 0.1095645 Test Loss: 0.1535336
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2012104 Vali Loss: 0.1122399 Test Loss: 0.1533150
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2013172 Vali Loss: 0.1096469 Test Loss: 0.1539253
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2011341 Vali Loss: 0.1094202 Test Loss: 0.1533936
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2020408 Vali Loss: 0.1120855 Test Loss: 0.1541190
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2019903 Vali Loss: 0.1109045 Test Loss: 0.1533356
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.2020499 Vali Loss: 0.1082347 Test Loss: 0.1540797
Validation loss decreased (0.109127 --> 0.108235).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.2013997 Vali Loss: 0.1110880 Test Loss: 0.1528374
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.2013826 Vali Loss: 0.1122652 Test Loss: 0.1534480
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.2011480 Vali Loss: 0.1111110 Test Loss: 0.1540336
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.2013638 Vali Loss: 0.1097617 Test Loss: 0.1533469
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.2018391 Vali Loss: 0.1099784 Test Loss: 0.1536892
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 118 | Train Loss: 0.2015272 Vali Loss: 0.1103698 Test Loss: 0.1531121
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 118 | Train Loss: 0.2014887 Vali Loss: 0.1124632 Test Loss: 0.1521743
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 118 | Train Loss: 0.2010936 Vali Loss: 0.1113229 Test Loss: 0.1540527
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 118 | Train Loss: 0.2014635 Vali Loss: 0.1111939 Test Loss: 0.1534938
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 118 | Train Loss: 0.2012967 Vali Loss: 0.1091497 Test Loss: 0.1545352
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SASOL_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 205
test shape: (205, 10, 1) (205, 10, 1)
test shape: (205, 10, 1) (205, 10, 1)


	mse:0.0023188344202935696, mae:0.036319833248853683, rmse:0.04815427586436272, r2:-0.04466080665588379, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0363, RMSE: 0.0482, RÂ²: -0.0447, MAPE: 17708220.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.544 MB of 0.545 MB uploadedwandb: \ 0.544 MB of 0.545 MB uploadedwandb: | 0.545 MB of 0.545 MB uploadedwandb: / 0.545 MB of 0.545 MB uploadedwandb: - 0.545 MB of 0.846 MB uploadedwandb: \ 0.846 MB of 0.846 MB uploadedwandb: | 0.846 MB of 0.846 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–ƒ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–‚â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‡â–‚â–„â–ƒâ–‡â–ƒâ–†â–ƒâ–‚â–…â–„â–â–„â–†â–„â–ƒâ–ƒâ–ƒâ–†â–…â–„â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 24
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.15454
wandb:                 train/loss 0.2013
wandb:   val/directional_accuracy 48.78049
wandb:                   val/loss 0.10915
wandb:                    val/mae 0.03632
wandb:                   val/mape 1770822000.0
wandb:                    val/mse 0.00232
wandb:                     val/r2 -0.04466
wandb:                   val/rmse 0.04815
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/7xdj5rmb
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_160152-7xdj5rmb/logs
Completed: SASOL H=10

Training: Informer on SASOL for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_160452-l25lya23
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/l25lya23
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SASOL_H22  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/l25lya23
>>>>>>>start training : long_term_forecast_Informer_SASOL_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 22
============================================================
train 3753
Overriding target from 'OT' to 'close' for stock data
val 192
Overriding target from 'OT' to 'close' for stock data
test 193
Epoch: 1, Steps: 118 | Train Loss: 0.2894649 Vali Loss: 0.1351358 Test Loss: 0.1775702
Validation loss decreased (inf --> 0.135136).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2327223 Vali Loss: 0.1220623 Test Loss: 0.1699597
Validation loss decreased (0.135136 --> 0.122062).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2894649293463109, 'val/loss': 0.13513583689928055, 'test/loss': 0.17757018761975424, '_timestamp': 1762783505.2876856}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23272233923613014, 'val/loss': 0.12206227208177249, 'test/loss': 0.169959690954004, '_timestamp': 1762783511.754134}).
Epoch: 3, Steps: 118 | Train Loss: 0.2192317 Vali Loss: 0.1264952 Test Loss: 0.1659706
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2148121 Vali Loss: 0.1198959 Test Loss: 0.1734116
Validation loss decreased (0.122062 --> 0.119896).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2121022 Vali Loss: 0.1189552 Test Loss: 0.1659726
Validation loss decreased (0.119896 --> 0.118955).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2095929 Vali Loss: 0.1224589 Test Loss: 0.1651822
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2095259 Vali Loss: 0.1181323 Test Loss: 0.1655672
Validation loss decreased (0.118955 --> 0.118132).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2085873 Vali Loss: 0.1194800 Test Loss: 0.1668458
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2087790 Vali Loss: 0.1194572 Test Loss: 0.1665995
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2077161 Vali Loss: 0.1182041 Test Loss: 0.1646332
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2086417 Vali Loss: 0.1185552 Test Loss: 0.1662053
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2101680 Vali Loss: 0.1180493 Test Loss: 0.1631974
Validation loss decreased (0.118132 --> 0.118049).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2088695 Vali Loss: 0.1176596 Test Loss: 0.1653805
Validation loss decreased (0.118049 --> 0.117660).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2086571 Vali Loss: 0.1198932 Test Loss: 0.1679434
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.2101467 Vali Loss: 0.1193277 Test Loss: 0.1657879
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.2080676 Vali Loss: 0.1182695 Test Loss: 0.1653716
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.2082842 Vali Loss: 0.1191205 Test Loss: 0.1651889
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.2079744 Vali Loss: 0.1203671 Test Loss: 0.1667329
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.2083161 Vali Loss: 0.1190478 Test Loss: 0.1673635
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.2091505 Vali Loss: 0.1184838 Test Loss: 0.1661919
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 118 | Train Loss: 0.2084892 Vali Loss: 0.1191058 Test Loss: 0.1643779
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 118 | Train Loss: 0.2084710 Vali Loss: 0.1207338 Test Loss: 0.1676211
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 118 | Train Loss: 0.2083017 Vali Loss: 0.1189044 Test Loss: 0.1676997
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SASOL_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 193
test shape: (193, 22, 1) (193, 22, 1)
test shape: (193, 22, 1) (193, 22, 1)


	mse:0.002349786926060915, mae:0.03646017238497734, rmse:0.04847460240125656, r2:-0.04708242416381836, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0365, RMSE: 0.0485, RÂ²: -0.0471, MAPE: 18624378.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.596 MB of 0.597 MB uploadedwandb: \ 0.596 MB of 0.597 MB uploadedwandb: | 0.596 MB of 0.597 MB uploadedwandb: / 0.597 MB of 0.597 MB uploadedwandb: - 0.597 MB of 0.597 MB uploadedwandb: \ 0.597 MB of 0.897 MB uploadedwandb: | 0.897 MB of 0.897 MB uploadedwandb: / 0.897 MB of 0.897 MB uploadedwandb: - 0.897 MB of 0.897 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ƒâ–ˆâ–ƒâ–‚â–ƒâ–„â–ƒâ–‚â–ƒâ–â–‚â–„â–ƒâ–‚â–‚â–ƒâ–„â–ƒâ–‚â–„â–„
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–‚â–…â–â–‚â–‚â–â–‚â–â–â–ƒâ–‚â–â–‚â–ƒâ–‚â–‚â–‚â–ƒâ–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.1677
wandb:                 train/loss 0.2083
wandb:   val/directional_accuracy 49.83962
wandb:                   val/loss 0.1189
wandb:                    val/mae 0.03646
wandb:                   val/mape 1862437800.0
wandb:                    val/mse 0.00235
wandb:                     val/r2 -0.04708
wandb:                   val/rmse 0.04847
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/l25lya23
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_160452-l25lya23/logs
Completed: SASOL H=22

Training: Informer on SASOL for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_160744-r765tgj5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/r765tgj5
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SASOL_H50  Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/r765tgj5
>>>>>>>start training : long_term_forecast_Informer_SASOL_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 50
============================================================
train 3725
Overriding target from 'OT' to 'close' for stock data
val 164
Overriding target from 'OT' to 'close' for stock data
test 165
Epoch: 1, Steps: 117 | Train Loss: 0.3136176 Vali Loss: 0.1485193 Test Loss: 0.1890390
Validation loss decreased (inf --> 0.148519).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 117 | Train Loss: 0.2449117 Vali Loss: 0.1503314 Test Loss: 0.3447332
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.31361756645716155, 'val/loss': 0.14851932724316916, 'test/loss': 0.18903902173042297, '_timestamp': 1762783677.326604}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2449116663545625, 'val/loss': 0.15033142020305, 'test/loss': 0.34473321338494617, '_timestamp': 1762783683.5892744}).
Epoch: 3, Steps: 117 | Train Loss: 0.2300865 Vali Loss: 0.1208799 Test Loss: 0.4310565
Validation loss decreased (0.148519 --> 0.120880).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 117 | Train Loss: 0.2219563 Vali Loss: 0.1457248 Test Loss: 0.3729957
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 117 | Train Loss: 0.2179748 Vali Loss: 0.1307503 Test Loss: 0.5032848
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 117 | Train Loss: 0.2161029 Vali Loss: 0.1298922 Test Loss: 0.4461260
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 117 | Train Loss: 0.2152006 Vali Loss: 0.1340847 Test Loss: 0.4674321
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 117 | Train Loss: 0.2151993 Vali Loss: 0.1323535 Test Loss: 0.5111486
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 117 | Train Loss: 0.2143589 Vali Loss: 0.1378263 Test Loss: 0.5074766
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 117 | Train Loss: 0.2139002 Vali Loss: 0.1286126 Test Loss: 0.4939386
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 117 | Train Loss: 0.2137212 Vali Loss: 0.1345146 Test Loss: 0.4944213
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 117 | Train Loss: 0.2148656 Vali Loss: 0.1354290 Test Loss: 0.4834265
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 117 | Train Loss: 0.2140699 Vali Loss: 0.1328444 Test Loss: 0.4918878
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SASOL_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 165
test shape: (165, 50, 1) (165, 50, 1)
test shape: (165, 50, 1) (165, 50, 1)


	mse:0.0024261882063001394, mae:0.03831610456109047, rmse:0.049256350845098495, r2:-0.1842212677001953, dtw:Not calculated


VAL - MSE: 0.0024, MAE: 0.0383, RMSE: 0.0493, RÂ²: -0.1842, MAPE: 35047256.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.653 MB of 0.655 MB uploadedwandb: \ 0.653 MB of 0.655 MB uploadedwandb: | 0.653 MB of 0.655 MB uploadedwandb: / 0.655 MB of 0.655 MB uploadedwandb: - 0.655 MB of 0.655 MB uploadedwandb: \ 0.655 MB of 0.954 MB uploadedwandb: | 0.879 MB of 0.954 MB uploadedwandb: / 0.954 MB of 0.954 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–„â–â–ˆâ–…â–†â–ˆâ–ˆâ–‡â–‡â–‡â–‡
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ˆâ–„â–„â–…â–„â–†â–ƒâ–…â–…â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.49189
wandb:                 train/loss 0.21407
wandb:   val/directional_accuracy 49.0538
wandb:                   val/loss 0.13284
wandb:                    val/mae 0.03832
wandb:                   val/mape 3504725600.0
wandb:                    val/mse 0.00243
wandb:                     val/r2 -0.18422
wandb:                   val/rmse 0.04926
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/r765tgj5
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_160744-r765tgj5/logs
Completed: SASOL H=50

Training: Informer on SASOL for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_160935-9vx3k1h3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/9vx3k1h3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_SASOL_H100 Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/9vx3k1h3
>>>>>>>start training : long_term_forecast_Informer_SASOL_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 100
============================================================
train 3675
Overriding target from 'OT' to 'close' for stock data
val 114
Overriding target from 'OT' to 'close' for stock data
test 115
Epoch: 1, Steps: 115 | Train Loss: 0.3474862 Vali Loss: 0.1605430 Test Loss: 0.4699519
Validation loss decreased (inf --> 0.160543).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 115 | Train Loss: 0.2769001 Vali Loss: 0.1776260 Test Loss: 0.4719694
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.34748615700265634, 'val/loss': 0.1605430394411087, 'test/loss': 0.46995190531015396, '_timestamp': 1762783788.0010452}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2769001038178154, 'val/loss': 0.17762602120637894, 'test/loss': 0.4719693697988987, '_timestamp': 1762783794.2504332}).
Epoch: 3, Steps: 115 | Train Loss: 0.2492125 Vali Loss: 0.1422329 Test Loss: 1.0029342
Validation loss decreased (0.160543 --> 0.142233).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 115 | Train Loss: 0.2344687 Vali Loss: 0.1470363 Test Loss: 1.2228365
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 115 | Train Loss: 0.2280984 Vali Loss: 0.1538719 Test Loss: 1.2515031
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 115 | Train Loss: 0.2265106 Vali Loss: 0.1540015 Test Loss: 1.2421963
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 115 | Train Loss: 0.2255149 Vali Loss: 0.1535423 Test Loss: 1.2915704
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 115 | Train Loss: 0.2251534 Vali Loss: 0.1543569 Test Loss: 1.3556556
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 115 | Train Loss: 0.2248513 Vali Loss: 0.1549467 Test Loss: 1.2382882
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 115 | Train Loss: 0.2238183 Vali Loss: 0.1544125 Test Loss: 1.2650474
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 115 | Train Loss: 0.2246592 Vali Loss: 0.1577516 Test Loss: 1.2447562
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 115 | Train Loss: 0.2235933 Vali Loss: 0.1561842 Test Loss: 1.2835989
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 115 | Train Loss: 0.2238960 Vali Loss: 0.1587827 Test Loss: 1.2788825
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_SASOL_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 115
test shape: (115, 100, 1) (115, 100, 1)
test shape: (115, 100, 1) (115, 100, 1)


	mse:0.0023024503607302904, mae:0.03762837499380112, rmse:0.04798385500907898, r2:-0.1602640151977539, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0376, RMSE: 0.0480, RÂ²: -0.1603, MAPE: 34668544.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.689 MB of 0.694 MB uploadedwandb: \ 0.689 MB of 0.694 MB uploadedwandb: | 0.689 MB of 0.694 MB uploadedwandb: / 0.694 MB of 0.694 MB uploadedwandb: - 0.694 MB of 0.694 MB uploadedwandb: \ 0.694 MB of 0.992 MB uploadedwandb: | 0.992 MB of 0.992 MB uploadedwandb: / 0.992 MB of 0.992 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–…â–†â–†â–‡â–ˆâ–†â–†â–†â–‡â–†
wandb:                 train/loss â–ˆâ–„â–‚â–‚â–‚â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ƒâ–†â–†â–†â–†â–†â–†â–ˆâ–‡â–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 1.27888
wandb:                 train/loss 0.2239
wandb:   val/directional_accuracy 48.57268
wandb:                   val/loss 0.15878
wandb:                    val/mae 0.03763
wandb:                   val/mape 3466854400.0
wandb:                    val/mse 0.0023
wandb:                     val/r2 -0.16026
wandb:                   val/rmse 0.04798
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/9vx3k1h3
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_160935-9vx3k1h3/logs
Completed: SASOL H=100

Training: Informer on DRD_GOLD for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_161124-0vuyw8vg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/0vuyw8vg
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_DRD_GOLD_H3Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/0vuyw8vg
>>>>>>>start training : long_term_forecast_Informer_DRD_GOLD_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
Overriding target from 'OT' to 'close' for stock data
val 237
Overriding target from 'OT' to 'close' for stock data
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.2998856 Vali Loss: 0.1546057 Test Loss: 0.1541489
Validation loss decreased (inf --> 0.154606).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2421106 Vali Loss: 0.1750490 Test Loss: 0.1621531
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29988562959925574, 'val/loss': 0.1546056941151619, 'test/loss': 0.1541488952934742, '_timestamp': 1762783898.4291878}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24211060048494124, 'val/loss': 0.17504902929067612, 'test/loss': 0.16215310618281364, '_timestamp': 1762783905.8313243}).
Epoch: 3, Steps: 133 | Train Loss: 0.2271032 Vali Loss: 0.1334777 Test Loss: 0.1299040
Validation loss decreased (0.154606 --> 0.133478).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2195832 Vali Loss: 0.1268301 Test Loss: 0.1263169
Validation loss decreased (0.133478 --> 0.126830).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2170246 Vali Loss: 0.1192484 Test Loss: 0.1249352
Validation loss decreased (0.126830 --> 0.119248).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2153026 Vali Loss: 0.1238865 Test Loss: 0.1240641
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2145366 Vali Loss: 0.1272617 Test Loss: 0.1245663
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2145594 Vali Loss: 0.1215386 Test Loss: 0.1234763
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2128902 Vali Loss: 0.1290355 Test Loss: 0.1241911
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2132523 Vali Loss: 0.1224103 Test Loss: 0.1244522
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2140173 Vali Loss: 0.1251059 Test Loss: 0.1240521
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2137082 Vali Loss: 0.1212773 Test Loss: 0.1245022
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2136691 Vali Loss: 0.1221583 Test Loss: 0.1243869
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2136622 Vali Loss: 0.1205229 Test Loss: 0.1235011
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2134966 Vali Loss: 0.1226810 Test Loss: 0.1241895
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_DRD_GOLD_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.0009369231411255896, mae:0.02168659120798111, rmse:0.03060919977724552, r2:-0.010297298431396484, dtw:Not calculated


VAL - MSE: 0.0009, MAE: 0.0217, RMSE: 0.0306, RÂ²: -0.0103, MAPE: 1158356.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.478 MB of 0.478 MB uploadedwandb: \ 0.478 MB of 0.478 MB uploadedwandb: | 0.478 MB of 0.478 MB uploadedwandb: / 0.478 MB of 0.478 MB uploadedwandb: - 0.478 MB of 0.478 MB uploadedwandb: \ 0.478 MB of 0.478 MB uploadedwandb: | 0.660 MB of 0.958 MB uploaded (0.002 MB deduped)wandb: / 0.958 MB of 0.958 MB uploaded (0.002 MB deduped)wandb: - 0.958 MB of 0.958 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–‚
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–‚â–â–â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–â–ƒâ–…â–‚â–†â–ƒâ–„â–‚â–‚â–‚â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.12419
wandb:                 train/loss 0.2135
wandb:   val/directional_accuracy 49.57806
wandb:                   val/loss 0.12268
wandb:                    val/mae 0.02169
wandb:                   val/mape 115835675.0
wandb:                    val/mse 0.00094
wandb:                     val/r2 -0.0103
wandb:                   val/rmse 0.03061
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/0vuyw8vg
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_161124-0vuyw8vg/logs
Completed: DRD_GOLD H=3

Training: Informer on DRD_GOLD for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_161340-4aubw52b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/4aubw52b
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_DRD_GOLD_H5Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/4aubw52b
>>>>>>>start training : long_term_forecast_Informer_DRD_GOLD_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
Overriding target from 'OT' to 'close' for stock data
val 235
Overriding target from 'OT' to 'close' for stock data
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.3042122 Vali Loss: 0.1341052 Test Loss: 0.1359774
Validation loss decreased (inf --> 0.134105).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2470905 Vali Loss: 0.1556690 Test Loss: 0.1444340
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3042121954206237, 'val/loss': 0.1341052269563079, 'test/loss': 0.1359774125739932, '_timestamp': 1762784034.2911284}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2470904892324505, 'val/loss': 0.15566902328282595, 'test/loss': 0.1444340329617262, '_timestamp': 1762784041.2146308}).
Epoch: 3, Steps: 133 | Train Loss: 0.2311652 Vali Loss: 0.1357427 Test Loss: 0.1312869
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2245336 Vali Loss: 0.1344709 Test Loss: 0.1294732
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2218284 Vali Loss: 0.1275266 Test Loss: 0.1286618
Validation loss decreased (0.134105 --> 0.127527).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2212271 Vali Loss: 0.1281260 Test Loss: 0.1297448
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2191628 Vali Loss: 0.1355399 Test Loss: 0.1287502
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2187313 Vali Loss: 0.1312646 Test Loss: 0.1296046
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2183966 Vali Loss: 0.1360982 Test Loss: 0.1293190
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2189002 Vali Loss: 0.1354277 Test Loss: 0.1287631
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2182611 Vali Loss: 0.1306353 Test Loss: 0.1294941
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2183537 Vali Loss: 0.1374622 Test Loss: 0.1297048
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2192194 Vali Loss: 0.1304797 Test Loss: 0.1297374
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2182672 Vali Loss: 0.1300028 Test Loss: 0.1292694
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2180054 Vali Loss: 0.1334631 Test Loss: 0.1296504
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_DRD_GOLD_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.0009413021034561098, mae:0.02177039533853531, rmse:0.030680647119879723, r2:-0.008040428161621094, dtw:Not calculated


VAL - MSE: 0.0009, MAE: 0.0218, RMSE: 0.0307, RÂ²: -0.0080, MAPE: 530981.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.501 MB of 0.502 MB uploadedwandb: \ 0.501 MB of 0.502 MB uploadedwandb: | 0.501 MB of 0.502 MB uploadedwandb: / 0.502 MB of 0.502 MB uploadedwandb: - 0.502 MB of 0.800 MB uploadedwandb: \ 0.502 MB of 0.800 MB uploadedwandb: | 0.800 MB of 0.800 MB uploadedwandb: / 0.800 MB of 0.800 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–„â–â–„â–ƒâ–â–ƒâ–„â–„â–ƒâ–„
wandb:                 train/loss â–ˆâ–„â–ƒâ–ƒâ–‚â–â–â–â–â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–†â–â–â–‡â–„â–‡â–‡â–ƒâ–ˆâ–ƒâ–ƒâ–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.12965
wandb:                 train/loss 0.21801
wandb:   val/directional_accuracy 50.31915
wandb:                   val/loss 0.13346
wandb:                    val/mae 0.02177
wandb:                   val/mape 53098100.0
wandb:                    val/mse 0.00094
wandb:                     val/r2 -0.00804
wandb:                   val/rmse 0.03068
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/4aubw52b
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_161340-4aubw52b/logs
Completed: DRD_GOLD H=5

Training: Informer on DRD_GOLD for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_161553-5ca5iew7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/5ca5iew7
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_DRD_GOLD_H10Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/5ca5iew7
>>>>>>>start training : long_term_forecast_Informer_DRD_GOLD_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
Overriding target from 'OT' to 'close' for stock data
val 230
Overriding target from 'OT' to 'close' for stock data
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.3156064 Vali Loss: 0.1657152 Test Loss: 0.1586165
Validation loss decreased (inf --> 0.165715).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2560507 Vali Loss: 0.1462645 Test Loss: 0.1417549
Validation loss decreased (0.165715 --> 0.146264).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3156063561152695, 'val/loss': 0.16571520268917084, 'test/loss': 0.15861649811267853, '_timestamp': 1762784166.6546552}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2560506647914872, 'val/loss': 0.14626446273177862, 'test/loss': 0.14175488986074924, '_timestamp': 1762784173.5757518}).
Epoch: 3, Steps: 133 | Train Loss: 0.2427652 Vali Loss: 0.1572892 Test Loss: 0.1522490
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2346256 Vali Loss: 0.1566609 Test Loss: 0.1407608
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2293614 Vali Loss: 0.1406926 Test Loss: 0.1372004
Validation loss decreased (0.146264 --> 0.140693).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2300494 Vali Loss: 0.1383452 Test Loss: 0.1412239
Validation loss decreased (0.140693 --> 0.138345).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2274691 Vali Loss: 0.1405360 Test Loss: 0.1393072
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2266840 Vali Loss: 0.1444909 Test Loss: 0.1374773
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2270110 Vali Loss: 0.1398904 Test Loss: 0.1389842
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2268917 Vali Loss: 0.1373586 Test Loss: 0.1379957
Validation loss decreased (0.138345 --> 0.137359).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2269242 Vali Loss: 0.1373984 Test Loss: 0.1382563
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2262677 Vali Loss: 0.1398921 Test Loss: 0.1370101
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2267951 Vali Loss: 0.1411923 Test Loss: 0.1377866
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2269633 Vali Loss: 0.1435634 Test Loss: 0.1381564
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2263911 Vali Loss: 0.1387903 Test Loss: 0.1380742
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2259725 Vali Loss: 0.1404445 Test Loss: 0.1375725
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2272953 Vali Loss: 0.1481735 Test Loss: 0.1374766
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2271147 Vali Loss: 0.1407469 Test Loss: 0.1374294
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2263872 Vali Loss: 0.1412138 Test Loss: 0.1371087
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2263723 Vali Loss: 0.1418827 Test Loss: 0.1378767
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_DRD_GOLD_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.0009545984212309122, mae:0.021958841010928154, rmse:0.03089657612144947, r2:-0.0066814422607421875, dtw:Not calculated


VAL - MSE: 0.0010, MAE: 0.0220, RMSE: 0.0309, RÂ²: -0.0067, MAPE: 1204837.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.528 MB of 0.528 MB uploadedwandb: \ 0.528 MB of 0.528 MB uploadedwandb: | 0.528 MB of 0.528 MB uploadedwandb: / 0.528 MB of 0.528 MB uploadedwandb: - 0.528 MB of 0.828 MB uploadedwandb: \ 0.528 MB of 0.828 MB uploadedwandb: | 0.828 MB of 0.828 MB uploadedwandb: / 0.828 MB of 0.828 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–â–ƒâ–‚â–â–‚â–â–‚â–â–â–‚â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–‚â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ˆâ–‚â–â–‚â–„â–‚â–â–â–‚â–‚â–ƒâ–‚â–‚â–…â–‚â–‚â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.13788
wandb:                 train/loss 0.22637
wandb:   val/directional_accuracy 51.98068
wandb:                   val/loss 0.14188
wandb:                    val/mae 0.02196
wandb:                   val/mape 120483725.0
wandb:                    val/mse 0.00095
wandb:                     val/r2 -0.00668
wandb:                   val/rmse 0.0309
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/5ca5iew7
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_161553-5ca5iew7/logs
Completed: DRD_GOLD H=10

Training: Informer on DRD_GOLD for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_161839-t4q0qgjt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/t4q0qgjt
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_DRD_GOLD_H22Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/t4q0qgjt
>>>>>>>start training : long_term_forecast_Informer_DRD_GOLD_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
Overriding target from 'OT' to 'close' for stock data
val 218
Overriding target from 'OT' to 'close' for stock data
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.3438484 Vali Loss: 0.1762202 Test Loss: 0.1656623
Validation loss decreased (inf --> 0.176220).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2724210 Vali Loss: 0.1820522 Test Loss: 0.1681773
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3438483780306397, 'val/loss': 0.17622024033750808, 'test/loss': 0.16566233124051774, '_timestamp': 1762784332.0189083}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.27242096581242303, 'val/loss': 0.18205223977565765, 'test/loss': 0.16817731303828104, '_timestamp': 1762784338.964476}).
Epoch: 3, Steps: 132 | Train Loss: 0.2566970 Vali Loss: 0.1753178 Test Loss: 0.1736610
Validation loss decreased (0.176220 --> 0.175318).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2454770 Vali Loss: 0.1766812 Test Loss: 0.1840725
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2411478 Vali Loss: 0.1674255 Test Loss: 0.1726320
Validation loss decreased (0.175318 --> 0.167426).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2384904 Vali Loss: 0.1776061 Test Loss: 0.1901337
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2374405 Vali Loss: 0.1738611 Test Loss: 0.1867304
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2375875 Vali Loss: 0.1749939 Test Loss: 0.1886422
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2366835 Vali Loss: 0.1722209 Test Loss: 0.1823398
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2363017 Vali Loss: 0.1709445 Test Loss: 0.1808875
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2359599 Vali Loss: 0.1731771 Test Loss: 0.1858741
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2364877 Vali Loss: 0.1694184 Test Loss: 0.1787143
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2366362 Vali Loss: 0.1706379 Test Loss: 0.1831086
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2361287 Vali Loss: 0.1699600 Test Loss: 0.1797609
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2368132 Vali Loss: 0.1705979 Test Loss: 0.1824082
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_DRD_GOLD_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.0009881820296868682, mae:0.022448882460594177, rmse:0.03143536299467087, r2:-0.03770112991333008, dtw:Not calculated


VAL - MSE: 0.0010, MAE: 0.0224, RMSE: 0.0314, RÂ²: -0.0377, MAPE: 2245544.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.594 MB of 0.595 MB uploadedwandb: \ 0.594 MB of 0.595 MB uploadedwandb: | 0.595 MB of 0.595 MB uploadedwandb: / 0.595 MB of 0.595 MB uploadedwandb: - 0.595 MB of 0.894 MB uploadedwandb: \ 0.894 MB of 0.894 MB uploadedwandb: | 0.894 MB of 0.894 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–†â–â–ˆâ–‡â–‡â–…â–„â–†â–ƒâ–…â–„â–…
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–â–‚â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–‡â–â–ˆâ–…â–†â–„â–ƒâ–…â–‚â–ƒâ–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.18241
wandb:                 train/loss 0.23681
wandb:   val/directional_accuracy 51.2014
wandb:                   val/loss 0.1706
wandb:                    val/mae 0.02245
wandb:                   val/mape 224554450.0
wandb:                    val/mse 0.00099
wandb:                     val/r2 -0.0377
wandb:                   val/rmse 0.03144
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/t4q0qgjt
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_161839-t4q0qgjt/logs
Completed: DRD_GOLD H=22

Training: Informer on DRD_GOLD for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_162052-qw8p3yc3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/qw8p3yc3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_DRD_GOLD_H50Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/qw8p3yc3
>>>>>>>start training : long_term_forecast_Informer_DRD_GOLD_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
Overriding target from 'OT' to 'close' for stock data
val 190
Overriding target from 'OT' to 'close' for stock data
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.3786274 Vali Loss: 0.2173154 Test Loss: 0.1652344
Validation loss decreased (inf --> 0.217315).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.3018540 Vali Loss: 0.2060905 Test Loss: 0.2186916
Validation loss decreased (0.217315 --> 0.206090).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.37862739192717004, 'val/loss': 0.21731542547543845, 'test/loss': 0.1652344142397245, '_timestamp': 1762784464.9787798}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.30185395072806964, 'val/loss': 0.20609047512213388, 'test/loss': 0.21869156757990518, '_timestamp': 1762784471.8192239}).
Epoch: 3, Steps: 132 | Train Loss: 0.2715439 Vali Loss: 0.2325109 Test Loss: 0.1781265
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2628058 Vali Loss: 0.2255414 Test Loss: 0.2788883
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2573038 Vali Loss: 0.2072445 Test Loss: 0.2073291
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2529469 Vali Loss: 0.2143914 Test Loss: 0.2378980
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2509608 Vali Loss: 0.2132940 Test Loss: 0.2326865
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2530084 Vali Loss: 0.2160246 Test Loss: 0.2490145
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2502198 Vali Loss: 0.2147394 Test Loss: 0.2489226
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2501819 Vali Loss: 0.2139331 Test Loss: 0.2497739
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2522115 Vali Loss: 0.2148675 Test Loss: 0.2401664
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2538746 Vali Loss: 0.2248909 Test Loss: 0.2594443
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_DRD_GOLD_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.0009464642498642206, mae:0.021819746121764183, rmse:0.030764659866690636, r2:-0.0164107084274292, dtw:Not calculated


VAL - MSE: 0.0009, MAE: 0.0218, RMSE: 0.0308, RÂ²: -0.0164, MAPE: 1562363.88%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.704 MB of 0.706 MB uploadedwandb: \ 0.704 MB of 0.706 MB uploadedwandb: | 0.704 MB of 0.706 MB uploadedwandb: / 0.706 MB of 0.706 MB uploadedwandb: - 0.706 MB of 0.706 MB uploadedwandb: \ 0.706 MB of 1.004 MB uploadedwandb: | 1.004 MB of 1.004 MB uploadedwandb: / 1.004 MB of 1.004 MB uploadedwandb: - 1.004 MB of 1.004 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–ƒâ–…â–…â–†â–†â–†â–…â–‡
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–‚â–â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.25944
wandb:                 train/loss 0.25387
wandb:   val/directional_accuracy 50.71966
wandb:                   val/loss 0.22489
wandb:                    val/mae 0.02182
wandb:                   val/mape 156236387.5
wandb:                    val/mse 0.00095
wandb:                     val/r2 -0.01641
wandb:                   val/rmse 0.03076
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/qw8p3yc3
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_162052-qw8p3yc3/logs
Completed: DRD_GOLD H=50

Training: Informer on DRD_GOLD for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_162247-l5sydojp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/l5sydojp
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_DRD_GOLD_H100Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          DRD_GOLD_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/l5sydojp
>>>>>>>start training : long_term_forecast_Informer_DRD_GOLD_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
Overriding target from 'OT' to 'close' for stock data
val 140
Overriding target from 'OT' to 'close' for stock data
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.4157275 Vali Loss: 0.3502413 Test Loss: 0.3028428
Validation loss decreased (inf --> 0.350241).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.3251570 Vali Loss: 0.2605278 Test Loss: 0.2770123
Validation loss decreased (0.350241 --> 0.260528).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.4157275275542186, 'val/loss': 0.3502412915229797, 'test/loss': 0.30284284353256224, '_timestamp': 1762784582.2937608}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.32515695003362804, 'val/loss': 0.26052782833576205, 'test/loss': 0.27701234817504883, '_timestamp': 1762784589.12639}).
Epoch: 3, Steps: 130 | Train Loss: 0.2924270 Vali Loss: 0.3278040 Test Loss: 0.4335250
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2781676 Vali Loss: 0.3228691 Test Loss: 0.4826454
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2703498 Vali Loss: 0.3058674 Test Loss: 0.4723053
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2679835 Vali Loss: 0.3394434 Test Loss: 0.4993810
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2667620 Vali Loss: 0.3193594 Test Loss: 0.4610517
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2665043 Vali Loss: 0.3276542 Test Loss: 0.5139998
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2646013 Vali Loss: 0.3240459 Test Loss: 0.4594281
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2660954 Vali Loss: 0.3112293 Test Loss: 0.4718580
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2653349 Vali Loss: 0.3441906 Test Loss: 0.5022483
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2652944 Vali Loss: 0.3092645 Test Loss: 0.4731467
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_DRD_GOLD_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0009249575668945909, mae:0.0214013010263443, rmse:0.03041311539709568, r2:-0.023697257041931152, dtw:Not calculated


VAL - MSE: 0.0009, MAE: 0.0214, RMSE: 0.0304, RÂ²: -0.0237, MAPE: 1053342.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.799 MB of 0.804 MB uploadedwandb: \ 0.799 MB of 0.804 MB uploadedwandb: | 0.799 MB of 0.804 MB uploadedwandb: / 0.804 MB of 0.804 MB uploadedwandb: - 0.804 MB of 0.804 MB uploadedwandb: \ 0.804 MB of 1.102 MB uploadedwandb: | 1.102 MB of 1.102 MB uploadedwandb: / 1.102 MB of 1.102 MB uploadedwandb: - 1.102 MB of 1.102 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–…â–„â–‡â–ƒâ–ˆâ–ƒâ–„â–‡â–„
wandb:                 train/loss â–ˆâ–„â–‚â–‚â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–„â–â–‡â–ƒâ–…â–„â–‚â–ˆâ–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 0.47315
wandb:                 train/loss 0.26529
wandb:   val/directional_accuracy 50.93074
wandb:                   val/loss 0.30926
wandb:                    val/mae 0.0214
wandb:                   val/mape 105334275.0
wandb:                    val/mse 0.00092
wandb:                     val/r2 -0.0237
wandb:                   val/rmse 0.03041
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD/runs/l5sydojp
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-DRD_GOLD
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_162247-l5sydojp/logs
Completed: DRD_GOLD H=100

Training: Informer on ANGLO_AMERICAN for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_162441-cqkl07if
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/cqkl07if
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ANGLO_AMERICAN_H3Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/cqkl07if
>>>>>>>start training : long_term_forecast_Informer_ANGLO_AMERICAN_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 3
============================================================
train 778
Overriding target from 'OT' to 'close' for stock data
val 44
Overriding target from 'OT' to 'close' for stock data
test 46
Epoch: 1, Steps: 25 | Train Loss: 0.4372993 Vali Loss: 0.6703516 Test Loss: 2.3736644
Validation loss decreased (inf --> 0.670352).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 25 | Train Loss: 0.3041068 Vali Loss: 0.6401435 Test Loss: 2.3182750
Validation loss decreased (0.670352 --> 0.640143).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 25 | Train Loss: 0.2843617 Vali Loss: 0.7076066 Test Loss: 2.5964931
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 25 | Train Loss: 0.2736692 Vali Loss: 0.6671996 Test Loss: 2.4811940
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 25 | Train Loss: 0.2667054 Vali Loss: 0.6741538 Test Loss: 2.4705954
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 25 | Train Loss: 0.2653437 Vali Loss: 0.6401517 Test Loss: 2.4242631
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 25 | Train Loss: 0.2666575 Vali Loss: 0.6929064 Test Loss: 2.4723685
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.4372992873191833, 'val/loss': 0.6703515648841858, 'test/loss': 2.373664379119873, '_timestamp': 1762784689.9465175}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3041068255901337, 'val/loss': 0.6401434540748596, 'test/loss': 2.318274974822998, '_timestamp': 1762784692.3982189}).
Epoch: 8, Steps: 25 | Train Loss: 0.2570243 Vali Loss: 0.7207616 Test Loss: 2.4791602
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 25 | Train Loss: 0.2700669 Vali Loss: 0.6811575 Test Loss: 2.4676609
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 25 | Train Loss: 0.2568083 Vali Loss: 0.6757492 Test Loss: 2.4717063
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 25 | Train Loss: 0.2579008 Vali Loss: 0.7081942 Test Loss: 2.4674805
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 25 | Train Loss: 0.2645786 Vali Loss: 0.6908103 Test Loss: 2.4986172
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ANGLO_AMERICAN_H3_Informer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 46
test shape: (46, 3, 1) (46, 3, 1)
test shape: (46, 3, 1) (46, 3, 1)


	mse:0.0068316408433020115, mae:0.056615062057971954, rmse:0.08265373855829239, r2:-0.008376836776733398, dtw:Not calculated


VAL - MSE: 0.0068, MAE: 0.0566, RMSE: 0.0827, RÂ²: -0.0084, MAPE: 31783620.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.439 MB of 0.439 MB uploadedwandb: \ 0.439 MB of 0.439 MB uploadedwandb: | 0.439 MB of 0.439 MB uploadedwandb: / 0.439 MB of 0.439 MB uploadedwandb: - 0.439 MB of 0.439 MB uploadedwandb: \ 0.439 MB of 0.439 MB uploadedwandb: | 0.439 MB of 0.439 MB uploadedwandb: / 0.439 MB of 0.439 MB uploadedwandb: - 0.621 MB of 0.918 MB uploaded (0.002 MB deduped)wandb: \ 0.918 MB of 0.918 MB uploaded (0.002 MB deduped)wandb: | 0.918 MB of 0.918 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–ƒâ–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„
wandb:                 train/loss â–ˆâ–…â–„â–ƒâ–„â–â–„â–â–â–ƒ
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–ƒâ–„â–â–†â–ˆâ–…â–„â–‡â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 2.49862
wandb:                 train/loss 0.26458
wandb:   val/directional_accuracy 52.17391
wandb:                   val/loss 0.69081
wandb:                    val/mae 0.05662
wandb:                   val/mape 3178362000.0
wandb:                    val/mse 0.00683
wandb:                     val/r2 -0.00838
wandb:                   val/rmse 0.08265
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/cqkl07if
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_162441-cqkl07if/logs
Completed: ANGLO_AMERICAN H=3

Training: Informer on ANGLO_AMERICAN for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_162538-3x5iwjv2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/3x5iwjv2
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ANGLO_AMERICAN_H5Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/3x5iwjv2
>>>>>>>start training : long_term_forecast_Informer_ANGLO_AMERICAN_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 5
============================================================
train 776
Overriding target from 'OT' to 'close' for stock data
val 42
Overriding target from 'OT' to 'close' for stock data
test 44
Epoch: 1, Steps: 25 | Train Loss: 0.4483039 Vali Loss: 0.6852684 Test Loss: 2.5656863
Validation loss decreased (inf --> 0.685268).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 25 | Train Loss: 0.3234691 Vali Loss: 0.6893086 Test Loss: 2.5125206
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 25 | Train Loss: 0.2965532 Vali Loss: 0.6473320 Test Loss: 2.3733810
Validation loss decreased (0.685268 --> 0.647332).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 25 | Train Loss: 0.2857315 Vali Loss: 0.6601645 Test Loss: 2.5233837
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 25 | Train Loss: 0.2751667 Vali Loss: 0.6209827 Test Loss: 2.4748241
Validation loss decreased (0.647332 --> 0.620983).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 25 | Train Loss: 0.2701421 Vali Loss: 0.7089214 Test Loss: 2.5380502
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.44830385088920593, 'val/loss': 0.6852684020996094, 'test/loss': 2.565686345100403, '_timestamp': 1762784745.7463713}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3234691125154495, 'val/loss': 0.6893086135387421, 'test/loss': 2.5125205516815186, '_timestamp': 1762784748.198408}).
Epoch: 7, Steps: 25 | Train Loss: 0.2765999 Vali Loss: 0.7215070 Test Loss: 2.5500849
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 25 | Train Loss: 0.2677467 Vali Loss: 0.7060853 Test Loss: 2.5406799
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 25 | Train Loss: 0.2690814 Vali Loss: 0.7144647 Test Loss: 2.5479809
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 25 | Train Loss: 0.2703148 Vali Loss: 0.6737667 Test Loss: 2.5631953
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 25 | Train Loss: 0.2661615 Vali Loss: 0.6514919 Test Loss: 2.5525246
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 25 | Train Loss: 0.2701947 Vali Loss: 0.6816717 Test Loss: 2.5419319
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 25 | Train Loss: 0.2713688 Vali Loss: 0.6950381 Test Loss: 2.5316184
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 25 | Train Loss: 0.2731052 Vali Loss: 0.7177343 Test Loss: 2.5400532
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 25 | Train Loss: 0.2727985 Vali Loss: 0.6925853 Test Loss: 2.5658749
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ANGLO_AMERICAN_H5_Informer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 44
test shape: (44, 5, 1) (44, 5, 1)
test shape: (44, 5, 1) (44, 5, 1)


	mse:0.007057441398501396, mae:0.05804368853569031, rmse:0.0840085819363594, r2:-0.0007833242416381836, dtw:Not calculated


VAL - MSE: 0.0071, MAE: 0.0580, RMSE: 0.0840, RÂ²: -0.0008, MAPE: 6428333.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.477 MB of 0.477 MB uploadedwandb: \ 0.477 MB of 0.477 MB uploadedwandb: | 0.477 MB of 0.477 MB uploadedwandb: / 0.477 MB of 0.776 MB uploadedwandb: - 0.477 MB of 0.776 MB uploadedwandb: \ 0.776 MB of 0.776 MB uploadedwandb: | 0.776 MB of 0.776 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–†â–…â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–‡â–ˆ
wandb:                 train/loss â–ˆâ–†â–ƒâ–‚â–ƒâ–â–‚â–‚â–â–‚â–‚â–ƒâ–ƒ
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–„â–â–‡â–ˆâ–‡â–ˆâ–…â–ƒâ–…â–†â–ˆâ–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 2.56587
wandb:                 train/loss 0.2728
wandb:   val/directional_accuracy 56.25
wandb:                   val/loss 0.69259
wandb:                    val/mae 0.05804
wandb:                   val/mape 642833300.0
wandb:                    val/mse 0.00706
wandb:                     val/r2 -0.00078
wandb:                   val/rmse 0.08401
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/3x5iwjv2
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_162538-3x5iwjv2/logs
Completed: ANGLO_AMERICAN H=5

Training: Informer on ANGLO_AMERICAN for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_162644-pkv4dgqy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/pkv4dgqy
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ANGLO_AMERICAN_H10Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/pkv4dgqy
>>>>>>>start training : long_term_forecast_Informer_ANGLO_AMERICAN_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 10
============================================================
train 771
Overriding target from 'OT' to 'close' for stock data
val 37
Overriding target from 'OT' to 'close' for stock data
test 39
Epoch: 1, Steps: 25 | Train Loss: 0.4642174 Vali Loss: 0.7144329 Test Loss: 2.5266808
Validation loss decreased (inf --> 0.714433).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 25 | Train Loss: 0.3357488 Vali Loss: 0.6819875 Test Loss: 2.3986588
Validation loss decreased (0.714433 --> 0.681987).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 25 | Train Loss: 0.3081523 Vali Loss: 0.7057440 Test Loss: 2.5392230
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 25 | Train Loss: 0.3018351 Vali Loss: 0.6892255 Test Loss: 2.5426151
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 25 | Train Loss: 0.2854817 Vali Loss: 0.8553205 Test Loss: 2.6710155
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 25 | Train Loss: 0.2892382 Vali Loss: 0.7127806 Test Loss: 2.5577627
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 25 | Train Loss: 0.2851458 Vali Loss: 0.6819728 Test Loss: 2.5803747
Validation loss decreased (0.681987 --> 0.681973).  Saving model ...
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.46421738862991335, 'val/loss': 0.7144328951835632, 'test/loss': 2.526680827140808, '_timestamp': 1762784811.8557594}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3357488113641739, 'val/loss': 0.6819874942302704, 'test/loss': 2.3986587524414062, '_timestamp': 1762784814.3561687}).
Epoch: 8, Steps: 25 | Train Loss: 0.2834309 Vali Loss: 0.7442275 Test Loss: 2.5869437
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 25 | Train Loss: 0.2851962 Vali Loss: 0.6536126 Test Loss: 2.5923668
Validation loss decreased (0.681973 --> 0.653613).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 25 | Train Loss: 0.2820899 Vali Loss: 0.7213697 Test Loss: 2.6064639
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 25 | Train Loss: 0.2838879 Vali Loss: 0.7114989 Test Loss: 2.5741616
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 25 | Train Loss: 0.2803442 Vali Loss: 0.7382806 Test Loss: 2.5841762
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 25 | Train Loss: 0.2832066 Vali Loss: 0.7460668 Test Loss: 2.5794050
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 25 | Train Loss: 0.2804832 Vali Loss: 0.7258209 Test Loss: 2.5903764
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 25 | Train Loss: 0.2867895 Vali Loss: 0.7477040 Test Loss: 2.6102126
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 25 | Train Loss: 0.2812223 Vali Loss: 0.7335997 Test Loss: 2.5993261
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 25 | Train Loss: 0.2828711 Vali Loss: 0.7044758 Test Loss: 2.5964274
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 25 | Train Loss: 0.2810166 Vali Loss: 0.7356832 Test Loss: 2.5731392
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 25 | Train Loss: 0.2797119 Vali Loss: 0.7417096 Test Loss: 2.5931323
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ANGLO_AMERICAN_H10_Informer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 39
test shape: (39, 10, 1) (39, 10, 1)
test shape: (39, 10, 1) (39, 10, 1)


	mse:0.007740939036011696, mae:0.06125187873840332, rmse:0.08798260986804962, r2:0.0009764432907104492, dtw:Not calculated


VAL - MSE: 0.0077, MAE: 0.0613, RMSE: 0.0880, RÂ²: 0.0010, MAPE: 2144598.75%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.501 MB of 0.501 MB uploadedwandb: \ 0.501 MB of 0.501 MB uploadedwandb: | 0.501 MB of 0.501 MB uploadedwandb: / 0.501 MB of 0.501 MB uploadedwandb: - 0.501 MB of 0.800 MB uploadedwandb: \ 0.501 MB of 0.800 MB uploadedwandb: | 0.800 MB of 0.800 MB uploadedwandb: / 0.800 MB of 0.800 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–â–ˆâ–‚â–ƒâ–„â–„â–…â–ƒâ–ƒâ–ƒâ–„â–…â–„â–„â–ƒâ–„
wandb:                 train/loss â–ˆâ–†â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–ƒâ–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–‚â–ˆâ–ƒâ–‚â–„â–â–ƒâ–ƒâ–„â–„â–„â–„â–„â–ƒâ–„â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 18
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 2.59313
wandb:                 train/loss 0.27971
wandb:   val/directional_accuracy 50.99715
wandb:                   val/loss 0.74171
wandb:                    val/mae 0.06125
wandb:                   val/mape 214459875.0
wandb:                    val/mse 0.00774
wandb:                     val/r2 0.00098
wandb:                   val/rmse 0.08798
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/pkv4dgqy
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_162644-pkv4dgqy/logs
Completed: ANGLO_AMERICAN H=10

Training: Informer on ANGLO_AMERICAN for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_162759-fbjrci72
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/fbjrci72
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ANGLO_AMERICAN_H22Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/fbjrci72
>>>>>>>start training : long_term_forecast_Informer_ANGLO_AMERICAN_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 22
============================================================
train 759
Overriding target from 'OT' to 'close' for stock data
val 25
Overriding target from 'OT' to 'close' for stock data
test 27
Epoch: 1, Steps: 24 | Train Loss: 0.4812976 Vali Loss: 0.7042102 Test Loss: 2.7118936
Validation loss decreased (inf --> 0.704210).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 24 | Train Loss: 0.3389055 Vali Loss: 0.8103695 Test Loss: 2.9535191
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 24 | Train Loss: 0.3207524 Vali Loss: 0.7612920 Test Loss: 2.7877369
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 24 | Train Loss: 0.3069434 Vali Loss: 0.7425706 Test Loss: 2.7564778
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 24 | Train Loss: 0.3006624 Vali Loss: 0.7368573 Test Loss: 2.7478213
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 24 | Train Loss: 0.2990666 Vali Loss: 0.7590631 Test Loss: 2.7805789
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 24 | Train Loss: 0.2961205 Vali Loss: 0.7401938 Test Loss: 2.7484095
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.4812976395090421, 'val/loss': 0.7042102217674255, 'test/loss': 2.7118935585021973, '_timestamp': 1762784888.1166005}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.33890552818775177, 'val/loss': 0.8103694915771484, 'test/loss': 2.953519105911255, '_timestamp': 1762784890.5277703}).
Epoch: 8, Steps: 24 | Train Loss: 0.2967500 Vali Loss: 0.7412269 Test Loss: 2.7399962
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 24 | Train Loss: 0.2944033 Vali Loss: 0.7371547 Test Loss: 2.7450702
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 24 | Train Loss: 0.2964453 Vali Loss: 0.7436728 Test Loss: 2.7561138
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 24 | Train Loss: 0.2945395 Vali Loss: 0.7428750 Test Loss: 2.7336380
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Informer_ANGLO_AMERICAN_H22_Informer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Overriding target from 'OT' to 'close' for stock data
test 27
test shape: (27, 22, 1) (27, 22, 1)
test shape: (27, 22, 1) (27, 22, 1)


	mse:0.007917435839772224, mae:0.06318467110395432, rmse:0.08897997438907623, r2:-0.008443355560302734, dtw:Not calculated


VAL - MSE: 0.0079, MAE: 0.0632, RMSE: 0.0890, RÂ²: -0.0084, MAPE: 231440.19%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.485 MB of 0.486 MB uploadedwandb: \ 0.485 MB of 0.486 MB uploadedwandb: | 0.486 MB of 0.486 MB uploadedwandb: / 0.486 MB of 0.486 MB uploadedwandb: - 0.486 MB of 0.785 MB uploadedwandb: \ 0.785 MB of 0.785 MB uploadedwandb: | 0.785 MB of 0.785 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–ƒâ–‡â–ƒâ–‚â–‚â–„â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–â–‚â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ƒâ–â–‡â–‚â–‚â–â–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (enc_embedd...
wandb: model/non_trainable_params 0
wandb:         model/total_params 11324422
wandb:     model/trainable_params 11324422
wandb:                  test/loss 2.73364
wandb:                 train/loss 0.29454
wandb:   val/directional_accuracy 50.61728
wandb:                   val/loss 0.74288
wandb:                    val/mae 0.06318
wandb:                   val/mape 23144018.75
wandb:                    val/mse 0.00792
wandb:                     val/r2 -0.00844
wandb:                   val/rmse 0.08898
wandb: 
wandb: ðŸš€ View run Informer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/fbjrci72
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_162759-fbjrci72/logs
Completed: ANGLO_AMERICAN H=22

Training: Informer on ANGLO_AMERICAN for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_162849-ppr6vucd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/ppr6vucd
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ANGLO_AMERICAN_H50Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/ppr6vucd
>>>>>>>start training : long_term_forecast_Informer_ANGLO_AMERICAN_H50_Informer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 50
============================================================
train 731
Overriding target from 'OT' to 'close' for stock data
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.005 MB of 0.020 MB uploadedwandb: / 0.024 MB of 0.024 MB uploadedwandb: ðŸš€ View run Informer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/ppr6vucd
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_162849-ppr6vucd/logs
Traceback (most recent call last):
  File "run.py", line 207, in <module>
    exp.train(setting)
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 93, in train
    vali_data, vali_loader = self._get_data(flag='val')
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 43, in _get_data
    data_set, data_loader = data_provider(self.args, flag)
  File "/home-mscluster/bkodze/research_project/forecast-research/data_provider/data_factory.py", line 79, in data_provider
    print(flag, len(data_set))
ValueError: __len__() should return >= 0
Completed: ANGLO_AMERICAN H=50

Training: Informer on ANGLO_AMERICAN for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251110_162912-ri7vw7s8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Informer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/ri7vw7s8
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Informer_ANGLO_AMERICAN_H100Model:              Informer            

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ANGLO_AMERICAN_normalized.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             5                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Informer_Exp        Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Informer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/ri7vw7s8
>>>>>>>start training : long_term_forecast_Informer_ANGLO_AMERICAN_H100_Informer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc5_ebtimeF_dtTrue_Informer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Overriding target from 'OT' to 'close' for stock data
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 934
Train: 840 samples (90%) - rows 0 to 839
Val: 46 samples (5%) - rows 840 to 885
Test: 48 samples (5%) - rows 886 to 933
Sequence length: 60, Prediction length: 100
============================================================
train 681
Overriding target from 'OT' to 'close' for stock data
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.005 MB of 0.005 MB uploadedwandb: / 0.024 MB of 0.024 MB uploadedwandb: ðŸš€ View run Informer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN/runs/ri7vw7s8
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ANGLO_AMERICAN
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251110_162912-ri7vw7s8/logs
Traceback (most recent call last):
  File "run.py", line 207, in <module>
    exp.train(setting)
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 93, in train
    vali_data, vali_loader = self._get_data(flag='val')
  File "/home-mscluster/bkodze/research_project/forecast-research/exp/exp_long_term_forecasting.py", line 43, in _get_data
    data_set, data_loader = data_provider(self.args, flag)
  File "/home-mscluster/bkodze/research_project/forecast-research/data_provider/data_factory.py", line 79, in data_provider
    print(flag, len(data_set))
ValueError: __len__() should return >= 0
Completed: ANGLO_AMERICAN H=100

Informer training completed for all datasets!
