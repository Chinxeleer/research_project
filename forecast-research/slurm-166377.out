##############################################################################
# Training Autoformer Model on All Datasets
##############################################################################
Training: Autoformer on NVIDIA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_110032-ume4p9tw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/ume4p9tw
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NVIDIA_H3Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/ume4p9tw
>>>>>>>start training : long_term_forecast_Autoformer_NVIDIA_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.3041071 Vali Loss: 0.1951918 Test Loss: 0.3391332
Validation loss decreased (inf --> 0.195192).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3041071154569325, 'val/loss': 0.19519175309687853, 'test/loss': 0.33913320675492287, '_timestamp': 1762333258.4225934}).
Epoch: 2, Steps: 133 | Train Loss: 0.2598916 Vali Loss: 0.1839015 Test Loss: 0.3174026
Validation loss decreased (0.195192 --> 0.183901).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.2462978 Vali Loss: 0.1817898 Test Loss: 0.3138133
Validation loss decreased (0.183901 --> 0.181790).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.259891592918482, 'val/loss': 0.18390145804733038, 'test/loss': 0.317402551881969, '_timestamp': 1762333265.5232935}).
Epoch: 4, Steps: 133 | Train Loss: 0.2412563 Vali Loss: 0.1729628 Test Loss: 0.3022252
Validation loss decreased (0.181790 --> 0.172963).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2365273 Vali Loss: 0.1968075 Test Loss: 0.3021743
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2343484 Vali Loss: 0.2011430 Test Loss: 0.3022677
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2331204 Vali Loss: 0.1707832 Test Loss: 0.3027658
Validation loss decreased (0.172963 --> 0.170783).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2336214 Vali Loss: 0.1762557 Test Loss: 0.3018244
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2332598 Vali Loss: 0.1810650 Test Loss: 0.3018641
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2331294 Vali Loss: 0.1727360 Test Loss: 0.3017516
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2328199 Vali Loss: 0.1783385 Test Loss: 0.3017008
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2325452 Vali Loss: 0.1747425 Test Loss: 0.3017463
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2327407 Vali Loss: 0.1750858 Test Loss: 0.3017640
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2326449 Vali Loss: 0.1749270 Test Loss: 0.3017366
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2324290 Vali Loss: 0.1864970 Test Loss: 0.3017394
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2322358 Vali Loss: 0.1785449 Test Loss: 0.3017385
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2316523 Vali Loss: 0.1947634 Test Loss: 0.3017390
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NVIDIA_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.001152194570749998, mae:0.025424981489777565, rmse:0.03394399210810661, r2:-0.027106761932373047, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0254, RMSE: 0.0339, RÂ²: -0.0271, MAPE: 774432.94%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.467 MB of 0.468 MB uploadedwandb: \ 0.467 MB of 0.468 MB uploadedwandb: | 0.468 MB of 0.468 MB uploadedwandb: / 0.468 MB of 0.468 MB uploadedwandb: - 0.468 MB of 0.468 MB uploadedwandb: \ 0.468 MB of 0.468 MB uploadedwandb: | 0.468 MB of 0.468 MB uploadedwandb: / 0.598 MB of 0.814 MB uploaded (0.002 MB deduped)wandb: - 0.814 MB of 0.814 MB uploaded (0.002 MB deduped)wandb: \ 0.814 MB of 0.814 MB uploaded (0.002 MB deduped)wandb: | 0.814 MB of 0.814 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–†â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–„â–‚â–‡â–ˆâ–â–‚â–ƒâ–â–ƒâ–‚â–‚â–‚â–…â–ƒâ–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.30174
wandb:                 train/loss 0.23165
wandb:   val/directional_accuracy 48.52321
wandb:                   val/loss 0.19476
wandb:                    val/mae 0.02542
wandb:                   val/mape 77443293.75
wandb:                    val/mse 0.00115
wandb:                     val/r2 -0.02711
wandb:                   val/rmse 0.03394
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/ume4p9tw
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_110032-ume4p9tw/logs
Completed: NVIDIA H=3

Training: Autoformer on NVIDIA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_110323-d16s6d2k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/d16s6d2k
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NVIDIA_H5Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/d16s6d2k
>>>>>>>start training : long_term_forecast_Autoformer_NVIDIA_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.2974592 Vali Loss: 0.2024027 Test Loss: 0.3360836
Validation loss decreased (inf --> 0.202403).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2597451 Vali Loss: 0.1822925 Test Loss: 0.3363506
Validation loss decreased (0.202403 --> 0.182292).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.297459210892369, 'val/loss': 0.20240267179906368, 'test/loss': 0.3360836310312152, '_timestamp': 1762333419.571441}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25974507988395545, 'val/loss': 0.18229245115071535, 'test/loss': 0.3363505695015192, '_timestamp': 1762333426.723495}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25974507988395545, 'val/loss': 0.18229245115071535, 'test/loss': 0.3363505695015192, '_timestamp': 1762333426.723495}).
Epoch: 3, Steps: 133 | Train Loss: 0.2446364 Vali Loss: 0.1813585 Test Loss: 0.3374871
Validation loss decreased (0.182292 --> 0.181359).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2399724 Vali Loss: 0.1813087 Test Loss: 0.3273924
Validation loss decreased (0.181359 --> 0.181309).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2365986 Vali Loss: 0.1785546 Test Loss: 0.3297556
Validation loss decreased (0.181309 --> 0.178555).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2355697 Vali Loss: 0.1782718 Test Loss: 0.3297663
Validation loss decreased (0.178555 --> 0.178272).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2354399 Vali Loss: 0.1795840 Test Loss: 0.3278094
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2350468 Vali Loss: 0.1790515 Test Loss: 0.3286813
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2344591 Vali Loss: 0.1800576 Test Loss: 0.3282703
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2347376 Vali Loss: 0.1932141 Test Loss: 0.3284743
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2354705 Vali Loss: 0.1756278 Test Loss: 0.3278676
Validation loss decreased (0.178272 --> 0.175628).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2348431 Vali Loss: 0.1803516 Test Loss: 0.3280765
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2340917 Vali Loss: 0.2003727 Test Loss: 0.3279122
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2344792 Vali Loss: 0.1945008 Test Loss: 0.3278745
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2346136 Vali Loss: 0.1935992 Test Loss: 0.3278679
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2343845 Vali Loss: 0.1925108 Test Loss: 0.3278690
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2341420 Vali Loss: 0.1859115 Test Loss: 0.3278700
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2349042 Vali Loss: 0.1944095 Test Loss: 0.3278694
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2340204 Vali Loss: 0.1854764 Test Loss: 0.3278693
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2344993 Vali Loss: 0.1792480 Test Loss: 0.3278693
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2347153 Vali Loss: 0.1778507 Test Loss: 0.3278695
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NVIDIA_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.0011710585094988346, mae:0.025819165632128716, rmse:0.03422073274850845, r2:-0.03717923164367676, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0258, RMSE: 0.0342, RÂ²: -0.0372, MAPE: 849704.69%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.521 MB of 0.521 MB uploadedwandb: \ 0.521 MB of 0.521 MB uploadedwandb: | 0.521 MB of 0.521 MB uploadedwandb: / 0.521 MB of 0.521 MB uploadedwandb: - 0.521 MB of 0.738 MB uploadedwandb: \ 0.738 MB of 0.738 MB uploadedwandb: | 0.738 MB of 0.738 MB uploadedwandb: / 0.738 MB of 0.738 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–ƒâ–ƒâ–â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–‚â–‚â–â–â–â–â–â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–†â–â–‚â–ˆâ–†â–†â–†â–„â–†â–„â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.32787
wandb:                 train/loss 0.23472
wandb:   val/directional_accuracy 44.78723
wandb:                   val/loss 0.17785
wandb:                    val/mae 0.02582
wandb:                   val/mape 84970468.75
wandb:                    val/mse 0.00117
wandb:                     val/r2 -0.03718
wandb:                   val/rmse 0.03422
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/d16s6d2k
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_110323-d16s6d2k/logs
Completed: NVIDIA H=5

Training: Autoformer on NVIDIA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_110622-vl7xrqep
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/vl7xrqep
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NVIDIA_H10Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/vl7xrqep
>>>>>>>start training : long_term_forecast_Autoformer_NVIDIA_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2896229 Vali Loss: 0.1963093 Test Loss: 0.3669249
Validation loss decreased (inf --> 0.196309).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2540137 Vali Loss: 0.1857466 Test Loss: 0.3596197
Validation loss decreased (0.196309 --> 0.185747).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2896228778855245, 'val/loss': 0.19630934484302998, 'test/loss': 0.3669249378144741, '_timestamp': 1762333595.3601713}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2540136878203629, 'val/loss': 0.18574663624167442, 'test/loss': 0.3596197087317705, '_timestamp': 1762333602.378589}).
Epoch: 3, Steps: 133 | Train Loss: 0.2444037 Vali Loss: 0.1828170 Test Loss: 0.3561010
Validation loss decreased (0.185747 --> 0.182817).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2403478 Vali Loss: 0.2057495 Test Loss: 0.3568897
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2386403 Vali Loss: 0.1927609 Test Loss: 0.3495979
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2371542 Vali Loss: 0.1750403 Test Loss: 0.3512473
Validation loss decreased (0.182817 --> 0.175040).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2367881 Vali Loss: 0.1828569 Test Loss: 0.3523902
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2370850 Vali Loss: 0.1831161 Test Loss: 0.3522008
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2374010 Vali Loss: 0.1808681 Test Loss: 0.3521478
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2370824 Vali Loss: 0.1939011 Test Loss: 0.3520969
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2363283 Vali Loss: 0.1884841 Test Loss: 0.3521564
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2366978 Vali Loss: 0.1734709 Test Loss: 0.3521619
Validation loss decreased (0.175040 --> 0.173471).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2367086 Vali Loss: 0.1996657 Test Loss: 0.3521712
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2369014 Vali Loss: 0.1846917 Test Loss: 0.3521754
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2355664 Vali Loss: 0.1910778 Test Loss: 0.3521762
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2363218 Vali Loss: 0.1756230 Test Loss: 0.3521782
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2380273 Vali Loss: 0.2133739 Test Loss: 0.3521792
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2365701 Vali Loss: 0.1731490 Test Loss: 0.3521788
Validation loss decreased (0.173471 --> 0.173149).  Saving model ...
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2359438 Vali Loss: 0.1802457 Test Loss: 0.3521794
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2368988 Vali Loss: 0.1769063 Test Loss: 0.3521790
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2366730 Vali Loss: 0.1783109 Test Loss: 0.3521789
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2367425 Vali Loss: 0.2091766 Test Loss: 0.3521792
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2376463 Vali Loss: 0.1794196 Test Loss: 0.3521787
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2370205 Vali Loss: 0.1751844 Test Loss: 0.3521793
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2360341 Vali Loss: 0.1997412 Test Loss: 0.3521791
EarlyStopping counter: 7 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2362515 Vali Loss: 0.1747341 Test Loss: 0.3521793
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.2360109 Vali Loss: 0.1747869 Test Loss: 0.3521793
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.2359721 Vali Loss: 0.1781565 Test Loss: 0.3521793
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NVIDIA_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.001177958445623517, mae:0.025914287194609642, rmse:0.03432139754295349, r2:-0.026878833770751953, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0259, RMSE: 0.0343, RÂ²: -0.0269, MAPE: 665302.38%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.563 MB of 0.563 MB uploadedwandb: \ 0.563 MB of 0.563 MB uploadedwandb: | 0.563 MB of 0.563 MB uploadedwandb: / 0.563 MB of 0.563 MB uploadedwandb: - 0.563 MB of 0.563 MB uploadedwandb: \ 0.563 MB of 0.781 MB uploadedwandb: | 0.781 MB of 0.781 MB uploadedwandb: / 0.781 MB of 0.781 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‡â–ˆâ–â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–ƒâ–‚â–â–‚â–‚â–‚â–ƒâ–‚â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–‡â–„â–â–ƒâ–ƒâ–‚â–…â–„â–â–†â–ƒâ–„â–â–ˆâ–â–‚â–‚â–‚â–‡â–‚â–â–†â–â–â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 27
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.35218
wandb:                 train/loss 0.23597
wandb:   val/directional_accuracy 50.57971
wandb:                   val/loss 0.17816
wandb:                    val/mae 0.02591
wandb:                   val/mape 66530237.5
wandb:                    val/mse 0.00118
wandb:                     val/r2 -0.02688
wandb:                   val/rmse 0.03432
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/vl7xrqep
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_110622-vl7xrqep/logs
Completed: NVIDIA H=10

Training: Autoformer on NVIDIA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_111004-74crp0v3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/74crp0v3
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NVIDIA_H22Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/74crp0v3
>>>>>>>start training : long_term_forecast_Autoformer_NVIDIA_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2827455 Vali Loss: 0.1948409 Test Loss: 0.4371988
Validation loss decreased (inf --> 0.194841).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2527917 Vali Loss: 0.1893283 Test Loss: 0.4284829
Validation loss decreased (0.194841 --> 0.189328).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.28274549634167645, 'val/loss': 0.19484093146664755, 'test/loss': 0.4371987815414156, '_timestamp': 1762333819.2853713}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25279172814705153, 'val/loss': 0.18932832777500153, 'test/loss': 0.4284828794854028, '_timestamp': 1762333826.3457499}).
Epoch: 3, Steps: 132 | Train Loss: 0.2457308 Vali Loss: 0.1878521 Test Loss: 0.4445304
Validation loss decreased (0.189328 --> 0.187852).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2425275 Vali Loss: 0.1880136 Test Loss: 0.4251011
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2405563 Vali Loss: 0.1872828 Test Loss: 0.4299313
Validation loss decreased (0.187852 --> 0.187283).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2395878 Vali Loss: 0.1907562 Test Loss: 0.4309905
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2392992 Vali Loss: 0.1886412 Test Loss: 0.4279313
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2388750 Vali Loss: 0.1892557 Test Loss: 0.4284472
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2387166 Vali Loss: 0.1896416 Test Loss: 0.4279843
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2386739 Vali Loss: 0.1892912 Test Loss: 0.4284197
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2385732 Vali Loss: 0.1890868 Test Loss: 0.4284310
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2383603 Vali Loss: 0.1879372 Test Loss: 0.4284987
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2383466 Vali Loss: 0.1888275 Test Loss: 0.4285217
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2384371 Vali Loss: 0.1879486 Test Loss: 0.4285276
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2385487 Vali Loss: 0.1899586 Test Loss: 0.4285328
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NVIDIA_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.0012169795809313655, mae:0.02628379315137863, rmse:0.034885235130786896, r2:-0.03146481513977051, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0263, RMSE: 0.0349, RÂ²: -0.0315, MAPE: 565354.94%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.585 MB of 0.587 MB uploadedwandb: \ 0.585 MB of 0.587 MB uploadedwandb: | 0.585 MB of 0.587 MB uploadedwandb: / 0.587 MB of 0.587 MB uploadedwandb: - 0.587 MB of 0.587 MB uploadedwandb: \ 0.587 MB of 0.802 MB uploadedwandb: | 0.802 MB of 0.802 MB uploadedwandb: / 0.802 MB of 0.802 MB uploadedwandb: - 0.802 MB of 0.802 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–‚â–â–ˆâ–„â–…â–†â–…â–…â–‚â–„â–‚â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.42853
wandb:                 train/loss 0.23855
wandb:   val/directional_accuracy 50.32765
wandb:                   val/loss 0.18996
wandb:                    val/mae 0.02628
wandb:                   val/mape 56535493.75
wandb:                    val/mse 0.00122
wandb:                     val/r2 -0.03146
wandb:                   val/rmse 0.03489
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/74crp0v3
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_111004-74crp0v3/logs
Completed: NVIDIA H=22

Training: Autoformer on NVIDIA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_111227-t7ukxs5f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/t7ukxs5f
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NVIDIA_H50Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/t7ukxs5f
>>>>>>>start training : long_term_forecast_Autoformer_NVIDIA_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.2879562 Vali Loss: 0.2083407 Test Loss: 0.5586436
Validation loss decreased (inf --> 0.208341).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.28795624942039, 'val/loss': 0.2083407218257586, 'test/loss': 0.5586436465382576, '_timestamp': 1762333963.9752107}).
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
Epoch: 2, Steps: 132 | Train Loss: 0.2601155 Vali Loss: 0.2106781 Test Loss: 0.5675659
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2536864 Vali Loss: 0.2035668 Test Loss: 0.5438871
Validation loss decreased (0.208341 --> 0.203567).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26011550178130466, 'val/loss': 0.21067810555299124, 'test/loss': 0.5675658583641052, '_timestamp': 1762333976.7123141}).
Epoch: 4, Steps: 132 | Train Loss: 0.2513126 Vali Loss: 0.2059532 Test Loss: 0.5525299
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2498110 Vali Loss: 0.2069520 Test Loss: 0.5377025
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2482990 Vali Loss: 0.2073976 Test Loss: 0.5343860
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2477920 Vali Loss: 0.2071969 Test Loss: 0.5359308
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2475883 Vali Loss: 0.2082284 Test Loss: 0.5360740
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2477890 Vali Loss: 0.2076752 Test Loss: 0.5366356
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2476541 Vali Loss: 0.2083948 Test Loss: 0.5367113
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2474274 Vali Loss: 0.2083076 Test Loss: 0.5368286
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2471703 Vali Loss: 0.2082650 Test Loss: 0.5368914
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2468791 Vali Loss: 0.2083664 Test Loss: 0.5368754
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NVIDIA_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.0012080906890332699, mae:0.02651206962764263, rmse:0.034757599234580994, r2:-0.015209078788757324, dtw:Not calculated


VAL - MSE: 0.0012, MAE: 0.0265, RMSE: 0.0348, RÂ²: -0.0152, MAPE: 334642.62%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.632 MB of 0.634 MB uploadedwandb: \ 0.634 MB of 0.634 MB uploadedwandb: | 0.634 MB of 0.634 MB uploadedwandb: / 0.634 MB of 0.849 MB uploadedwandb: - 0.849 MB of 0.849 MB uploadedwandb: \ 0.849 MB of 0.849 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–ˆâ–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–†â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–„â–†â–‡â–†â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.53688
wandb:                 train/loss 0.24688
wandb:   val/directional_accuracy 51.28894
wandb:                   val/loss 0.20837
wandb:                    val/mae 0.02651
wandb:                   val/mape 33464262.5
wandb:                    val/mse 0.00121
wandb:                     val/r2 -0.01521
wandb:                   val/rmse 0.03476
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/t7ukxs5f
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_111227-t7ukxs5f/logs
Completed: NVIDIA H=50

Training: Autoformer on NVIDIA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_111440-efx0jyl7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/efx0jyl7
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NVIDIA_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NVIDIA_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/efx0jyl7
>>>>>>>start training : long_term_forecast_Autoformer_NVIDIA_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.3054019 Vali Loss: 0.2303090 Test Loss: 0.8291389
Validation loss decreased (inf --> 0.230309).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
Epoch: 2, Steps: 130 | Train Loss: 0.2802192 Vali Loss: 0.2287487 Test Loss: 0.8282064
Validation loss decreased (0.230309 --> 0.228749).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3054018753079268, 'val/loss': 0.23030900061130524, 'test/loss': 0.8291389286518097, '_timestamp': 1762334095.6875303}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2802191673563077, 'val/loss': 0.22874867916107178, 'test/loss': 0.828206354379654, '_timestamp': 1762334102.8387487}).
Epoch: 3, Steps: 130 | Train Loss: 0.2755358 Vali Loss: 0.2373754 Test Loss: 0.8075912
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2734030 Vali Loss: 0.2284187 Test Loss: 0.8197072
Validation loss decreased (0.228749 --> 0.228419).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2721373 Vali Loss: 0.2356132 Test Loss: 0.8149086
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2715899 Vali Loss: 0.2368632 Test Loss: 0.8119757
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2718637 Vali Loss: 0.2423612 Test Loss: 0.8200044
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2715518 Vali Loss: 0.2385795 Test Loss: 0.8175327
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2715472 Vali Loss: 0.2333576 Test Loss: 0.8176840
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2712464 Vali Loss: 0.2371187 Test Loss: 0.8177169
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2713411 Vali Loss: 0.2408916 Test Loss: 0.8176148
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2715080 Vali Loss: 0.2374091 Test Loss: 0.8175793
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 130 | Train Loss: 0.2714388 Vali Loss: 0.2391462 Test Loss: 0.8175742
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 130 | Train Loss: 0.2712570 Vali Loss: 0.2397065 Test Loss: 0.8175781
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NVIDIA_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.0013063077349215746, mae:0.02748447097837925, rmse:0.036142878234386444, r2:-0.014674663543701172, dtw:Not calculated


VAL - MSE: 0.0013, MAE: 0.0275, RMSE: 0.0361, RÂ²: -0.0147, MAPE: 237255.91%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.682 MB of 0.687 MB uploadedwandb: \ 0.682 MB of 0.687 MB uploadedwandb: | 0.682 MB of 0.687 MB uploadedwandb: / 0.687 MB of 0.687 MB uploadedwandb: - 0.687 MB of 0.902 MB uploadedwandb: \ 0.902 MB of 0.902 MB uploadedwandb: | 0.902 MB of 0.902 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ˆâ–…â–ƒâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:                 train/loss â–ˆâ–…â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–â–…â–…â–ˆâ–†â–ƒâ–…â–‡â–†â–†â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 13
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.81758
wandb:                 train/loss 0.27126
wandb:   val/directional_accuracy 49.7619
wandb:                   val/loss 0.23971
wandb:                    val/mae 0.02748
wandb:                   val/mape 23725590.625
wandb:                    val/mse 0.00131
wandb:                     val/r2 -0.01467
wandb:                   val/rmse 0.03614
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA/runs/efx0jyl7
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NVIDIA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_111440-efx0jyl7/logs
Completed: NVIDIA H=100

Training: Autoformer on APPLE for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_111658-w6uw90tt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/w6uw90tt
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_APPLE_H3 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/w6uw90tt
>>>>>>>start training : long_term_forecast_Autoformer_APPLE_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.2977462 Vali Loss: 0.0966509 Test Loss: 0.1430479
Validation loss decreased (inf --> 0.096651).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2530787 Vali Loss: 0.0970310 Test Loss: 0.1427962
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29774621420336844, 'val/loss': 0.0966509310528636, 'test/loss': 0.14304788131266832, '_timestamp': 1762334234.2118204}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2530787094195086, 'val/loss': 0.09703101590275764, 'test/loss': 0.14279618579894304, '_timestamp': 1762334241.1831033}).
Epoch: 3, Steps: 133 | Train Loss: 0.2357283 Vali Loss: 0.0883246 Test Loss: 0.1386417
Validation loss decreased (0.096651 --> 0.088325).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2294953 Vali Loss: 0.0897471 Test Loss: 0.1349155
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2246271 Vali Loss: 0.0858094 Test Loss: 0.1348069
Validation loss decreased (0.088325 --> 0.085809).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2238654 Vali Loss: 0.0826458 Test Loss: 0.1330095
Validation loss decreased (0.085809 --> 0.082646).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2224208 Vali Loss: 0.0849322 Test Loss: 0.1331571
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2222341 Vali Loss: 0.0848230 Test Loss: 0.1329965
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2219603 Vali Loss: 0.0855513 Test Loss: 0.1325129
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2212924 Vali Loss: 0.0833097 Test Loss: 0.1323896
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2221982 Vali Loss: 0.0846731 Test Loss: 0.1323928
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2226983 Vali Loss: 0.0888893 Test Loss: 0.1323518
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2218408 Vali Loss: 0.0860995 Test Loss: 0.1323418
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2218328 Vali Loss: 0.0877028 Test Loss: 0.1323221
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2218410 Vali Loss: 0.0889757 Test Loss: 0.1323194
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2214636 Vali Loss: 0.0826706 Test Loss: 0.1323196
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_APPLE_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.00020889288862235844, mae:0.010622061789035797, rmse:0.014453127048909664, r2:-0.04476344585418701, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0106, RMSE: 0.0145, RÂ²: -0.0448, MAPE: 1059653.88%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.473 MB of 0.473 MB uploadedwandb: \ 0.473 MB of 0.473 MB uploadedwandb: | 0.473 MB of 0.473 MB uploadedwandb: / 0.473 MB of 0.473 MB uploadedwandb: - 0.473 MB of 0.473 MB uploadedwandb: \ 0.473 MB of 0.473 MB uploadedwandb: | 0.473 MB of 0.473 MB uploadedwandb: / 0.473 MB of 0.473 MB uploadedwandb: - 0.604 MB of 0.819 MB uploaded (0.002 MB deduped)wandb: \ 0.819 MB of 0.819 MB uploaded (0.002 MB deduped)wandb: | 0.819 MB of 0.819 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–„â–‚â–‚â–‚â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‡â–ˆâ–„â–â–ƒâ–ƒâ–„â–‚â–ƒâ–‡â–„â–†â–‡â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.13232
wandb:                 train/loss 0.22146
wandb:   val/directional_accuracy 49.78903
wandb:                   val/loss 0.08267
wandb:                    val/mae 0.01062
wandb:                   val/mape 105965387.5
wandb:                    val/mse 0.00021
wandb:                     val/r2 -0.04476
wandb:                   val/rmse 0.01445
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/w6uw90tt
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_111658-w6uw90tt/logs
Completed: APPLE H=3

Training: Autoformer on APPLE for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_111930-sbxbwh43
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/sbxbwh43
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_APPLE_H5 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/sbxbwh43
>>>>>>>start training : long_term_forecast_Autoformer_APPLE_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.2882581 Vali Loss: 0.0955466 Test Loss: 0.1452185
Validation loss decreased (inf --> 0.095547).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2494175 Vali Loss: 0.0899851 Test Loss: 0.1440141
Validation loss decreased (0.095547 --> 0.089985).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.28825812467506956, 'val/loss': 0.09554659482091665, 'test/loss': 0.14521849900484085, '_timestamp': 1762334386.0907736}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.24941753377591758, 'val/loss': 0.08998507913202047, 'test/loss': 0.14401409961283207, '_timestamp': 1762334393.21133}).
Epoch: 3, Steps: 133 | Train Loss: 0.2364686 Vali Loss: 0.0912502 Test Loss: 0.1436039
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2313922 Vali Loss: 0.0893447 Test Loss: 0.1392399
Validation loss decreased (0.089985 --> 0.089345).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2297292 Vali Loss: 0.0900219 Test Loss: 0.1367058
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2262313 Vali Loss: 0.0857650 Test Loss: 0.1371122
Validation loss decreased (0.089345 --> 0.085765).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2272256 Vali Loss: 0.0851614 Test Loss: 0.1368920
Validation loss decreased (0.085765 --> 0.085161).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2270115 Vali Loss: 0.0871649 Test Loss: 0.1366277
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2273915 Vali Loss: 0.0855858 Test Loss: 0.1364761
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2265542 Vali Loss: 0.0845611 Test Loss: 0.1363419
Validation loss decreased (0.085161 --> 0.084561).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2261083 Vali Loss: 0.0845785 Test Loss: 0.1362371
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2269770 Vali Loss: 0.0850459 Test Loss: 0.1362964
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2257166 Vali Loss: 0.0851925 Test Loss: 0.1363001
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2261621 Vali Loss: 0.0846068 Test Loss: 0.1362986
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2265769 Vali Loss: 0.0851936 Test Loss: 0.1362966
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2264701 Vali Loss: 0.0841117 Test Loss: 0.1362955
Validation loss decreased (0.084561 --> 0.084112).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2258962 Vali Loss: 0.0856541 Test Loss: 0.1362956
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2251398 Vali Loss: 0.0846221 Test Loss: 0.1362951
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2263736 Vali Loss: 0.0854300 Test Loss: 0.1362959
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2260944 Vali Loss: 0.0882015 Test Loss: 0.1362955
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2274388 Vali Loss: 0.0846424 Test Loss: 0.1362956
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2260867 Vali Loss: 0.0883495 Test Loss: 0.1362954
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2262209 Vali Loss: 0.0867415 Test Loss: 0.1362955
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2259764 Vali Loss: 0.0840271 Test Loss: 0.1362958
Validation loss decreased (0.084112 --> 0.084027).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2260638 Vali Loss: 0.0862721 Test Loss: 0.1362958
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2260493 Vali Loss: 0.0846222 Test Loss: 0.1362956
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.2261979 Vali Loss: 0.0841344 Test Loss: 0.1362957
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.2267100 Vali Loss: 0.0850182 Test Loss: 0.1362956
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.2264487 Vali Loss: 0.0858416 Test Loss: 0.1362956
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 133 | Train Loss: 0.2254862 Vali Loss: 0.0871783 Test Loss: 0.1362956
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 133 | Train Loss: 0.2256934 Vali Loss: 0.0861248 Test Loss: 0.1362956
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 133 | Train Loss: 0.2261253 Vali Loss: 0.0850613 Test Loss: 0.1362956
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 133 | Train Loss: 0.2258818 Vali Loss: 0.0877213 Test Loss: 0.1362956
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 133 | Train Loss: 0.2265097 Vali Loss: 0.0849089 Test Loss: 0.1362956
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_APPLE_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.00021334974735509604, mae:0.010591523721814156, rmse:0.014606496319174767, r2:-0.06218361854553223, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0106, RMSE: 0.0146, RÂ²: -0.0622, MAPE: 285074.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.522 MB of 0.522 MB uploadedwandb: \ 0.522 MB of 0.522 MB uploadedwandb: | 0.522 MB of 0.522 MB uploadedwandb: / 0.522 MB of 0.522 MB uploadedwandb: - 0.522 MB of 0.740 MB uploadedwandb: \ 0.539 MB of 0.740 MB uploadedwandb: | 0.740 MB of 0.740 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–‡â–ƒâ–‚â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–ƒâ–‚â–‚â–…â–‚â–…â–„â–â–ƒâ–‚â–â–‚â–ƒâ–„â–ƒâ–‚â–…â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 33
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.1363
wandb:                 train/loss 0.22651
wandb:   val/directional_accuracy 44.68085
wandb:                   val/loss 0.08491
wandb:                    val/mae 0.01059
wandb:                   val/mape 28507450.0
wandb:                    val/mse 0.00021
wandb:                     val/r2 -0.06218
wandb:                   val/rmse 0.01461
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/sbxbwh43
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_111930-sbxbwh43/logs
Completed: APPLE H=5

Training: Autoformer on APPLE for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_112358-pvmmleu2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/pvmmleu2
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_APPLE_H10Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/pvmmleu2
>>>>>>>start training : long_term_forecast_Autoformer_APPLE_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2774970 Vali Loss: 0.1027223 Test Loss: 0.1470547
Validation loss decreased (inf --> 0.102722).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2446195 Vali Loss: 0.0970822 Test Loss: 0.1401248
Validation loss decreased (0.102722 --> 0.097082).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.27749700351317125, 'val/loss': 0.10272233281284571, 'test/loss': 0.14705466851592064, '_timestamp': 1762334653.2825131}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2446195415984419, 'val/loss': 0.09708218649029732, 'test/loss': 0.14012477733194828, '_timestamp': 1762334660.4196827}).
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.27749700351317125, 'val/loss': 0.10272233281284571, 'test/loss': 0.14705466851592064, '_timestamp': 1762334653.2825131}).
Epoch: 3, Steps: 133 | Train Loss: 0.2350080 Vali Loss: 0.0925843 Test Loss: 0.1435907
Validation loss decreased (0.097082 --> 0.092584).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2294170 Vali Loss: 0.0953054 Test Loss: 0.1393723
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2277872 Vali Loss: 0.0910377 Test Loss: 0.1402791
Validation loss decreased (0.092584 --> 0.091038).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2255515 Vali Loss: 0.0912228 Test Loss: 0.1391905
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2250311 Vali Loss: 0.0905290 Test Loss: 0.1386153
Validation loss decreased (0.091038 --> 0.090529).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2252435 Vali Loss: 0.0891865 Test Loss: 0.1391256
Validation loss decreased (0.090529 --> 0.089186).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2256349 Vali Loss: 0.0893685 Test Loss: 0.1387934
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2242586 Vali Loss: 0.0910594 Test Loss: 0.1385226
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2243104 Vali Loss: 0.0885718 Test Loss: 0.1385332
Validation loss decreased (0.089186 --> 0.088572).  Saving model ...
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2237504 Vali Loss: 0.0933969 Test Loss: 0.1384110
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2247316 Vali Loss: 0.0901323 Test Loss: 0.1385380
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2251691 Vali Loss: 0.0917889 Test Loss: 0.1386027
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2238306 Vali Loss: 0.0898691 Test Loss: 0.1386041
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2244468 Vali Loss: 0.0908291 Test Loss: 0.1386037
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2252084 Vali Loss: 0.0907019 Test Loss: 0.1386038
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2238765 Vali Loss: 0.0917578 Test Loss: 0.1386035
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2243081 Vali Loss: 0.0911380 Test Loss: 0.1386037
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2249056 Vali Loss: 0.0907053 Test Loss: 0.1386037
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2257636 Vali Loss: 0.0909356 Test Loss: 0.1386035
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_APPLE_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.00021910472423769534, mae:0.010746784508228302, rmse:0.014802186749875546, r2:-0.08217644691467285, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0107, RMSE: 0.0148, RÂ²: -0.0822, MAPE: 700140.44%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.583 MB of 0.583 MB uploadedwandb: \ 0.583 MB of 0.583 MB uploadedwandb: | 0.583 MB of 0.800 MB uploadedwandb: / 0.785 MB of 0.800 MB uploadedwandb: - 0.800 MB of 0.800 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–„â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–â–â–â–‚â–‚â–â–â–‚â–â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–ˆâ–„â–„â–ƒâ–‚â–‚â–„â–â–†â–ƒâ–„â–‚â–ƒâ–ƒâ–„â–„â–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 20
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.1386
wandb:                 train/loss 0.22576
wandb:   val/directional_accuracy 50.04831
wandb:                   val/loss 0.09094
wandb:                    val/mae 0.01075
wandb:                   val/mape 70014043.75
wandb:                    val/mse 0.00022
wandb:                     val/r2 -0.08218
wandb:                   val/rmse 0.0148
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/pvmmleu2
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_112358-pvmmleu2/logs
Completed: APPLE H=10

Training: Autoformer on APPLE for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_112659-935467ph
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/935467ph
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_APPLE_H22Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/935467ph
>>>>>>>start training : long_term_forecast_Autoformer_APPLE_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2778098 Vali Loss: 0.0946686 Test Loss: 0.1386246
Validation loss decreased (inf --> 0.094669).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.277809767673413, 'val/loss': 0.09466862146343503, 'test/loss': 0.13862461809601104, '_timestamp': 1762334833.782577}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2467058113578594, 'val/loss': 0.0918048939534596, 'test/loss': 0.13575890873159682, '_timestamp': 1762334840.7319863}).
Epoch: 2, Steps: 132 | Train Loss: 0.2467058 Vali Loss: 0.0918049 Test Loss: 0.1357589
Validation loss decreased (0.094669 --> 0.091805).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2389134 Vali Loss: 0.0920424 Test Loss: 0.1409032
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2353256 Vali Loss: 0.0921175 Test Loss: 0.1411711
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2334664 Vali Loss: 0.0901761 Test Loss: 0.1392581
Validation loss decreased (0.091805 --> 0.090176).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2320363 Vali Loss: 0.0908282 Test Loss: 0.1404491
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2317869 Vali Loss: 0.0900784 Test Loss: 0.1403434
Validation loss decreased (0.090176 --> 0.090078).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2314926 Vali Loss: 0.0906649 Test Loss: 0.1402694
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2312765 Vali Loss: 0.0904346 Test Loss: 0.1402152
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2309437 Vali Loss: 0.0908336 Test Loss: 0.1401426
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2310214 Vali Loss: 0.0905861 Test Loss: 0.1401338
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2306516 Vali Loss: 0.0908206 Test Loss: 0.1401210
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2305740 Vali Loss: 0.0907678 Test Loss: 0.1401173
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2304058 Vali Loss: 0.0905263 Test Loss: 0.1400863
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2310109 Vali Loss: 0.0904553 Test Loss: 0.1400895
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2308989 Vali Loss: 0.0905383 Test Loss: 0.1400912
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.2311822 Vali Loss: 0.0907366 Test Loss: 0.1400912
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_APPLE_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.00022270044428296387, mae:0.010784860700368881, rmse:0.014923151582479477, r2:-0.07323193550109863, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0108, RMSE: 0.0149, RÂ²: -0.0732, MAPE: 376049.47%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.651 MB of 0.652 MB uploadedwandb: \ 0.651 MB of 0.652 MB uploadedwandb: | 0.652 MB of 0.652 MB uploadedwandb: / 0.652 MB of 0.868 MB uploadedwandb: - 0.868 MB of 0.868 MB uploadedwandb: \ 0.868 MB of 0.868 MB uploadedwandb: | 0.868 MB of 0.868 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‡â–ˆâ–â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–„
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–ˆâ–â–„â–â–ƒâ–‚â–„â–ƒâ–„â–ƒâ–ƒâ–‚â–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.14009
wandb:                 train/loss 0.23118
wandb:   val/directional_accuracy 49.03888
wandb:                   val/loss 0.09074
wandb:                    val/mae 0.01078
wandb:                   val/mape 37604946.875
wandb:                    val/mse 0.00022
wandb:                     val/r2 -0.07323
wandb:                   val/rmse 0.01492
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/935467ph
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_112659-935467ph/logs
Completed: APPLE H=22

Training: Autoformer on APPLE for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_112933-r6u0er9p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/r6u0er9p
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_APPLE_H50Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/r6u0er9p
>>>>>>>start training : long_term_forecast_Autoformer_APPLE_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.2839285 Vali Loss: 0.1002721 Test Loss: 0.1611220
Validation loss decreased (inf --> 0.100272).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2548240 Vali Loss: 0.0961139 Test Loss: 0.1555223
Validation loss decreased (0.100272 --> 0.096114).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.28392846227595303, 'val/loss': 0.10027213891347249, 'test/loss': 0.1611220327516397, '_timestamp': 1762334989.0557184}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.25482396578246896, 'val/loss': 0.09611392517884572, 'test/loss': 0.1555223191777865, '_timestamp': 1762334996.1388817}).
Epoch: 3, Steps: 132 | Train Loss: 0.2450044 Vali Loss: 0.0942678 Test Loss: 0.1545716
Validation loss decreased (0.096114 --> 0.094268).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2419354 Vali Loss: 0.0930892 Test Loss: 0.1561792
Validation loss decreased (0.094268 --> 0.093089).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2406018 Vali Loss: 0.0909065 Test Loss: 0.1534960
Validation loss decreased (0.093089 --> 0.090906).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2394408 Vali Loss: 0.0929473 Test Loss: 0.1557992
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2383848 Vali Loss: 0.0934847 Test Loss: 0.1565546
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2378540 Vali Loss: 0.0934859 Test Loss: 0.1563623
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2384113 Vali Loss: 0.0937696 Test Loss: 0.1567838
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2385875 Vali Loss: 0.0938107 Test Loss: 0.1568960
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2386373 Vali Loss: 0.0937534 Test Loss: 0.1567295
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2380864 Vali Loss: 0.0936253 Test Loss: 0.1566578
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2380385 Vali Loss: 0.0936492 Test Loss: 0.1566123
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2374199 Vali Loss: 0.0937256 Test Loss: 0.1566110
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2377335 Vali Loss: 0.0936543 Test Loss: 0.1566042
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_APPLE_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.0002359729551244527, mae:0.011133217252790928, rmse:0.01536141149699688, r2:-0.06353998184204102, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0111, RMSE: 0.0154, RÂ²: -0.0635, MAPE: 368502.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.725 MB of 0.728 MB uploadedwandb: \ 0.725 MB of 0.728 MB uploadedwandb: | 0.725 MB of 0.728 MB uploadedwandb: / 0.728 MB of 0.728 MB uploadedwandb: - 0.728 MB of 0.728 MB uploadedwandb: \ 0.728 MB of 0.943 MB uploadedwandb: | 0.943 MB of 0.943 MB uploadedwandb: / 0.943 MB of 0.943 MB uploadedwandb: - 0.943 MB of 0.943 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ƒâ–‡â–â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡
wandb:                 train/loss â–ˆâ–…â–„â–ƒâ–‚â–â–‚â–‚â–‚â–‚â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.1566
wandb:                 train/loss 0.23773
wandb:   val/directional_accuracy 48.23845
wandb:                   val/loss 0.09365
wandb:                    val/mae 0.01113
wandb:                   val/mape 36850200.0
wandb:                    val/mse 0.00024
wandb:                     val/r2 -0.06354
wandb:                   val/rmse 0.01536
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/r6u0er9p
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_112933-r6u0er9p/logs
Completed: APPLE H=50

Training: Autoformer on APPLE for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_113202-16vyin1q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/16vyin1q
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_APPLE_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          APPLE_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/16vyin1q
>>>>>>>start training : long_term_forecast_Autoformer_APPLE_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.2972880 Vali Loss: 0.0995323 Test Loss: 0.1655730
Validation loss decreased (inf --> 0.099532).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 130 | Train Loss: 0.2709302 Vali Loss: 0.0952675 Test Loss: 0.1651533
Validation loss decreased (0.099532 --> 0.095267).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29728797525167466, 'val/loss': 0.09953228682279587, 'test/loss': 0.16557296514511108, '_timestamp': 1762335137.4829953}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2709301755978511, 'val/loss': 0.09526747465133667, 'test/loss': 0.16515327990055084, '_timestamp': 1762335144.5435727}).
Epoch: 3, Steps: 130 | Train Loss: 0.2642930 Vali Loss: 0.0998368 Test Loss: 0.1680554
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2614499 Vali Loss: 0.0970256 Test Loss: 0.1680683
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.2599090 Vali Loss: 0.0973001 Test Loss: 0.1692845
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2595234 Vali Loss: 0.0976762 Test Loss: 0.1676178
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2591214 Vali Loss: 0.0981609 Test Loss: 0.1684157
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2582971 Vali Loss: 0.0984167 Test Loss: 0.1676551
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2582388 Vali Loss: 0.0975666 Test Loss: 0.1679286
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2580623 Vali Loss: 0.0978139 Test Loss: 0.1678941
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2583199 Vali Loss: 0.0978237 Test Loss: 0.1678476
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2585325 Vali Loss: 0.0980314 Test Loss: 0.1678728
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_APPLE_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.00024839461548253894, mae:0.011314813047647476, rmse:0.01576053909957409, r2:-0.05202126502990723, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0113, RMSE: 0.0158, RÂ²: -0.0520, MAPE: 977500.31%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.729 MB of 0.734 MB uploadedwandb: \ 0.729 MB of 0.734 MB uploadedwandb: | 0.734 MB of 0.734 MB uploadedwandb: / 0.734 MB of 0.734 MB uploadedwandb: - 0.734 MB of 0.949 MB uploadedwandb: \ 0.734 MB of 0.949 MB uploadedwandb: | 0.949 MB of 0.949 MB uploadedwandb: / 0.949 MB of 0.949 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ƒâ–ƒâ–ˆâ–â–„â–â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–‚â–ƒâ–„â–„â–‚â–ƒâ–ƒâ–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.16787
wandb:                 train/loss 0.25853
wandb:   val/directional_accuracy 48.9899
wandb:                   val/loss 0.09803
wandb:                    val/mae 0.01131
wandb:                   val/mape 97750031.25
wandb:                    val/mse 0.00025
wandb:                     val/r2 -0.05202
wandb:                   val/rmse 0.01576
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE/runs/16vyin1q
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-APPLE
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_113202-16vyin1q/logs
Completed: APPLE H=100

Training: Autoformer on SP500 for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_113357-amx1a4d2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/amx1a4d2
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SP500_H3 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/amx1a4d2
>>>>>>>start training : long_term_forecast_Autoformer_SP500_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 3
============================================================
train 4241
val 237
test 238
Epoch: 1, Steps: 133 | Train Loss: 0.2431118 Vali Loss: 0.0793828 Test Loss: 0.0857464
Validation loss decreased (inf --> 0.079383).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2056598 Vali Loss: 0.0748744 Test Loss: 0.0801788
Validation loss decreased (0.079383 --> 0.074874).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.24311180530410065, 'val/loss': 0.0793827697634697, 'test/loss': 0.08574639400467277, '_timestamp': 1762335249.3507562}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.205659832441269, 'val/loss': 0.07487442530691624, 'test/loss': 0.08017883775755763, '_timestamp': 1762335256.3244138}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.205659832441269, 'val/loss': 0.07487442530691624, 'test/loss': 0.08017883775755763, '_timestamp': 1762335256.3244138}).
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.24311180530410065, 'val/loss': 0.0793827697634697, 'test/loss': 0.08574639400467277, '_timestamp': 1762335249.3507562}).
Epoch: 3, Steps: 133 | Train Loss: 0.1930449 Vali Loss: 0.0721058 Test Loss: 0.0809788
Validation loss decreased (0.074874 --> 0.072106).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1891252 Vali Loss: 0.0700776 Test Loss: 0.0751680
Validation loss decreased (0.072106 --> 0.070078).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1864100 Vali Loss: 0.0714771 Test Loss: 0.0750114
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1852527 Vali Loss: 0.0685722 Test Loss: 0.0743930
Validation loss decreased (0.070078 --> 0.068572).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1851544 Vali Loss: 0.0698446 Test Loss: 0.0750270
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1838959 Vali Loss: 0.0703273 Test Loss: 0.0747891
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1839028 Vali Loss: 0.0712958 Test Loss: 0.0745316
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1852426 Vali Loss: 0.0708679 Test Loss: 0.0746986
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1837868 Vali Loss: 0.0695307 Test Loss: 0.0746419
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1837560 Vali Loss: 0.0701435 Test Loss: 0.0746044
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1843192 Vali Loss: 0.0677414 Test Loss: 0.0746167
Validation loss decreased (0.068572 --> 0.067741).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1833428 Vali Loss: 0.0698776 Test Loss: 0.0746107
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1836571 Vali Loss: 0.0733723 Test Loss: 0.0745559
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1848037 Vali Loss: 0.0706567 Test Loss: 0.0745813
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1834578 Vali Loss: 0.0696751 Test Loss: 0.0745814
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1840763 Vali Loss: 0.0695561 Test Loss: 0.0745814
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1840463 Vali Loss: 0.0712936 Test Loss: 0.0745810
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1832601 Vali Loss: 0.0700733 Test Loss: 0.0745812
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1838510 Vali Loss: 0.0696462 Test Loss: 0.0745813
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1838829 Vali Loss: 0.0707076 Test Loss: 0.0745811
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1851372 Vali Loss: 0.0703359 Test Loss: 0.0745810
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SP500_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 238
test shape: (238, 3, 1) (238, 3, 1)
test shape: (238, 3, 1) (238, 3, 1)


	mse:6.699670484522358e-05, mae:0.006081330124288797, rmse:0.008185151033103466, r2:-0.030540108680725098, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0082, RÂ²: -0.0305, MAPE: 1.97%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.502 MB of 0.502 MB uploadedwandb: \ 0.502 MB of 0.502 MB uploadedwandb: | 0.502 MB of 0.502 MB uploadedwandb: / 0.502 MB of 0.502 MB uploadedwandb: - 0.502 MB of 0.502 MB uploadedwandb: \ 0.502 MB of 0.502 MB uploadedwandb: | 0.502 MB of 0.502 MB uploadedwandb: / 0.502 MB of 0.502 MB uploadedwandb: - 0.502 MB of 0.502 MB uploadedwandb: \ 0.633 MB of 0.849 MB uploaded (0.002 MB deduped)wandb: | 0.849 MB of 0.849 MB uploaded (0.002 MB deduped)wandb: / 0.849 MB of 0.849 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–‚â–â–â–‚â–â–â–‚â–â–‚â–‚â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–„â–†â–‚â–„â–„â–…â–…â–ƒâ–„â–â–„â–ˆâ–…â–ƒâ–ƒâ–…â–„â–ƒâ–…â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.07458
wandb:                 train/loss 0.18514
wandb:   val/directional_accuracy 45.37815
wandb:                   val/loss 0.07034
wandb:                    val/mae 0.00608
wandb:                   val/mape 196.82101
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.03054
wandb:                   val/rmse 0.00819
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/amx1a4d2
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_113357-amx1a4d2/logs
Completed: SP500 H=3

Training: Autoformer on SP500 for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_113714-bt3801li
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/bt3801li
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SP500_H5 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/bt3801li
>>>>>>>start training : long_term_forecast_Autoformer_SP500_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 5
============================================================
train 4239
val 235
test 236
Epoch: 1, Steps: 133 | Train Loss: 0.2376393 Vali Loss: 0.0925077 Test Loss: 0.1064501
Validation loss decreased (inf --> 0.092508).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.1995682 Vali Loss: 0.0734287 Test Loss: 0.0874373
Validation loss decreased (0.092508 --> 0.073429).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23763926344034367, 'val/loss': 0.0925077497959137, 'test/loss': 0.10645011905580759, '_timestamp': 1762335450.629774}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19956818620737335, 'val/loss': 0.07342868251726031, 'test/loss': 0.08743734378367662, '_timestamp': 1762335457.6206672}).
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23763926344034367, 'val/loss': 0.0925077497959137, 'test/loss': 0.10645011905580759, '_timestamp': 1762335450.629774}).
Epoch: 3, Steps: 133 | Train Loss: 0.1873580 Vali Loss: 0.0750128 Test Loss: 0.0803808
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.1837101 Vali Loss: 0.0727066 Test Loss: 0.0796822
Validation loss decreased (0.073429 --> 0.072707).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1806611 Vali Loss: 0.0718587 Test Loss: 0.0816793
Validation loss decreased (0.072707 --> 0.071859).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1794160 Vali Loss: 0.0694732 Test Loss: 0.0810097
Validation loss decreased (0.071859 --> 0.069473).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1784065 Vali Loss: 0.0687023 Test Loss: 0.0793424
Validation loss decreased (0.069473 --> 0.068702).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1783917 Vali Loss: 0.0699432 Test Loss: 0.0792098
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1791789 Vali Loss: 0.0680514 Test Loss: 0.0791054
Validation loss decreased (0.068702 --> 0.068051).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1775955 Vali Loss: 0.0694501 Test Loss: 0.0791050
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1777168 Vali Loss: 0.0698344 Test Loss: 0.0791133
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1783254 Vali Loss: 0.0694965 Test Loss: 0.0791063
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1789131 Vali Loss: 0.0698469 Test Loss: 0.0791092
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1774595 Vali Loss: 0.0688566 Test Loss: 0.0791115
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1778082 Vali Loss: 0.0679543 Test Loss: 0.0791125
Validation loss decreased (0.068051 --> 0.067954).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1780461 Vali Loss: 0.0687233 Test Loss: 0.0791136
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1775919 Vali Loss: 0.0711576 Test Loss: 0.0791135
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1776840 Vali Loss: 0.0697058 Test Loss: 0.0791130
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1780343 Vali Loss: 0.0717333 Test Loss: 0.0791135
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1780673 Vali Loss: 0.0692342 Test Loss: 0.0791135
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1780543 Vali Loss: 0.0696950 Test Loss: 0.0791135
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1777819 Vali Loss: 0.0705144 Test Loss: 0.0791132
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1781495 Vali Loss: 0.0714394 Test Loss: 0.0791132
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.1776177 Vali Loss: 0.0678026 Test Loss: 0.0791133
Validation loss decreased (0.067954 --> 0.067803).  Saving model ...
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.1780998 Vali Loss: 0.0695403 Test Loss: 0.0791134
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.1772640 Vali Loss: 0.0682613 Test Loss: 0.0791132
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 133 | Train Loss: 0.1778093 Vali Loss: 0.0701943 Test Loss: 0.0791133
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 133 | Train Loss: 0.1781478 Vali Loss: 0.0694988 Test Loss: 0.0791133
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.450580596923828e-13
Epoch: 29, Steps: 133 | Train Loss: 0.1778968 Vali Loss: 0.0695207 Test Loss: 0.0791133
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.725290298461914e-13
Epoch: 30, Steps: 133 | Train Loss: 0.1774359 Vali Loss: 0.0696617 Test Loss: 0.0791133
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.862645149230957e-13
Epoch: 31, Steps: 133 | Train Loss: 0.1776177 Vali Loss: 0.0712420 Test Loss: 0.0791133
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.313225746154786e-14
Epoch: 32, Steps: 133 | Train Loss: 0.1775693 Vali Loss: 0.0695663 Test Loss: 0.0791133
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.656612873077393e-14
Epoch: 33, Steps: 133 | Train Loss: 0.1777848 Vali Loss: 0.0700999 Test Loss: 0.0791133
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.3283064365386964e-14
Epoch: 34, Steps: 133 | Train Loss: 0.1777813 Vali Loss: 0.0686424 Test Loss: 0.0791133
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SP500_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
test shape: (236, 5, 1) (236, 5, 1)
test shape: (236, 5, 1) (236, 5, 1)


	mse:6.999906327109784e-05, mae:0.006258220877498388, rmse:0.008366544730961323, r2:-0.07755041122436523, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0063, RMSE: 0.0084, RÂ²: -0.0776, MAPE: 2.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.528 MB of 0.528 MB uploadedwandb: \ 0.528 MB of 0.528 MB uploadedwandb: | 0.528 MB of 0.528 MB uploadedwandb: / 0.528 MB of 0.528 MB uploadedwandb: - 0.528 MB of 0.747 MB uploadedwandb: \ 0.747 MB of 0.747 MB uploadedwandb: | 0.747 MB of 0.747 MB uploadedwandb: / 0.747 MB of 0.747 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–„â–ƒâ–ˆâ–†â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–â–â–‚â–‚â–â–â–‚â–â–â–‚â–‚â–‚â–â–‚â–â–‚â–â–â–‚â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–…â–ƒâ–‚â–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–â–‚â–„â–ƒâ–…â–‚â–ƒâ–„â–…â–â–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 33
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.07911
wandb:                 train/loss 0.17778
wandb:   val/directional_accuracy 45.33898
wandb:                   val/loss 0.06864
wandb:                    val/mae 0.00626
wandb:                   val/mape 250.2738
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.07755
wandb:                   val/rmse 0.00837
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/bt3801li
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_113714-bt3801li/logs
Completed: SP500 H=5

Training: Autoformer on SP500 for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_114144-4mekfy2b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/4mekfy2b
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SP500_H10Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/4mekfy2b
>>>>>>>start training : long_term_forecast_Autoformer_SP500_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 10
============================================================
train 4234
val 230
test 231
Epoch: 1, Steps: 133 | Train Loss: 0.2235763 Vali Loss: 0.0748921 Test Loss: 0.0842203
Validation loss decreased (inf --> 0.074892).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.22357633004063054, 'val/loss': 0.07489207480102777, 'test/loss': 0.0842202641069889, '_timestamp': 1762335720.1540394}).
Epoch: 2, Steps: 133 | Train Loss: 0.1923578 Vali Loss: 0.0747720 Test Loss: 0.0849686
Validation loss decreased (0.074892 --> 0.074772).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.1854841 Vali Loss: 0.0729875 Test Loss: 0.0816993
Validation loss decreased (0.074772 --> 0.072988).  Saving model ...
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19235778112608687, 'val/loss': 0.07477195374667645, 'test/loss': 0.08496862184256315, '_timestamp': 1762335728.1805992}).
Epoch: 4, Steps: 133 | Train Loss: 0.1814150 Vali Loss: 0.0723868 Test Loss: 0.0814486
Validation loss decreased (0.072988 --> 0.072387).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.1807510 Vali Loss: 0.0725403 Test Loss: 0.0812439
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.1791257 Vali Loss: 0.0713995 Test Loss: 0.0811244
Validation loss decreased (0.072387 --> 0.071399).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.1792823 Vali Loss: 0.0734195 Test Loss: 0.0806942
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.1793818 Vali Loss: 0.0696649 Test Loss: 0.0804601
Validation loss decreased (0.071399 --> 0.069665).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.1787618 Vali Loss: 0.0700924 Test Loss: 0.0805012
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.1785510 Vali Loss: 0.0699686 Test Loss: 0.0804566
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.1785060 Vali Loss: 0.0718874 Test Loss: 0.0804551
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.1796542 Vali Loss: 0.0697865 Test Loss: 0.0804128
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.1790986 Vali Loss: 0.0724043 Test Loss: 0.0804254
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.1785089 Vali Loss: 0.0707116 Test Loss: 0.0804249
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.1781321 Vali Loss: 0.0696310 Test Loss: 0.0804250
Validation loss decreased (0.069665 --> 0.069631).  Saving model ...
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.1787075 Vali Loss: 0.0693088 Test Loss: 0.0804248
Validation loss decreased (0.069631 --> 0.069309).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.1819125 Vali Loss: 0.0710707 Test Loss: 0.0804254
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.1788755 Vali Loss: 0.0707310 Test Loss: 0.0804252
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.1784596 Vali Loss: 0.0735660 Test Loss: 0.0804252
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.1795703 Vali Loss: 0.0720549 Test Loss: 0.0804253
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.1786442 Vali Loss: 0.0743965 Test Loss: 0.0804252
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.1792090 Vali Loss: 0.0718603 Test Loss: 0.0804252
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.1784581 Vali Loss: 0.0694334 Test Loss: 0.0804254
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.1785195 Vali Loss: 0.0703826 Test Loss: 0.0804252
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.1804561 Vali Loss: 0.0701928 Test Loss: 0.0804253
EarlyStopping counter: 9 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.1792541 Vali Loss: 0.0698197 Test Loss: 0.0804253
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SP500_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 231
test shape: (231, 10, 1) (231, 10, 1)
test shape: (231, 10, 1) (231, 10, 1)


	mse:6.765079160686582e-05, mae:0.00612590042874217, rmse:0.00822500977665186, r2:-0.04073667526245117, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0082, RÂ²: -0.0407, MAPE: 2.92%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.559 MB of 0.560 MB uploadedwandb: \ 0.559 MB of 0.560 MB uploadedwandb: | 0.560 MB of 0.560 MB uploadedwandb: / 0.560 MB of 0.777 MB uploadedwandb: - 0.560 MB of 0.777 MB uploadedwandb: \ 0.777 MB of 0.777 MB uploadedwandb: | 0.777 MB of 0.777 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‡â–†â–…â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–‚â–‚â–â–â–‚â–…â–‚â–â–‚â–â–‚â–â–â–ƒâ–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–…â–…â–„â–‡â–â–‚â–‚â–…â–‚â–…â–ƒâ–â–â–ƒâ–ƒâ–‡â–…â–ˆâ–…â–â–‚â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 25
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.08043
wandb:                 train/loss 0.17925
wandb:   val/directional_accuracy 46.27225
wandb:                   val/loss 0.06982
wandb:                    val/mae 0.00613
wandb:                   val/mape 292.32848
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.04074
wandb:                   val/rmse 0.00823
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/4mekfy2b
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_114144-4mekfy2b/logs
Completed: SP500 H=10

Training: Autoformer on SP500 for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_114527-o3c09iez
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/o3c09iez
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SP500_H22Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/o3c09iez
>>>>>>>start training : long_term_forecast_Autoformer_SP500_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 22
============================================================
train 4222
val 218
test 219
Epoch: 1, Steps: 132 | Train Loss: 0.2191711 Vali Loss: 0.0726866 Test Loss: 0.0730067
Validation loss decreased (inf --> 0.072687).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.1901417 Vali Loss: 0.0749931 Test Loss: 0.0737332
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.21917113097328128, 'val/loss': 0.07268660249454635, 'test/loss': 0.07300672573702675, '_timestamp': 1762335945.2229326}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.19014174378279483, 'val/loss': 0.07499305052416665, 'test/loss': 0.07373316532799176, '_timestamp': 1762335952.373726}).
Epoch: 3, Steps: 132 | Train Loss: 0.1841718 Vali Loss: 0.0745236 Test Loss: 0.0746339
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.1818750 Vali Loss: 0.0721823 Test Loss: 0.0728676
Validation loss decreased (0.072687 --> 0.072182).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.1807037 Vali Loss: 0.0720563 Test Loss: 0.0727388
Validation loss decreased (0.072182 --> 0.072056).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.1801669 Vali Loss: 0.0720280 Test Loss: 0.0732927
Validation loss decreased (0.072056 --> 0.072028).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.1800263 Vali Loss: 0.0723537 Test Loss: 0.0728049
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.1797467 Vali Loss: 0.0721521 Test Loss: 0.0730124
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.1793723 Vali Loss: 0.0722334 Test Loss: 0.0726928
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.1793698 Vali Loss: 0.0720247 Test Loss: 0.0728141
Validation loss decreased (0.072028 --> 0.072025).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.1794057 Vali Loss: 0.0721218 Test Loss: 0.0728584
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.1794161 Vali Loss: 0.0724024 Test Loss: 0.0728401
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.1794055 Vali Loss: 0.0720121 Test Loss: 0.0728260
Validation loss decreased (0.072025 --> 0.072012).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.1793686 Vali Loss: 0.0721301 Test Loss: 0.0728283
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.1794320 Vali Loss: 0.0721398 Test Loss: 0.0728302
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.1794634 Vali Loss: 0.0720810 Test Loss: 0.0728303
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.1794515 Vali Loss: 0.0720614 Test Loss: 0.0728305
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 132 | Train Loss: 0.1795207 Vali Loss: 0.0719500 Test Loss: 0.0728304
Validation loss decreased (0.072012 --> 0.071950).  Saving model ...
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 132 | Train Loss: 0.1795919 Vali Loss: 0.0719991 Test Loss: 0.0728302
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 132 | Train Loss: 0.1794534 Vali Loss: 0.0720003 Test Loss: 0.0728304
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 132 | Train Loss: 0.1794624 Vali Loss: 0.0719586 Test Loss: 0.0728303
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 132 | Train Loss: 0.1793991 Vali Loss: 0.0721529 Test Loss: 0.0728302
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 132 | Train Loss: 0.1795064 Vali Loss: 0.0720495 Test Loss: 0.0728302
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 132 | Train Loss: 0.1796683 Vali Loss: 0.0721217 Test Loss: 0.0728303
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 132 | Train Loss: 0.1794716 Vali Loss: 0.0720713 Test Loss: 0.0728302
EarlyStopping counter: 7 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 132 | Train Loss: 0.1794240 Vali Loss: 0.0719723 Test Loss: 0.0728302
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.9802322387695314e-12
Epoch: 27, Steps: 132 | Train Loss: 0.1794297 Vali Loss: 0.0721960 Test Loss: 0.0728302
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.4901161193847657e-12
Epoch: 28, Steps: 132 | Train Loss: 0.1795293 Vali Loss: 0.0722878 Test Loss: 0.0728302
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SP500_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 219
test shape: (219, 22, 1) (219, 22, 1)
test shape: (219, 22, 1) (219, 22, 1)


	mse:6.661019142484292e-05, mae:0.006063314154744148, rmse:0.008161506615579128, r2:-0.043303847312927246, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0082, RÂ²: -0.0433, MAPE: 3.68%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.647 MB of 0.648 MB uploadedwandb: \ 0.647 MB of 0.648 MB uploadedwandb: | 0.647 MB of 0.648 MB uploadedwandb: / 0.648 MB of 0.648 MB uploadedwandb: - 0.648 MB of 0.648 MB uploadedwandb: \ 0.648 MB of 0.865 MB uploadedwandb: | 0.865 MB of 0.865 MB uploadedwandb: / 0.865 MB of 0.865 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–â–ƒâ–â–‚â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‚â–â–â–‚â–‚â–‚â–â–â–‚â–â–â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–‚â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 27
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.07283
wandb:                 train/loss 0.17953
wandb:   val/directional_accuracy 49.53251
wandb:                   val/loss 0.07229
wandb:                    val/mae 0.00606
wandb:                   val/mape 367.56077
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.0433
wandb:                   val/rmse 0.00816
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/o3c09iez
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_114527-o3c09iez/logs
Completed: SP500 H=22

Training: Autoformer on SP500 for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_114915-o3tg3ncn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/o3tg3ncn
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SP500_H50Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/o3tg3ncn
>>>>>>>start training : long_term_forecast_Autoformer_SP500_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 50
============================================================
train 4194
val 190
test 191
Epoch: 1, Steps: 132 | Train Loss: 0.2199752 Vali Loss: 0.0776398 Test Loss: 0.0816611
Validation loss decreased (inf --> 0.077640).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.1929109 Vali Loss: 0.0743075 Test Loss: 0.0803417
Validation loss decreased (0.077640 --> 0.074308).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.21997522173280065, 'val/loss': 0.07763983185092609, 'test/loss': 0.08166109770536423, '_timestamp': 1762336170.2655506}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.1929109473007195, 'val/loss': 0.07430751870075862, 'test/loss': 0.08034169984360535, '_timestamp': 1762336177.3100126}).
Epoch: 3, Steps: 132 | Train Loss: 0.1882150 Vali Loss: 0.0751737 Test Loss: 0.0788048
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.1855439 Vali Loss: 0.0728787 Test Loss: 0.0746163
Validation loss decreased (0.074308 --> 0.072879).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.1909301 Vali Loss: 0.0730557 Test Loss: 0.0763648
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.1845075 Vali Loss: 0.0732721 Test Loss: 0.0767777
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.1843090 Vali Loss: 0.0727572 Test Loss: 0.0758153
Validation loss decreased (0.072879 --> 0.072757).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.1893162 Vali Loss: 0.0728863 Test Loss: 0.0763520
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.1854066 Vali Loss: 0.0729080 Test Loss: 0.0761730
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.1838796 Vali Loss: 0.0728686 Test Loss: 0.0760923
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.1838322 Vali Loss: 0.0728465 Test Loss: 0.0760849
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.1837612 Vali Loss: 0.0728572 Test Loss: 0.0760663
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.1841639 Vali Loss: 0.0728964 Test Loss: 0.0760394
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.1841996 Vali Loss: 0.0728757 Test Loss: 0.0760363
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.1858346 Vali Loss: 0.0728560 Test Loss: 0.0760366
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.1873193 Vali Loss: 0.0728393 Test Loss: 0.0760381
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 132 | Train Loss: 0.1840957 Vali Loss: 0.0728701 Test Loss: 0.0760389
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SP500_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 191
test shape: (191, 50, 1) (191, 50, 1)
test shape: (191, 50, 1) (191, 50, 1)


	mse:6.615891470573843e-05, mae:0.006013923790305853, rmse:0.008133812807500362, r2:-0.01732027530670166, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0060, RMSE: 0.0081, RÂ²: -0.0173, MAPE: 2.91%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.704 MB of 0.707 MB uploadedwandb: \ 0.704 MB of 0.707 MB uploadedwandb: | 0.707 MB of 0.707 MB uploadedwandb: / 0.707 MB of 0.707 MB uploadedwandb: - 0.707 MB of 0.923 MB uploadedwandb: \ 0.923 MB of 0.923 MB uploadedwandb: | 0.923 MB of 0.923 MB uploadedwandb: / 0.923 MB of 0.923 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–â–„â–…â–ƒâ–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–…â–ƒâ–ˆâ–‚â–‚â–†â–ƒâ–â–â–â–â–â–ƒâ–„â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 16
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.07604
wandb:                 train/loss 0.1841
wandb:   val/directional_accuracy 51.34096
wandb:                   val/loss 0.07287
wandb:                    val/mae 0.00601
wandb:                   val/mape 290.97168
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.01732
wandb:                   val/rmse 0.00813
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/o3tg3ncn
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_114915-o3tg3ncn/logs
Completed: SP500 H=50

Training: Autoformer on SP500 for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_115154-mevvakl9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/mevvakl9
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SP500_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SP500_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/mevvakl9
>>>>>>>start training : long_term_forecast_Autoformer_SP500_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4782
Train: 4303 samples (90%) - rows 0 to 4302
Val: 239 samples (5%) - rows 4303 to 4541
Test: 240 samples (5%) - rows 4542 to 4781
Sequence length: 60, Prediction length: 100
============================================================
train 4144
val 140
test 141
Epoch: 1, Steps: 130 | Train Loss: 0.2300266 Vali Loss: 0.0695385 Test Loss: 0.0839049
Validation loss decreased (inf --> 0.069538).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.23002655792694826, 'val/loss': 0.06953848302364349, 'test/loss': 0.08390492647886276, '_timestamp': 1762336328.6521544}).
Epoch: 2, Steps: 130 | Train Loss: 0.2034308 Vali Loss: 0.0678986 Test Loss: 0.0838680
Validation loss decreased (0.069538 --> 0.067899).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.1985523 Vali Loss: 0.0697889 Test Loss: 0.0848211
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.20343084868330222, 'val/loss': 0.06789856404066086, 'test/loss': 0.08386795669794082, '_timestamp': 1762336336.9578822}).
Epoch: 4, Steps: 130 | Train Loss: 0.1972315 Vali Loss: 0.0687324 Test Loss: 0.0839486
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 130 | Train Loss: 0.1959496 Vali Loss: 0.0693612 Test Loss: 0.0845648
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.1959847 Vali Loss: 0.0691301 Test Loss: 0.0843353
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.1968982 Vali Loss: 0.0702870 Test Loss: 0.0855159
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.1954287 Vali Loss: 0.0695630 Test Loss: 0.0848072
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.1958225 Vali Loss: 0.0694748 Test Loss: 0.0849375
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.1955819 Vali Loss: 0.0696938 Test Loss: 0.0850322
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.1958283 Vali Loss: 0.0698190 Test Loss: 0.0850800
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.1954254 Vali Loss: 0.0693189 Test Loss: 0.0850465
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SP500_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 141
test shape: (141, 100, 1) (141, 100, 1)
test shape: (141, 100, 1) (141, 100, 1)


	mse:6.914738332852721e-05, mae:0.006139859557151794, rmse:0.008315490558743477, r2:-0.009328007698059082, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0061, RMSE: 0.0083, RÂ²: -0.0093, MAPE: 3.41%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.791 MB of 0.796 MB uploadedwandb: \ 0.791 MB of 0.796 MB uploadedwandb: | 0.791 MB of 0.796 MB uploadedwandb: / 0.796 MB of 0.796 MB uploadedwandb: - 0.796 MB of 1.011 MB uploadedwandb: \ 0.796 MB of 1.011 MB uploadedwandb: | 1.011 MB of 1.011 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–â–„â–ƒâ–ˆâ–…â–…â–†â–†â–†
wandb:                 train/loss â–ˆâ–…â–‚â–‚â–„â–â–‚â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–â–„â–ƒâ–ˆâ–…â–„â–…â–†â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.08505
wandb:                 train/loss 0.19543
wandb:   val/directional_accuracy 53.42073
wandb:                   val/loss 0.06932
wandb:                    val/mae 0.00614
wandb:                   val/mape 341.19997
wandb:                    val/mse 7e-05
wandb:                     val/r2 -0.00933
wandb:                   val/rmse 0.00832
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500/runs/mevvakl9
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SP500
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_115154-mevvakl9/logs
Completed: SP500 H=100

Training: Autoformer on NASDAQ for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_115359-55n8l1hm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/55n8l1hm
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NASDAQ_H3Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/55n8l1hm
>>>>>>>start training : long_term_forecast_Autoformer_NASDAQ_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 237
Epoch: 1, Steps: 133 | Train Loss: 0.3219785 Vali Loss: 0.1676580 Test Loss: 0.1560067
Validation loss decreased (inf --> 0.167658).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2746880 Vali Loss: 0.1444808 Test Loss: 0.1391101
Validation loss decreased (0.167658 --> 0.144481).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.321978518053105, 'val/loss': 0.16765804775059223, 'test/loss': 0.15600671712309122, '_timestamp': 1762336454.8909945}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2746879861767131, 'val/loss': 0.14448076765984297, 'test/loss': 0.139110098592937, '_timestamp': 1762336461.8977451}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2746879861767131, 'val/loss': 0.14448076765984297, 'test/loss': 0.139110098592937, '_timestamp': 1762336461.8977451}).
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
Epoch: 3, Steps: 133 | Train Loss: 0.2586522 Vali Loss: 0.1432642 Test Loss: 0.1304620
Validation loss decreased (0.144481 --> 0.143264).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2503965 Vali Loss: 0.1391363 Test Loss: 0.1292247
Validation loss decreased (0.143264 --> 0.139136).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2455394 Vali Loss: 0.1362014 Test Loss: 0.1271399
Validation loss decreased (0.139136 --> 0.136201).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2432290 Vali Loss: 0.1389531 Test Loss: 0.1268274
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2411296 Vali Loss: 0.1371084 Test Loss: 0.1262976
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2403659 Vali Loss: 0.1339716 Test Loss: 0.1262675
Validation loss decreased (0.136201 --> 0.133972).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2403286 Vali Loss: 0.1324156 Test Loss: 0.1267580
Validation loss decreased (0.133972 --> 0.132416).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2407079 Vali Loss: 0.1353689 Test Loss: 0.1268387
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2391335 Vali Loss: 0.1332217 Test Loss: 0.1268436
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2407208 Vali Loss: 0.1354900 Test Loss: 0.1268498
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2398894 Vali Loss: 0.1383615 Test Loss: 0.1268474
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2398997 Vali Loss: 0.1358395 Test Loss: 0.1268442
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2393710 Vali Loss: 0.1368046 Test Loss: 0.1268432
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2402642 Vali Loss: 0.1355399 Test Loss: 0.1268426
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2406100 Vali Loss: 0.1354660 Test Loss: 0.1268422
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2395167 Vali Loss: 0.1355250 Test Loss: 0.1268422
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2393914 Vali Loss: 0.1376851 Test Loss: 0.1268422
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NASDAQ_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 237
test shape: (237, 3, 1) (237, 3, 1)
test shape: (237, 3, 1) (237, 3, 1)


	mse:0.00014092863420955837, mae:0.008712934330105782, rmse:0.011871336959302425, r2:-0.03541147708892822, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0087, RMSE: 0.0119, RÂ²: -0.0354, MAPE: 5285813.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.498 MB of 0.498 MB uploadedwandb: \ 0.498 MB of 0.498 MB uploadedwandb: | 0.498 MB of 0.498 MB uploadedwandb: / 0.498 MB of 0.498 MB uploadedwandb: - 0.498 MB of 0.498 MB uploadedwandb: \ 0.498 MB of 0.498 MB uploadedwandb: | 0.498 MB of 0.498 MB uploadedwandb: / 0.498 MB of 0.498 MB uploadedwandb: - 0.498 MB of 0.498 MB uploadedwandb: \ 0.498 MB of 0.498 MB uploadedwandb: | 0.628 MB of 0.844 MB uploaded (0.002 MB deduped)wandb: / 0.844 MB of 0.844 MB uploaded (0.002 MB deduped)wandb: - 0.844 MB of 0.844 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–†â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–‚â–â–‚â–â–â–â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–…â–ƒâ–…â–„â–‚â–â–ƒâ–‚â–ƒâ–…â–ƒâ–„â–ƒâ–ƒâ–ƒâ–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 18
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.12684
wandb:                 train/loss 0.23939
wandb:   val/directional_accuracy 51.47679
wandb:                   val/loss 0.13769
wandb:                    val/mae 0.00871
wandb:                   val/mape 528581300.0
wandb:                    val/mse 0.00014
wandb:                     val/r2 -0.03541
wandb:                   val/rmse 0.01187
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/55n8l1hm
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_115359-55n8l1hm/logs
Completed: NASDAQ H=3

Training: Autoformer on NASDAQ for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_115702-mq7bx5wr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/mq7bx5wr
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NASDAQ_H5Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/mq7bx5wr
>>>>>>>start training : long_term_forecast_Autoformer_NASDAQ_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 235
Epoch: 1, Steps: 133 | Train Loss: 0.3077539 Vali Loss: 0.1536355 Test Loss: 0.1453563
Validation loss decreased (inf --> 0.153635).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2702460 Vali Loss: 0.1544486 Test Loss: 0.1475836
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3077538656560998, 'val/loss': 0.15363547578454018, 'test/loss': 0.14535633381456137, '_timestamp': 1762336640.2434583}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.27024602509082707, 'val/loss': 0.1544486377388239, 'test/loss': 0.14758356474339962, '_timestamp': 1762336647.2522414}).
Epoch: 3, Steps: 133 | Train Loss: 0.2552717 Vali Loss: 0.1507019 Test Loss: 0.1356350
Validation loss decreased (0.153635 --> 0.150702).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2480155 Vali Loss: 0.1456548 Test Loss: 0.1352343
Validation loss decreased (0.150702 --> 0.145655).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2448904 Vali Loss: 0.1365259 Test Loss: 0.1332162
Validation loss decreased (0.145655 --> 0.136526).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2425077 Vali Loss: 0.1374088 Test Loss: 0.1331342
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2418591 Vali Loss: 0.1467672 Test Loss: 0.1343131
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2422004 Vali Loss: 0.1470836 Test Loss: 0.1333892
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2419760 Vali Loss: 0.1380568 Test Loss: 0.1335157
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2412283 Vali Loss: 0.1467414 Test Loss: 0.1336272
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2408960 Vali Loss: 0.1387393 Test Loss: 0.1336459
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2419009 Vali Loss: 0.1425198 Test Loss: 0.1336942
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2416557 Vali Loss: 0.1483965 Test Loss: 0.1337182
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2414631 Vali Loss: 0.1391320 Test Loss: 0.1337171
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2415906 Vali Loss: 0.1465464 Test Loss: 0.1337178
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NASDAQ_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 235
test shape: (235, 5, 1) (235, 5, 1)
test shape: (235, 5, 1) (235, 5, 1)


	mse:0.00014892379113007337, mae:0.008927647955715656, rmse:0.01220343355089426, r2:-0.08860111236572266, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0089, RMSE: 0.0122, RÂ²: -0.0886, MAPE: 5508348.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.536 MB of 0.536 MB uploadedwandb: \ 0.536 MB of 0.536 MB uploadedwandb: | 0.536 MB of 0.536 MB uploadedwandb: / 0.536 MB of 0.536 MB uploadedwandb: - 0.536 MB of 0.752 MB uploadedwandb: \ 0.752 MB of 0.752 MB uploadedwandb: | 0.752 MB of 0.752 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‡â–â–â–„â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒ
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–â–‚â–‚â–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–â–â–†â–†â–‚â–†â–‚â–„â–‡â–‚â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.13372
wandb:                 train/loss 0.24159
wandb:   val/directional_accuracy 49.14894
wandb:                   val/loss 0.14655
wandb:                    val/mae 0.00893
wandb:                   val/mape 550834850.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.0886
wandb:                   val/rmse 0.0122
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/mq7bx5wr
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_115702-mq7bx5wr/logs
Completed: NASDAQ H=5

Training: Autoformer on NASDAQ for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_115931-v92idn2d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/v92idn2d
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NASDAQ_H10Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/v92idn2d
>>>>>>>start training : long_term_forecast_Autoformer_NASDAQ_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 230
Epoch: 1, Steps: 133 | Train Loss: 0.2956630 Vali Loss: 0.1606172 Test Loss: 0.1466465
Validation loss decreased (inf --> 0.160617).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.2619469 Vali Loss: 0.1530751 Test Loss: 0.1410232
Validation loss decreased (0.160617 --> 0.153075).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29566298492420884, 'val/loss': 0.16061720997095108, 'test/loss': 0.14664645958691835, '_timestamp': 1762336786.2982435}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2619469178126271, 'val/loss': 0.1530750971287489, 'test/loss': 0.14102323725819588, '_timestamp': 1762336793.3401408}).
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29566298492420884, 'val/loss': 0.16061720997095108, 'test/loss': 0.14664645958691835, '_timestamp': 1762336786.2982435}).
Epoch: 3, Steps: 133 | Train Loss: 0.2513856 Vali Loss: 0.1486515 Test Loss: 0.1411872
Validation loss decreased (0.153075 --> 0.148652).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2461038 Vali Loss: 0.1457725 Test Loss: 0.1388367
Validation loss decreased (0.148652 --> 0.145772).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2447373 Vali Loss: 0.1439506 Test Loss: 0.1375261
Validation loss decreased (0.145772 --> 0.143951).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2424675 Vali Loss: 0.1441450 Test Loss: 0.1374278
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2411863 Vali Loss: 0.1595733 Test Loss: 0.1384854
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2425590 Vali Loss: 0.1468912 Test Loss: 0.1384267
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2423357 Vali Loss: 0.1484738 Test Loss: 0.1383311
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2405984 Vali Loss: 0.1431232 Test Loss: 0.1381781
Validation loss decreased (0.143951 --> 0.143123).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2401247 Vali Loss: 0.1542589 Test Loss: 0.1381228
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2414013 Vali Loss: 0.1469757 Test Loss: 0.1381344
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2410019 Vali Loss: 0.1483414 Test Loss: 0.1381170
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2421216 Vali Loss: 0.1471060 Test Loss: 0.1381213
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2412728 Vali Loss: 0.1463385 Test Loss: 0.1381180
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2408899 Vali Loss: 0.1456108 Test Loss: 0.1381181
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2422381 Vali Loss: 0.1477975 Test Loss: 0.1381192
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2401343 Vali Loss: 0.1484681 Test Loss: 0.1381197
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2411129 Vali Loss: 0.1581395 Test Loss: 0.1381198
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2415176 Vali Loss: 0.1589218 Test Loss: 0.1381195
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NASDAQ_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 230
test shape: (230, 10, 1) (230, 10, 1)
test shape: (230, 10, 1) (230, 10, 1)


	mse:0.00014999235281720757, mae:0.009063391014933586, rmse:0.012247136794030666, r2:-0.08355557918548584, dtw:Not calculated


VAL - MSE: 0.0001, MAE: 0.0091, RMSE: 0.0122, RÂ²: -0.0836, MAPE: 7552914.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.612 MB of 0.612 MB uploadedwandb: \ 0.612 MB of 0.612 MB uploadedwandb: | 0.612 MB of 0.612 MB uploadedwandb: / 0.612 MB of 0.612 MB uploadedwandb: - 0.612 MB of 0.829 MB uploadedwandb: \ 0.829 MB of 0.829 MB uploadedwandb: | 0.829 MB of 0.829 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–â–â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–ƒâ–‚â–â–â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ƒâ–‚â–â–â–ˆâ–ƒâ–ƒâ–â–†â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–‡â–ˆ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.13812
wandb:                 train/loss 0.24152
wandb:   val/directional_accuracy 49.13043
wandb:                   val/loss 0.15892
wandb:                    val/mae 0.00906
wandb:                   val/mape 755291450.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.08356
wandb:                   val/rmse 0.01225
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/v92idn2d
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_115931-v92idn2d/logs
Completed: NASDAQ H=10

Training: Autoformer on NASDAQ for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_120231-1ua6blsx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/1ua6blsx
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NASDAQ_H22Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/1ua6blsx
>>>>>>>start training : long_term_forecast_Autoformer_NASDAQ_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 218
Epoch: 1, Steps: 132 | Train Loss: 0.2921134 Vali Loss: 0.1570803 Test Loss: 0.1333307
Validation loss decreased (inf --> 0.157080).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29211341600978014, 'val/loss': 0.1570802628993988, 'test/loss': 0.13333067723682948, '_timestamp': 1762336964.6694446}).
Epoch: 2, Steps: 132 | Train Loss: 0.2601581 Vali Loss: 0.1529209 Test Loss: 0.1367752
Validation loss decreased (0.157080 --> 0.152921).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 132 | Train Loss: 0.2506390 Vali Loss: 0.1573927 Test Loss: 0.1384223
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2457470 Vali Loss: 0.1557856 Test Loss: 0.1382714
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26015814382470015, 'val/loss': 0.15292087197303772, 'test/loss': 0.1367752445595605, '_timestamp': 1762336972.7333336}).
Epoch: 5, Steps: 132 | Train Loss: 0.2432760 Vali Loss: 0.1552082 Test Loss: 0.1384789
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2428357 Vali Loss: 0.1555922 Test Loss: 0.1375711
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2412143 Vali Loss: 0.1551588 Test Loss: 0.1373521
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2411603 Vali Loss: 0.1554365 Test Loss: 0.1383655
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2410207 Vali Loss: 0.1553793 Test Loss: 0.1386639
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2409940 Vali Loss: 0.1550263 Test Loss: 0.1387107
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2406151 Vali Loss: 0.1557269 Test Loss: 0.1387569
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2409203 Vali Loss: 0.1548354 Test Loss: 0.1387682
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NASDAQ_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 218
test shape: (218, 22, 1) (218, 22, 1)
test shape: (218, 22, 1) (218, 22, 1)


	mse:0.00015165656805038452, mae:0.009013224393129349, rmse:0.012314892373979092, r2:-0.08383810520172119, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0090, RMSE: 0.0123, RÂ²: -0.0838, MAPE: 4102418.25%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.654 MB of 0.655 MB uploadedwandb: \ 0.654 MB of 0.655 MB uploadedwandb: | 0.655 MB of 0.655 MB uploadedwandb: / 0.655 MB of 0.655 MB uploadedwandb: - 0.655 MB of 0.870 MB uploadedwandb: \ 0.870 MB of 0.870 MB uploadedwandb: | 0.870 MB of 0.870 MB uploadedwandb: / 0.870 MB of 0.870 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–†â–†â–‡â–‚â–â–†â–‡â–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–ƒâ–
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.13877
wandb:                 train/loss 0.24092
wandb:   val/directional_accuracy 48.60201
wandb:                   val/loss 0.15484
wandb:                    val/mae 0.00901
wandb:                   val/mape 410241825.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.08384
wandb:                   val/rmse 0.01231
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/1ua6blsx
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_120231-1ua6blsx/logs
Completed: NASDAQ H=22

Training: Autoformer on NASDAQ for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_120448-1as1t7iv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/1as1t7iv
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NASDAQ_H50Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/1as1t7iv
>>>>>>>start training : long_term_forecast_Autoformer_NASDAQ_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 190
Epoch: 1, Steps: 132 | Train Loss: 0.2907474 Vali Loss: 0.1777459 Test Loss: 0.1392463
Validation loss decreased (inf --> 0.177746).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.2621666 Vali Loss: 0.1705515 Test Loss: 0.1449143
Validation loss decreased (0.177746 --> 0.170552).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2907473652651816, 'val/loss': 0.17774590104818344, 'test/loss': 0.1392462986210982, '_timestamp': 1762337102.8824348}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26216655269716727, 'val/loss': 0.1705515185991923, 'test/loss': 0.14491425330440202, '_timestamp': 1762337110.0151017}).
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2907473652651816, 'val/loss': 0.17774590104818344, 'test/loss': 0.1392462986210982, '_timestamp': 1762337102.8824348}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.26216655269716727, 'val/loss': 0.1705515185991923, 'test/loss': 0.14491425330440202, '_timestamp': 1762337110.0151017}).
Epoch: 3, Steps: 132 | Train Loss: 0.2552156 Vali Loss: 0.1711851 Test Loss: 0.1390946
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.2515098 Vali Loss: 0.1700738 Test Loss: 0.1390074
Validation loss decreased (0.170552 --> 0.170074).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2506525 Vali Loss: 0.1697178 Test Loss: 0.1390705
Validation loss decreased (0.170074 --> 0.169718).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2484890 Vali Loss: 0.1694346 Test Loss: 0.1384091
Validation loss decreased (0.169718 --> 0.169435).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2472235 Vali Loss: 0.1703565 Test Loss: 0.1380391
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2470793 Vali Loss: 0.1705975 Test Loss: 0.1382447
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2475266 Vali Loss: 0.1709388 Test Loss: 0.1381063
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2478766 Vali Loss: 0.1710706 Test Loss: 0.1381884
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2475741 Vali Loss: 0.1708521 Test Loss: 0.1382285
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2475518 Vali Loss: 0.1707978 Test Loss: 0.1382342
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 132 | Train Loss: 0.2471244 Vali Loss: 0.1707710 Test Loss: 0.1382520
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 132 | Train Loss: 0.2466721 Vali Loss: 0.1707910 Test Loss: 0.1382572
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 132 | Train Loss: 0.2468302 Vali Loss: 0.1708354 Test Loss: 0.1382608
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 132 | Train Loss: 0.2476244 Vali Loss: 0.1705881 Test Loss: 0.1382643
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NASDAQ_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 190
test shape: (190, 50, 1) (190, 50, 1)
test shape: (190, 50, 1) (190, 50, 1)


	mse:0.0001525919942650944, mae:0.008963336236774921, rmse:0.01235281303524971, r2:-0.05348014831542969, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0090, RMSE: 0.0124, RÂ²: -0.0535, MAPE: 6231621.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.717 MB of 0.719 MB uploadedwandb: \ 0.717 MB of 0.719 MB uploadedwandb: | 0.717 MB of 0.719 MB uploadedwandb: / 0.719 MB of 0.719 MB uploadedwandb: - 0.719 MB of 0.935 MB uploadedwandb: \ 0.719 MB of 0.935 MB uploadedwandb: | 0.935 MB of 0.935 MB uploadedwandb: / 0.935 MB of 0.935 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‡â–ˆâ–ƒâ–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–„â–‚â–â–â–‚â–‚â–‚â–‚â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–„â–‚â–â–…â–†â–‡â–ˆâ–‡â–†â–†â–†â–‡â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 15
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.13826
wandb:                 train/loss 0.24762
wandb:   val/directional_accuracy 53.23308
wandb:                   val/loss 0.17059
wandb:                    val/mae 0.00896
wandb:                   val/mape 623162150.0
wandb:                    val/mse 0.00015
wandb:                     val/r2 -0.05348
wandb:                   val/rmse 0.01235
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/1as1t7iv
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_120448-1as1t7iv/logs
Completed: NASDAQ H=50

Training: Autoformer on NASDAQ for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_120713-02kyzsg7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/02kyzsg7
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_NASDAQ_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          NASDAQ_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/02kyzsg7
>>>>>>>start training : long_term_forecast_Autoformer_NASDAQ_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4780
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 239 samples (5%) - rows 4541 to 4779
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 140
Epoch: 1, Steps: 130 | Train Loss: 0.2994045 Vali Loss: 0.1910580 Test Loss: 0.1444535
Validation loss decreased (inf --> 0.191058).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2994044756660095, 'val/loss': 0.191057950258255, 'test/loss': 0.14445347636938094, '_timestamp': 1762337245.8478835}).
Epoch: 2, Steps: 130 | Train Loss: 0.2723751 Vali Loss: 0.1798800 Test Loss: 0.1481731
Validation loss decreased (0.191058 --> 0.179880).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.2659621 Vali Loss: 0.1863925 Test Loss: 0.1468453
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.2632424 Vali Loss: 0.1878578 Test Loss: 0.1460874
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2723751182739551, 'val/loss': 0.17988003492355348, 'test/loss': 0.14817313253879547, '_timestamp': 1762337253.9883149}).
Epoch: 5, Steps: 130 | Train Loss: 0.2617058 Vali Loss: 0.1867607 Test Loss: 0.1456662
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.2605025 Vali Loss: 0.1846608 Test Loss: 0.1463524
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.2605256 Vali Loss: 0.1872806 Test Loss: 0.1463714
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.2599839 Vali Loss: 0.1860206 Test Loss: 0.1465174
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.2598643 Vali Loss: 0.1819852 Test Loss: 0.1464781
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.2592712 Vali Loss: 0.1850439 Test Loss: 0.1465103
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.2593645 Vali Loss: 0.1859470 Test Loss: 0.1464856
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 130 | Train Loss: 0.2595324 Vali Loss: 0.1852769 Test Loss: 0.1464868
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_NASDAQ_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 140
test shape: (140, 100, 1) (140, 100, 1)
test shape: (140, 100, 1) (140, 100, 1)


	mse:0.00015787068696226925, mae:0.008816294372081757, rmse:0.012564660049974918, r2:-0.0408555269241333, dtw:Not calculated


VAL - MSE: 0.0002, MAE: 0.0088, RMSE: 0.0126, RÂ²: -0.0409, MAPE: 2920379.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.731 MB of 0.736 MB uploadedwandb: \ 0.731 MB of 0.736 MB uploadedwandb: | 0.736 MB of 0.736 MB uploadedwandb: / 0.736 MB of 0.951 MB uploadedwandb: - 0.736 MB of 0.951 MB uploadedwandb: \ 0.951 MB of 0.951 MB uploadedwandb: | 0.951 MB of 0.951 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–â–…â–…â–†â–†â–†â–†â–†
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–†â–ˆâ–‡â–„â–‡â–†â–â–…â–†â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.14649
wandb:                 train/loss 0.25953
wandb:   val/directional_accuracy 51.79654
wandb:                   val/loss 0.18528
wandb:                    val/mae 0.00882
wandb:                   val/mape 292037950.0
wandb:                    val/mse 0.00016
wandb:                     val/r2 -0.04086
wandb:                   val/rmse 0.01256
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ/runs/02kyzsg7
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-NASDAQ
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_120713-02kyzsg7/logs
Completed: NASDAQ H=100

Training: Autoformer on ABSA for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_120912-k5dxkycn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/k5dxkycn
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ABSA_H3  Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/k5dxkycn
>>>>>>>start training : long_term_forecast_Autoformer_ABSA_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 3
============================================================
train 4240
val 237
test 238
Epoch: 1, Steps: 133 | Train Loss: 0.3692649 Vali Loss: 0.1924892 Test Loss: 0.1777710
Validation loss decreased (inf --> 0.192489).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.3181029 Vali Loss: 0.1783161 Test Loss: 0.1701691
Validation loss decreased (0.192489 --> 0.178316).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.36926491267250894, 'val/loss': 0.1924892384558916, 'test/loss': 0.17777102440595627, '_timestamp': 1762337367.6000943}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.31810291559624493, 'val/loss': 0.17831612844020128, 'test/loss': 0.1701691234484315, '_timestamp': 1762337374.7491474}).
Epoch: 3, Steps: 133 | Train Loss: 0.2996172 Vali Loss: 0.1696818 Test Loss: 0.1598913
Validation loss decreased (0.178316 --> 0.169682).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2865619 Vali Loss: 0.1669392 Test Loss: 0.1594784
Validation loss decreased (0.169682 --> 0.166939).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2832409 Vali Loss: 0.1687484 Test Loss: 0.1583705
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2828341 Vali Loss: 0.1649478 Test Loss: 0.1564461
Validation loss decreased (0.166939 --> 0.164948).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2800595 Vali Loss: 0.1664772 Test Loss: 0.1549399
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2796883 Vali Loss: 0.1638160 Test Loss: 0.1547666
Validation loss decreased (0.164948 --> 0.163816).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2802483 Vali Loss: 0.1636839 Test Loss: 0.1550528
Validation loss decreased (0.163816 --> 0.163684).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2800077 Vali Loss: 0.1646879 Test Loss: 0.1551030
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2798861 Vali Loss: 0.1647002 Test Loss: 0.1551555
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2793113 Vali Loss: 0.1648460 Test Loss: 0.1551576
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2805239 Vali Loss: 0.1594865 Test Loss: 0.1551856
Validation loss decreased (0.163684 --> 0.159487).  Saving model ...
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2801809 Vali Loss: 0.1621773 Test Loss: 0.1551782
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2783553 Vali Loss: 0.1625843 Test Loss: 0.1551745
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2791078 Vali Loss: 0.1648565 Test Loss: 0.1551720
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2799718 Vali Loss: 0.1658691 Test Loss: 0.1551723
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2789736 Vali Loss: 0.1668583 Test Loss: 0.1551722
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2786854 Vali Loss: 0.1660384 Test Loss: 0.1551720
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2785040 Vali Loss: 0.1691259 Test Loss: 0.1551719
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2786408 Vali Loss: 0.1660471 Test Loss: 0.1551717
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2799530 Vali Loss: 0.1620653 Test Loss: 0.1551718
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2785971 Vali Loss: 0.1631273 Test Loss: 0.1551720
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ABSA_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 238
test shape: (238, 3, 1) (238, 3, 1)
test shape: (238, 3, 1) (238, 3, 1)


	mse:0.0004758087743539363, mae:0.016684507951140404, rmse:0.02181304059922695, r2:-0.04490840435028076, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0167, RMSE: 0.0218, RÂ²: -0.0449, MAPE: 1.96%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.494 MB of 0.494 MB uploadedwandb: \ 0.494 MB of 0.494 MB uploadedwandb: | 0.494 MB of 0.494 MB uploadedwandb: / 0.494 MB of 0.494 MB uploadedwandb: - 0.494 MB of 0.494 MB uploadedwandb: \ 0.494 MB of 0.494 MB uploadedwandb: | 0.494 MB of 0.494 MB uploadedwandb: / 0.494 MB of 0.494 MB uploadedwandb: - 0.625 MB of 0.842 MB uploaded (0.002 MB deduped)wandb: \ 0.842 MB of 0.842 MB uploaded (0.002 MB deduped)wandb: | 0.842 MB of 0.842 MB uploaded (0.002 MB deduped)wandb: / 0.842 MB of 0.842 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‡â–†â–ƒâ–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–â–â–‚â–â–â–â–â–‚â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–‡â–…â–†â–„â–„â–…â–…â–…â–â–ƒâ–ƒâ–…â–…â–†â–…â–ˆâ–†â–ƒâ–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 22
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.15517
wandb:                 train/loss 0.2786
wandb:   val/directional_accuracy 48.10924
wandb:                   val/loss 0.16313
wandb:                    val/mae 0.01668
wandb:                   val/mape 195.7739
wandb:                    val/mse 0.00048
wandb:                     val/r2 -0.04491
wandb:                   val/rmse 0.02181
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/k5dxkycn
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_120912-k5dxkycn/logs
Completed: ABSA H=3

Training: Autoformer on ABSA for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_121237-ielmypm6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/ielmypm6
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ABSA_H5  Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/ielmypm6
>>>>>>>start training : long_term_forecast_Autoformer_ABSA_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 5
============================================================
train 4238
val 235
test 236
Epoch: 1, Steps: 133 | Train Loss: 0.3657577 Vali Loss: 0.1931968 Test Loss: 0.1817556
Validation loss decreased (inf --> 0.193197).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.36575771778597865, 'val/loss': 0.1931968405842781, 'test/loss': 0.18175558932125568, '_timestamp': 1762337571.3389463}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3210427407035254, 'val/loss': 0.18820582330226898, 'test/loss': 0.1816343143582344, '_timestamp': 1762337578.35243}).
Epoch: 2, Steps: 133 | Train Loss: 0.3210427 Vali Loss: 0.1882058 Test Loss: 0.1816343
Validation loss decreased (0.193197 --> 0.188206).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 133 | Train Loss: 0.3012665 Vali Loss: 0.1781768 Test Loss: 0.1697632
Validation loss decreased (0.188206 --> 0.178177).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.2929231 Vali Loss: 0.1768187 Test Loss: 0.1669789
Validation loss decreased (0.178177 --> 0.176819).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2898967 Vali Loss: 0.1781504 Test Loss: 0.1709511
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2869836 Vali Loss: 0.1687546 Test Loss: 0.1676758
Validation loss decreased (0.176819 --> 0.168755).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2858485 Vali Loss: 0.1730435 Test Loss: 0.1674438
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2845068 Vali Loss: 0.1754200 Test Loss: 0.1669336
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2862302 Vali Loss: 0.1725122 Test Loss: 0.1666586
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2849909 Vali Loss: 0.1674998 Test Loss: 0.1665905
Validation loss decreased (0.168755 --> 0.167500).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2858050 Vali Loss: 0.1723117 Test Loss: 0.1664827
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2853718 Vali Loss: 0.1739934 Test Loss: 0.1665838
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2851861 Vali Loss: 0.1725085 Test Loss: 0.1666132
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2842437 Vali Loss: 0.1738921 Test Loss: 0.1666684
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2846434 Vali Loss: 0.1731087 Test Loss: 0.1666483
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2855958 Vali Loss: 0.1728686 Test Loss: 0.1666491
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2840066 Vali Loss: 0.1705376 Test Loss: 0.1666486
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2851198 Vali Loss: 0.1745309 Test Loss: 0.1666488
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2841784 Vali Loss: 0.1770689 Test Loss: 0.1666483
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2857539 Vali Loss: 0.1739841 Test Loss: 0.1666482
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ABSA_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 236
test shape: (236, 5, 1) (236, 5, 1)
test shape: (236, 5, 1) (236, 5, 1)


	mse:0.00048358560889028013, mae:0.01679394207894802, rmse:0.02199058048427105, r2:-0.05581212043762207, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0168, RMSE: 0.0220, RÂ²: -0.0558, MAPE: 1.66%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.528 MB of 0.528 MB uploadedwandb: \ 0.528 MB of 0.528 MB uploadedwandb: | 0.528 MB of 0.528 MB uploadedwandb: / 0.528 MB of 0.528 MB uploadedwandb: - 0.528 MB of 0.744 MB uploadedwandb: \ 0.744 MB of 0.744 MB uploadedwandb: | 0.744 MB of 0.744 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–†â–‚â–ˆâ–ƒâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–‚â–â–‚â–‚â–â–â–â–‚â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–‡â–ˆâ–‚â–…â–†â–„â–â–„â–…â–„â–…â–…â–…â–ƒâ–†â–‡â–…
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 19
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.16665
wandb:                 train/loss 0.28575
wandb:   val/directional_accuracy 47.35169
wandb:                   val/loss 0.17398
wandb:                    val/mae 0.01679
wandb:                   val/mape 165.5946
wandb:                    val/mse 0.00048
wandb:                     val/r2 -0.05581
wandb:                   val/rmse 0.02199
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/ielmypm6
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_121237-ielmypm6/logs
Completed: ABSA H=5

Training: Autoformer on ABSA for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_121530-w418wzo7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/w418wzo7
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ABSA_H10 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/w418wzo7
>>>>>>>start training : long_term_forecast_Autoformer_ABSA_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 10
============================================================
train 4233
val 230
test 231
Epoch: 1, Steps: 133 | Train Loss: 0.3583406 Vali Loss: 0.1839347 Test Loss: 0.1734673
Validation loss decreased (inf --> 0.183935).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 133 | Train Loss: 0.3207687 Vali Loss: 0.1980777 Test Loss: 0.1717175
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.35834056833632905, 'val/loss': 0.18393470719456673, 'test/loss': 0.17346727568656206, '_timestamp': 1762337745.1327994}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.32076872952450486, 'val/loss': 0.19807765446603298, 'test/loss': 0.17171749379485846, '_timestamp': 1762337752.1442833}).
Epoch: 3, Steps: 133 | Train Loss: 0.3071223 Vali Loss: 0.1841148 Test Loss: 0.1686533
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 133 | Train Loss: 0.3003927 Vali Loss: 0.1813202 Test Loss: 0.1652222
Validation loss decreased (0.183935 --> 0.181320).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 133 | Train Loss: 0.2985549 Vali Loss: 0.1790882 Test Loss: 0.1647153
Validation loss decreased (0.181320 --> 0.179088).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 133 | Train Loss: 0.2941251 Vali Loss: 0.1824758 Test Loss: 0.1659743
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 133 | Train Loss: 0.2937809 Vali Loss: 0.1847692 Test Loss: 0.1653997
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 133 | Train Loss: 0.2938286 Vali Loss: 0.1779081 Test Loss: 0.1653532
Validation loss decreased (0.179088 --> 0.177908).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 133 | Train Loss: 0.2939734 Vali Loss: 0.1820190 Test Loss: 0.1651583
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 133 | Train Loss: 0.2926915 Vali Loss: 0.1802664 Test Loss: 0.1650653
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 133 | Train Loss: 0.2924701 Vali Loss: 0.1789427 Test Loss: 0.1652431
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 133 | Train Loss: 0.2919867 Vali Loss: 0.1771851 Test Loss: 0.1652738
Validation loss decreased (0.177908 --> 0.177185).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 133 | Train Loss: 0.2933530 Vali Loss: 0.1834159 Test Loss: 0.1652765
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 133 | Train Loss: 0.2942498 Vali Loss: 0.1819611 Test Loss: 0.1652999
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 133 | Train Loss: 0.2924773 Vali Loss: 0.1778391 Test Loss: 0.1652917
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 133 | Train Loss: 0.2912095 Vali Loss: 0.1763737 Test Loss: 0.1652967
Validation loss decreased (0.177185 --> 0.176374).  Saving model ...
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 133 | Train Loss: 0.2954703 Vali Loss: 0.1826978 Test Loss: 0.1652961
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 133 | Train Loss: 0.2929597 Vali Loss: 0.1808715 Test Loss: 0.1652962
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 133 | Train Loss: 0.2929985 Vali Loss: 0.1822302 Test Loss: 0.1652961
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 133 | Train Loss: 0.2916805 Vali Loss: 0.1856824 Test Loss: 0.1652963
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 133 | Train Loss: 0.2928749 Vali Loss: 0.1906733 Test Loss: 0.1652958
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 133 | Train Loss: 0.2924433 Vali Loss: 0.1851749 Test Loss: 0.1652962
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.76837158203125e-11
Epoch: 23, Steps: 133 | Train Loss: 0.2954766 Vali Loss: 0.1802321 Test Loss: 0.1652959
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.384185791015625e-11
Epoch: 24, Steps: 133 | Train Loss: 0.2961123 Vali Loss: 0.1796567 Test Loss: 0.1652960
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.1920928955078126e-11
Epoch: 25, Steps: 133 | Train Loss: 0.2931252 Vali Loss: 0.1786786 Test Loss: 0.1652960
EarlyStopping counter: 9 out of 10
Updating learning rate to 5.960464477539063e-12
Epoch: 26, Steps: 133 | Train Loss: 0.2925369 Vali Loss: 0.1817606 Test Loss: 0.1652960
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ABSA_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 231
test shape: (231, 10, 1) (231, 10, 1)
test shape: (231, 10, 1) (231, 10, 1)


	mse:0.0004867443349212408, mae:0.01681392453610897, rmse:0.022062283009290695, r2:-0.05429863929748535, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0168, RMSE: 0.0221, RÂ²: -0.0543, MAPE: 1.73%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.585 MB of 0.586 MB uploadedwandb: \ 0.585 MB of 0.586 MB uploadedwandb: | 0.585 MB of 0.586 MB uploadedwandb: / 0.586 MB of 0.586 MB uploadedwandb: - 0.586 MB of 0.803 MB uploadedwandb: \ 0.586 MB of 0.803 MB uploadedwandb: | 0.803 MB of 0.803 MB uploadedwandb: / 0.803 MB of 0.803 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–ƒâ–‚â–‚â–â–‚â–‚â–ƒâ–ƒâ–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–…â–ƒâ–‚â–„â–…â–‚â–„â–ƒâ–‚â–â–„â–„â–‚â–â–„â–ƒâ–„â–†â–ˆâ–…â–ƒâ–ƒâ–‚â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 25
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.1653
wandb:                 train/loss 0.29254
wandb:   val/directional_accuracy 52.52525
wandb:                   val/loss 0.18176
wandb:                    val/mae 0.01681
wandb:                   val/mape 173.33132
wandb:                    val/mse 0.00049
wandb:                     val/r2 -0.0543
wandb:                   val/rmse 0.02206
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/w418wzo7
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_121530-w418wzo7/logs
Completed: ABSA H=10

Training: Autoformer on ABSA for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_121902-rnyjyb54
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/rnyjyb54
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ABSA_H22 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/rnyjyb54
>>>>>>>start training : long_term_forecast_Autoformer_ABSA_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 22
============================================================
train 4221
val 218
test 219
Epoch: 1, Steps: 132 | Train Loss: 0.3592382 Vali Loss: 0.1831331 Test Loss: 0.1657678
Validation loss decreased (inf --> 0.183133).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.3257916 Vali Loss: 0.1830869 Test Loss: 0.1632473
Validation loss decreased (0.183133 --> 0.183087).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3592381909715407, 'val/loss': 0.18313309337411607, 'test/loss': 0.16576780804565974, '_timestamp': 1762337955.840145}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.325791570724863, 'val/loss': 0.1830868763583047, 'test/loss': 0.16324728833777563, '_timestamp': 1762337962.816134}).
Epoch: 3, Steps: 132 | Train Loss: 0.3116394 Vali Loss: 0.1919208 Test Loss: 0.1688757
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.3043722 Vali Loss: 0.1884179 Test Loss: 0.1682427
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.2993174 Vali Loss: 0.1892926 Test Loss: 0.1701005
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.2970410 Vali Loss: 0.1888559 Test Loss: 0.1690437
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.2957371 Vali Loss: 0.1889245 Test Loss: 0.1684158
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.2963179 Vali Loss: 0.1891331 Test Loss: 0.1678416
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.2950855 Vali Loss: 0.1882717 Test Loss: 0.1681370
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.2957216 Vali Loss: 0.1882743 Test Loss: 0.1681278
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.2948328 Vali Loss: 0.1884349 Test Loss: 0.1680920
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 132 | Train Loss: 0.2952645 Vali Loss: 0.1888432 Test Loss: 0.1680652
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ABSA_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 219
test shape: (219, 22, 1) (219, 22, 1)
test shape: (219, 22, 1) (219, 22, 1)


	mse:0.00048661098117008805, mae:0.01692788116633892, rmse:0.022059259936213493, r2:-0.038609981536865234, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0169, RMSE: 0.0221, RÂ²: -0.0386, MAPE: 1.71%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.656 MB of 0.657 MB uploadedwandb: \ 0.656 MB of 0.657 MB uploadedwandb: | 0.657 MB of 0.657 MB uploadedwandb: / 0.657 MB of 0.872 MB uploadedwandb: - 0.872 MB of 0.872 MB uploadedwandb: \ 0.872 MB of 0.872 MB uploadedwandb: | 0.872 MB of 0.872 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–„â–‚â–ˆâ–…â–ƒâ–â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–ƒâ–‚â–‚â–ƒâ–â–â–â–‚
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.16807
wandb:                 train/loss 0.29526
wandb:   val/directional_accuracy 51.20678
wandb:                   val/loss 0.18884
wandb:                    val/mae 0.01693
wandb:                   val/mape 171.08269
wandb:                    val/mse 0.00049
wandb:                     val/r2 -0.03861
wandb:                   val/rmse 0.02206
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/rnyjyb54
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_121902-rnyjyb54/logs
Completed: ABSA H=22

Training: Autoformer on ABSA for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_122058-0hxoq7gn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/0hxoq7gn
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ABSA_H50 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/0hxoq7gn
>>>>>>>start training : long_term_forecast_Autoformer_ABSA_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 50
============================================================
train 4193
val 190
test 191
Epoch: 1, Steps: 132 | Train Loss: 0.3697438 Vali Loss: 0.1875352 Test Loss: 0.1688321
Validation loss decreased (inf --> 0.187535).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 132 | Train Loss: 0.3385596 Vali Loss: 0.1954260 Test Loss: 0.1704864
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.3697437715124, 'val/loss': 0.18753521144390106, 'test/loss': 0.1688321183125178, '_timestamp': 1762338074.110479}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.33855961680863844, 'val/loss': 0.19542600959539413, 'test/loss': 0.17048643777767816, '_timestamp': 1762338081.2250512}).
Epoch: 3, Steps: 132 | Train Loss: 0.3269973 Vali Loss: 0.1993974 Test Loss: 0.1723131
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 132 | Train Loss: 0.3220135 Vali Loss: 0.1995559 Test Loss: 0.1732493
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 132 | Train Loss: 0.3179441 Vali Loss: 0.1992775 Test Loss: 0.1756532
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 132 | Train Loss: 0.3162177 Vali Loss: 0.2043269 Test Loss: 0.1729992
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 132 | Train Loss: 0.3149330 Vali Loss: 0.2032234 Test Loss: 0.1733048
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 132 | Train Loss: 0.3142294 Vali Loss: 0.2030196 Test Loss: 0.1739721
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 132 | Train Loss: 0.3146159 Vali Loss: 0.2027943 Test Loss: 0.1737221
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 132 | Train Loss: 0.3154494 Vali Loss: 0.2029140 Test Loss: 0.1737012
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 132 | Train Loss: 0.3154037 Vali Loss: 0.2030619 Test Loss: 0.1736216
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ABSA_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 191
test shape: (191, 50, 1) (191, 50, 1)
test shape: (191, 50, 1) (191, 50, 1)


	mse:0.000520066125318408, mae:0.017736472189426422, rmse:0.02280495874583721, r2:-0.06901717185974121, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0177, RMSE: 0.0228, RÂ²: -0.0690, MAPE: 1.50%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.770 MB of 0.772 MB uploadedwandb: \ 0.770 MB of 0.772 MB uploadedwandb: | 0.772 MB of 0.772 MB uploadedwandb: / 0.772 MB of 0.987 MB uploadedwandb: - 0.772 MB of 0.987 MB uploadedwandb: \ 0.987 MB of 0.987 MB uploadedwandb: | 0.987 MB of 0.987 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–ƒâ–ˆâ–‚â–ƒâ–„â–„â–„â–„
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–‚â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–â–â–ˆâ–†â–†â–†â–†â–†
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.17362
wandb:                 train/loss 0.3154
wandb:   val/directional_accuracy 50.76397
wandb:                   val/loss 0.20306
wandb:                    val/mae 0.01774
wandb:                   val/mape 149.87197
wandb:                    val/mse 0.00052
wandb:                     val/r2 -0.06902
wandb:                   val/rmse 0.0228
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/0hxoq7gn
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_122058-0hxoq7gn/logs
Completed: ABSA H=50

Training: Autoformer on ABSA for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_122248-q6aqzz2h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/q6aqzz2h
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_ABSA_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          ABSA_normalized.csv Features:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/q6aqzz2h
>>>>>>>start training : long_term_forecast_Autoformer_ABSA_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4781
Train: 4302 samples (90%) - rows 0 to 4301
Val: 239 samples (5%) - rows 4302 to 4540
Test: 240 samples (5%) - rows 4541 to 4780
Sequence length: 60, Prediction length: 100
============================================================
train 4143
val 140
test 141
Epoch: 1, Steps: 130 | Train Loss: 0.4089620 Vali Loss: 0.1916920 Test Loss: 0.1701124
Validation loss decreased (inf --> 0.191692).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.4089620189024852, 'val/loss': 0.19169196486473083, 'test/loss': 0.17011238485574723, '_timestamp': 1762338180.3023973}).
Epoch: 2, Steps: 130 | Train Loss: 0.3808896 Vali Loss: 0.2044047 Test Loss: 0.1669137
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
Epoch: 3, Steps: 130 | Train Loss: 0.3722378 Vali Loss: 0.2024115 Test Loss: 0.1679908
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 130 | Train Loss: 0.3683510 Vali Loss: 0.2021458 Test Loss: 0.1667379
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.3808896495745732, 'val/loss': 0.20440468788146973, 'test/loss': 0.16691369116306304, '_timestamp': 1762338188.6863642}).
Epoch: 5, Steps: 130 | Train Loss: 0.3657514 Vali Loss: 0.2029891 Test Loss: 0.1680875
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 130 | Train Loss: 0.3643871 Vali Loss: 0.2039468 Test Loss: 0.1683921
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 130 | Train Loss: 0.3648192 Vali Loss: 0.2039461 Test Loss: 0.1688756
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 130 | Train Loss: 0.3635069 Vali Loss: 0.2034669 Test Loss: 0.1689941
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 130 | Train Loss: 0.3635709 Vali Loss: 0.2035192 Test Loss: 0.1690691
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 130 | Train Loss: 0.3629934 Vali Loss: 0.2021735 Test Loss: 0.1690345
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 130 | Train Loss: 0.3632583 Vali Loss: 0.2028670 Test Loss: 0.1690745
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_ABSA_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 141
test shape: (141, 100, 1) (141, 100, 1)
test shape: (141, 100, 1) (141, 100, 1)


	mse:0.0005378981586545706, mae:0.01768907718360424, rmse:0.02319263108074665, r2:-0.04237008094787598, dtw:Not calculated


VAL - MSE: 0.0005, MAE: 0.0177, RMSE: 0.0232, RÂ²: -0.0424, MAPE: 1.24%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.805 MB of 0.810 MB uploadedwandb: \ 0.805 MB of 0.810 MB uploadedwandb: | 0.805 MB of 0.810 MB uploadedwandb: / 0.810 MB of 0.810 MB uploadedwandb: - 0.810 MB of 1.025 MB uploadedwandb: \ 1.025 MB of 1.025 MB uploadedwandb: | 1.025 MB of 1.025 MB uploadedwandb: / 1.025 MB of 1.025 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–…â–â–…â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–â–„â–ˆâ–ˆâ–†â–†â–â–„
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.16907
wandb:                 train/loss 0.36326
wandb:   val/directional_accuracy 48.99348
wandb:                   val/loss 0.20287
wandb:                    val/mae 0.01769
wandb:                   val/mape 123.80189
wandb:                    val/mse 0.00054
wandb:                     val/r2 -0.04237
wandb:                   val/rmse 0.02319
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA/runs/q6aqzz2h
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-ABSA
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_122248-q6aqzz2h/logs
Completed: ABSA H=100

Training: Autoformer on SASOL for H=3
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_122433-ji1uo9sk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl3
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/ji1uo9sk
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SASOL_H3 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           3                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl3
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/ji1uo9sk
>>>>>>>start training : long_term_forecast_Autoformer_SASOL_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 3
============================================================
train 3772
val 211
test 212
Epoch: 1, Steps: 118 | Train Loss: 0.2944106 Vali Loss: 0.1201976 Test Loss: 0.1622136
Validation loss decreased (inf --> 0.120198).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2368346 Vali Loss: 0.1210807 Test Loss: 0.1679794
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29441059418654036, 'val/loss': 0.12019764525549752, 'test/loss': 0.16221360223633902, '_timestamp': 1762338285.7597063}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23683456028417005, 'val/loss': 0.12108072851385389, 'test/loss': 0.16797935643366405, '_timestamp': 1762338292.2885273}).
Epoch: 3, Steps: 118 | Train Loss: 0.2197715 Vali Loss: 0.1157872 Test Loss: 0.1550706
Validation loss decreased (0.120198 --> 0.115787).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2125454 Vali Loss: 0.1131024 Test Loss: 0.1502725
Validation loss decreased (0.115787 --> 0.113102).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2100776 Vali Loss: 0.1091805 Test Loss: 0.1470778
Validation loss decreased (0.113102 --> 0.109180).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2066099 Vali Loss: 0.1099168 Test Loss: 0.1475066
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2061498 Vali Loss: 0.1112689 Test Loss: 0.1460823
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2054692 Vali Loss: 0.1127225 Test Loss: 0.1466836
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2057726 Vali Loss: 0.1090824 Test Loss: 0.1467711
Validation loss decreased (0.109180 --> 0.109082).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2040122 Vali Loss: 0.1096397 Test Loss: 0.1467383
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2054494 Vali Loss: 0.1095511 Test Loss: 0.1468358
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2057612 Vali Loss: 0.1071515 Test Loss: 0.1467745
Validation loss decreased (0.109082 --> 0.107151).  Saving model ...
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2046242 Vali Loss: 0.1093052 Test Loss: 0.1467903
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2037948 Vali Loss: 0.1086660 Test Loss: 0.1467614
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.2041835 Vali Loss: 0.1094873 Test Loss: 0.1467585
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.103515625e-09
Epoch: 16, Steps: 118 | Train Loss: 0.2049677 Vali Loss: 0.1072825 Test Loss: 0.1467589
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.0517578125e-09
Epoch: 17, Steps: 118 | Train Loss: 0.2032101 Vali Loss: 0.1081641 Test Loss: 0.1467599
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.52587890625e-09
Epoch: 18, Steps: 118 | Train Loss: 0.2045285 Vali Loss: 0.1094631 Test Loss: 0.1467601
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.62939453125e-10
Epoch: 19, Steps: 118 | Train Loss: 0.2045313 Vali Loss: 0.1102145 Test Loss: 0.1467593
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.814697265625e-10
Epoch: 20, Steps: 118 | Train Loss: 0.2041721 Vali Loss: 0.1102471 Test Loss: 0.1467595
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.9073486328125e-10
Epoch: 21, Steps: 118 | Train Loss: 0.2039930 Vali Loss: 0.1130851 Test Loss: 0.1467596
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.5367431640625e-11
Epoch: 22, Steps: 118 | Train Loss: 0.2045043 Vali Loss: 0.1102184 Test Loss: 0.1467598
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SASOL_H3_Autoformer_custom_ftM_sl60_ll30_pl3_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 212
test shape: (212, 3, 1) (212, 3, 1)
test shape: (212, 3, 1) (212, 3, 1)


	mse:0.0022755651734769344, mae:0.03518905118107796, rmse:0.047702886164188385, r2:-0.03293740749359131, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0352, RMSE: 0.0477, RÂ²: -0.0329, MAPE: 9001002.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.468 MB of 0.468 MB uploadedwandb: \ 0.468 MB of 0.468 MB uploadedwandb: | 0.468 MB of 0.468 MB uploadedwandb: / 0.468 MB of 0.468 MB uploadedwandb: - 0.468 MB of 0.468 MB uploadedwandb: \ 0.468 MB of 0.468 MB uploadedwandb: | 0.468 MB of 0.468 MB uploadedwandb: / 0.468 MB of 0.468 MB uploadedwandb: - 0.468 MB of 0.468 MB uploadedwandb: \ 0.468 MB of 0.468 MB uploadedwandb: | 0.599 MB of 0.815 MB uploaded (0.002 MB deduped)wandb: / 0.815 MB of 0.815 MB uploaded (0.002 MB deduped)wandb: - 0.815 MB of 0.815 MB uploaded (0.002 MB deduped)wandb: \ 0.815 MB of 0.815 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 train/loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–‚â–â–‚â–‚â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–†â–ƒâ–ƒâ–„â–†â–ƒâ–ƒâ–ƒâ–â–ƒâ–‚â–ƒâ–â–‚â–ƒâ–ƒâ–„â–†â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 21
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.14676
wandb:                 train/loss 0.2045
wandb:   val/directional_accuracy 50.4717
wandb:                   val/loss 0.11022
wandb:                    val/mae 0.03519
wandb:                   val/mape 900100200.0
wandb:                    val/mse 0.00228
wandb:                     val/r2 -0.03294
wandb:                   val/rmse 0.0477
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl3 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/ji1uo9sk
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_122433-ji1uo9sk/logs
Completed: SASOL H=3

Training: Autoformer on SASOL for H=5
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_122731-8m0w6f77
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl5
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/8m0w6f77
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SASOL_H5 Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           5                   Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl5
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/8m0w6f77
>>>>>>>start training : long_term_forecast_Autoformer_SASOL_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 5
============================================================
train 3770
val 209
test 210
Epoch: 1, Steps: 118 | Train Loss: 0.2916864 Vali Loss: 0.1315487 Test Loss: 0.1683555
Validation loss decreased (inf --> 0.131549).  Saving model ...
Updating learning rate to 0.0001
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.29168637652518387, 'val/loss': 0.13154868675129755, 'test/loss': 0.16835548090083258, '_timestamp': 1762338466.0272672}).
Epoch: 2, Steps: 118 | Train Loss: 0.2411896 Vali Loss: 0.1187389 Test Loss: 0.1705071
Validation loss decreased (0.131549 --> 0.118739).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2411896400785042, 'val/loss': 0.11873894397701536, 'test/loss': 0.1705070776598794, '_timestamp': 1762338474.1368284}).
Epoch: 3, Steps: 118 | Train Loss: 0.2212723 Vali Loss: 0.1089148 Test Loss: 0.1661246
Validation loss decreased (0.118739 --> 0.108915).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2147503 Vali Loss: 0.1131369 Test Loss: 0.1572983
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2110254 Vali Loss: 0.1137322 Test Loss: 0.1585675
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2088917 Vali Loss: 0.1131375 Test Loss: 0.1573602
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2070856 Vali Loss: 0.1114833 Test Loss: 0.1571806
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2076942 Vali Loss: 0.1110215 Test Loss: 0.1559339
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2072286 Vali Loss: 0.1108674 Test Loss: 0.1560594
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2074950 Vali Loss: 0.1109362 Test Loss: 0.1561612
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2069109 Vali Loss: 0.1132669 Test Loss: 0.1562423
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2067564 Vali Loss: 0.1115846 Test Loss: 0.1563329
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2085551 Vali Loss: 0.1127034 Test Loss: 0.1562292
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SASOL_H5_Autoformer_custom_ftM_sl60_ll30_pl5_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 210
test shape: (210, 5, 1) (210, 5, 1)
test shape: (210, 5, 1) (210, 5, 1)


	mse:0.0022912505082786083, mae:0.035303741693496704, rmse:0.04786700755357742, r2:-0.032373785972595215, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0353, RMSE: 0.0479, RÂ²: -0.0324, MAPE: 18173028.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.502 MB of 0.503 MB uploadedwandb: \ 0.503 MB of 0.503 MB uploadedwandb: | 0.503 MB of 0.503 MB uploadedwandb: / 0.503 MB of 0.503 MB uploadedwandb: - 0.503 MB of 0.503 MB uploadedwandb: \ 0.503 MB of 0.717 MB uploadedwandb: | 0.717 MB of 0.717 MB uploadedwandb: / 0.717 MB of 0.717 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–‚â–ƒâ–‚â–‚â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–‚
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–‡â–ˆâ–‡â–…â–„â–„â–„â–‡â–…â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 12
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.15623
wandb:                 train/loss 0.20856
wandb:   val/directional_accuracy 54.04762
wandb:                   val/loss 0.1127
wandb:                    val/mae 0.0353
wandb:                   val/mape 1817302800.0
wandb:                    val/mse 0.00229
wandb:                     val/r2 -0.03237
wandb:                   val/rmse 0.04787
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl5 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/8m0w6f77
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_122731-8m0w6f77/logs
Completed: SASOL H=5

Training: Autoformer on SASOL for H=10
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_122933-qyy1nhri
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl10
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/qyy1nhri
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SASOL_H10Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           10                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl10
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/qyy1nhri
>>>>>>>start training : long_term_forecast_Autoformer_SASOL_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 10
============================================================
train 3765
val 204
test 205
Epoch: 1, Steps: 118 | Train Loss: 0.2812426 Vali Loss: 0.1220120 Test Loss: 0.1640196
Validation loss decreased (inf --> 0.122012).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2374643 Vali Loss: 0.1227412 Test Loss: 0.1716445
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.28124255636485956, 'val/loss': 0.12201202022177833, 'test/loss': 0.16401963574545725, '_timestamp': 1762338588.6827364}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.23746430747589822, 'val/loss': 0.12274115319762911, 'test/loss': 0.17164452161107743, '_timestamp': 1762338595.9372888}).
Epoch: 3, Steps: 118 | Train Loss: 0.2233616 Vali Loss: 0.1118739 Test Loss: 0.1630136
Validation loss decreased (0.122012 --> 0.111874).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2177818 Vali Loss: 0.1115351 Test Loss: 0.1615523
Validation loss decreased (0.111874 --> 0.111535).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2149150 Vali Loss: 0.1111413 Test Loss: 0.1624585
Validation loss decreased (0.111535 --> 0.111141).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2131027 Vali Loss: 0.1112526 Test Loss: 0.1621714
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2129420 Vali Loss: 0.1171748 Test Loss: 0.1607002
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2121184 Vali Loss: 0.1122877 Test Loss: 0.1610249
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2127473 Vali Loss: 0.1117799 Test Loss: 0.1606166
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2120143 Vali Loss: 0.1117967 Test Loss: 0.1606091
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2121153 Vali Loss: 0.1147947 Test Loss: 0.1605277
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 118 | Train Loss: 0.2125095 Vali Loss: 0.1124670 Test Loss: 0.1604771
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 118 | Train Loss: 0.2129670 Vali Loss: 0.1147565 Test Loss: 0.1604783
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.44140625e-08
Epoch: 14, Steps: 118 | Train Loss: 0.2124072 Vali Loss: 0.1140788 Test Loss: 0.1604757
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.220703125e-08
Epoch: 15, Steps: 118 | Train Loss: 0.2124140 Vali Loss: 0.1162874 Test Loss: 0.1604803
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SASOL_H10_Autoformer_custom_ftM_sl60_ll30_pl10_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 205
test shape: (205, 10, 1) (205, 10, 1)
test shape: (205, 10, 1) (205, 10, 1)


	mse:0.0022872306872159243, mae:0.035240594297647476, rmse:0.04782500118017197, r2:-0.03042292594909668, dtw:Not calculated


VAL - MSE: 0.0023, MAE: 0.0352, RMSE: 0.0478, RÂ²: -0.0304, MAPE: 11352801.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.549 MB of 0.550 MB uploadedwandb: \ 0.549 MB of 0.550 MB uploadedwandb: | 0.549 MB of 0.550 MB uploadedwandb: / 0.550 MB of 0.550 MB uploadedwandb: - 0.550 MB of 0.765 MB uploadedwandb: \ 0.550 MB of 0.765 MB uploadedwandb: | 0.765 MB of 0.765 MB uploadedwandb: / 0.765 MB of 0.765 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–ˆâ–„â–†â–†â–‚â–ƒâ–â–â–â–â–â–â–
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–‚â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–‚â–â–â–â–ˆâ–‚â–‚â–‚â–…â–ƒâ–…â–„â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 14
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.16048
wandb:                 train/loss 0.21241
wandb:   val/directional_accuracy 50.84011
wandb:                   val/loss 0.11629
wandb:                    val/mae 0.03524
wandb:                   val/mape 1135280100.0
wandb:                    val/mse 0.00229
wandb:                     val/r2 -0.03042
wandb:                   val/rmse 0.04783
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl10 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/qyy1nhri
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_122933-qyy1nhri/logs
Completed: SASOL H=10

Training: Autoformer on SASOL for H=22
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_123151-bu0j9fui
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl22
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/bu0j9fui
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SASOL_H22Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           22                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl22
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/bu0j9fui
>>>>>>>start training : long_term_forecast_Autoformer_SASOL_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 22
============================================================
train 3753
val 192
test 193
Epoch: 1, Steps: 118 | Train Loss: 0.2886765 Vali Loss: 0.1130945 Test Loss: 0.1686451
Validation loss decreased (inf --> 0.113094).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 118 | Train Loss: 0.2529186 Vali Loss: 0.1223141 Test Loss: 0.1721579
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.2886764707706742, 'val/loss': 0.11309446642796199, 'test/loss': 0.16864510093416488, '_timestamp': 1762338726.5184193}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2529185541858107, 'val/loss': 0.12231412654121716, 'test/loss': 0.172157883644104, '_timestamp': 1762338733.0355415}).
Epoch: 3, Steps: 118 | Train Loss: 0.2382275 Vali Loss: 0.1152879 Test Loss: 0.1675595
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 118 | Train Loss: 0.2316674 Vali Loss: 0.1167450 Test Loss: 0.1751487
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 118 | Train Loss: 0.2273509 Vali Loss: 0.1156518 Test Loss: 0.1754446
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 118 | Train Loss: 0.2263335 Vali Loss: 0.1165232 Test Loss: 0.1741437
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 118 | Train Loss: 0.2240217 Vali Loss: 0.1166908 Test Loss: 0.1744935
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 118 | Train Loss: 0.2228192 Vali Loss: 0.1172137 Test Loss: 0.1763753
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 118 | Train Loss: 0.2233843 Vali Loss: 0.1171620 Test Loss: 0.1750317
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 118 | Train Loss: 0.2238866 Vali Loss: 0.1171174 Test Loss: 0.1749709
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 118 | Train Loss: 0.2230716 Vali Loss: 0.1170579 Test Loss: 0.1749092
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SASOL_H22_Autoformer_custom_ftM_sl60_ll30_pl22_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 193
test shape: (193, 22, 1) (193, 22, 1)
test shape: (193, 22, 1) (193, 22, 1)


	mse:0.002386556239798665, mae:0.035778775811195374, rmse:0.04885239154100418, r2:-0.06346714496612549, dtw:Not calculated


VAL - MSE: 0.0024, MAE: 0.0358, RMSE: 0.0489, RÂ²: -0.0635, MAPE: 13317366.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.609 MB of 0.610 MB uploadedwandb: \ 0.609 MB of 0.610 MB uploadedwandb: | 0.610 MB of 0.610 MB uploadedwandb: / 0.610 MB of 0.825 MB uploadedwandb: - 0.610 MB of 0.825 MB uploadedwandb: \ 0.825 MB of 0.825 MB uploadedwandb: | 0.825 MB of 0.825 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–‡â–‡â–†â–‡â–ˆâ–‡â–‡â–‡
wandb:                 train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–†â–‚â–…â–†â–ˆâ–ˆâ–ˆâ–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.17491
wandb:                 train/loss 0.22307
wandb:   val/directional_accuracy 47.9398
wandb:                   val/loss 0.11706
wandb:                    val/mae 0.03578
wandb:                   val/mape 1331736600.0
wandb:                    val/mse 0.00239
wandb:                     val/r2 -0.06347
wandb:                   val/rmse 0.04885
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl22 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/bu0j9fui
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_123151-bu0j9fui/logs
Completed: SASOL H=22

Training: Autoformer on SASOL for H=50
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_123340-cis343jl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl50
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/cis343jl
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SASOL_H50Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           50                  Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl50
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/cis343jl
>>>>>>>start training : long_term_forecast_Autoformer_SASOL_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 50
============================================================
train 3725
val 164
test 165
Epoch: 1, Steps: 117 | Train Loss: 0.3166531 Vali Loss: 0.1080585 Test Loss: 0.1893578
Validation loss decreased (inf --> 0.108058).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 117 | Train Loss: 0.2692852 Vali Loss: 0.1186511 Test Loss: 0.2222265
EarlyStopping counter: 1 out of 10
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.31665312403287643, 'val/loss': 0.10805847744146983, 'test/loss': 0.18935784449179968, '_timestamp': 1762338834.485193}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.2692852183284923, 'val/loss': 0.11865105479955673, 'test/loss': 0.22222649802764258, '_timestamp': 1762338842.081455}).
Epoch: 3, Steps: 117 | Train Loss: 0.2512845 Vali Loss: 0.1261074 Test Loss: 0.2182233
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 117 | Train Loss: 0.2425670 Vali Loss: 0.1156548 Test Loss: 0.2175708
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 117 | Train Loss: 0.2386895 Vali Loss: 0.1235341 Test Loss: 0.2181687
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 117 | Train Loss: 0.2372341 Vali Loss: 0.1228796 Test Loss: 0.2195207
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 117 | Train Loss: 0.2355840 Vali Loss: 0.1238730 Test Loss: 0.2176405
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 117 | Train Loss: 0.2354513 Vali Loss: 0.1190537 Test Loss: 0.2198369
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 117 | Train Loss: 0.2350673 Vali Loss: 0.1219205 Test Loss: 0.2212945
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 117 | Train Loss: 0.2350056 Vali Loss: 0.1219248 Test Loss: 0.2217021
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 117 | Train Loss: 0.2345009 Vali Loss: 0.1182731 Test Loss: 0.2216684
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SASOL_H50_Autoformer_custom_ftM_sl60_ll30_pl50_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 165
test shape: (165, 50, 1) (165, 50, 1)
test shape: (165, 50, 1) (165, 50, 1)


	mse:0.0021906469482928514, mae:0.034704189747571945, rmse:0.04680434614419937, r2:-0.06925380229949951, dtw:Not calculated


VAL - MSE: 0.0022, MAE: 0.0347, RMSE: 0.0468, RÂ²: -0.0693, MAPE: 12549523.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.650 MB of 0.653 MB uploadedwandb: \ 0.650 MB of 0.653 MB uploadedwandb: | 0.653 MB of 0.653 MB uploadedwandb: / 0.653 MB of 0.867 MB uploadedwandb: - 0.653 MB of 0.867 MB uploadedwandb: \ 0.867 MB of 0.867 MB uploadedwandb: | 0.867 MB of 0.867 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–‚â–â–‚â–„â–â–…â–‡â–ˆâ–ˆ
wandb:                 train/loss â–ˆâ–„â–ƒâ–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–ˆâ–â–†â–†â–‡â–ƒâ–…â–…â–ƒ
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 10
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.22167
wandb:                 train/loss 0.2345
wandb:   val/directional_accuracy 47.92826
wandb:                   val/loss 0.11827
wandb:                    val/mae 0.0347
wandb:                   val/mape 1254952300.0
wandb:                    val/mse 0.00219
wandb:                     val/r2 -0.06925
wandb:                   val/rmse 0.0468
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl50 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/cis343jl
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_123340-cis343jl/logs
Completed: SASOL H=50

Training: Autoformer on SASOL for H=100
wandb: Currently logged in as: blessingeleer-programming (blessingeleer-programming-university-of-the-witwatersrand). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home-mscluster/bkodze/research_project/forecast-research/wandb/run-20251105_123530-42zlm14x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Autoformer_custom_sl60_pl100
wandb: â­ï¸ View project at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: ðŸš€ View run at https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/42zlm14x
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           Autoformer_SASOL_H100Model:              Autoformer          

[1mData Loader[0m
  Data:               custom              Root Path:          ../dataset/processed_data/
  Data Path:          SASOL_normalized.csvFeatures:           M                   
  Target:             pct_chg             Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            60                  Label Len:          30                  
  Pred Len:           100                 Seasonal Patterns:  Monthly             
  Inverse:            1                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              6                   d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       100                 Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.0001              
  Des:                Autoformer_Exp      Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
âœ… W&B initialized: Autoformer_custom_sl60_pl100
   Dashboard: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/42zlm14x
>>>>>>>start training : long_term_forecast_Autoformer_SASOL_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 4261
Train: 3834 samples (90%) - rows 0 to 3833
Val: 213 samples (5%) - rows 3834 to 4046
Test: 214 samples (5%) - rows 4047 to 4260
Sequence length: 60, Prediction length: 100
============================================================
train 3675
val 114
test 115
Epoch: 1, Steps: 115 | Train Loss: 0.3811456 Vali Loss: 0.1240217 Test Loss: 0.1873610
Validation loss decreased (inf --> 0.124022).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 115 | Train Loss: 0.3454182 Vali Loss: 0.1189030 Test Loss: 0.2084678
Validation loss decreased (0.124022 --> 0.118903).  Saving model ...
Updating learning rate to 5e-05
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 2. Dropping entry: {'epoch': 0, 'train/loss': 0.38114561697711113, 'val/loss': 0.12402169965207577, 'test/loss': 0.18736101314425468, '_timestamp': 1762338945.3761518}).
wandb: WARNING (User provided step: 1 is less than current step: 2. Dropping entry: {'epoch': 1, 'train/loss': 0.34541824369326884, 'val/loss': 0.11890302039682865, 'test/loss': 0.20846781134605408, '_timestamp': 1762338951.8168213}).
Epoch: 3, Steps: 115 | Train Loss: 0.3303631 Vali Loss: 0.1258293 Test Loss: 0.2289669
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 115 | Train Loss: 0.3249066 Vali Loss: 0.1360785 Test Loss: 0.2352526
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 115 | Train Loss: 0.3223526 Vali Loss: 0.1345420 Test Loss: 0.2332077
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 115 | Train Loss: 0.3205905 Vali Loss: 0.1327551 Test Loss: 0.2320085
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 115 | Train Loss: 0.3196077 Vali Loss: 0.1349166 Test Loss: 0.2394643
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 115 | Train Loss: 0.3190868 Vali Loss: 0.1313377 Test Loss: 0.2314441
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 115 | Train Loss: 0.3190163 Vali Loss: 0.1318309 Test Loss: 0.2347353
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 115 | Train Loss: 0.3192495 Vali Loss: 0.1332818 Test Loss: 0.2339734
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 115 | Train Loss: 0.3187273 Vali Loss: 0.1319262 Test Loss: 0.2341584
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 115 | Train Loss: 0.3190837 Vali Loss: 0.1341357 Test Loss: 0.2341757
EarlyStopping counter: 10 out of 10
>>>>>>>testing : long_term_forecast_Autoformer_SASOL_H100_Autoformer_custom_ftM_sl60_ll30_pl100_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_Autoformer_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 115
test shape: (115, 100, 1) (115, 100, 1)
test shape: (115, 100, 1) (115, 100, 1)


	mse:0.002018795581534505, mae:0.03301683068275452, rmse:0.04493100941181183, r2:-0.017323017120361328, dtw:Not calculated


VAL - MSE: 0.0020, MAE: 0.0330, RMSE: 0.0449, RÂ²: -0.0173, MAPE: 7447622.00%

ðŸ“Š Logging comprehensive val results to W&B...
âœ… Val results logged to W&B
wandb: - 0.656 MB of 0.656 MB uploadedwandb: \ 0.656 MB of 0.656 MB uploadedwandb: | 0.656 MB of 0.656 MB uploadedwandb: / 0.656 MB of 0.656 MB uploadedwandb: - 0.656 MB of 0.871 MB uploadedwandb: \ 0.656 MB of 0.871 MB uploadedwandb: | 0.871 MB of 0.871 MB uploadedwandb: / 0.871 MB of 0.871 MB uploadedwandb: 
wandb: Run history:
wandb:                      epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb: model/non_trainable_params â–
wandb:         model/total_params â–
wandb:     model/trainable_params â–
wandb:                  test/loss â–â–…â–„â–ƒâ–ˆâ–ƒâ–…â–„â–„â–„
wandb:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–
wandb:   val/directional_accuracy â–
wandb:                   val/loss â–â–ˆâ–‡â–†â–‡â–…â–…â–†â–…â–‡
wandb:                    val/mae â–
wandb:                   val/mape â–
wandb:                    val/mse â–
wandb:                     val/r2 â–
wandb:                   val/rmse â–
wandb: 
wandb: Run summary:
wandb:                      epoch 11
wandb:         model/architecture Model(
wandb:   (decomp): s...
wandb: model/non_trainable_params 0
wandb:         model/total_params 10530822
wandb:     model/trainable_params 10530822
wandb:                  test/loss 0.23418
wandb:                 train/loss 0.31908
wandb:   val/directional_accuracy 50.51383
wandb:                   val/loss 0.13414
wandb:                    val/mae 0.03302
wandb:                   val/mape 744762200.0
wandb:                    val/mse 0.00202
wandb:                     val/r2 -0.01732
wandb:                   val/rmse 0.04493
wandb: 
wandb: ðŸš€ View run Autoformer_custom_sl60_pl100 at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL/runs/42zlm14x
wandb: â­ï¸ View project at: https://wandb.ai/blessingeleer-programming-university-of-the-witwatersrand/financial-forecasting-SASOL
wandb: Synced 6 W&B file(s), 8 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251105_123530-42zlm14x/logs
Exception in thread ChkStopThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread IntMsgThr:
Traceback (most recent call last):
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 300, in check_internal_messages
    self._target(*self._args, **self._kwargs)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 286, in check_stop_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    self._loop_check_status(
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 840, in deliver_stop_status
    local_handle = request()
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 856, in deliver_internal_messages
    return self._deliver_internal_messages(internal_message)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 516, in _deliver_internal_messages
    return self._deliver_stop_status(status)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 494, in _deliver_stop_status
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    return self._deliver_record(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 459, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    interface._publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self.send_server_request(server_req)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._send_message(msg)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    self._sendall_with_error_handle(header + data)
  File "/home-mscluster/bkodze/miniconda3/envs/finalenv/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Completed: SASOL H=100

Autoformer training completed for all datasets!
