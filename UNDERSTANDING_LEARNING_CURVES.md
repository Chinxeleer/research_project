# Understanding Learning Curves - Complete Guide

**Date:** 2025-11-04
**For:** Financial Forecasting Research

---

## What Are Learning Curves?

Learning curves show how your model's **loss (error)** changes during training.

### The Graph Axes:

```
Loss (MSE)
    â†‘
    |  Training Loss (Blue)
    |  Validation Loss (Orange)
    |
    |
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epochs
       (Training iterations)
```

- **X-axis (Epochs):** Number of times model sees entire training dataset
- **Y-axis (Loss):** Error/mistake size (Lower = Better)
- **Blue Line:** Error on training data (data model is learning from)
- **Orange Line:** Error on validation data (data model has NEVER seen)

---

## Understanding Your Graph (mamba_100.png)

Let me explain what you saw:

### Your Graph Details:

```
Loss
0.8 â”¤
    â”‚ Blue (Training): Starts ~0.8, drops to ~0.65, then FLAT
0.7 â”¤           â•²
    â”‚            â•²___________________________
0.6 â”¤             â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾
    â”‚
0.5 â”¤
    â”‚
0.4 â”¤
    â”‚
0.3 â”¤
    â”‚ Orange (Validation): COMPLETELY FLAT at ~0.25
0.2 â”¤ ________________________________________________
    â”‚
0.1 â”¤
    â”‚
0.0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
    0    10   20   30   40   50  Epochs
```

### What This Tells Us:

#### ğŸš¨ **Problem 1: Training Loss Plateaus Too High**
- Starts at 0.8
- Drops to 0.65
- **STUCK at 0.65** for 45+ epochs
- **Meaning:** Model stopped learning!

#### ğŸš¨ **Problem 2: Validation Loss WAY Lower Than Training**
- Training: 0.65
- Validation: 0.25
- **Gap of 0.40** (huge!)
- **Meaning:** Something is fundamentally wrong

#### ğŸš¨ **Problem 3: Validation Loss Completely Flat**
- No variation at all for 50 epochs
- **Meaning:** Model predicting the same thing every time

### Why This Happened:

**This graph was from the OLD run with:**
1. âŒ Wrong data (2006-2015 only)
2. âŒ Wrong pct_chg formula (log-space values)
3. âŒ Possible data leakage or normalization issue

**Your NEW runs should look COMPLETELY different!**

---

## Normal/Healthy Learning Curves

### âœ… **Ideal Pattern:**

```
Loss
0.5 â”¤
    â”‚  Both lines decreasing
0.4 â”¤  â•²
    â”‚   â•²  Blue (Train)
0.3 â”¤    â•²_____
    â”‚     â•²    â€¾â€¾â€¾â•²___
0.2 â”¤      â•²         â€¾â€¾â€¾â•²__
    â”‚       â•²  Orange (Val) â€¾â€¾â•²__
0.1 â”¤        â•²___________________â€¾â€¾â€¾
    â”‚          (Both converge)
0.0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
    0    10   20   30   40   50
```

**What to look for:**
1. âœ… Both lines **start high** and **decrease**
2. âœ… Both **converge** toward a minimum
3. âœ… **Training loss â‰¥ Validation loss** (usually)
4. âœ… Lines **smooth** (not jumping around)
5. âœ… Eventually **plateau** (no more improvement)

### âœ… **Your Latest Run (Looks Good!):**

From your SLURM output (slurm-166276.out):
```
Epoch 1: Train=0.39, Val=0.15  â† Good start
Epoch 2: Train=0.36, Val=0.14  â† Both decreasing âœ“
Epoch 3: Train=0.32, Val=0.14  â† Converging âœ“
Epoch 5: Train=0.29, Val=0.13  â† Getting better âœ“
Epoch 8: Train=0.28, Val=0.13  â† Approaching minimum âœ“
```

**This is HEALTHY!** Both decreasing, approaching convergence.

---

## Common Patterns and What They Mean

### Pattern 1: Overfitting ğŸ”´

```
Loss
0.5 â”¤
    â”‚  Training keeps improving
0.4 â”¤  â•²
    â”‚   â•²________________  â† Train keeps dropping
0.3 â”¤    â•²               â€¾â€¾â€¾â•²___
    â”‚     â•²                      â€¾â€¾â€¾â€¾
0.2 â”¤      â•²
    â”‚       â•²     â•±â€¾â€¾â€¾â€¾â€¾  â† Val INCREASES!
0.1 â”¤        â•²__â•±   (Bad!)
    â”‚
0.0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
```

**Signs:**
- âŒ Training loss decreasing
- âŒ Validation loss INCREASING
- âŒ Gap between them growing

**What it means:**
- Model memorizing training data
- Not learning generalizable patterns
- Will perform poorly on new data

**Fix:**
- âœ… Early stopping (you have this!)
- âœ… Increase dropout
- âœ… Add regularization
- âœ… Get more training data

---

### Pattern 2: Underfitting ğŸ”´

```
Loss
0.5 â”¤  Both lines high and flat
    â”‚  ________________________
0.4 â”¤ â•±â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾
    â”‚â•±
0.3 â”¤   Both stuck!
    â”‚
0.2 â”¤  No learning happening
    â”‚
0.1 â”¤
    â”‚
0.0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
```

**Signs:**
- âŒ Both losses stay HIGH
- âŒ No improvement after many epochs
- âŒ Lines are flat or barely decreasing

**What it means:**
- Model too simple
- Learning rate too low
- Not enough training

**Fix:**
- âœ… Increase model capacity (more layers/neurons)
- âœ… Increase learning rate
- âœ… Train longer
- âœ… Check data preprocessing

---

### Pattern 3: Good Fit âœ…

```
Loss
0.5 â”¤
    â”‚  Both decreasing smoothly
0.4 â”¤  â•²
    â”‚   â•²
0.3 â”¤    â•²___
    â”‚     â•²  â€¾â€¾â€¾â•²___  Both converge
0.2 â”¤      â•²       â€¾â€¾â€¾â•²__
    â”‚       â•²____________â€¾â€¾â€¾  to similar level
0.1 â”¤
    â”‚  Small gap maintained
0.0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
```

**Signs:**
- âœ… Both decrease together
- âœ… Small gap between them (0.01-0.05 typical)
- âœ… Both plateau at similar level
- âœ… Smooth curves (no wild jumps)

**What it means:**
- Model learning well!
- Good generalization
- Ready for testing

---

### Pattern 4: Validation Lower Than Training (Your Case) ğŸŸ¡

```
Loss
0.4 â”¤  Train (Blue)
    â”‚  â•²
0.3 â”¤   â•²_____________
    â”‚    â•²           â€¾â€¾â€¾  â† Train higher
0.2 â”¤     â•²
    â”‚      â•²________  â† Val lower
0.1 â”¤       â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾
    â”‚
0.0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
```

**When this is OK:** âœ…
1. **Smaller validation set** (yours: 239 val vs 4,302 train with 90/5/5 split)
   - Less data = less noise in average
   - Validation can get "lucky"

2. **No dropout/augmentation during validation**
   - Training uses dropout (adds noise)
   - Validation runs clean (lower loss)

3. **Validation data is easier**
   - Different time period
   - Less volatile
   - Easier patterns

4. **Both are still decreasing**
   - Main thing is improvement
   - Absolute gap matters less

**When to worry:** ğŸš¨
1. Gap is HUGE (>0.3 like your old graph)
2. Validation is FLAT (no variation)
3. Both curves don't make sense

---

## Financial Forecasting Specific Notes

### Expected Loss Ranges:

For percentage change forecasting (pct_chg):

```
GOOD:     MSE < 0.0001  (0.01% average error)
OKAY:     MSE = 0.0001-0.001  (0.01-0.03% error)
ACCEPTABLE: MSE = 0.001-0.01  (0.03-0.1% error)
POOR:     MSE > 0.01  (>0.1% error)
```

Your losses in latest run:
- Train: 0.28-0.39 (normalized space)
- Val: 0.13-0.15 (normalized space)

These get denormalized for final metrics (should be ~0.0001-0.001 range).

### Why Financial Data is Special:

1. **Very noisy** - Random walk component
2. **Non-stationary** - Patterns change over time
3. **Low predictability** - RÂ² near 0 is normal
4. **Validation can be easier** - Different market regimes

**So some "weird" patterns are actually normal!**

---

## How to Use Learning Curves for Debugging

### Step 1: Check Initial Loss (Epoch 1)

**Normal:**
- Train: 0.3-0.8
- Val: Similar to train (Â±0.1)

**Problems:**
- Loss > 1.0 â†’ Learning rate too high
- Loss = NaN â†’ Numerical instability
- Loss not decreasing â†’ Wrong initialization

### Step 2: Watch First 5 Epochs

**Good signs:**
- âœ… Both decreasing
- âœ… Smooth decline
- âœ… No sudden jumps

**Bad signs:**
- âŒ Increasing
- âŒ Wild oscillations
- âŒ Staying flat

### Step 3: Monitor Convergence (Epoch 10-30)

**Good signs:**
- âœ… Still improving
- âœ… Rate of improvement slowing
- âœ… Lines getting closer

**Bad signs:**
- âŒ Validation going up
- âŒ No improvement for 10+ epochs
- âŒ Gap widening

### Step 4: Check Final Performance

**Good signs:**
- âœ… Both plateaued at low level
- âœ… Small gap (<0.05)
- âœ… Early stopping triggered appropriately

**Bad signs:**
- âŒ Still decreasing (need more epochs)
- âŒ Large gap (overfitting)
- âŒ Both high (underfitting)

---

## Your Specific Cases

### Case 1: Old Graph (mamba_100.png) ğŸ”´

```
Issue: Train=0.65, Val=0.25, both flat
Diagnosis: Data problem (wrong preprocessing)
Status: FIXED with new preprocessing
Action: Ignore this graph, it's from bad data
```

### Case 2: Latest Run (slurm-166276.out) âœ…

```
Pattern: Train=0.28, Val=0.13, both decreasing
Diagnosis: HEALTHY learning!
Status: GOOD - continue training
Action: Wait for early stopping, then evaluate
```

---

## Quick Decision Guide

### Is My Training OK? Decision Tree:

```
Are both lines decreasing?
â”œâ”€ Yes â†’ GOOD! Keep going
â”‚   â””â”€ Is val loss increasing after epoch 20?
â”‚       â”œâ”€ Yes â†’ Overfitting, early stop will catch it âœ“
â”‚       â””â”€ No â†’ Perfect! âœ“
â”‚
â””â”€ No â†’ PROBLEM
    â”œâ”€ Both flat â†’ Underfitting
    â”‚   â””â”€ Increase learning rate or model size
    â”‚
    â”œâ”€ Both increasing â†’ Learning rate too high
    â”‚   â””â”€ Decrease learning rate
    â”‚
    â””â”€ Wild oscillations â†’ Unstable training
        â””â”€ Check data normalization
```

### Is the Gap Between Train/Val OK?

```
Gap = |Train Loss - Val Loss|

Gap < 0.05:  âœ… EXCELLENT - Generalizing well
Gap = 0.05-0.15: âœ… GOOD - Normal for small val set
Gap = 0.15-0.30: ğŸŸ¡ OKAY - Monitor for overfitting
Gap > 0.30: ğŸ”´ BAD - Likely data issue or overfitting

Your latest: ~0.15 â†’ GOOD! âœ“
Your old graph: ~0.40 â†’ BAD (data was wrong)
```

---

## What to Report in Your Paper

### Good Example:

> "Figure X shows the learning curves for the Mamba model on NVIDIA stock data.
> Both training and validation losses converge after approximately 25 epochs,
> with early stopping triggered at epoch 28 (patience=10). The final training
> loss of 0.28 and validation loss of 0.13 indicate good model fit without
> overfitting. The small gap between training and validation loss suggests
> the model generalizes well to unseen data."

### What NOT to say:

> âŒ "Validation loss is lower than training loss, which is weird."
>
> Better: âœ… "Validation loss is slightly lower than training loss (0.13 vs 0.28),
> which is common with smaller validation sets (125 vs 2201 samples) and
> no dropout applied during validation."

---

## Summary - Key Takeaways

### âœ… What "Good" Looks Like:

1. **Both lines decrease** (learning is happening)
2. **Converge to low values** (model is accurate)
3. **Small gap** (<0.15 typical)
4. **Smooth curves** (stable training)
5. **Plateau eventually** (found minimum)

### ğŸš¨ What "Bad" Looks Like:

1. **Validation increases** (overfitting)
2. **Both stay high** (underfitting)
3. **Huge gap** (>0.3 - data problem)
4. **Wild jumps** (unstable)
5. **Completely flat** (not learning)

### ğŸ¯ Your Current Status:

**Latest run (166276):** âœ… HEALTHY
- Both decreasing âœ“
- Converging âœ“
- Gap reasonable (~0.15) âœ“
- Will continue to improve âœ“

**Old run (mamba_100.png):** ğŸ”´ BROKEN
- From wrong data âœ—
- Ignore completely âœ—
- Already fixed âœ“

---

## Interactive Checklist for Your Runs

When you see a new learning curve, ask:

- [ ] Are both lines going down?
  - **Yes** â†’ Good!
  - **No** â†’ Check learning rate/data

- [ ] Do they converge eventually?
  - **Yes** â†’ Good!
  - **No** â†’ May need more epochs

- [ ] Is the gap reasonable (<0.3)?
  - **Yes** â†’ Good!
  - **No** â†’ Check for data issues

- [ ] Are the curves smooth?
  - **Yes** â†’ Good!
  - **No** â†’ Check batch size/learning rate

- [ ] Did early stopping trigger appropriately?
  - **Yes** â†’ Good!
  - **No** â†’ Adjust patience if needed

**If all checks pass â†’ Your model is training correctly!** âœ…

---

## Additional Resources

### Viewing on WandB:

1. Go to your WandB project
2. Click on a run
3. Look for "Charts" tab
4. Find "learning_curves" plot
5. Hover over lines to see exact values
6. Compare multiple runs side-by-side

### Common WandB Patterns:

```
Smooth line: Stable training âœ“
Jagged line: High variance (increase batch size)
Horizontal: No learning (check data/LR)
Exponential growth: Exploding gradients (lower LR)
```

---

**Generated:** 2025-11-04
**Use this guide to interpret all your training runs!**
